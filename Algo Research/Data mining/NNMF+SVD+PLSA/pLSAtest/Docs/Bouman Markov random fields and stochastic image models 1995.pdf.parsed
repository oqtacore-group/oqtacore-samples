
Markov Random Fields and Stochastic Image Models Charles A. Bouman School of Electrical and Computer Engineering Purdue University Phone: (317) 494-0340 Fax: (317) 494-3358 email bouman@ecn.purdue.edu Available from: http://dynamo.ecn.purdue.edu/"bouman/ Tutorial Presented at: 1995 IEEE International Conference on Image Processing 23-26 October 1995 Washington, D.C. Special thanks to: Ken Sauer Department of Electrical Engineering University of Notre Dame Suhail Saquib School of Electrical and Computer Engineering Purdue University 1
Overview of Topics 1. Introduction 2. The Bayesian Approach 3. Discrete Models (a) Markov Chains (b) Markov Random Fields (MRF) (c) Simulation (d) Parameter estimation 4. Application of MRF's to Segmentation (a) The Model (b) Bayesian Estimation (c) MAP Optimization (d) Parameter Estimation (e) Other Approaches 5. Continuous Models (a) Gaussian Random Process Models i. Autoregressive (AR) models ii. Simultaneous AR (SAR) models iii. Gaussian MRF's iv. Generalization to 2-D (b) Non-Gaussian MRF's i. Quadratic functions ii. Non-Convex functions iii. Continuous MAP estimation iv. Convex functions (c) Parameter Estimation i. Estimation of ¾ ii. Estimation of T and p parameters 6. Application to Tomography (a) Tomographic system and data models (b) MAP Optimization (c) Parameter estimation 7. Multiscale Stochastic Models (a) Continuous models (b) Discrete models 8. High Level Image Models 2
References in Statistical Image Modeling 1. Overview references [100, 89, 50, 54, 162, 4, 44] 2. Type of Random Field Model (a) Discrete Models i. Hidden Markov models [134, 135] ii. Markov Chains [41, 42, 156, 132] iii. Ising model [127, 126, 122, 130, 100, 131] iv. Discrete MRF [13, 14, 160, 48, 161, 47, 16, 169, 36, 51, 49, 116, 167, 99, 50, 72, 104, 157, 55, 181, 121, 123, 23, 91, 176, 92, 37, 125, 128, 140, 168, 97, 119, 11, 39, 77, 172, 93] v. MRF with Line Processes[68, 53, 177, 175, 178, 173, 171] (b) Continuous Models i. AR and Simultaneous AR [95, 94, 115] ii. Gaussian MRF [18, 15, 87, 95, 94, 33, 114, 153, 38, 106, 147] iii. Nonconvex potential functions [70, 71, 21, 81, 107, 66, 32, 143] iv. Convex potential functions [17, 75, 107, 108, 155, 24, 146, 90, 25, 27, 32, 149, 26, 148, 150] 3. Regularization approaches (a) Quadratic [165, 158, 137, 102, 103, 98, 138, 60] (b) Nonconvex [139, 85, 88, 159, 19, 20] (c) Convex [155, 3] 4. Simulation and Stochastic Optimization Methods [118, 80, 129, 100, 68, 141, 61, 76, 62, 63] 5. Computational Methods used with MRF Models (a) Simulation based estimators [116, 157, 55, 39, 26] (b) Discrete optimization i. Simulated annealing [68, 167, 55, 181] ii. Recursive optimization [48, 49, 169, 156, 91, 172, 173, 93] iii. Greedy optimization [160, 16, 161, 36, 51, 104, 157, 55, 125, 92] iv. Multiscale optimization [22, 72, 23, 128, 97, 105, 110] v. Mean ¯eld theory [176, 177, 175, 178, 171] (c) Continuous optimization i. Simulated annealing [153] ii. Gradient ascent [87, 149, 150] iii. Conjugate gradient [10] iv. EM [70, 71, 81, 107, 75, 82] v. ICM/Gauss-Seidel/ICD [24, 146, 147, 25, 27] vi. Continuation methods [19, 20, 153, 143] 6. Parameter Estimation (a) For MRF i. Discrete MRF A. Maximum likelihood [130, 64, 71, 131, 121, 108] 3
B. Coding/maximum pseudolikelihood [15, 16, 18, 69, 104] C. Least squares [49, 77] ii. Continuous MRF A. Gaussian [95, 94, 33, 114, 38, 115, 106] B. Non-Gaussian [124, 148, 26, 133, 145, 144] iii. EM based [71, 176, 177, 39, 180, 26, 178, 133, 145, 144] (b) For other models i. EM algorithm for HMM's and mixture models [9, 8, 46, 170, 136, 1] ii. Order identi¯cation [2, 94, 142, 37, 179, 180] 7. Application (a) Texture classi¯cation [95, 33, 38, 115] (b) Texture modeling [56, 94] (c) Segmentation remotely sensed imagery [160, 161, 99, 181, 140, 28, 92, 29] (d) Segmentation of documents [157, 55] (e) Segmentation (nonspeci¯c) [48, 16, 47, 36, 49, 51, 167, 116, 96, 104, 114, 23, 37, 115, 125, 168, 97, 120, 11, 39, 110, 180, 172, 93] (f) Boundary and edge detection [41, 42, 57, 156, 65, 175] (g) Image restoration [87, 68, 169, 96, 153, 91, 90, 177, 82, 150] (h) Image interpolation [149] (i) Optical 'ow estimation [88, 101, 83, 111, 143, 178] (j) Texture modeling [95, 94, 44, 33, 38, 123, 115, 112, 56, 113] (k) Tomography [79, 70, 71, 81, 75, 107, 108, 24, 146, 25, 147, 32, 26, 27] (l) Crystallography [53] (m) Template matching [166, 154] (n) Image interpretation [119] 8. Multiscale Bayesian Models (a) Discrete model [28, 29, 40, 154] (b) Continuous model [12, 34, 5, 6, 7, 112, 35, 52, 111, 113, 166] (c) Parameter estimation [34, 29, 166, 154] 9. Multigrid techniques [78, 30, 31, 117, 58] 4
The Bayesian Approach u - Random ¯eld model parameters X - Unknown image Á - Physical system model parameters Y - Observed data Random Field X Model q Physical System Y Data Collection f 2 Random ¯eld may model: { Achromatic/color/multispectral image { Image of discrete pixel classi¯cations { Model of object cross-section 2 Physical system may model: { Optics of image scanner { Spectral re'ectivity of ground covers (remote sensing) { Tomographic data collection 5
Bayesian Versus Frequentist? 2 How does the Bayesian approach direr? { Bayesian makes assumptions about prior behavior. { Bayesian requires that you choose a model. { A good prior model can improve accuracy. { But model mismatch can impair accuracy 2 When should you use the frequentist approach? { When (# of data samples)>>(# of unknowns). { When an accurate prior model does not exist. { When prior model is not needed. 2 When should you use the Bayesian approach? { When (# of data samples)¼(# of unknowns). { When model mismatch is tolerable. { When accuracy without prior is poor. 6
Examples of Bayesian Versus Frequentist? Random Field X Model q Physical System Y Data Collection f 2 Bayesian model of image X { (# of image points)¼(# of data points.) { Images have unique behaviors which may be modeled. { Maximum likelihood estimation works poorly. { Reduce model mismatch by estimating parameter u. 2 Frequentist model for u and Á { (# of model parameters)<<(# of data points.) { Parameters are di±cult to model. { Maximum likelihood estimation works well. 7
Markov Chains 2 Topics to be covered: { 1-D properties { Parameter estimation { 2-D Markov Chains 2 Notation: Upper case ) Random variable 8
Markov Chains X0 X1 X2 X3 X4 2 De¯nition of (homogeneous) Markov chains p(xnjxi i < n) = p(xnjxn!1) 2 Therefore, we may show that the probability of a sequence is given by p(x) = p(x0) NY n=1 p(xnjxn!1) 2 Notice: Xn is not independent of Xn+1 p(xnjxi i 6= n) = p(xnjxn!1; xn+1) 9
Parameters of Markov Chain 2 Transition parameters are: uj;i = p(xn = ijxn!1 = j) 2 Example: u = 26641 ! ½ ½ ½ 1 ! ½ 3775 01 01 r 1-r 1-rr 01 r 1-r 1-r r 01 r 1-r 1-r r X0 X1 X2 X3 01 r 1-r 1-r r X4 2 ½ is the probability of changing state. 10 20 30 40 50 60 70 80 90 100 −0.50 0.51 1.5 Binary Valued Markov Chain: rho = 0.050000 discrete time, n yaxis 10 20 30 40 50 60 70 80 90 100 −0.50 0.51 1.5 Binary Valued Markov Chain: rho = 0.200000 discrete time, n yaxis ½ = 0:05 ½ = 0:2 10
Parameter Estimation for Markov Chains 2 Maximum likelihood (ML) parameter estimation ^u = arg max u p(xju) 2 For Markov chain ^uj;i = hj;i Xk hj;k where hj;i is the histogram of transitions hj;i = Xn ±(xn = i & xn!1 = j) 2 Example xn = 0; 0; 0; 1; 1; 1; 0; 1; 1; 1; 1; 1 u = 2664h0;0 h0;1 h1;0 h1;1 3775= 26642 2 1 6 3775 11
2-D Markov Chains X(1,0) X(1,1) X(1,2) X(1,3) X(1,4) X(2,0) X(2,1) X(2,2) X(2,3) X(2,4) X(3,0) X(3,1) X(3,2) X(3,3) X(3,4) X(0,0) X(0,1) X(0,2) X(0,3) X(0,4) 2 Advantages: { Simple expressions for probability { Simple parameter estimation 2 Disadvantages: { No natural ordering of pixels in image { Anisotropic model behavior 12
Discrete State Markov Random Fields 2 Topics to be covered: { De¯nitions and theorems { 1-D MRF's { Ising model { M-Level model { Line process model 13
Markov Random Fields 2 Noncausal model 2 Advantages of MRF's { Isotropic behavior { Only local dependencies 2 Disadvantages of MRF's { Computing probability is di±cult { Parameter estimation is di±cult 2 Key theoretical result: Hammersley-Clirord theorem 14
De¯nition of Neighborhood System and Clique 2 De¯ne S - set of lattice points s - a lattice point, s 2 S Xs - the value of X at s @s - the neighboring points of s 2 A neighborhood system @s must be symmetric r 2 @s ) s 2 @r also s 62 @s 2 A clique is a set of points, c, which are all neighbors of each other 8s; r 2 c, r 2 @s 15
Example of Neighborhood System and Clique 2 Example of 8 point neighborhood X(1,0) X(1,1) X(1,2) X(1,3) X(1,4) X(2,0) X(2,1) X(2,2) X(2,3) X(2,4) X(3,0) X(3,1) X(3,2) X(3,3) X(3,4) X(0,0) X(0,1) X(0,2) X(0,3) X(0,4) X(4,0) X(4,1) X(4,2) X(4,3) X(4,4) Neighbors of X(2,2) 2 Example of cliques for 8 point neighborhood 1-point clique 2-point cliques 3-point cliques 4-point cliques Not a clique 16
Gibbs Distribution xc - The value of X at the points in clique c. Vc(xc) - A potential function is any function of xc. 2 A (discrete) density is a Gibbs distribution if p(x) = 1Z exp 8>< >:! X c2C Vc(xc)9>= >; C is the set of all cliques Z is the normalizing constant for the density. 2 Z is known as the partition function. 2 U(x) = X c2C Vc(xc) is known as the energy function. 17
Markov Random Field 2 De¯nition: A random object X on the lattice S with neighborhood system @s is said to be a Markov random ¯eld if for all s 2 S p(xsjxr for r 6= s) = p(xsjx@r) 18
Hammersley-Clirord Theorem[14] 0BBBBBB@ X is a Markov random ¯eld & 8x; PfX = xg > 0 1CCCCCCA() 0BB@PfX = xg has the form of a Gibbs distribution 1CCA 2 Gives you a method for writing the density for a MRF 2 Does not give the value of Z, the partition function. 2 Positivity, PfX = xg > 0, is a technical condition which we will generally assume. 19
Markov Chains are MRF's Xn-2 Xn-1 Xn Xn+1 Xn+2 Neighbors of Xn 2 Neighbors of n are @n = fn ! 1; n + 1g 2 Cliques have the form c = fn ! 1; ng 2 Density has the form p(x) = p(x0) NY n=1 p(xnjxn!1) = p(x0) exp 8>< >: NX n=1 log p(xnjxn!1)9>= >; 2 The potential functions have the form V (xn; xn!1) = log p(xnjxn!1) 20
1-D MRF's are Markov Chains 2 Let Xn be a 1-D MRF with @n = fn ! 1; n + 1g 2 The discrete density has the form of a Gibbs distribution p(x) = p(x0) exp 8>< >: NX n=1 V (xn; xn!1)9>= >; 2 It may be shown that this is a Markov Chain. 2 Transition probabilities may be di±cult to compute. 21
The Ising Model: A 2-D MRF[100] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 Cliques: Xr Xs Xr Xs Boundary: 2 Potential functions are given by V (xr; xs) = ¯±(xr 6= xs) where ¯ is a model parameter. 2 Energy function is given by X c2C Vc(xc) = ¯(Boundary length) 2 Longer boundaries ) less probable22
Critical Temperature Behavior[127, 126, 100] Center Pixel X0: B B B B B 0 0 0 B 0 0 1 B 0 0 1 B B B B 0 1 1 B 1 1 1 B 1 0 B B 0 0 0 B 0 0 0 B 0 0 0 B B B B 0 1 0 B 1 1 0 B 0 1 0 B B B B B 1 B 0 0 0 0 1 1 B B000000B0 N N 2 1¯ is analogous to temperature. 2 Peierls showed that for ¯ > ¯c lim N!1P(X0 = 0jB = 0) 6= lim N!1P(X0 = 0jB = 1) 2 The erect of the boundary does not diminish as N ! 1! 2 ¯c ¼ :88 is known as the critical temperature. 23
Critical Temperature Analysis[122] 2 Amazingly, Onsager was able to compute E[X0jB = 1] = 8>>>>< >>>>: Ã1 ! 1 (sinh(¯))4!1=8 if ¯ > ¯c 0 if ¯ < ¯c 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 −0.50 0.51 1.5 Inverse Temperature Mean Field Value 2 Onsager also computed an analytic expression for Z(T)! 24
M-Level MRF[16] 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 2 2 0 0 2 2 0 0 0 2 0 0 0 0 2 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 Cliques: Xr Xs Xr Xs Xr Xs Xr Xs Neighbors: Xs 2 De¯ne C1 4=( hor./vert. cliques) and C2 4=( diag. cliques) 2 Then V (xr; xs) = 8>>< >>: ¯1±(xr 6= xs) for fxr; xsg 2 C1 ¯2±(xr 6= xs) for fxr; xsg 2 C2 2 De¯ne t1(x) 4=X fs;rg2C1 ±(xr 6= xs) t2(x) 4=X fs;rg2C2 ±(xr 6= xs) 2 Then the probability is given by p(x) = 1Z exp f!(¯1t1(x) + ¯2t2(x))g 25
Conditional Probability of a Pixel Neighbors Xs Xs Cliques Containing Xs X4 Xs X1 Xs X7 Xs X6 Xs X3 Xs Xs X2 X8 Xs X5 Xs X4 X1 X7 X6 X3 X2 X8 X5 2 The probability of a pixel given all other pixels is p(xsjxi6=s) = 1Z exp f! Pc2C Vc(xc)g PM!1 xs=0 1Z exp f! Pc2C Vc(xc)g 2 Notice: Any term Vc(xc) which does not include xs cancels. p(xsjxi6=s) = exp ½!¯1 P4i=1 ±(xs 6= xi) ! ¯2 P8i=5 ±(xs 6= xi)¾ PM!1 xs=0 exp f!¯1 P4i=1 ±(xs 6= xi) ! ¯2 P8i=5 ±(xs 6= xi)g 26
Conditional Probability of a Pixel (Continued) Neighbors Xs xs 1 V1(0,x
s 1 ) = 2 10 0 000 V1(1,x
s) = 2 V2(0,x
s) = 1 V2(1,x
s) = 3 2 De¯ne v1(xs; @xs) 4=# of horz./vert. neighbors 6= xs v2(xs; @xs) 4=# of diag. neighbors 6= xs 2 Then p(xsjxi6=s) = 1 Z0 exp f!¯1v1(xs; @xs) ! ¯2v2(xs; @xs)g where Z0 is an easily computed normalizing constant 2 When ¯1; ¯2 > 0, Xs is most likely to be the majority neighboring class. 27
Line Process MRF [68] Pixels Line sites MRF b1=0 b2=2.7 b3=1.8 b4=0.9 b5=1.8 b6=2.7 Clique Potentials 2 Line sites fall between pixels 2 The values ¯1; ¢ ¢ ¢ ; ¯2 determine the potential of line sites 2 The potential of pixel values is V (xs; xr; lr;s) = 8>>< >>: (xs ! xr)2 if lr;s = 0 0 if lr;s = 1 2 The ¯eld is { Smooth between line sites { Discontinuous at line sites 28
Simulation 2 Topics to be covered: { Metropolis sampler { Gibbs sampler { Generalized Metropolis sampler29
Generating Samples from a Gibbs Distribution 2 How do we generate a random variable X with a Gibbs distribution? p(x) = 1Z exp f!U(x)g 2 Generally, this problem is di±cult. 2 Markov Chains can be generated sequentially 2 Non-causal structure of MRF's makes simulation di±cult. 30
The Metropolis Sampler[118, 100] 2 How do we generate a sample from a Gibbs distribution? p(x) = 1Z exp f!U(x)g 2 Start with the sample xk, and generate a new sample W with probability q(wjxk). Note: q(wjxk) must be symmetric. q(wjxk) = q(xkjw) 2 Compute ¢E(W) = U(W) ! U(xk), then do the following: If ¢E(W) < 0 { Accept: Xk+1 = W If ¢E(W) ¸ 0 { Accept: Xk+1 = W with probability expf!¢E(W)g { Reject: Xk+1 = xk with probability 1 ! expf!¢E(W)g 31
Ergodic Behavior of Metropolis Sampler 2 The sequence of random ¯elds, Xk, form a Markov chain. 2 Let p(xk+1jxk) be the transition probabilities of the Markov chain. 2 Then Xk is reversible p(xk+1jxk) expf!U(xk)g = expf!U(xk+1)gp(xkjxk+1) 2 Therefore, if the Markov chain is irreducible, then lim k!1PfXk = xg = 1Z expf!U(x)g 2 If every state can be reached, then as k ! 1, Xk will be a sample from the Gibbs distribution. 32
Example Metropolis Sampler for Ising Model xs 0 1 0 0 2 Assume xks= 0. 2 Generate a binary R.V., W, such that PfW = 0g = 0:5. ¢E(W) = U(W) ! U(xks) = 8>>< >>: 0 if W = 0 2¯ if W = 1 If ¢E(W) < 0 { Accept Xk+1 s = W If ¢E(W) ¸ 0 { Accept: Xk+1 s = W with probability expf!¢E(W)g { Reject: Xk+1 s = xkswith probability 1 ! expf!¢E(W)g 2 Repeat this procedure for each pixel. 2Warning: for ¯ > ¯c convergence can be extremely slow! 33
Example Simulation for Ising Model(¯ = 1:0) 2 Test 1 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 10 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 50 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 100 2 4 6 8 10 12 14 16 2468 10 12 14 16 2 Test 2 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 10 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 50 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 100 2 4 6 8 10 12 14 16 2468 10 12 14 16 2 Test 3 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 10 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 50 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 100 2 4 6 8 10 12 14 16 2468 10 12 14 16 2 Test 3 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 10 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 50 2 4 6 8 10 12 14 16 2468 10 12 14 16 Ising model: Beta = 1.000000, Iteration = 100 2 4 6 8 10 12 14 16 2468 10 12 14 16 34
Advantages and Disadvantages of Metropolis Sampler 2 Advantages { Can be implemented whenever ¢E is easy to compute. { Has guaranteed geometric convergence. 2 Disadvantages { Can be slow if there are many rejections. { Is constrained to use a symmetric transition function q(xk+1jxk). 35
Gibbs Sampler[68] 2 Replace each point with a sample from its conditional distribution p(xsjxkii 6= s) = p(xsjx@s) 2 Scan through all the points in the image. 2 Advantage { Eliminates need for rejections ) faster convergence 2 Disadvantage { Generating samples from p(xsjx@s) can be di±cult. 36
Generalized Metropolis Sampler[80, 129] 2 Hastings and Peskun generalized the Metropolis sampler for transition func- tions q(wjxk) which are not symmetric. 2 The acceptance probability is then r(xks;w) = min 8>>< >>:1; q(xkjw) q(wjxk) expf!¢E(w)g9>>= >>; 2 Special cases q(wjxk) = q(xkjz) ) conventional Metropolis q(wsjxk) = p(xksjxk@s)¯¯¯¯ xks=ws ) Gibbs sampler 2 Advantage { Transition function may be chosen to minimize rejections[76] 37
Parameter Estimation for Discrete State MRF's 2 Topics to be covered: { Why is it di±cult? { Coding/maximum pseudolikehood { Least squares 38
Why is Parameter Estimation Di±cult? 2 Consider the ML estimate of ¯ for an Ising model. 2 Remember that t1(x) = (# horz. and vert. neighbors of direrent value.) 2 Then the ML estimate of ¯ is ^ ¯ = arg max ¯ 8>>< >>: 1 Z(¯) exp f!¯t1(x)g9>>= >>; = arg max ¯ f!¯t1(x) ! log Z(¯)g 2 However, log Z(¯) has an intractable form log Z(¯) = log Xx exp f!¯t1(x)g 2 Partition function can not be computed. 39
Coding Method/Maximum Pseudolikelihood[15, 16] 4 pt Neighborhood Code 1 Code 2 Code 3 Code 4 2 Assume a 4 point neighborhood 2 Separate points into four groups or codes. 2 Group (code) contains points which are conditionally independent given the other groups (codes). ^ ¯ = arg max ¯ Y s2Codek p(xsjx@s) 2 This is tractable (but not necessarily easy) to compute 40
Least Squares Parameter Estimation[49] 2 It can be shown that for an Ising model log PfXs = 1jx@sg PfXs = 0jx@sg = !¯ (V1(1jx@s) ! V1(0jx@s)) 2 For each unique set of neighboring pixel values, x@s, we may compute { The observed rate of log PfXs=1jx@sg PfXs=0jx@sg { The value of (V1(1jx@s) ! V1(0jx@s)) { This produces a set of over-determined linear equations which can be solved for ¯. 2 This least squares method is easily implemented. 41
Theoretical Results in Parameter Estimation for MRF's 2 Inconsistency of ML estimate for Ising model[130, 131] { Caused by critical temperature behavior. { Single sample of Ising model cannot distinguish between high ¯ with mean 1/2, and low ¯ with large mean. { Not identi¯able 2 Consistency of maximum pseudolikelihood estimate[69] { Requires an identi¯able parameterization. 42
Application of MRF's to Segmentation 2 Topics to be covered: { The Model { Bayesian Estimation { MAP Optimization { Parameter Estimation { Other Approaches 43
Bayesian Segmentation Model 1 2 3 0 Y -Texture feature vectors observed from image. X -Unobserved field containing the class of each pixel 2 Discrete MRF is used to model the segmentation ¯eld. 2 Each class is represented by a value Xs 2 f0; ¢ ¢ ¢ ;M ! 1g 2 The joint probability of the data and segmentation is PfY 2 dy;X = xg = p(yjx)p(x) where { p(yjx) is the data model { p(x) is the segmentation model 44
Bayes Estimation 2 C(x;X) is the cost of guessing x when X is the correct answer. 2 ^Xis the estimated value of X. 2 E[C( ^X;X)] is the expected cost (risk). 2 Objective: Choose the estimator ^Xwhich minimizes E[C( ^X;X)]. 45
Maximum A Posteriori (MAP) Estimation 2 Let C(x;X) = ±(x 6= X) 2 Then the optimum estimator is given by ^XMAP = arg max x pxjy(xjY ) = arg max x log py;x(Y; x) py(Y ) = arg max x flog p(Y jx) + log p(x)g 2 Advantage: { Can be computed through direct optimization 2 Disadvantage: { Cost function is unreasonable for many applications 46
Maximizer of the Posterior Marginals (MPM) Estimation[116] 2 Let C(x;X) = X s2S ±(xs 6= Xs) 2 Then the optimum estimator is given by ^XMPM = arg max xs pxsjY (xsjY ) 2 Compute the most likely class for each pixel 2 Method: { Use simulation method to generate samples from pxjy(xjy). { For each pixel, choose the most frequent class. 2 Advantage: { Minimizes number of misclassi¯ed pixels 2 Disadvantage: { Di±cult to compute 47
MAP Optimization for Segmentation 2 Assume the data model pyjx(yjx) = Y s2S p(ysjxs) 2 And the prior model (Ising model) px(x) = 1 Z0 expf!¯t1(x)g 2 Then the MAP estimate has the form ^x = arg min x ½!log pyjx(yjx) + ¯t1(x)¾ 2 This optimization problem is very di±cult 48
Iterated Conditional Modes [16] 2 The problem:^xMAP = arg min x 8>< >:! X s2S log pysjxs(ysjxs) + ¯t1(x)9>= >; 2 Iteratively minimize the function with respect to each pixel, xs. ^xs = arg min xs ½!log pysjxs(ysjxs) + ¯v1(xsjx@s)¾ 2 This converges to a local minimum in the cost function 49
Simulated Annealing [68] 2 Consider the Gibbs distribution 1Z exp 8>< >:!1T U(x)9>= >; where U(x) = X s2S log pysjxs(ysjxs) + ¯t1(x) 2 As T ! 0, the distribution becomes clustered about ^xMAP . 2 Use simulation method to generate samples from distribution. 2 Slowly let T ! 0. 2 If Tk = T1 1+log k for iteration k, the the simulation converges to ^xMAP almost surely. 2 Problem: This is very slow! 50
Multiscale MAP Segmentation 2 Renormalization theory[72] { Theoretically results in the exact MAP segmentation { Requires the computation of intractable functions { Can be implemented with approximation 2 Multiscale resolution segmentation[23] { Performs ICM segmentation in a coarse-to-¯ne sequence { Each MAP optimization is initialized with the solution from the previous coarser resolution { Used the fact that a discrete MRF constrained to be block constant is still a MRF. 2 Multiscale Markov random ¯elds[97] { Extended MRF to the third dimension of scale { Formulated a parallel computational approach 51
Segmentation Example 2 Iterated Conditional Modes (ICM): ML ; ICM 1; ICM 5; ICM 10 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 2 Simulated Annealing (SA): ML ; SA 1; SA 5; SA 10 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30 52
Texture Segmentation Example a b c d a) Synthetic image with 3 textures b) ICM - 29 iterations c) Simulated Annealing - 100 iterations d) Multiresolution - 7.8 iterations 53
Parameter Estimation Random Field X Model q Physical System Y Data Collection f 2 Question: How do we estimate u from Y ? 2 Problem: We don't know X! 2 Solution 1: Joint MAP estimation [104] (^u; ^x) = arg max u;x p(y; xju) { Problem: The solution is biased. 2 Solution 2: Expectation maximization algorithm [9, 70] ^uk+1 = arg max u E[log p(Y;Xju)jY = y; uk] { Expectation may be computed using simulation techniques or mean ¯eld theory. 54
Other Approaches to using Discrete MRFs 2 Dynamic programming does not work in 2-D, but a number of researchers have formulated approximate recursive solutions to MAP estimation[48, 169]. 2 Mean ¯eld theory has also been studied as a method for computing the MPM estimate[176]. 55
Gaussian Random Process Models 2 Topics to be covered: { Autoregressive (AR) models { Simultaneous Autoregressive (SAR) models { Gaussian MRF's { Generalization to 2-D 56
Autoregressive (AR) Models en = xn ! 1X k=1 xn!khk Xn-3 Xn-2 Xn-1 Xn Xn+1 Xn+2 Xn+3 H(ejw) + -en 2 H(ej!) is an optimal predictor ) e(n) is white noise. 2 The density for the N point vector X is given by px(x) = 1Z exp 8< :!12xtAtAx9= ; where A = 2666664 1 !hm!n . . . 0 1 3777775 Z = (2¼)N=2jAj!1 = (2¼)N=2 2 The power spectrum of X is Sx(ej!) = ¾2e j1 ! H(ej!)j2 57
Simultaneous Autoregressive (SAR) Models[95, 94] en = xn ! 1X k=1(xn!k ! xn+k)hk Xn-3 Xn-2 Xn-1 Xn Xn+1 Xn+2 Xn+3 H(ejw) + -en 2 e(n) is white noise ) H(ej!) is not an optimal non-causal predictor. 2 The density for the N point vector X is given by px(x) = 1Z exp 8< :!12xtAtAx9= ; where A = 2666664 1 !hm!n . . . !hn!m 1 3777775 Z = (2¼)N=2jAj!1 ¼ (2¼)N=2 exp 8< :!N2¼ Z ¼ !¼ log j1 ! H(ej!)jd!9= ; 2 The power spectrum of X is Sx(ej!) = ¾2e j1 ! H(ej!)j2 58
Conditional Markov (CM) Models (i.e. MRF's)[95, 94] en = xn ! 1X k=1(xn!k ! xn+k)gk Xn-3 Xn-2 Xn-1 Xn Xn+1 Xn+2 Xn+3 G(ejw) + -en 2 G(ej!) is an optimal non-causal predictor ) e(n) is not white noise. 2 The density for the N point vector X is given by px(x) = 1Z exp 8< :!12xtBx9= ; where B = 2666664 1 !gm!n . . . !gn!m 1 3777775 Z = (2¼)N=2jBj!1=2 ¼ (2¼)N=2 exp 8< :!N4¼ Z ¼ !¼ log(1 ! G(ej!))d!9= ; 2 The power spectrum of X is Sx(ej!) = ¾2e 1 ! G(ej!) 59
Generalization to 2-D 2 Same basic properties hold. 2 Circulant matrices become circulant block circulant. 2 Toeplitz matrices become Toeplitz block Toeplitz. 2 SAR and MRF models are more important in 2-D. 60
Non-Gaussian Continuous State MRF's 2 Topics to be covered: { Quadratic functions { Non-Convex functions { Continuous MAP estimation { Convex functions 61
Why use Non-Gaussian MRF's? 2 Gaussian MRF's do not model edges well. 2 In applications such as image restoration and tomography, Gaussian MRF's either { Blur edges { Leave excessive amounts of noise62
Gaussian MRF's 2 Gaussian MRF's have density functions with the form p(x) = 1Z exp 8>>< >>:! X s2S asx2s! X fs;rg2C bsrjxs ! xrj29>>= >>; 2 We will assume as = 0. 2 The terms jxs ! xrj2 penalize rapid changes in gray level. 2 MAP estimate has the form ^x = arg min x 8>>< >>:!log p(yjx) + X fs;rg2C bsrjxs ! xrj29>>= >>; 2 Problem: Quadratic function, j ¢ j2, excessively penalizes image edges. 63
Non-Gaussian MRF's Based on Pair-Wise Cliques 2 We will consider MRF's with pair-wise cliques p(x) = 1Z exp 8>>< >>:! X fs;rg2C bsr½ 0B@xs ! xr ¾ 1CA 9>>= >>; jxs ! xrj - is the change in gray level. ¾ - controls the gray level variation or scale. ½(¢): { Known as the potential function. { Determines the cost of abrupt changes in gray level. { ½(¢) = j¢j2 is the Gaussian model. ½0(¢) = d½(¢) d¢ : { Known as the in'uence function from \M-estimation"[139, 85]. { Determines the attraction of a pixel to neighboring gray levels. 64
Non-Convex Potential Functions Authors ½(¢) Ref. Potential func. In'uence func. Geman and McClure ¢2 1+¢2 [70, 71] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Geman_McClure Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Geman_McClure Influence Function Blake and Zisserman min ½¢2; 1¾ [20, 19] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Blake_Zisserman Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Blake_Zisserman Influence Function Hebert and Leahy log u1 + ¢2
 [81] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Hebert_Leahy Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Hebert_Leahy Influence Function Geman and Reynolds j¢j 1+j¢j [66] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Geman_Reynolds Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Geman_Reynolds Influence Function 65
Properties of Non-Convex Potential Functions 2 Advantages { Very sharp edges { Very general class of potential functions 2 Disadvantages { Di±cult (impossible) to compute MAP estimate { Usually requires the choice of an edge threshold { MAP estimate is a discontinuous function of the data 66
Continuous (Stable) MAP Estimation[25] 2 Minimum of non-convex function can change abruptly. x1 x2 location of minimum x1 x2 location of minimum 2 Discontinuous MAP estimate for Blake and Zisserman potential. -2 -10123456 0 5 10 15 20 25 30 35 40 45 50 Noisy Signals signal #1 signal #2 Unstable Reconstructions signal #1 signal #2 -2 -10123456 0 5 10 15 20 25 30 35 40 45 50 2 Theorem:[25] - If the log of the posterior density is strictly convex, then the MAP estimate is a continuous function of the data. 67
Convex Potential Functions Authors(Name) ½(¢) Ref. Potential func. In'uence func. Besag j¢j [17] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Besage Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Besage Influence Function Green log cosh¢ [75] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Green Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Green Influence Function Stevenson and Delp (Huber function) min nj¢j2; 2j¢j!1o [155] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Stevenson_Delp Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Stevenson_Delp Influence Function Bouman and Sauer (Generalized Gaus- sian MRF) j¢jp [25] −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 0 0.51 1.52 2.53 Bouman_Sauer Potential Function −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 −3 −2 −10123 Bouman_Sauer Influence Function 68
Properties of Convex Potential Functions 2 Both log cosh(¢) and Huber functions { Quadratic for j¢j << 1 { Linear for j¢j >> 1 { Transition from quadratic to linear determines edge threshold. 2 Generalized Gaussian MRF (GGMRF) functions { Include j¢j function { Do not require an edge threshold parameter. { Convex and direrentable for p > 1. 69
Parameter Estimation for Continuous MRF's 2 Topics to be covered: { Estimation of scale parameter, ¾ { Estimation of temperature, T, and shape, p 70
ML Estimation of Scale Parameter, ¾, for Continuous MRF's [26] 2 For any continuous state Gibbs distribution p(x) = 1 Z(¾) exp f!U(x=¾)g the partition function has the form Z(¾) = ¾NZ(1) 2 Using this result the ML estimate of ¾ is given by ¾N d d¾U(x=¾)¯¯¯¯¯¯¯ ¾=^¾ ! 1 = 0 2 This equation can be solved numerically using any root ¯nding method. 71
ML Estimation of ¾ for GGMRF's [108, 26] 2 For a Generalized Gaussian MRF (GGMRF) p(x) = 1 ¾NZ(1) exp 8>>< >>:! 1 p¾pU(x)9>>= >>; where the energy function has the property that for all r > 0 U(rx) = rpU(x) 2 Then the ML estimate of ¾ is ^¾ = 0B@1 NU(x)1CA (1=p) 2 Notice for that for the i.i.d. Gaussian case, this is ^¾ = vuuuut1 N Xs jxsj2 72
Estimation of Temperature, T, and Shape, p, Parameters 2 ML estimation of T[71] { Used to estimate T for any distribution. { Based on \or line" computation of log partition function. 2 Adaptive method [133] { Used to estimate p parameter of GGMRF. { Based on measurement of kurtosis. 2 ML estimation of p[145, 144] { Used to estimate p parameter of GGMRF. { Based on \or line" computation of log partition function. 73
Example Estimation of p Parameter 0.8 1 1.2 1.4 1.6 1.8 2 −7.4 −7.2 −7 −6.8 −6.6 −6.4 −6.2 p −−−> −log-likelihood −−−> (a) 0.8 1 1.2 1.4 1.6 1.8 2 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 p---> -log-likelihood ---> (b) 0.8 1 1.2 1.4 1.6 1.8 2 -2.308 -2.306 -2.304 -2.302 -2.3 -2.298 -2.296 -2.294 p ---> -log likelihood ---> (c) 2 ML estimation of p for (a) transmission phantom (b) natural image (c) image corrupted with Gaussian noise. The plot below each image shows the corresponding negative log- likelihood as a function of p. The ML estimate is the value of p that minimizes the plotted function. 74
Application to Tomography 2 Topics to be covered: { Tomographic system and data models { MAP Optimization { Parameter estimation 75
The Tomography Problem 2 Recover image cross-section from integral projections 2 Transmission problem Emitter Detector i Y  -detected events  i yT -dosage x -absorption of pixel j j 2 Emission problem Detector i Detector i x  -detection rate  j Pij x -emission rate j 76
Statistical Data Model[27] 2 Notation { y - vector of photon counts { x - vector of image pixels { P - projection matrix { Pj;¤ - jth row of projection matrix 2 Emission formulation log p(yjx) = MX i=1 (!Pi¤x + yi logfPi¤xg ! log(yi!)) 2 Transmission formulation log p(yjx) = MX i=1 u!yT e!Pi¤x + yi(log yT ! Pi¤x) ! log(yi!)
 2 Common form log p(yjx) = !PMi=1 fi(Pi¤x) { fi(¢) is a convex function { Not a hard problem! 77
Maximum A Posteriori Estimation (MAP) 2 MAP estimate incorporates prior knowledge about image ^x = arg max x p(xjy) = arg max x>0 8>>< >>:! MX i=1 fi(Pi¤x) ! X k<j bk;j ½(xk ! xj)9>>= >>; 2 Can be solved using direct optimization 2 Incorporates positivity constraint 78
MAP Optimization Strategies 2 Expectation maximization (EM) based optimization strategies { ML reconstruction[151, 107] { MAP reconstruction[81, 75, 84] { Slow convergence; Similar to gradient search. { Accelerated EM approach[59] 2 Direct optimization { Preconditioned gradient descent with soft positivity constraint[45] { ICM iterations (also known as ICD and Gauss-Seidel)[27] 79
Convergence of ICM Iterations: MAP with Generalized Gaussian Prior q = 1:1 2 ICM also known as iterative coordinate descent (ICD) and Gauss-Seidel 0 10 20 30 40 50 Iteration Number -5.5e+03 -4.5e+03 -3.5e+03 Log A Posteriori Likelihood GGMRF Prior, q=1.1 g = 3.0 ICD/NR GEM OSL DePierro's 2 Convergence of MAP estimates using ICD/Newton-Raphson updates, Green's (OSL), and Hebert/Leahy's GEM, and De Pierro's method, and a general- ized Gaussian prior model with q = 1:1 and ' = 3:0. 80
Estimation of ¾ from Tomographic Data 2 Assume a GGMRF prior distribution of the form p(x) = 1 ¾NZ(1) exp 8>>< >>: 1 p¾pU(x)9>>= >>; 2 Problem: We don't know X! 2 EM formulation for incomplete data problem ¾(k+1) = arg max ¾ E (log p(Xj¾)jY = y; ¾(k)) = 0B @E 8>< >: 1 NU(X)jY = y; ¾(k)9>= >; 1CA 1=p 2 Iterations converge toward the ML estimate. 2 Expectations may be computed using stochastic simulation. 81
Example of Estimation of ¾ from Tomographic Data Accelerated Metropolis Metropolis Projected sigma 0 5 10 15 20 25 30 0.12 0.14 0.16 0.18 0.2 0.22 0.24 0.26 0.28 No. of iterations −−−> Sigma −−−> 2 The above plot shows the EM updates for ¾ for the emission phantom modeled by a GGMRF prior (p = 1:1) using conventional Metropolis (CM) method, accelerated Metropolis (AM) and the extrapolation method. The parameter s denotes the standard deviation of the symmetric transition distribution for the CM method. 82
Example of Tomographic Reconstructions a b c d e 2 (a) Original transmission phantom and (b) CBP reconstruction. Recon- structed transmission phantom using GGMRF prior with p = 1:1 The scale parameter ¾ is (c) ^¾ML ¼ ^¾CBP , (d) 12 ^¾ML, and (e) 2^¾ML 2 Phantom courtesy of J. Fessler, University of Michigan 83
Multiscale Stochastic Models 2 Generate a Markov chain in scale 2 Some references { Continuous models[12, 5, 111] { Discrete models[29, 111] 2 Advantages: { Does not require a causal ordering of image pixels { Computational advantages of Markov chain versus MRF { Allows joint and marginal probabilities to be computed using forward/backward algorithm of HMM's. 84
Multiscale Stochastic Models for Continuous State Estimation 2 Theory of 1-D systems can be extended to multiscale trees[6, 7]. 2 Can be used to e±ciently estimate optical 'ow[111]. 2 These models can approximate MRF's[112]. 2 The structure of the model allows exact calculation of log likelihoods for texture segmentation[113]. 85
Multiscale Stochastic Models for Segmentation[29] 2 Multiscale model results in non-iterative segmentation 2 Sequential MAP (SMAP) criteria minimizes size of largest misclassi¯cation. 2 Computational comparison Replacements per pixel SMAP SMAP + par. est. SA 500 SA 100 ICM image1 1.33 3.13 504 105 28 image2 1.33 3.55 506 108 28 image3 1.33 3.14 505 104 10 86
Segmentation of Synthetic Test Image Synthetic Image Correct Segmentation SMAP 100 Iterations of SA 87
Multispectral Spot Image Segmentation SPOT image SMAP Maximum Likelihood 88
High Level Image Models 2 MRF's have been used to { model the relative location of objects in a scene[119]. { model relational constraints for object matching problems[109]. 2 Multiscale stochastic models { have been used to model complex assemblies for automated inspection[166]. { have been used to model 2-D patterns for application in image search[154]. 89
References [1] M. Aitkin and D. B. Rubin. Estimation and hypothesis testing in ¯nite mixture models. Journal of the Royal Statistical Society B, 47(1):67{75, 1985. [2] H. Akaike. A new look at the statistical model identi¯cation. IEEE Trans. Automat. Contr., AC-19(6):716{723, December 1974. [3] S. Alliney and S. A. Ruzinsky. An algorithm for the minimization of mixed l1 and l2 norms with application to Bayesian estimation. IEEE Trans. on Signal Processing, 42(3):618{627, March 1994. [4] P. Barone, A. Frigessi, and M. Piccioni, editors. Stochastic models, statistical methods, and algorithms in image analysis. Springer-Verlag, Berlin, 1992. [5] M. Basseville, A. Benveniste, K. Chou, S. Golden, and R. Nikoukhah. Modeling and estimation of multiresolution stochastic processes. IEEE Trans. on Information Theory, 38(2):766{784, 1992. [6] M. Basseville, A. Benveniste, and A. Willsky. Multiscale autoregressive processes, part i: Schur-Levinson parametrizations. IEEE Trans. on Signal Processing, 40(8):1915{1934, 1992. [7] M. Basseville, A. Benveniste, and A. Willsky. Multiscale autoregressive processes, part ii: Lattice structures for whitening and modeling. IEEE Trans. on Signal Processing, 40(8):1915{1934, 1992. [8] L. Baum, T. Petrie, G. Soules, and N.Weiss. A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Ann. Math. Statistics, 41(1):164{171, 1970. [9] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of ¯nite state Markov chains. Ann. Math. Statistics, 37:1554{1563, 1966. [10] F. Beckman. The solution of linear equations by the conjugate gradient method. In A. Ralston, H. Wilf, and K. Enslein, editors, Mathematical Methods for Digital Computers. Wiley, 1960. [11] M. G. Bello. A combined Markov random ¯eld and wave-packet transform-based approach for image segmentation. IEEE Trans. on Image Processing, 3(6):834{846, November 1994. [12] A. Benveniste, R. Nikoukhah, and A. Willsky. Multiscale system theory. In Proceedings of the 29th Conference on Decision and Control, volume 4, pages 2484{2489, Honolulu, Hawaii, December 5-7 1990. 90
[13] J. Besag. Nearest-neighbour systems and the auto-logistic model for binary data. Journal of the Royal Statistical Society B, 34(1):75{83, 1972. [14] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society B, 36(2):192{236, 1974. [15] J. Besag. E±ciency of pseudolikelihood estimation for simple Gaussian ¯elds. Biometrica, 64(3):616{618, 1977. [16] J. Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society B, 48(3):259{302, 1986. [17] J. Besag. Towards Bayesian image analysis. Journal of Applied Statistics, 16(3):395{407, 1989. [18] J. E. Besag and P. A. P. Moran. On the estimation and testing of spatial interaction in Gaussian lattice processes. Biometrika, 62(3):555{562, 1975. [19] A. Blake. Comparison of the e±ciency of deterministic and stochastic algorithms for visual reconstruction. IEEE Trans. on Pattern Analysis and Machine Intelligence, 11(1):2{30, January 1989. [20] A. Blake and A. Zisserman. Visual Reconstruction. MIT Press, Cambridge, Massachusetts, 1987. [21] C. A. Bouman and B. Liu. A multiple resolution approach to regularization. In Proc. of SPIE Conf. on Visual Commu- nications and Image Processing, pages 512{520, Cambridge, MA, Nov. 9-11 1988. [22] C. A. Bouman and B. Liu. Segmentation of textured images using a multiple resolution approach. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., pages 1124{1127, New York, NY, April 11-14 1988. [23] C. A. Bouman and B. Liu. Multiple resolution segmentation of textured images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 13:99{113, Feb. 1991. [24] C. A. Bouman and K. Sauer. An edge-preserving method for image reconstruction from integral projections. In Proc. of the Conference on Information Sciences and Systems, pages 382{387, The Johns Hopkins University, Baltimore, MD, March 20-22 1991. [25] C. A. Bouman and K. Sauer. A generalized Gaussian image model for edge-preserving map estimation. IEEE Trans. on Image Processing, 2:296{310, July 1993. [26] C. A. Bouman and K. Sauer. Maximum likelihood scale estimation for a class of Markov random ¯elds. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., volume 5, pages 537{540, Adelaide, South Australia, April 19-22 1994. 91
[27] C. A. Bouman and K. Sauer. A uni¯ed approach to statistical tomography using coordinate descent optimization. IEEE Trans. on Image Processing, 5(3):480{492, March 1996. [28] C. A. Bouman and M. Shapiro. Multispectral image segmentation using a multiscale image model. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., pages III{565 { III{568, San Francisco, California, March 23-26 1992. [29] C. A. Bouman and M. Shapiro. A multiscale random ¯eld model for Bayesian image segmentation. IEEE Trans. on Image Processing, 3(2):162{177, March 1994. [30] A. Brandt. Multigrid Techniques: 1984 Guide with Applications to Fluid Dynamics, volume Nr. 85. GMD-Studien, 1984. ISBN:3-88457-081-1. [31] W. Briggs. A Multigrid Tutorial. Society for Industrial and Applied Mathematics, Philadelphia, 1987. [32] P. Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud. Two deterministic half-quadratic reqularization algorithms for computed imaging. In Proc. of IEEE Int'l Conf. on Image Proc., volume 2, pages 168{176, Austin, TX, November, 13-16 1994. [33] R. Chellappa and S. Chatterjee. Classi¯cation of textures using Gaussian Markov random ¯elds. IEEE Trans. on Acoust. Speech and Signal Proc., ASSP-33(4):959{963, August 1985. [34] K. Chou, S. Golden, and A. Willsky. Modeling and estimation of multiscale stochastic processes. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., pages 1709{1712, Toronto, Canada, May 14-17 1991. [35] B. Claus and G. Chartier. Multiscale signal processing: Isotropic random ¯elds on homogeneous trees. IEEE Trans. on Circ. and Sys.:Analog and Dig. Signal Proc., 41(8):506{517, August 1994. [36] F. Cohen and D. Cooper. Simple parallel hierarchical and relaxation algorithms for segmenting noncausal Markovian random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-9(2):195{219, March 1987. [37] F. S. Cohen and Z. Fan. Maximum likelihood unsupervised texture image segmentation. CVGIP:Graphical Models and Image Proc., 54(3):239{251, May 1992. [38] F. S. Cohen, Z. Fan, and M. A. Patel. Classi¯cation of rotated and scaled textured images using Gaussian Markov random ¯eld models. IEEE Trans. on Pattern Analysis and Machine Intelligence, 13(2):192{202, February 1991. [39] M. L. Comer and E. J. Delp. Parameter estimation and segmentation of noisy or textured images using the EM algorithm and MPM estimation. In Proc. of IEEE Int'l Conf. on Image Proc., volume II, pages 650{654, Austin, Texas, November 1994. 92
[40] M. L. Comer and E. J. Delp. Multiresolution image segmentation. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., pages 2415{2418, Detroit, Michigan, May 1995. [41] D. B. Cooper. Maximum likelihood estimation of Markov process blob boundaries in noisy images. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-1:372{384, 1979. [42] D. B. Cooper, H. Elliott, F. Cohen, L. Reiss, and P. Symosek. Stochastic boundary estimation and object recognition. Comput. Vision Graphics and Image Process., 12:326{356, April 1980. [43] R. Cristi. Markov and recursive least squares methods for the estimation of data with discontinuities. IEEE Trans. on Acoust. Speech and Signal Proc., 38(11):1972{1980, March 1990. [44] G. R. Cross and A. K. Jain. Markov random ¯eld texture models. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-5(1):25{39, January 1983. [45] E. ÄU. Mumcuo-glu, R. Leahy, S. R. Cherry, and Z. Zhou. Fast gradient-based methods for Bayesian reconstruction of transmission and emission pet images. IEEE Trans. on Medical Imaging, 13(4):687{701, December 1994. [46] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39(1):1{38, 1977. [47] H. Derin and W. Cole. Segmentation of textured images using Gibbs random ¯elds. Comput. Vision Graphics and Image Process., 35:72{98, 1986. [48] H. Derin, H. Elliot, R. Cristi, and D. Geman. Bayes smoothing algorithms for segmentation of binary images modeled by Markov random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-6(6):707{719, November 1984. [49] H. Derin and H. Elliott. Modeling and segmentation of noisy and textured images using Gibbs random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-9(1):39{55, January 1987. [50] H. Derin and P. A. Kelly. Discrete-index Markov-type random processes. Proc. of the IEEE, 77(10):1485{1510, October 1989. [51] H. Derin and C. Won. A parallel image segmentation algorithm using relaxation with varying neighborhoods and its mapping to array processors. Comput. Vision Graphics and Image Process., 40:54{78, October 1987. [52] R. W. Dijkerman and R. R. Mazumdar. Wavelet representations of stochastic processes and multiresolution stochastic models. IEEE Trans. on Signal Processing, 42(7):1640{1652, July 1994. 93
[53] P. C. Doerschuk. Bayesian signal reconstruction, Markov random ¯elds, and X-ray crystallography. J. Opt. Soc. Am. A, 8(8):1207{1221, 1991. [54] R. Dubes and A. Jain. Random ¯eld models in image analysis. Journal of Applied Statistics, 16(2):131{164, 1989. [55] R. Dubes, A. Jain, S. Nadabar, and C. Chen. MRF model-based algorithms for image segmentation. In Proc. of the 10th Internat. Conf. on Pattern Recognition, pages 808{814, Atlantic City, NJ, June 1990. [56] I. M. Elfadel and R. W. Picard. Gibbs random ¯elds, cooccurrences and texture modeling. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(1):24{37, January 1994. [57] H. Elliott, D. B. Cooper, F. S. Cohen, and P. F. Symosek. Implementation, interpretation, and analysis of a suboptimal boundary ¯nding algorithm. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-4(2):167{182, March 1982. [58] W. Enkelmann. Investigations of multigrid algorithms. Comput. Vision Graphics and Image Process., 43:150{177, 1988. [59] J. Fessler and A. Hero. Space-alternating generalized expectation-maximization algorithms. IEEE Trans. on Acoust. Speech and Signal Proc., 42(10):2664{2677, October 1994. [60] N. P. Galatsanos and A. K. Katsaggelos. Methods for choosing the regularization parameter and estimating the noise variance in image restoration and their relation. IEEE Trans. on Image Processing, 1(3):322{336, July 1992. [61] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in rd. SIAM Journal on Control and Optimization, 29(5):999{1018, September 1991. [62] S. B. Gelfand and S. K. Mitter. Metropolis-type annealing algorithms for global optimization in rd. SIAM Journal on Control and Optimization, 31(1):111{131, January 1993. [63] S. B. Gelfand and S. K. Mitter. On sampling methods and annealing algorithms. In R. Chellappa and A. Jain, editors, Markov Random Fields: Theory and Applications, pages 499{515. Academic Press, Inc., Boston, 1993. [64] D. Geman. Bayesian image analysis by adaptive annealing. In Digest 1985 Int. Geoscience and remote sensing symp., Amherst, MA, October 7-9 1985. [65] D. Geman, S. Geman, C. Gra±gne, and P. Dong. Boundary detection by constrained optimization. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12(7):609{628, July 1990. 94
[66] D. Geman and G. Reynolds. Constrained restoration and the recovery of discontinuities. IEEE Trans. on Pattern Analysis and Machine Intelligence, 14(3):367{383, 1992. [67] D. Geman, G. Reynolds, and C. Yang. Stochastic algorithms for restricted image spaces and experiments in deblurring. In R. Chellappa and A. Jain, editors, Markov Random Fields: Theory and Applications, pages 39{68. Academic Press, Inc., Boston, 1993. [68] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-6:721{741, Nov. 1984. [69] S. Geman and C. Gra±gne. Markov random ¯eld image models and their applications to computer vision. In Proc. of the Intl Congress of Mathematicians, pages 1496{1517, Berkeley, California, 1986. [70] S. Geman and D. McClure. Bayesian images analysis: An application to single photon emission tomography. In Proc. Statist. Comput. sect. Amer. Stat. Assoc., pages 12{18, Washington, DC, 1985. [71] S. Geman and D. McClure. Statistical methods for tomographic image reconstruction. Bull. Int. Stat. Inst., LII-4:5{21, 1987. [72] B. Gidas. A renormalization group approach to image processing problems. IEEE Trans. on Pattern Analysis and Machine Intelligence, 11(2):164{180, February 89. [73] J. Goutsias. Unilateral approximation of Gibbs random ¯eld images. CVGIP:Graphical Models and Image Proc., 53(3):240{ 257, May 1991. [74] A. Gray, J. Kay, and D. Titterington. An empirical study of the simulation of various models used for images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(5):507{513, May 1994. [75] P. J. Green. Bayesian reconstruction from emission tomography data using a modi¯ed EM algorithm. IEEE Trans. on Medical Imaging, 9(1):84{93, March 1990. [76] P. J. Green and X. liang Han. Metropolis methods, Gaussian proposals and antithetic variables. In P. Barone, A. Frigessi, and M. Piccioni, editors, Stochastic Models, Statistical methods, and Algorithms in Image Analysis, pages 142{164. Springer-Verlag, Berlin, 1992. [77] M. I. Gurelli and L. Onural. On a parameter estimation method for Gibbs-Markov random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(4):424{430, April 1994. [78] W. Hackbusch. Multi-Grid Methods and Applications. Sprinter-Verlag, Berlin, 1980. 95
[79] K. Hanson and G. Wecksung. Bayesian approach to limited-angle reconstruction computed tomography. J. Opt. Soc. Am., 73(11):1501{1509, November 1983. [80] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97{109, 1970. [81] T. Hebert and R. Leahy. A generalized EM algorithm for 3-d Bayesian reconstruction from Poisson data using Gibbs priors. IEEE Trans. on Medical Imaging, 8(2):194{202, June 1989. [82] T. J. Hebert and K. Lu. Expectation-maximization algorithms, null spaces, and MAP image restoration. IEEE Trans. on Image Processing, 4(8):1084{1095, August 1995. [83] F. Heitz and P. Bouthemy. Multimodal estimation of discontinuous optical 'ow using Markov random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 15(12):1217{1232, December 1993. [84] G. T. Herman, A. R. De Pierro, and N. Gai. On methods for maximum a posteriori image reconstruction with normal prior. J. Visual Comm. Image Rep., 3(4):316{324, December 1992. [85] P. Huber. Robust Statistics. John Wiley & Sons, New York, 1981. [86] B. Hunt. The application of constrained least squares estimation to image restoration by digital computer. IEEE Trans. on Comput., c-22(9):805{812, September 1973. [87] B. Hunt. Bayesian methods in nonlinear digital image restoration. IEEE Trans. on Comput., c-26(3):219{229, 1977. [88] J. Hutchinson, C. Koch, J. Luo, and C. Mead. Computing motion using analog and binary resistive networks. Computer, 21:53{63, March 1988. [89] A. Jain. Advances in mathematical models for image processing. Proc. of the IEEE, 69:502{528, May 1981. [90] B. D. Jers and M. Gunsay. Restoration of blurred star ¯eld images by maximally sparse optimization. IEEE Trans. on Image Processing, 2(2):202{211, April 1993. [91] F.-C. Jeng and J. W. Woods. Compound Gauss-Markov random ¯elds for image estimation. IEEE Trans. on Signal Processing, 39(3):683{697, March 1991. [92] B. Jeon and D. Landgrebe. Classi¯cation with spatio-temporal interpixel class dependency contexts. IEEE Trans. on Geoscience and Remote Sensing, 30(4), JULY 1992. 96
[93] S. R. Kadaba, S. B. Gelfand, and R. L. Kashyap. Bayesian decision feedback for segmentation of binary images. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., pages 2543{2546, Detroit, Michigan, May 8-12 1995. [94] R. Kashyap and R. Chellappa. Estimation and choice of neighbors in spatial-interaction models of images. IEEE Trans. on Information Theory, IT-29(1):60{72, January 1983. [95] R. Kashyap, R. Chellappa, and A. Khotanzad. Texture classi¯cation using features derived from random ¯eld models. Pattern Recogn. Let., 1(1):43{50, October 1982. [96] R. L. Kashyap and K.-B. Eom. Robust image modeling techniques with an image restoration application. IEEE Trans. on Acoust. Speech and Signal Proc., 36(8):1313{1325, August 1988. [97] Z. Kato, M. Berthod, and J. Zerubia. Parallel image classi¯cation using multiscale Markov random ¯elds. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., volume 5, pages 137{140, Minneapolis, MN, April 27-30 1993. [98] A. Katsaggelos. Image identi¯cation and restoration based on the expectation-maximization algorithm. Optical Engineer- ing, 29(5):436{445, May 1990. [99] P. Kelly, H. Derin, and K. Hartt. Adaptive segmentation of speckled images using a hierarchical random ¯eld model. IEEE Trans. on Acoust. Speech and Signal Proc., 36(10):1628{1641, October 1988. [100] R. Kindermann and J. Snell. Markov Random Fields and their Applications. American Mathematical Society, Providence, 1980. [101] J. Konrad and E. Dubois. Bayesian estimation of motion vector ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 14(9):910{927, September 1992. [102] R. Lagendijk, A. Tekalp, and J. Biemond. Maximum likelihood image and blur identi¯cation: A unifying approach. Optical Engineering, 29(5):422{435, May 1990. [103] R. L. Lagendijk, J. Biemond, and D. E. Boekee. Identi¯cation and restoration of noisy blurred images using the expectation-maximization algorithm. IEEE Trans. on Acoust. Speech and Signal Proc., 38(7):1180{1191, July 1990. [104] S. Lakshmanan and H. Derin. Simultaneous parameter estimation and segmentation of Gibbs random ¯elds using simulated annealing. IEEE Trans. on Pattern Analysis and Machine Intelligence, 11(8):799{813, August 1989. [105] S. Lakshmanan and H. Derin. Gaussian Markov random ¯elds at multiple resolutions. In R. Chellappa and A. Jain, editors, Markov Random Fields: Theory and Applications, pages 131{157. Academic Press, Inc., Boston, 1993. 97
[106] S. Lakshmanan and H. Derin. Valid parameter space for 2-d Gaussian Markov random ¯elds. IEEE Trans. on Information Theory, 39(2):703{709, March 1993. [107] K. Lange. Convergence of EM image reconstruction algorithms with Gibbs smoothing. IEEE Trans. on Medical Imaging, 9(4):439{446, December 1990. [108] K. Lange. An overview of Bayesian methods in image reconstruction. In Proc. of the SPIE Conference on Digital Image Synthesis and Inverse Optics, volume SPIE-1351, pages 270{287, San Diego, CA, 1990. [109] S. Z. Li. Markov Random Field Modeling in Computer Vision. to be published, 1996. [110] J. Liu and Y.-H. Yang. Multiresolution color image segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(7):689{700, July 1994. [111] M. R. Luettgen, W. C. Karl, and A. S. Willsky. E±cient multiscale regularization with applications to the computation of optical 'ow. IEEE Trans. on Image Processing, 3(1), January 1994. [112] M. R. Luettgen, W. C. Karl, A. S. Willsky, and R. R. Tenney. Multiscale representations of Markov random ¯elds. IEEE Trans. on Signal Processing, 41(12), December 1993. special issue on Wavelets and Signal Processing. [113] M. R. Luettgen and A. S. Willsky. Likelihood calculation for a class of multiscale stochastic models, with application to texture discrimination. IEEE Trans. on Image Processing, 4(2):194{207, February 1995. [114] B. Manjunath, T. Simchony, and R. Chellappa. Stochastic and deterministic networks for texture segmentation. IEEE Trans. on Acoust. Speech and Signal Proc., 38(6):1039{1049, June 1990. [115] J. Mao and A. K. Jain. Texture classi¯cation and segmentation using multiresolution simultaneous autoregressive models. Pattern Recognition, 25(2):173{188, 1992. [116] J. Marroquin, S. Mitter, and T. Poggio. Probabilistic solution of ill-posed problems in computational vision. Journal of the American Statistical Association, 82:76{89, March 1987. [117] S. McCormick, editor. Multigrid Methods. Society for Industrial and Applied Mathematics, Philadelphia, 1987. [118] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. Equations of state calculations by fast computing machines. J. Chem. Phys., 21:1087{1091, 1953. [119] J. W. Modestino and J. Zhang. A Markov random ¯eld model-based approach to image interpretation. In R. Chellappa and A. Jain, editors, Markov Random Fields: Theory and Applications, pages 369{408. Academic Press, Inc., Boston, 1993. 98
[120] H. H. Nguyen and P. Cohen. Gibbs random ¯elds, fuzzy clustering, and the unsupervised segmentation of textured images. CVGIP:Graphical Models and Image Proc., 55(1):1{19, January 1993. [121] Y. Ogata. A Monte Carlo method for an objective Bayesian procedure. Ann. Inst. of Statist. Math., 42(3):403{433, 1990. [122] L. Onsager. Crystal statistics i. a two-dimensional model. Physical Review, 65:117{149, 1944. [123] L. Onural. Generating connected textured fractal patterns using Markov random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 13(8):819{825, August 1991. [124] L. Onural, M. B. Alp, and M. I. Gurelli. Gibbs random ¯eld model based weight selection for the 2-d adaptive weighted median ¯lter. IEEE Trans. on Pattern Analysis and Machine Intelligence, 16(8):831{837, August 1994. [125] T. Pappas. An adaptive clustering algorithm for image segmentation. IEEE Trans. on Signal Processing, 4:901{914, April 1992. [126] R. E. Peierls. On Ising's model of ferromagnetism. Proc. Camb. Phil. Soc., 32:477{481, 1936. [127] R. E. Peierls. Statistical theory of adsorption with interaction between the adsorbed atoms. Proc. Camb. Phil. Soc., 32:471{476, 1936. [128] P. Perez and F. Heitz. Multiscale Markov random ¯elds and constrained relaxation in low level image analysis. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., volume 3, pages 61{64, San Francisco, CA, March 23-26 1992. [129] P. H. Peskun. Optimum Monte-Carlo sampling using Markov chains. Biometrika, 60(3):607{612, 1973. [130] D. Pickard. Asymptotic inference for an ising lattice iii. non-zero ¯eld and ferromagnetic states. J. Appl. Prob., 16:12{24, 1979. [131] D. Pickard. Inference for discrete Markov ¯elds: The simplest nontrivial case. Journal of the American Statistical Association, 82:90{96, March 1987. [132] D. N. Politis. Markov chains in many dimensions. Adv. Appl. Prob., 26:756{774, 1994. [133] W. Pun and B. Jers. Shape parameter estimation for generalized Gaussian Markov random ¯eld models used in MAP image restoration. In 29th Asilomar Conference on Signals, Systems, and Computers, Oct. 29 - Nov. 1 1995. [134] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. of the IEEE, 77(2):257{286, February 1989. 99
[135] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models. IEEE Signal Proc. Magazine, 3(1):4{16, January 1986. [136] E. Redner and H. Walker. Mixture densities, maximum likelihood and the EM algorithm. SIAM Review, 26(2), April 1984. [137] S. Reeves and R. Mersereau. Optimal estimation of the regularization parameter and stabilizing functional for regularized image restoration. Optical Engineering, 29(5):446{454, May 1990. [138] S. J. Reeves and R. M. Mersereau. Blur identi¯cation by the method of generalized cross-validation. IEEE Trans. on Image Processing, 1(3):301{321, July 1992. [139] W. Rey. Introduction to Robust and Quasi-Robust Statistical Methods. Springer-Verlag, Berlin, 1980. [140] E. Rignot and R. Chellappa. Segmentation of polarimetric synthetic aperature radar data. IEEE Trans. on Image Processing, 1(3):281{300, July 1992. [141] B. D. Ripley. Stochastic Simulation. John Wiley & Sons, New York, 1987. [142] J. Rissanen. A universal prior for integers and estimation by minimum description length. Annals of Statistics, 11(2):417{ 431, 1983. [143] B. Rouchouze, P. Mathieu, T. Gaidon, and M. Barlaud. Motion estimation based on Markov random ¯elds. In Proc. of IEEE Int'l Conf. on Image Proc., volume 3, pages 270{274, Austin, TX, November, 13-16 1994. [144] S. S. Saquib, C. A. Bouman, and K. Sauer. ML parameter estimation for Markov random ¯elds, with applications to Bayesian tomography. Technical Report TR-ECE 95-24, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, October 1995. [145] S. S. Saquib, C. A. Bouman, and K. Sauer. E±cient ML estimation of the shape parameter for generalized Gaussian MRF. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., volume 4, pages 2229{2232, Atlanta, GA, May 7-10 1996. [146] K. Sauer and C. Bouman. Bayesian estimation of transmission tomograms using segmentation based optimization. IEEE Trans. on Nuclear Science, 39:1144{1152, 1992. [147] K. Sauer and C. A. Bouman. A local update strategy for iterative reconstruction from projections. IEEE Trans. on Signal Processing, 41(2), February 1993. 100
[148] R. Schultz, R. Stevenson, and A. Lumsdaine. Maximum likelihood parameter estimation for non-Gaussian prior signal models. In Proc. of IEEE Int'l Conf. on Image Proc., volume 2, pages 700{704, Austin, TX, November 1994. [149] R. R. Schultz and R. L. Stevenson. A Bayesian approach to image expansion for improved de¯nition. IEEE Trans. on Image Processing, 3(3):233{242, May 1994. [150] R. R. Schultz and R. L. Stevenson. Stochastic modeling and estimation of multispectral image data. IEEE Trans. on Image Processing, 4(8):1109{1119, August 1995. [151] L. Shepp and Y. Vardi. Maximum likelihood reconstruction for emission tomography. IEEE Trans. on Medical Imaging, MI-1(2):113{122, October 1982. [152] J. Silverman and D. Cooper. Bayesian clustering for unsupervised estimation of surface and texture models. IEEE Trans. on Pattern Analysis and Machine Intelligence, 10(4):482{495, July 1988. [153] T. Simchony, R. Chellappa, and Z. Lichtenstein. Relaxation algorithms for MAP estimation of gray-level images with multiplicative noise. IEEE Trans. on Information Theory, 36(3):608{613, May 1990. [154] S. Sista, C. A. Bouman, and J. P. Allebach. Fast image search using a multiscale stochastic model. In Proc. of IEEE Int'l Conf. on Image Proc., pages 225{228, Washington, DC, October 23-26 1995. [155] R. Stevenson and E. Delp. Fitting curves with discontinuities. Proc. of the ¯rst international workshop on robust computer vision, pages 127{136, October 1-3 1990. [156] H. Tan, S. Gelfand, and E. Delp. A comparative cost function approach to edge detection. IEEE Trans. on Systems Man and Cybernetics, 19(6):1337{1349, December 1989. [157] T. Taxt, P. Flynn, and A. Jain. Segmentation of document images. IEEE Trans. on Pattern Analysis and Machine Intelligence, 11(12):1322{1329, December 1989. [158] D. Terzopoulos. Image analysis using multigrid relaxation methods. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-8(2):129{139, March 1986. [159] D. Terzopoulos. The computation of visible-surface representations. IEEE Trans. on Pattern Analysis and Machine Intelligence, 10(4):417{438, July 1988. [160] C. Therrien. An estimation-theoretic approach to terrain image segmentation. Comput. Vision Graphics and Image Process., 22:313{326, 1983. 101
[161] C. Therrien, T. Quatieri, and D. Dudgeon. Statistical model-based algorithm for image analysis. Proc. of the IEEE, 74(4):532{551, April 1986. [162] C. W. Therrien, editor. Decision Estimation and Classi¯cation: An Introduction to Pattern Recognition and Related Topics. John Wiley & Sons, New York, 1989. [163] A. Thompson, J. Kay, and D. Titterington. A cautionary note about the cross validatory choice. J. statist. Comput. Simul., 33:199{216, 1989. [164] A. M. Thompson, J. C. Brown, J. W. Kay, and D. M. Titterington. A study of methods of choosing the smoothing parameter in image restoration. IEEE Trans. on Pattern Analysis and Machine Intelligence, 13(4):326{339, 1991. [165] A. Tikhonov and V. Arsenin. Solutions of Ill-Posed Problems. Winston and Sons, New York, 1977. [166] D. Tretter, C. A. Bouman, K. Khawaja, and A. Maciejewski. A multiscale stochastic image model for automated inspection. IEEE Trans. on Image Processing, 4(12):507{517, December 1995. [167] C. Won and H. Derin. Segmentation of noisy textured images using simulated annealing. In Proc. of IEEE Int'l Conf. on Acoust., Speech and Sig. Proc., pages 14.4.1{14.4.4, Dallas, TX, 1987. [168] C. S. Won and H. Derin. Unsupervised segmentation of noisy and textured images using Markov random ¯elds. CVGIP:Graphical Models and Image Proc., 54(4):308{328, July 1992. [169] J. Woods, S. Dravida, and R. Mediavilla. Image estimation using double stochastic Gaussian random ¯eld models. IEEE Trans. on Pattern Analysis and Machine Intelligence, PAMI-9(2):245{253, March 1987. [170] C. Wu. On the convergence properties of the EM algorithm. Annals of Statistics, 11(1):95{103, 1983. [171] C.-h. Wu and P. C. Doerschuk. Cluster expansions for the deterministic computation of Bayesian estimators based on Markov random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 17(3):275{293, March 1995. [172] C.-h. Wu and P. C. Doerschuk. Texture-based segmentation using Markov random ¯eld models and approximate Bayesian estimators based on trees. J. Math. Imaging and Vision, 5(4), December 1995. [173] C.-h. Wu and P. C. Doerschuk. Tree approximations to Markov random ¯elds. IEEE Trans. on Pattern Analysis and Machine Intelligence, 17(4):391{402, April 1995. [174] Z. Wu and R. Leahy. An approximate method of evaluating the joint likelihood for ¯rst-order GMRFs. IEEE Trans. on Image Processing, 2(4):520{523, October 1993. 102
[175] J. Zerubia and R. Chellappa. Mean ¯eld annealing using compound Gauss-Markov random ¯elds for edge detection and image estimation. IEEE Trans. on Neural Networks, 4(4):703{709, July 1993. [176] J. Zhang. The mean ¯eld theory in EM procedures for Markov random ¯elds. IEEE Trans. on Signal Processing, 40(10):2570{2583, October 1992. [177] J. Zhang. The mean ¯eld theory in EM procedures for blind Markov random ¯eld image restoration. IEEE Trans. on Image Processing, 2(1):27{40, January 1993. [178] J. Zhang and G. G. Hanauer. The application of mean ¯eld theory to image motion estimation. IEEE Trans. on Image Processing, 4(1):19{33, January 1995. [179] J. Zhang and J. W. Modestino. A model-¯tting approach to cluster validation with application to stochastic model-based image segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 12(10):1009{1017, October 1990. [180] J. Zhang, J. W. Modestino, and D. A. Langan. Maximum-likelihood parameter estimation for unsupervised stochastic model-based image segmentation. IEEE Trans. on Image Processing, 3(4):404{420, July 1994. [181] M. Zhang, R. Haralick, and J. Campbell. Multispectral image context classi¯cation using stochastic relaxation. IEEE Trans. on Systems Man and Cybernetics, vol. 20(1):128{140, February 1990. 103