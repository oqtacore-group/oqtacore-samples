 Тема 08 "Поиск похожих пользователей в социальных сетях".
Реферат
Целью данной работы является создание прототипа рекомендательной системы для поиска интересных людей в социальных сетях. Для функционирования системы требуется обеспечить сбор данных, их обработку, и представление результатов.
В результате выполнения НИОКР был создан робот для сбора данных из  LiveJournal . Его скорость работы пока не является достаточно высокой по ряду причин, однако уже позволяет строить на основе собранных данных сервис. Также было реализовано несколько известных алгоритмов и разработан собственный. Развернута альфа-версия сервиса в режиме ограниченной доступности. Таки образом, прототип сервиса с минимальными возможностями запущен, работу можно считать успешной. Однако существует множество направлений его доработки, которые предполагается выполнить на следующем этапе, с учетом отзывов пользователей.
В настоящее время для анализа используется ряд выборок, наибольшая из которых содержит 11 тысяч пользователей. Результатом поиска является один из пользователей из выборки.
Ключевые слова:  Web Usage Mining ,  Data Minng , анализ данных, социальные сети
Отчет состоит из 6 (шести) страниц.
Основная часть
Основной целью первого этапа было научиться собирать данные из социальной сети и разработать вспомогательный инструментарий, необходимый для анализа собранных данных. 
Было проанализировано несколько социальных сетей (ВКонтакте, МирТесен, Facebook, MySpace, LiveJournal)  и изучены возможности извлечения данных пользователей для отладки алгоритмов. В качестве социальной сети, на которой будет проводиться тестирование, была выбрана социальная сеть-блогхостинг LiveJournal (LJ), так как информация про большую часть пользователей является открытой и легкодоступной. 
На данном этапе развития проекта было решено использовать для анализа информацию о пользователях, их интересы и списки друзей, т.к. задача анализа текстов не входила в план работ на первый год по проекту. Процесс сбора этой информации облегчается благодаря тому, что в социальной сети LiveJournal предусмотрены специальные URL для получения интересов пользователя и его друзей.
Так как при создании прототипа важное значение имеет скорость реализации концепта, то для написания первой версии робота-краулера был выбран язык PHP вследствие простоты реализации веб-приложений на нем и наличии большого количества удобных расширений. В качестве хранилища скачанных данных использовалась база в бесплатной СУБД MySQL, а сам робот был запущен на сервере под OS Fedora Core. 
Опишем методологию сбора данных роботом. При запуске робот получал некоторое количество имен пользователей. На практике в качестве таких стартовых выступали либо заранее известные пользователи (например, автор данного проекта или его друзья), либо 50 произвольных пользователей. Для выбора произвольных пользователей использовался специальный запрос к LJ, так как подобная функциональность предоставляется для роботов движком социальной сети.  После этого робот выкачивал всех друзей заданных пользователей, а затем и их друзей, т.е. пользователей  из второго круга начальных пользователей. По всем этим пользователям выкачивались их интересы. Однако не все эти данные предназначались для анализа. Например, довольно большое число интересов было уникальным, т.е. отмечены только одним пользователем. В коллаборативной фильтрации отношения часто представляют в виде матриц. В нашей задаче можно выделить матрицу интересов (строки - пользователи, столбцы - интересы, элемент равен 1, если пользователь отметил интерес, и 0 в противном случае) и матрицу дружбы (пользователь-пользователь, элемент равен 1, если один из них добавил в друзья другого). Особенностью социальной сети LiveJournal является то, что дружба не взаимна, т.е. если A - друг B, то необязательно B - друг A. Получается, что матрица дружбы не является симметричной. Заметим, что в случае другой социальной сети эта матрица могла бы быть симметричной. Анализ собранных данных показал, что обе матрицы являются высокоразреженными. Для их уплотнения было рещено отбросить часть данных - "обрезать" выборки. Были оставлены только 1000 самых популярных интересов, а из пользователей убраны те, у кого нет интересов или друзей. Таким образом было получено несколько выборок. Дальнейшие эксперименты проводились именно на них.
Другим интересным фактом, обнаруженным при анализе собранных данных,  является то, что нельзя провести четкую границу между более и менее популярными интересами - "хвост" гистограммы числа интересов с заданной популярностью оказался очень "размазанным".
Пилотная версия робота выявила узкие места его архитектуры, которые предполагалось устранить в ходе выполнения работ по следующим этапам вне утвержденного плана работ.
Для работы с выборками была создана первая версия библиотеки разреженных матриц. В качестве хранилища нетривиальных элементов использовался контейнер STL map. Для быстрого передвижения по нетривиальным элементам строки или столбца матрицы вместе со значением элемента хранится указатель на следующий элемент. Такая архитектура матриц показала неплохую производительность при чтении данных, однако время заполнения матрицы с ростом числа элементов оказывается слишком большим. Основное время в этом случае занимает операция выделения памяти для новых элементов. Испытания показали, что требуется дальнейшая доработка матриц, которая и осуществлялась на втором этапе.
В процессе доработки была предпринята попытка ускорить работу, изменив процедуру выделения памяти, т.к. это было узким местом работы. Для этого был разработан собственный менеджер памяти. Однако это не принесло желаемого успеха. После этого были проанализированы все сценарии использования разреженных матриц в проекте. Было выделено несколько различных сценариев и принято решение для каждого сценария использования разработать свою структуру данных, оптимальную именно для этого сценария. Также был найден баг в старой реализации, позволивший ускорить работу на порядок.
С учетом опыта реализации пилотной версии робота и выявленных при этом узких мест его архитектуры, он был почти полностью переписан. В первой версии робота основная задержка была связана с обработкой скачанного контента. Для этого обработка данных после скачивания была перенесена на C++. Значительно была переработана его архитектура. Благодаря этому удалось значительно повысить скорость разбора скачанных страниц. В результате узким звеном робота стало само скачивание. По условиям использования материалов LiveJournal, с одного IP-адреса в минуту можно делать не более 5 запросов. Один из способов решения этой проблемы заключается в подключении через прокси-сервера. Для этого необходимо создать балансировщик запросов, который будет перенаправлять запросы через свободные бесплатные прокси. Другой вариант решения проблемы заключается в создании распределенной системы роботов. В этом случае разные экземпляры роботов можно будет разместить на разных площадках, в том числе, у поклонников проекта. В этом случае необходима доработка робота для того, чтобы он мог работать на клиентских ПК, была возможность задавать ему расписание и удаленно администрировать. У обоих вариантов есть достаточно большое пересечение по внутренней реализации. Сейчас разрабатываются оба варианта, однако ни один  из них окончательно не готов.
На третьем этапе основное внимание уделялось разработке рекомендательных алгоритмов. В качестве опорного алгоритма для сравнения качества рекомендаций был имплементирован исходный рекомендательный алгоритм Amazon (User-Based и Item-Based подходы). В качестве другого опорного алгоритма был использован один из алгоритмов агломеративной кластеризации, адаптированный для рассматриваемой задачи. 
С точки зрения исполнителей проекта, крайне важным является возможность интерпретировать и оценить результаты экспертом предметной области. Поэтому основной упор делался на разработку алгоритмов, относящихся к семейству латентных семантических моделей (Latent Semantic Analysis, Indexing, Models). В этом случае выбирается какая-то параметрическая модель, которая зависит от большого количества скрытых (латентных) переменных, а задача алгоритма - найти эти переменные. В качестве базовой модели был выбран вероятностный подход, т.е. скрытые переменные описывают некоторые вероятности. 
Была построена явная вероятностная модель предметной области. В этой модели каждый объект описывается вектором действительных чисел. Каждый элемент вектора - оценка близости объекта некоторой теме, топику. Такой вектор называется скрытым профилем. Так как интересы пользователя описываются таким же скрытым профилем, то топики получаются интерпретируемыми - для каждого топика можно выделить ключевые слова, интересы, которые находятся ближе всего к нему. Важно, что такое описание строится автоматически. Кроме того, по результатам экспериментов, топики оказались хорошо интерпретируемыми. Приведем ниже часть топиков.
Строка состоит из номера топика и перечисления ключевых слов, характеризующих данный топик. После ключевого слова через тире записана его близость этому топику (ключевые слова могут лежать в нескольких топиках).
откровенность - 0.939754, прикосновения - 0.881453, естественность - 0.697073, нежность - 0.67714, влюблённость - 0.66517
поэзия - 0.645104, философия - 0.592495, проза - 0.548364, литература - 0.535733
биатлон - 1, хоккей - 0.961607, футбол - 0.897914, спорт - 0.854447, зенит - 0.531885
заниматься любовью - 0.694134, эротика - 0.604637, целоваться - 0.588622, флирт - 0.570511, мужчины - 0.509843
nikon - 1, фильмы - 0.722648, писатели - 0.60598, Интернет - 0.576279, шашлык - 0.535255
фантастика - 0.82129, фэнтези - 0.73237, фэнтэзи - 0.507741
антропология - 1, легенды - 0.917523, мифы - 0.87516, куклы - 0.651683
Санкт-Петербург - 0.964368, Питер - 0.828839, Москва - 0.821722, Ленинград - 0.552004
спать - 1, читать - 0.793402, томатный сок - 0.552268, гулять по городу - 0.506864
бизнес - 0.777838, деньги - 0.771308, карьера - 0.721223
манга  - 0.930975,  анимация  - 0.764651, manga - 0.64152, anime - 0.612239, japan - 0.532072
 готика - 0.678163, мистика - 0.591149, драконы - 0.532378, магия - 0.505657
рассвет - 1, закат - 0.96301, ливень - 0.519836
чувства - 1, эмоции - 0.870456, отношения - 0.537811, мысли - 0.52095, радость - 0.514146
графика - 0.930522, Акварель - 0.835214, черно-белое - 0.767079, кинематограф - 0.69454, рисунок - 0.642439
язычество - 0.799194, фолк - 0.729498, славяне - 0.685875, Мельница - 0.512949
Арда - 0.979407, Сильмариллион - 0.947733, Толкин - 0.787062, фехтование - 0.733752, tolkien - 0.699156
Таро - 0.943115, руны - 0.870123
кельты - 0.963644, Муми-тролли - 0.957091, Ирландия - 0.892341, Шотландия - 0.7885, Норвегия - 0.531985
верховая езда - 1, лошади - 0.694111
ведьмы - 0.961627, оборотни - 0.783417, вампиры - 0.775565
хиппи - 0.855658, радуга - 0.740084, фенечки - 0.665093
Алиса - 0.877646, пилот - 0.763177, Чиж - 0.714062, Анархия - 0.653225, ДДТ - 0.603713
fashion - 0.919008, Мода - 0.7344, стиль - 0.568554, одежда - 0.546855
сми - 1, телевидение - 0.958438, журналистика - 0.695149, демократия - 0.671702, новости - 0.670763
Предложенный алгоритм на тестовых данных показал лучшее качество подбора людей и более высокую скорость сходимости по сравнению с алгоритмом [2].
Также для оценки качества была создана специальная страница для аксессоров, позволявшая оценивать близость двух интересов, для которых также вычислялась близость с помощью алгоритма. В работе приняли участие 12 аксессоров, различные по социально-демографическим факторам (студенты младших курсов, молодые выпускники МФТИ и МГУ и люди в возрасте 40-50 лет; с физико-математическим образованием и гуманитарным). Результаты тестирования также показали хорошее качество предложенного алгоритма.
Главной задачей на IV этап являлся запуск сайта для пользователей в тестовую эксплуатацию. Для этого необходимо было разработать непосредственной сайт как средство отображения информации и способ передачи ему результатов подбора. 
Для ускорения разработки было решено создавать сайт на технологии ASP.Net 3.5. Для увеличения потенциала масштабирования в качестве архитектуры была выбрана сервисно-ориентированная архитектура. В этом случае пользователь взаимодействует с веб-страницами. Скрипы на стороне веб-сервера на основе совершенных пользователем действий формируют запрос к рекомендательному сервису, и на основе полученных от него рекомендаций формируют ответ-страничку для пользователя, дополняя рекомендации статической информацией из БД, собранной роботом.
Сам рекомендательный сервис создан на основе технологии WCF (Windows Communication Foundation).  Это облегчает построение коммуникации между рекомендательным движком и клиентами (одним из клиентов является сайт).  Благодаря этому можно легко подключать новые способы передачи данных к клиентам (например, использование бинарного протокола вместо SOAP). Также можно подключать другие клиенты к рекомендательному сервису, например, новые разделы на сайте или продажа услуг по подбору как сервиса. 
Для создания рекомендаций сервис использует написанные на C++ библиотеки, которые выдают рекомендации на основе заранее просчитанных скрытых профилей пользователей и интересов. Благодаря этому с точки зрения сервиса в будущем будет возможен подбор похожих не только на данного пользователя, но и на нескольких. Сейчас эта функциональность до конца не реализована. Кроме того, сервис обращается к своей БД для того, чтобы сопоставить имя сущности (пользователя, интереса) идентификаторам, с которыми работает ядро сервиса - собственно, написанные на C++ алгоритмы.
В настоящее время на сайте реализована возможность подбора интересных пользователей, похожих на заданного пользователя или ключевое слово (интерес) и поиск интересов, похожих на заданный интерес, т.е. семантически близких. Последний сервис, в первую очередь, создан для более удобного взаимодействия с аксессорами, однако не исключен вариант и дальнейшей коммерциализации этого сервиса. 
Сервис подбора пользователей доступен по адресу: http://mycolornet.com/fSearch. В настоящее время он функционирует в формате альфа-версии, в него часто вносятся изменения(которые обычно улучшают сервис, однако иногда их приходится отменять). В связи с этим с главной страницы на альфа-версию ссылка пока не проставлена.
Заключение
В результате выполнения НИОКР был создан робот для сбора данных из  LiveJournal . Его скорость работы пока не является достаточно высокой по ряду причин, однако уже позволяет строить на основе собранных данных сервис. Осуществляется его дальнейшая доработка.
Также было реализовано несколько известных алгоритмов и разработан и внедрен собственный. В дальнейшем основные работы будут вестись именно над рекомендательным алгоритмом. Одна из важнейших задач - улучшение его масштабируемости для обработки большего объема данных. Сейчас этот момент является слабым звеном алгоритма.
Развернута альфа-версия сервиса в режиме ограниченной доступности. 
Таким образом, прототип сервиса с минимальными возможностями запущен, работу можно считать успешной. Однако существует множество направлений его доработки, которые предполагается выполнить на следующем этапе, с учетом отзывов пользователей.
Список литературы.
 Resnick, Iacovou, Suchak, Bergstrom and Riedl. Grouplens: An open architecture for collaborative filtering of netnews.// Proceedings of the ACM - 1994.
Leksin V. A. Symmetrization and overfitting in probabilistic latent semantic analysis // Pattern Recognition and Image Analysis. - 2009. - Vol. 19, No. 4 - P. 565-574.
 Пустовойтов Н.Ю. Поиск схожих пользователей в социальных сетях методами коллаборативной фильтрации. // Труды 52-й научной конференции МФТИ "Современные проблемы фундаментальных и прикладных наук". - 2009. -Часть  VII .  Управление и прикладная математика. Том  2 .  - С.94-96. 
 Мэтью Мак-Дональд, Марио Шпушта.  Microsoft ASP . Net  3.5 с примерами на  C #2008 и  Silverlight  2 для профессионалов. 3-е изд.// М.:ООО "И.Д. Вильямс", 2009.- 1408с.
Дж .  Лёве .  Создание служб  Windows communication Foundation. //  СПб .:  Питер , 2008.-  592с. 
 
