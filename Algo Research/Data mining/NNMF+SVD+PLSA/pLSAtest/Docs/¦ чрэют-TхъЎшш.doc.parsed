 Лекция 1
1. ВВЕДЕНИЕ
В различных областях человеческой деятельности (экономике, финансах, медицине, бизнесе, геологии, химии, и др.) повседневно возникает необходимость решения задач анализа, прогноза и диагностики, выявления скрытых зависимостей и поддержки принятия оптимальных решений. В настоящее время вследствие бурного роста объема информации, развития технологий ее сбора, хранения и организации в базах и хранилищах данных (в том числе Интернет-технологий), точные методы их анализа и моделирования зачастую отстают от потребностей реальной жизни. Здесь требуются универсальные, простые и надежные подходы, пригодные для обработки информации из различных областей, в том числе для решения проблем, которые могут возникнуть в ближайшем будущем. В качестве подобного базиса могут быть использованы технологии и подходы математической теории распознавания и классификации [1, 2, 8]. 
Действительно, данные подходы в качестве исходной информации используют лишь выборки описаний-наблюдений объектов, предметов, ситуаций или процессов (выборки прецедентов), при этом каждое отдельное наблюдение-прецедент записывается в виде вектора числовых значений отдельных его свойств-признаков. Выборки признаковых описаний являются обычно первичными исходными данными, которые повседневно возникают в различных предметных областях, и которые могут быть использованы для решения следующих задач:     
распознавание (классификация, диагностика) ситуаций, явлений, объектов или процессов с обоснованием решений; 
прогнозирование ситуаций, явлений, процессов и состояний по выборкам динамических данных; 
кластерный анализ и исследование структуры данных; 
выявление существенных признаков и минимизация описаний объектов; 
нахождение эмпирических закономерностей различного вида; 
нахождение нестандартных или критических случаев; 
формирование эталонных описаний образов. 
Данные задачи возникают в различных предметных областях. Приведем некоторые примеры подобных приложений: 
обработка данных социологических опросов;  
прогнозирование тенденций изменения макроэкономических показателей; 
анализ финансовых данных и прогноз финансовых показателей; 
оценка экономического состояния предприятий и перспектив их инвестирования; 
проблемы прогнозирования экологических последствий по малым выборкам прецедентов; 
широкий круг задач медицины, связанных с созданием систем поддержки принятия диагностических решений, обработкой медицинской статистики, анализа эффективности лекарств и прогноза последствий лечения;
задачи геологического прогнозирования; 
задачи экспериментальной физики, связанные с анализом накопленного экспериментального материала на этапах выявления качественных взаимосвязей между физическими параметрами и созданием приближенных математических моделей; 
задачи прогнозирования свойств новых органических соединений в химии на основе имеющегося банка исследованных органических соединений; 
обработка и анализ данных в биологии, с целью оптимизации селекционных и генетических исследований; 
обширный круг задач распознавания изображений. 
1. Основные задачи анализа данных, распознавания, классификации и прогноза по прецедентам. 

Исходной информацией являются описания объектов (ситуаций , предметов, явлений или процессов)  S   в виде векторов значений признаков , где признаки  , характеризуют различные стороны-свойства  S .  Одно из "свойств"  y ( S ) объектов  S   (не входящее в состав признаков) считается "основным". Свойство  y ( S ) принимает конечное число значений и для некоторых объектов  считается известным. Предполагается, что существует прямая связь между признаками и основным свойством (неизвестная пользователю). 
Задача распознавания (прогноза, идентификации, "классификации с учителем") по прецедентам состоит в определении значения свойства  y ( S )  объекта  S  по информации  (обучающей или эталонной выборке). Обычно вместо термина "основное свойство объекта" используют термин "класс объекта". Объекты, имеющие равные значения основного свойства считаются принадлежащими одному множеству (образу, классу объектов), и задача распознавания по прецедентам формулируется как задача отнесения объекта к одному из классов. 
Задачу распознавания далее мы будем рассматривать далее как задачу классификации с учителем, и использовать следующую постановку и обозначения. 
1. Пусть некоторое множество объектов является объединением конечного числа непересекающихся подмножеств, именуемых классами:   (, . Данное разбиение известно лишь частично в виде выборки объектов  из данного множества, содержащей представителей всех классов. Для определенности будем считать, что  . 
2. Описание произвольного объекта  из  задается в виде совокупности из  n  значений признаков :  где  - значение признака  на объекте . Здесь множества  задают область допустимых значений признака. Признак, как некоторое свойство  объекта, может быть произвольной природы (некоторая числовая характеристика, наличие или отсутствие какого-то свойства, изображение, функция, и т.д.). Мы будем рассматривать случаи числовых признаков, а именно: 
а)  - признак бинарный, обозначает отсутствие или наличие какого-либо свойства; 
б)   - признак  k  - значный, выражает степень выраженности некоторого свойства с конечным числом значений; 
в) , где числа, либо символы . 
Числовые признаки являются наиболее простыми и распространенными. Признаки номинальные (при сравнении которых нельзя использовать отношения "больше", "меньше", например "цвет", "социальное положение", "пол"), порядковые (где существенны или известны лишь отношения <, >, но не сама величина различия между значениями признаков), и другие более "сложные " признаки рассматриваться не будут. На практике, данные признаки сводятся к числовым, или для задач со сложными признаками создаются специальные методы. В качестве подобных примеров можно привести задачи распознавания зрительных и слуховых образов.  Далее, для простоты записи, мы будем отождествлять объект с его описанием: . 
Обучающая выборка будет задаваться таблицей обучения  из  m  строк и  n  столбцов, в которой строками являются признаковые описания объектов, причем первые  объектов из первого класса, следующие  - из второго, и т.д. Т.е. класс  представлен  эталонами, . Строка  таблицы является  признаковым описанием эталонного объекта , а столбец  содержит значения признака  на эталонной выборке. 
	Примерами подобных задач являются: 
-  задачи медицинской диагностики, в которых по совокупности симптомов, данных лабораторных обследований и т.п. требуется поставить диагноз при заданном конечном наборе возможных их вариантов (здесь "основное свойство" есть наличие/отсутствие определенного заболевания); 
- задачи технической диагностики, когда по набору значений косвенных технических параметров, показаниям датчиков и приборов требуется определить наличие или вид неисправности; 
- прогноз эффективности инвестирования предприятия по его финансово-экономическим показателям (здесь "основное свойство" есть оценка эффективности инвестирования, качественная или в баллах); 
- прогноз тенденций в политике, финансах и экономике, выявление и оценивание скрытых факторов; 
- прогноз свойств органических/неорганических химических соединений и сплавов по составляющим компонентам и технологии производства; 
- прогноз урожайности (интервала сбора культуры с единицы площади) сельскохозяйственных культур по описанию их состояния на различных стадиях роста и климатических условий; 
- распознавание изображений, рукописных и других символов, подписей. 
Задача распознавания объекта  S  состоит в определении класса которому принадлежит объект, на основе описания объекта  и таблице обучения . Данная задача обычно решается в два этапа. Сначала по таблице обучения подбирается алгоритм, который наилучшим образом соответствует в каком-либо смысле таблице обучения. Данный этап называют этапом обучения распознаванию. На втором этапе, подобранный алгоритм непосредственно применяется для классификации нового объекта.  
Данная постановка задачи распознавания имеет простую геометрическую интерпретацию. Множеству  M  (соответственно классам) соответствуют область (подобласти)  n -мерного векторного пространства признаковых описаний. Исходная информация об областях представлена в виде отдельных их точек. По данной исходной информации требуется определять принадлежность новых точек к одной из подобластей. 
В практическом распознавании, в качестве допустимых решений, принимаются "отказы от распознавания", когда распознаваемый объект не похож на все предыдущие прецеденты, или когда он похож приблизительно в равной степени на объекты двух и более классов. 
Задача автоматической классификации (классификации без учителя, кластерного анализа, таксономии) состоит в автоматическом разбиении заданной выборки объектов на классы (группировки) так, чтобы по совокупности значений признаков объекты одной группировки были близки друг другу, а объекты разных группировок - далеки. Полученные группировки являются приближенным макроописанием исходной выборки. Для простоты изложения, чтобы не возникало разночтений и путаницы между задачами классификации с учителем (распознавания) и классификации без учителя, для последней далее будут использоваться как правило термины "кластерный анализ", "кластеризация", и вместо терминов "классы" - термин "кластеры". 
	Задача оценки информативности признаков и объектов состоит в вычислении относительного вклада признака (объекта) в процесс распознавания. 
	Задача минимизации признакового пространства состоит в нахождении минимального набора признаков, обеспечивающего незначительное ухудшение качества (точности) распознавания относительно исходного набора признаков. 
	Задача поиска логических закономерностей (логических зависимостей, извлечения знаний,  data mining ) состоит в нахождении таких значений (интервалов значений) признаков, которые свойственны многим объектам одного класса (с одинаковым значением свойства  y ). Это выражается в правилах следующего вида: 
1. "для 80% эталонных объектов  второго класса ( y ( S )=2) выполнены условия:  &  &  & ".
2. "если &&&, то с вероятностью 0.9 выполнено   y ( S )=1 (объект  S  принадлежит первому классу)". 
Существуют и другие функции, параметры, величины, которые могут быть вычислены (хотя бы приближенно) по эталонным выборкам, и которые имеют интерпретацию и практическую ценность для пользователя (логические описания классов, логические корреляции, и др.). 
Лекция 2
2. Алгоритмы распознавания, основанные на  принципе частичной прецедентности 

Принципиальная идея алгоритмов частичной прецедентности состоит в отнесении распознаваемого объекта в тот класс, в котором имеется большее число "информативных" фрагментов эталонов (частичных прецедентов), приблизительно равных соответствующим фрагментам  объекта  S  [1, 2]. Вычисляются близости - "голоса" (равные 1 или 0) распознаваемого объекта к эталонам некоторого класса по определенным различным подмножествам признаков. Данные близости ("голоса") суммируются и нормируются на число эталонов класса. В результате вычисляется нормированное число голосов, или оценка объекта  S  за класс  - эвристическая степень близости объекта  S  к классу . После вычисления оценок объекта за каждый из классов, осуществляется отнесение объекта к одному из классов (т.е. распознавание класса объекта) с помощью решающего правила. Оптимальные значения параметров алгоритмов распознавания (если он содержит некоторые неизвестные параметры), определяются из решения задачи оптимизации данной модели распознавания - находятся такие значения параметров, при которых точность распознавания является максимальной на некоторой заданной контрольной выборке. 

2.1. Тестовый алгоритм распознавания

Тестовый алгоритм является одним из первых представителей широкого класса алгоритмов распознавания, основанных на принципе частичной прецедентности, в которых сравнение распознаваемого объекта с эталонными осуществляется по различным "информативным" и "сложно" вычисляемым подмножествам признаков. В качестве подобных подсистем признаков используются тупиковые тесты и их аналоги [1,2,4]. 
Для случая признаков целочисленных ( k  -значных)  известны понятия теста и тупикового теста [21]. Определение. Подмножество столбцов  таблицы эталонов  называется тестом, если любые две строки подтаблицы, образованной данными столбцами, различны при условии их принадлежности разным классам. Тупиковым называется тест, любое собственное подмножество которого не является тестом. 
Для вещественнозначных таблиц  легко вводится аналог тестам таблиц конечнозначных, если строки подтаблиц считать различными при их различии с точностью до параметров . 
Тупиковый тест - минимальная подсистема признаков, разделяющая эталоны разных классов. 
Пример 1.  Таблица : 

Здесь {1,2,3,4}, {2,3,4} - тесты; {1,2} - не является тестом. Таблица имеет два тупиковых теста: {2,3,4},{1,2,3}.  
 Пример  2 .  Таблица :  
1	2	3	4	5	6		0	1	1	1	0	0		1	0	0	1	1	0		0	1	1	0	0	0		0	0	1	1	0	0		
 Здесь 6-й столбец не входит ни в один тупиковый тест. Столбец 4-й входит во все тесты, поскольку после его удаления будут равны строки разных классов (1-я и 4-я). 
Рассмотрим кратко вопрос нахождения тупиковых тестов. Пусть  , где . Таблице  ставится в соответствие матрица сравнения , где , , . 
Пример 3. 
0
1
1
0

1
0
1
1

0
0
0
0

0
1
1
1

0
0
0
1



Таблица обучения 	0
1
1
0

0
0
0
1

0
1
1
1

1
0
1
1

1
1
0
0

1
0
1
0

                   
Матрица сравнения 		Столбцы  образуют покрытие строк матрицы , если  : . 
Покрытие называется тупиковым, если произвольное его собственное подмножество не является покрытием.  Нетрудно убедиться, что наборы {1,2,4}, {1,3,4}, {2,3,4} образуют все тупиковые покрытия матрицы сравнения из примера 3. Каждому тупиковому тесту соответствует тупиковое покрытие строк матрицы сравнения и наоборот. 
Тупиковый тест, состоящий из минимального числа столбцов, называется минимальным тупиковым тестом. Задача поиска минимального тупикового теста может быть сформулирована как задача целочисленного линейного программирования. 
Рассмотрим следующую оптимизационную задачу:  
, 
							(1)
 
Легко видеть, что единичные компоненты решения данной задачи определяют минимальный тест. Если понимать под локальным минимумом данной задачи любой допустимый бинарный набор , в котором любая замена некоторой единицы на ноль делает его недопустимым, то множество локально-оптимальных решений задачи определяет множество тупиковых тестов.   
Тестовый алгоритм распознавания определяется следующим образом.  
Пусть заданы значения неотрицательных числовых параметров  и для заданной таблицы обучения  найдено множество  всех тупиковых тестов. Пусть  - некоторый тупиковый тест таблицы  , заданный соответствующими номерами ее столбцов. Для простоты обозначений тупиковые тесты будем записывать как множество номеров образующих тест столбцов. 
Определим функцию близости между частями описаний некоторого эталонного объекта  и  S  , соответствующим данному тупиковому тесту: 
=				(2)
Назовем оценкой объекта  S   за класс  ("мерой близости" к классу) следующую величину: 
 . 				(3)
Таким образом, близость объекта к классу определяется как нормированная на число эталонов класса сумма близостей объекта ко всем эталонам данного класса по всем тупиковым тестам. Пусть вычислены оценки объекта  S   за каждый из классов. Объект  S    считается принадлежащим классу , если . В случае наличия двух или более равных максимальных оценок, алгоритм отказывается от классификации  S . 

ЛЕКЦИЯ 3 
(тестовый алгоритм, продолжение)
Существуют эффективные алгоритмы поиска тупиковых тестов [17,18]. Тем не менее, задача нахождения множества всех тупиковых тестов является вычислительно сложной комбинаторной задачей и не может быть решена на современных компьютерах даже для относительно небольших таблиц обучения (сотни объектов и признаков). Поэтому при решении практических задач вычисляют и используют в процедурах голосования обычно лишь часть тупиковых тестов [17,18] . 
В Системе РАСПОЗНАВАНИЕ реализован стохастический вариант идеи тестового алгоритма. Из таблицы обучения выбираются случайно  N  подтаблиц, каждая из которых состоит из 3 строк таблицы обучения,  N  подтаблиц,  состоящих из 4 строк таблицы обучения, и т.д.,  N  подтаблиц,  состоящих из  k  строк таблицы обучения (здесь  N   и  k  - управляющие параметры программы). Каждая подтаблица не обязана содержать эталоны из каждого класса, т.е. допускаются подтаблицы с числом строк меньшим числа классов. Это не приводит к дальнейшему использованию "плохих" тестов, так как каждому тесту впоследствии сопоставляется вес (качество) уже по полной обучающей выборке. Для каждой подтаблицы находятся все тупиковые тесты либо один минимальный тест  в зависимости от выбранного алгоритма поиска. В последнем случае для таблицы обучения находится не более  N * ( k -2) минимальных тестов случайных подтаблиц. 
Обозначим множество всех найденных тупиковых тестов для подтаблиц как и ранее через . Пусть  M 1 ={,} множество пар строк таблицы обучения, принадлежащих равным классам, а  M 2  - множество пар строк из разных классов. Число элементов множеств  M 1 и  M 2 обозначим, соответственно, через  n 1 и  n 2. Антиблизостью объектов по опорному множеству   назовем бинарную величину 
. 
Определим "вес" опорного множества (в нашем случае теста  T  )   согласно выражению (4)
		(4)
а через  - его удельный вес. Данные величины показывают, как часто бывают близки эталонные объекты одного класса и далеки объекты разных классов по выбранному опорному множеству.  
Окончательно, оценки распознаваемого объекта за классы ,  j =1,2,., l , вычисляются согласно следующей формуле: 
. 
Классификация осуществляется с помощью простейшего решающего правила. 
В случаях практических задач с плохой отделимостью классов тупиковые тесты будут иметь большое число столбцов или могут вообще отсутствовать. Для управления отделимостью классов введен управляющий параметр программы (делитель ( - порогов), позволяющий увеличивать-уменьшать близость объектов. Для небольших по количеству признаков таблиц обучения возможно вычисление всех тупиковых тестов и, соответственно, голосование по всем тупиковым тестам. Для реализации данного варианта в Системе предусмотрена кнопка "переборный алгоритм" . 

2.2. Алгоритмы распознавания, основанные на вычислении оценок

Тестовый алгоритм стал первым широко известным в теории распознавания подходом, основанным на принципе частичной прецедентности. Ю.И.Журавлевым была предложена более общая формализация с различными способами выбора информативных подсистем признаков и формулами для вычисления оценок. Данный класс алгоритмов получил название "алгоритмы вычисления оценок" и включал "тестовый алгоритм" как частный случай. В настоящем разделе будут рассмотрены наиболее используемые алгоритмы, при этом тестовый алгоритм будет как и ранее специально выделенным. 
Алгоритмы вычисления оценок (АВО) определяются как последовательное выполнение шести этапов, для каждого из которых имеются различные пути реализации. Ниже будут приведены лишь некоторые основные способы их выполнения. Подробно данные вопросы рассмотрены в [] и в прилагаемой библиографии. 

2.2.1. Основные определения и этапы алгоритмов вычисления оценок
1. Задание системы опорных множеств алгоритма. Первым шагом определения АВО является задание множества подсистем признаков, по которым осуществляется сравнение объектов. Пусть  - некоторая система подмножеств множества {1,2,., n }, называемая системой опорных множеств алгоритма  A . Элементы (=( называются опорными множествами алгоритма. Они определяют номера признаков, по которым сравниваются части эталонных и распознаваемых объектов. Каждому подмножеству (= можно поставить во взаимно однозначное соответствие булевский вектор , в котором , а остальные компоненты равны нулю. 
Геометрически, множество всех   n -мерных булевских векторов определяет дискретный единичный куб , . 
Теоретические исследования свойств тупиковых тестов для случайных бинарных таблиц показали, что характеристические векторы "почти всех тупиковых тестов" имеют асимптотически (при неограниченном возрастании размерности таблицы обучения) приблизительно одну и ту же длину. Это явилось одним из обоснований выбора в качестве множества  всевозможных подмножеств {1,2,., n } длины  k . Значение  k  находится из решения задачи обучения (оптимизации модели) или задается экспертом. В итоге, широко распространенными подходами к выбору  являются  (наряду с тупиковыми тестами) следующие два: 
а) =; 
 b )  (. 
Второй способ выбора системы опорных множеств, как всевозможных подсистем  {1,2,., n }, является "усреднением" первого и не требует нахождения неизвестных параметров. 
2. Задание функции близости.  Пусть фиксировано некоторое опорное множество ( и соответствующий ему характеристический вектор (. Фрагмент  объекта , соответствующий всем единичным компонентам вектора (, называется (-частью объекта, и обозначается ( S . Под функцией близости  будет пониматься функция от соответствующих (-частей сравниваемых объектов, принимающая значение 1 ("объекты близки") или 0 ("объекты далеки"). Приведем примеры подобных функций. 
а)  =
Здесь  - неотрицательные параметры, именуемые "точности измерения признаков". 
 b ) =
Здесь ( также некоторый неотрицательный параметр алгоритма. 
3. Оценка близости объекта  S  к эталонному объекту  для заданной (-части. Данная числовая величина формируется на основе функции близости и, возможно,  дополнительных параметров. 
 a )   =. 

 b )   =, где  - "вес" опорного множества. 
 c )   =. Здесь  - параметры, характеризующие степень важности объекта  (информативность объекта), а  - веса (информативность) признаков.  
4. Оценка объекта  S  за класс  для заданной (-части. 
 a )   . 
5. Оценка объекта  S  за класс . 
 a )   . 
 b )   , где  - "вес" класса . Например, в статистической теории распознавания аналогами параметров  являются  априорные вероятности классов, которые характеризуют, насколько часто встречаются объекты различных классов . 
6. Решающее правило. 
Решающее правило - правило (алгоритм, оператор), относящее объект по вектору оценок за классы  в один из классов, или вырабатывающее для объекта "отказ от распознавания". Отказ возникает обычно  в случаях, когда оценки объекта малы за все классы (объект является принципиально новым, аналоги которого отсутствуют в обучающей выборке), или он имеет две или более близкие максимальные оценки за различные классы (объект лежит на границе классов). Таким образом, решающее правило  r  вычисляет для распознаваемого объекта  S   булевский вектор , где  означает отнесение объекта в класс ,  - принятие решения: "объект  S  не принадлежит классу ".
Простейшее решающее правило - отнесение объекта в класс, за который он имеет максимальную оценку, и отказ от распознавания в случае двух и более максимальных оценок. 
 
b )  
 c ) 						(5)
Здесь  - параметры алгоритма. В последнем случае наличие двух или более единиц интерпретируется как "объект вероятно принадлежит нескольким классам". Когда бинарный вектор состоит из одних нулей говорят, что данный объект - выброс, он  не похож ни на один из классов, близких его аналогов ранее не наблюдалось.  
Использование решающего правила означает фактически переход из признакового пространства в пространство оценок, в котором в качестве разделяющих классы функций используются гиперплоскости, проходящие через начало координат симметрично относительно новых координатных осей (случай  a ), пары гиперплоскостей (случай  b ), и наборы из  l -1 гиперплоскостей. 

ЛЕКЦИЯ  4
2.2.2. Эффективные формулы вычисления оценок
После последовательной подстановки выражений на этапах 2-5 могут быть получены различные общие формулы для вычисления оценок . Например, выбирая первые примеры реализации различных этапов будет получена следующая общая формула для вычисления оценок объекта  S  за классы  ,  j =1,2,., l . 
. 				(6)
При выборе системы опорных множеств согласно вариантам  a ) или  b ) прямое вычисление оценок (6) представляется весьма трудоемким. Действительно, при вычислении оценок (6) согласно  a ) требуется  вычислений значений функции близости. В действительности необходимость выполнения всех данных вычислений отсутствует, поскольку существуют эффективные комбинаторные формулы вычисления оценок при многих вариантах реализации этапов 2-5 и различных  системах опорных множеств. 

Теорема 1. Пусть в модели вычисления оценок используются варианты  a ) выполнения этапов, тогда справедлива формула
  ,  
где   . 
Доказательство. 
Для доказательства достаточно показать, что 
. 
Действительно, данная сумма является общим числом "совпадений" фрагментов  и  S   длины  k . Но данное число и равно . 
Следствие.  Пусть в модели вычисления оценок используются варианты  a ) выполнения этапов 2-5 и  b ) - первого, тогда справедлива формула
  , 
где   . 
Действительно, =   =   =  . 
Теорема 2. Пусть в модели вычисления оценок используются варианты  a ) выполнения этапов 1,2,4,5, и  c ) этапа 3, тогда справедлива формула
  ,
где  , . 
 Доказательство. 
По определению, 
 = 
= . 
Действительно, из определения функции близости следует, что в сумму могут войти лишь те веса признаков, по которым объекты не различимы с точностью до соответствующего порога. Коэффициенты при данных для фиксированного  будут равны в силу симметрии. Но тогда  будет равно числу способов выбора из оставшихся -1  признаков по  k -1 признаку, т.е. . Теорема доказана. 
В теории алгоритмов вычисления оценок существуют и другие эффективные формулы вычисления оценок для более сложно определенных способов их вычисления. 

2.3. Алгоритмы голосования по представительным наборам
Другими алгоритмами данного вида являются алгоритмы типа "Кора" /16,12/, в которых опорные множества связаны со значениями признаков конкретных объектов. 
Пусть . Набор  называется представительным  набором (или просто набором) для класса , если для любого  система неравенств  несовместна. Представительный набор называется тупиковым, если любой его собственный поднабор не является представительным набором, т.е. для любого его поднабора существует равный ему поднабор в каком-либо другом классе. 
Пусть множество тупиковых представительных наборов для класса . Оценку объекта  по заданному тупиковому представительному набору  u  вычисляем по одной из формул: 
 a ) , где  (оценка совпадает с близостью набора и соответствующей ( - части объекта ); 
 b )  (близость умножается на вес набора); 
 c )  (вес близости оценивается как сумма весов соответствующих признаков). 
Тогда   . В дальнейшем для краткости прилагательное "тупиковый" будем убирать. 
Отметим, что для нахождения множества представительных наборов класса необходимо объединить множества представительных объектов, связанных с отдельными эталонами. Для нахождения же представительных наборов, связанных с фиксированным эталоном, достаточно найти тупиковые тесты таблицы, состоящей из выделенного эталона  и эталонов дополнения  данного класса. 
Лекция № 5
2.4. Оптимизация моделей распознавания
Описания всех трех приведенных подходов включают неизвестные числовые параметры. Фактически, каждый из подходов представляет параметрическое множество алгоритмов распознавания, где конкретный алгоритм задается фиксацией значений параметров. 
Так в описание тестового алгоритма при голосовании по тупиковым тестам могут быть введены параметры, характеризующие веса признаков и эталонов. Пусть выбрано также общее решающее правило  c ). Тогда модель тестовых алгоритмов  содержит множество алгоритмов 
. 
Общее число параметров модели равно . Аналогично, модель  вычисления оценок, в которой систему опорных множеств составляют всевозможные подмножества из  k  признаков, является множеством алгоритмов 
. 
Модель голосования по представительным наборам , при вычислении оценок объекта по представительному набору согласно  c ),  является параметрическим множеством алгоритмов 
. 
Стандартная постановка задачи поиска наилучших алгоритмов заданной модели состоит в следующем. 
Пусть дано параметрическое множество распознающих алгоритмов  и на нем определен числовой функционал  качества алгоритм. Требуется найти такой алгоритм , который доставляет экстремум функционалу: . 
Обычно проблема оптимизации решается следующим образом. 
Пусть задана таблица контрольных объектов , аналогичная таблице обучения, т.е. состоящая из разбитых на  l  классов  m  числовых строк - описаний объектов  с помощью  n  признаков. Для определенности считаем, что  
. 
Пусть  Обозначим также 
Определение. Стандартным функционалом качества распознавания называется функционал . 
Мы будем обычно использовать не данный функционал (доля ошибок), а ему обратную величину - долю правильных ответов. 
Пусть используется общее решающее правило  c ), когда условием правильной классификации является выполнение системы из  l  неравенств. Тогда условием правильного распознавания некоторого объекта  является выполнение следующей системы неравенств: 
					(7)
Пусть  Z  - система, состоящая из  m   подсистем (7) относительно переменных . Тогда задача оптимизации стандартного функционала качества может быть сформулирована в терминах систем неравенств следующим образом. 
Найти совместную подсистему системы  Z  при условии , содержащую максимальное  число систем (7), и некоторое его допустимое решение. 

2.5. Оценка информативности признаков и эталонов 
Алгоритмы вычисления оценок являются удобным инструментом для решения задач оценки информативности (важности) признаков и эталонов. Под данными характеристиками понимаются числовые оценки того, насколько высок вклад признака/эталона в процессе распознавания. 
Пусть заданы обучающая и контрольная выборки и некоторый алгоритм. Обозначим через  значение функционала качества распознавания алгоритмом, через  - значение функционала качества алгоритма, построенного по данным обучения без учета  i -го признака, а через  -  значение функционала качества алгоритма, построенного по данным обучения без  j -го эталона. 
Определение. Мерой информативности (весом) признака называется величина . Мерой представительности (весом) объекта называется величина . 
Данный подход естественен, нагляден и пригоден при использовании любого алгоритма распознавания. 
При малых выборках данные величины принимают небольшой набор значений и могут быть грубыми оценками. В данном случае можно использовать другие, эвристические величины, характеризующие факт влияния признака (объекта). 
Пусть  A  - некоторый алгоритм типа вычисления оценок,  - оценка контрольного объекта  за класс ,  - та же оценка без учета признака ,  оценка контрольного объекта  за класс  без учета эталона . 
Определение. Мерой информативности (весом) признака называется величина . Мерой представительности (весом) объекта называется величина .  
Данное определение подразумевает, что при удалении признака (эталона) теряется часть сходства объектов к своим классам, и эти потери тем больше, чем более важен данный признак (соответственно объект). 

Лекция № 6

3. Логические закономерности классов. 
В настоящем разделе будут рассмотрены методы поиска закономерностей  вида "если  то ". Здесь  A 1 ,  A 2 ,.,  Ak   - одноместные предикаты "простейшего вида", зависящие от одного какого-либо признака. Предполагается, что условия данного вида выполняются если не на всех объектах обучающей выборки из некоторого класса, то по крайней мере на многих эталонах класса. Отметим, что к данной постановке сводятся задачи выявления логических связей между набором некоторых независимых величин (признаков) и зависимой. Если зависимая величина является дискретной, то разбиение на классы задают ее значения, если непрерывной (или с высоким уровнем значности) - интервалами значений. 
Представительные наборы  классов (и их ( - окрестности) также могут рассматриваться как закономерности классов. Здесь представляет интерес к расширению определению понятия представительных наборов путем введения критериев из качества и создания процедур нахождения оптимальных наборов. Второй принципиальный вопрос состоит в выборе ( - порогов, причем, естественно, данные пороги должны быть индивидуальными для каждого набора. В настоящем разделе будет рассмотрен вопрос обобщения представительных наборов - поиск оптимальных признаковых подпространств и окрестностей значений признаков будут осуществляться в рамках решения различных оптимизационных задач. 
Рассмотрим следующее множество элементарных предикатов, параметрически зависящих от числовых неизвестных : 

Пусть . 
Определение. Предикат  	(1)
называется логической закономерностью класса  K(  , если 
 ( St ( K( :    
 (  St ( K( ,     
 , где ( - критерий качества предиката. 
В 3. поиск экстремума проводится по множеству всевозможных предикатов вида (1). 
Предикат (1), удовлетворяющий только первым двум ограничениям, называется допустимым предикатом рассматриваемого класса. 
Предикат (1), удовлетворяющий только первому и третьему ограничениям, называется частичной логической закономерностью класса  K( . 
















Рис.1 Геометрическое представление логической закономерности класса  K(   с опорным эталоном  St 
Стандартным критерием качества предиката класса  K(  будем называть следующий критерий: .  
Логические закономерности со стандартным критерием качества имеют простую геометрическую интерпретацию. По данным обучающей выборки требуется найти прямоугольный гиперпараллелипипед, лежащий в некотором признаковом подпространстве, содержащий максимальное число эталонов из класса  K(   и только класса  K(  .  

2.6.1.  Нахождение оптимальных логических закономерностей (поиск оптимальных окрестностей представительных наборов ).
 
Задача поиска логических закономерностей классов для стандартного критерия качества предикатов является сложной задачей нелинейной дискретной оптимизации. Создание методов поиска 
Для создания эффективных методов их поиска рассмотрим "более слабый" вариант, основанный на следующих двух ограничениях. 
Множество логических закономерностей "базируется" на эталонах. На геометрическом языке, в центре каждого искомого гиперпараллелипипеда находится один из эталонов класса. 
Вместо стандартного функционала качества предикатов рассматривается некоторый линейный по признакам эвристический функционал. 
Пусть фиксирован произвольный эталон  St ( K(  . 
Будем искать предикаты вида
, 
где , 
				(2)
Отметим, что  Pt ( St )=1, т.е. первое условие определения логических закономерностей класса  K(  выполнено. Объект  St   будем называть опорным для предикатов (2). 
В качестве критерия оптимальности предиката возьмем функционал , 
	(3) 
где антиблизость 
Здесь  означают число эталонных объектов принадлежащих и не принадлежащих отмеченному классу соответственно. 

Лекция № 7 
Вычислим значения ,  i =1,2,., m , и упорядочим все различные их значения по возрастанию в виде следующих последовательностей 
 rj  :   при  v < w . 				(4)
Очевидно, 
Подпоследовательность  назовем подпоследовательностью первого типа последовательности (4), если 
а) для любого ее элемента не существует строк 
 Si ( CK( : , 
б) любое возможное расширение подпоследовательности дополнением элементов слева или справа нарушает свойство  a ). 
Подпоследовательность  назовем подпоследовательностью  второго типа последовательности (4), если 
а) для любого ее элемента не существует строк 
 Si ( K( :, , 
б) любое возможное расширение подпоследовательности дополнением элементов слева или справа нарушает свойство  a ). 
Докажем, что область  D ={ (j( 0,  j =1,2,. n } допустимых значений параметров ( можно ограничить некоторым конечным множеством  D *( D  таким, что .					(5)
Очевидно, что условие (5) будет выполнено, если в качестве  D * взять множество  D '={ ( =( ( 1, ( 2,., (n ):  (i( ri  ,  i =1,2,. n }. 
Покажем, что множество  D ' может быть далее существенно сокращено с помощью трех алгоритмов сокращения последовательностей  (4) (или их подпоследовательностей, которые будем обозначать так же).  

Первый алгоритм сокращения. 
В последовательности  rj  выделяется подпоследовательность первого типа  Из  rj  вычеркиваются все элементы данной подпоследовательности кроме  Данная процедура повторяется для всех подпоследовательностей первого типа  последовательностей  rj . 

2. Второй алгоритм сокращения. 
В последовательности  rj  выделяется подпоследовательность второго типа  Из  rj  вычеркиваются все элементы данной подпоследовательности. Данная процедура повторяется для всех подпоследовательностей второго типа  последовательностей  rj . 

3. Пусть дана произвольная числовая последовательность С:    c 1 , c 2 ,., ck  и функция действительного аргумента (( x ) ( 0. Последовательности  С соответствует числовая последовательность ( ( c 1), ( ( c 2),., ( ( ck ), которую будем называть функциональной.  
Третий алгоритм сокращения произвольной конечной числовой последовательности С состоит в выделении такой ее подпоследовательности   , 									(6)
которой соответствует специальная строго убывающая подпоследовательность функциональной последовательности 
( ( c 1), ( ( c 2),., ( ( ck ). 

Третий алгоритм сокращения. 
Полагаем  v = w =1,  i 1=1. 
Шаг w ( 1. Пусть из  c 1 , c 2 ,., cw   выделена подпоследовательность   . Если  w = k  подпоследовательность (6) считается построенной и переход на этап 3. 
В противном случае w увеличивается на единицу.  
Если , тогда  v  увеличивается на единицу,  iv = w  и осуществляется переход на начало этапа 2. 
Конец алгоритма.

Пусть   - произвольный допустимый предикат такой, что  (j( rj  , , где  rj  - последовательность (4) или ее подпоследовательность, а  - подпоследовательность первого типа последовательности  rj . 
Рассмотрим предикат , для которого 
Ясно, что если предикат  - допустимый, то будет допустимым также и предикат . 
Из условий  для  Si ( K(   и  для  Si  (  K(   следует, что . 
Рассмотрим теперь случай, когда  - подпоследовательность второго типа последовательности  rj . 
Рассмотрим предикат , для которого 
Тогда, если предикат  - допустимый, то будет допустимым также и предикат  в силу неравенства е j < (j  . Кроме того выполнено . 
Таким образом, после последовательного применения первого и второго алгоритмов сокращения к последовательностям  rj  ,  j =1,2,. n , вычисляется такое подмножество  D '' множества  D ' , минимум на котором основного функционала совпадает с минимумом на  D '. Обозначим сокращенные последовательности снова как  rj  ,  D ''= r 1  ( r 2  (  .  ( rn  . Применим к последовательностям  rj  ,  j =1,2,. n , третий алгоритм сокращения, рассматривая в качестве соответствующих функциональных последовательностей последовательности . 
  В итоге получим множество  D *= ( 1  ( ( 2  (  .  ( (n, 
 где  (j={(ji ,i=1,2,.,kj },(ju <(jv , при  u<v. 
 В силу свойств третьего алгоритма сокращения для любого допустимого предиката ,  (j( rj , существует  e =( e 1 , e 2 ,., en ),  ej = (jv ( j )( (j  , для которого , т.е. ( допустимый предикат , для которого . 
После явного описания процедуры нахождения множества  D * покажем, что задача поиска логических закономерностей может быть сформулирована в виде специальной задачи целочисленного линейного программирования. 
Введем следующие обозначения: 
     , 
, 
,							(7)
	  .			(8)
По определению коэффициентов и в силу свойств третьего алгоритма сокращения для задачи (7-8) имеют место следующие свойства коэффициентов: 
   при    	(9)
В силу свойств коэффициентов (9) задачу (7-8) будем называть задачей целочисленного линейного программирования с блочно-монотонными столбцами (задача БМС-ЦЛП). 
Пример задачи (7-8).  

 i=1	 			 i=2 				 i=n 							0.5	0.4	0.3	0.1	0.8	0.6	0.3	.	0.9	0.8	0.6	0.4	0.2	 ( 		1	1	1	0	1	1	1	.	1	1	1	1	0			1	0	0	0	1	1	1	.	1	1	1	1	1			1	1	0	0	1	0	0	.	0	0	0	0	0			1	1	0	0	1	1	0	.	1	0	0	0	0	 ( 		1	0	0	0	0	0	0	.	0	0	0	0	0			1	0	0	0	1	0	0	.	1	1	1	0	0			1	1	1	1	0	0	0	.	0	0	0	0	0			.	.	.	.	.	.	.	.	.	.	.	.	.			1	1	1	0	1	1	0	.	1	1	0	0	0			В силу свойств коэффициентов функции (7) оптимальное решение  содержит не более одной единичной компоненты для каждой из  n  групп параметров . 
Примечание. В случае  это проверяется просто. При  , по построению,  ki  =1 и данное свойство также выполняется. 
Множество единичных компонент однозначно определяет предикат . Действительно, множество ( признаков предиката определяется первыми индексами единичных значений компонент  y * ij  , а соответствующие значения  ( -порогов задаются вторыми индексами ( (i  = (ij ). Найденные предикаты естественно называть оптимальными окрестностями представительных наборов, поскольку алгоритм находит фрагменты эталонных описаний и их окрестности.  


Лекция № 8 (27 октября)

3. Распознавание на основе логических закономерностей. 

Пусть имеется некоторый алгоритм поиска локально-оптимальных решений задачи (7-8). Найденному множеству локально-оптимальных решений задачи (7-8) соответствует некоторое множество логических закономерностей {}, связанных с объектом S t   класса  Kj  и функционалом  f . Обозначим через  множество логических закономерностей класса  Kj  как объединение множеств предикатов {}, найденных для всех эталонов   класса  Kj .  
Общая схема, которая используется в алгоритмах распознавания, основанных на голосовании по системам логических закономерностей, включает три последовательных этапа и является частным случаем общей последовательности выполнения этапов в моделях вычисления оценок. 
1. Для каждого класса Kj по обучающей информации I0 согласно описанному ранее алгоритму находится множество логических закономерностей , из которого вычеркиваются предикаты с малыми значениями стандартного функционала качества.  
2. Для произвольного распознаваемого объекта S вычисляется "мера близости" Gj=(( t   объекта S к классу Kj, где суммирование проводится по всем  (. Здесь Gj является "взвешенной суммой голосов" за класс Kj , или, следуя терминологии [2], оценкой S за класс Kj. Нормировочные коэффициенты (i могут вычисляться различными способами и задают тип процедуры голосования.
3. По значениям Gj вычисляется информационный вектор ((A1(S), (A2(S), .,  (A l (S)). 
 Например, (Ar(I(S))=1 , если Gr=max{Gj, j=1,2,...,l}. В противном случае (Ar(I(S))=0. Строка (A(S) = ((A1(S), (A2(S),..., (Al(S)), содержащая ровно одну единицу, означает однозначное решение задачи распознавания - отнесение алгоритмом распознавания A объекта S в один из  l  классов. Строка (A(S), содержащая несколько единиц, означает многозначное решение задачи распознавание. В данной ситуации алгоритм распознавания указывает несколько классов, которым может принадлежать объект S. Строка (A(S), содержащая одни нули, интерпретируется как отсутствие классов, на которые похож распознаваемый объект. 

Нахождение логических закономерностей при обучающих выборках большой длины. 
 Изложенный в п.2 подход для построения множества логических закономерностей предполагает нахождение предикатов вида (2) для каждого эталонного объекта, т.е. решение задач БМС-ЦЛП (7-8) для каждого эталона. Решение большого числа задач БМС-ЦЛП приводит к существенным затратам процессорного времени для поиска предикатов и памяти для хранения предикатов, времени распознавания новых объектов. Ясно, что найденные множества логических закономерностей будут содержать много похожих предикатов (2), а само их число будет "избыточным". Таким образом представляет интерес вычисление множеств логических закономерностей различной мощности. 
Естественным подходом для решения данной задачи является реализация следующей общей схемы. 
Пусть задан критерий (( S j) оценки важности (представительности) произвольного объекта  S j обучающей выборки,  I ={ S 1,  S 2,.,  S m }, ((1 - некоторое фиксированное число, 
Рассмотрим следующий алгоритм вычисления множеств ,  j =1,2,. l . 
Полагаем (,  j =1,2,., l ,  v =1.  
Пусть - эталонный объект, , для которого выполнено . Решается задача БМС-ЦЛП (7-8) относительно  и находится множество логических закономерностей , для которых  является опорным. Множество  дополняется предикатами из . 
Для каждого  i ( I   величина  увеличивается на число предикатов из , принимающих единичные значения на объекте   Si  . 	 
Из  I  исключается и все  Si , для которых ((. 
Если  I =(, множества ,  j =1,2,. l , считаются построенными и алгоритм заканчивает свою работу. 
В противном случае значение  v  увеличивается на единицу и осуществляется переход на второй этап. 
При выборе (=1 множество  содержит множество предикатов (2), причем для каждого эталонного объекта из  Kj  имеется хотя бы один предикат, принимающий на данном объекте значение 1. При выборе (= m  множество  является, как правило, объединением всех предикатов (2), вычисленных для всех задач БМС-ЦЛП с использованием всех эталонов  Kj   в качестве опорных. 
Рассмотрим теперь вопрос выбора функции (( Sp ), оценивающей "представительность" объекта  S p. 
Здесь можно использовать различные подходы, например, следующий. Выбирается некоторая метрика (полуметрика)  в пространстве признаковых описаний и при некотором фиксированном значении  k   представительность (( S j) объекта  определяется как число объектов данного класса из ближайших  k  соседей , т.е. если  - множество ближайших  k  соседей к  Sj , то (( S j)=. 

5. Оптимизация логических закономерностей. 
Пусть по данным обучающей выборки с использованием эвристического функционала (3) вычислены некоторые множества логических закономерностей  по всем классам  Kj  ,  j =1,2,., l . 
Полученные множества "элементарных знаний" содержат большое число элементов, которые используются как в процедурах голосования при классификации новых объектов, так имеют и самостоятельную ценность для понимания и описания классов. Независимо от метода их поиска наиболее естественным критерием их оценки является (понятный и наглядный) стандартный функционал качества предикатов. 
В самом начале сложная задача поиска логических закономерностей (1) со стандартным критерием их оценки была заменена более простой. Множества "элементарных знаний"  каждого  класса вычислялись при двух существенных ограничениях: поиск логических закономерностей осуществляется лишь на множестве предикатов c опорными эталонами и с использованием эвристического функционала качества. При этом, конкретный используемый метод решения задачи БМС-ЦЛП не гарантирует вычисление всех локально-оптимальных экстремумов (тем более, на длинных выборках) или нахождение глобального экстремума. В итоге, найденные множества логических закономерностей содержат близкие или даже равные элементы. С другой стороны, существуют логические закономерности, имеющие более высокие значения стандартного критерия качества, но данные "элементарные знания" найти не удалось в силу ограниченности процедур поиска, использования более простых функционалов и множества предикатов частного вида. Таким образом, возникает вопрос о поиске множеств  логических закономерностей с более высокими значениями стандартного функционала качества чем у найденных из , используя  множества  в качестве начальных приближений и естественные гипотезы о связи "близких" знаний. Если данная задача решается положительно, то "элементарные знания" из  могут рассматриваться как обобщения множеств "элементарных знаний" .
Пусть Pt(x)(. 
Обозначим через  - оценочный вектор предиката Pt(x), где . 
Предикат Pt(x) из множества  назовем  (  - допустимым, если   ,    Далее предполагаем, что множества  содержат лишь  ( -допустимые предикаты, а ( для краткости будем опускать. 
Пусть P1(x), P2( x ) - пара допустимых предикатов из , 
  ,   , где 
 ( ,  ( ,  ( . 
Допустимый предикат  назовем расширением предиката P1( x ) по предикату P2(x), если , . Допустимый предикат    назовем расширением предиката   , если , . Очевидно, расширение P2(S) предиката P1(S)  является расширением P1(S) по P1(S). В случаях, когда , , переменная x j   становится фиктивной. Множество предикатов  назовем расширением множества предикатов , если оно состоит из всех расширений предикатов множества  по предикатам этого же множества. 
Допустимый предикат  P ( x ) назовем максимальным по множеству , если не существует его расширений ни по одному предикату из . 



Рис.2.  ABCD  и  EFGH  соответствуют элементарным предикатам.  AKMD  и  GHBK   их максимальные допустимые обобщения при  ( =0. Обобщение  AKGL  не является допустимым.

В результате построения последовательных расширений ,,.,,., будут построены множества , состоящие только из максимальных расширений. Конечность настоящего процесса очевидна. В случае  ( =0 по исходным множествам логических закономерностей классов будут построены множества логических закономерностей классов, обладающие максимально возможными значениями  стандартного функционала качества среди всевозможных допустимых расширений начальных . 
Построенные множества максимальных предикатов  обладают рядом благоприятных свойств, которые делают их более предпочтительными относительно исходных . Сам процесс построения по множествам  (исходным "элементарным знаниям") множеств  естественно называть оптимизацией исходных элементарных знаний. Максимальные предикаты из  имеют более высокие показатели для  чем их "предки", причем значения ,  i(j , не превосходят установленных ограничений сверху. 


ЛЕКЦИЯ № 9

6. Нахождение логических закономерностей, оптимальных по стандартному критерию качества
Вернемся к общей задаче поиска логических закономерностей. Пусть задано множество элементарных предикатов, параметрически зависящих от числовых неизвестных : 

и  . Ранее было введено общее определение логической закономерности.
Определение. Предикат  	(1)
называется логической закономерностью класса  K(  , если 
 ( St ( K( :    
 (  St ( K( ,     
 , где .  
В настоящем разделе будет рассмотрен метод прямого поиска логических закономерностей классов. 
Пусть   ,    - некоторые варианты выбора возможных значений левых и правых границ предикатов (1), соответственно,   . Для простоты записи, мы рассмотрим случай, когда значения   u  и  v  равны для всех признаков  i  .
 Построим числовую матрицу   ,   , 
  ,    ,   ,
   ,   ,  ,
  ,    ,   ,
  ,    ,   ,  where 
         			(2)
        			(3)
Как следует из (2), (3), выполняются следующие ограничения для элементов   : 
  ,    ,							(4)
  ,    . 							(5)
Таким образом, матрица    имеет структуру, как показано в Таблице 1. 
 i=1	i=2	i=3	.	i=n		0000011	1111111	0111111	1111111	1111111	0000001	.	0000001	1111111		 0011111	1111111	1111111	0001111	0000001	1111111	 . 	0011111	1111111	 	 1111111	0001111	0000001	1111111	0001111	1111111	 .	 1111111	0000111	 	 1111111	0000011	1111111	0000011	0011111	1111111	 .	 1111111	0011111	 	 .	.	.	.	.	.	.	.	.	 	 1110000	0000000	0000000	1111100	1100000	0000000	 .	 0000000	1000000	 	 1000000	0000000	1110000	0000000	1110000	0000000	 .	 1000000	0000000	 	 1111000	0000000	1000000	0000000	1111000	0000000	 .	 1100000	0000000	 	 0000000	1100000	0000000	1100000	0000000	1100000	 . 	0000000	1111000	 	 .	.	.	.	.	.	.	.	.	 	 Таблица 1.Иллюстрация структуры матрицы коэффициентов Проблемы  Z 
Множество всех предикатов (1) с возможными границами    находится во взаимнооднозначном соответствии с множеством векторов {  },
  =   при ограничениях   i =1,2,., n . 
В виду данного соответствия, мы будем использовать также запись  ( (  )  для стандартного критерия оптимальности. 
Единицы в {  } соответствуют выбору значений параметров . 
Для выполнения условия 2) должны выполняться неравенства (6). 
  	(6)
Наконец, стандартный критерий оптимальности предиката  ( (  ) равен числу выполненных равенств системе (7). 
   	(7)
Таким образом, проблема поиска оптимального предиката (1) может быть сформулирована как специальная дискретная оптимизационная задача.:
Задача  Z : 
 ( (  )=<число выполненных уравнений в (7)> →  max , 
при ограничениях (8-9)
  . 				(8)
  i =1,2,., n ,				(9)

Поставим в соответствие задаче (7-9)  аналогичную задачу (10-13) относительно вещественных переменных. 
Задача  ZC : 		
<число выполненных неравенств в (10)> →  max , 			
при ограничениях (11-13)
  . 			(10)
  . 				(11)
  ,  i =1,2,., n ,	 j =1,2,., u ,						(12)
  ,  i =1,2,., n ,	 j =1,2,., v .						(13)
Пусть   , где    является некоторым решением задачи (10-13). 
Пусть номер признака  i =1,2,., n  фиксирован, и   ,   . Пусть          После выполнения аналогичных операций для  i =1,2,., n , будет определен вектор   . 
Теорема. Вектор   является решением Задачи  Z . 
Доказательство.
 ( (  )= ( (  ),  а вектор    удовлетворяет ограничениям (9) по построению. Покажем справедливость (11) для   . 
При любом  i  ,   =0 при  j < p .  Действительно, если    >0, тогда    =0 и , что противоречит условию . Аналогично,   =0 при  j < r . 
Тогда    ( p , r  зависят от  i ). 
В силу (5),   ,  , т.ч. . 
Окончательно,   , т.е. (11) выполнено. 
Данная теорема дает основу для создания алгоритма поиска предикатов (1) для каждого класса:  
Вычисление опорного объекта .  
 Вычисление множеств   . 
 Решение Проблемы  ZC , и нахождение решения { Q ,   } проблемы  Z . 
Замечания . 
 Для нахождения множества логических закономерностей некоторого класса формируется множество его опорных эталонов. Находятся логические закономерности, соответствующие локально-оптимальным решениям Проблемы  Z , связанной с конкретным опорным элементом.  Объединение всех полученных логических закономерностей образует конечное множество логических закономерностей класса. Для выбора опорных эталонов можно использовать алгоритм  [4]. 
 Возможно вычисление "полных множеств"   , которые включают значения параметров оптимальных предикатов (1). Доказательство аналогично рассмотренному в [5] для некоторого эвристического критерия оптимальности логических закономерностей. В случае практически больших размерностей множеств   , некоторые приближения могут быть использованы для сокращения размерностей. 
Для нахождения максимальной совместной подсистемы системы линейных неравенств был успешно использован итеративный практический алгоритм [6]. 

6. Обобщение знаний по выборкам прецедентов

В настоящем разделе будет рассмотрена задача обобщения знаний (множества логических закономерностей), под которой понимается нахождение и вычисление обобщенных характеристик признаков и классов на основе ранее полученных множеств логических закономерностей классов. 
Определение. Логическим описанием класса Kj назовем логическую сумму 
 Dj ( S ) = ( Pt ( S ), 
где дизъюнкция берется по множеству  логических закономерностей  класса Kj. 
Ясно, что Dj(St)=1 для всех обучающих объектов из класса Kj  (если обучающая выборка не противоречива), и Dj(St)=0 для всех эталонных объектов, не принадлежащих классу . Таким образом, Dj(S) совпадает на множестве описаний эталонных объектов с характеристической функцией класса Kj. 
Определение. Кратчайшим логическим описанием  класса Kj назовем логическую сумму 
, 
суммирование в которой проводится по подмножеству множества , содержащему минимальное число конъюнкций Pt(S), и совпадающей с функцией Dj(S) на эталонных объектах. 



Рис.3. Покрытие эталонных объектов класса "белый кружок" множеством его логических закономерностей. 


Рис.4. Кратчайшее покрытие эталонных объектов класса "белый кружок" множеством его логических закономерностей
Определение. Минимальным логическим описанием класса Kj назовем логическую сумму 
, 
суммирование в которой проводится по подмножеству множества , содержащую минимальное общее число символов x1(S),x2(S),...,xn(S) в своей записи, и совпадающей с функцией Dj(S) на эталонных объектах. 
Логические (кратчайшие, минимальные) описания классов являются аналогами представлений частичных булевых функций  в виде сокращенных дизъюнктивных нормальных форм (кратчайших, минимальных), а геометрические образы логических закономерностей классов - аналогами максимальных интервалов [6,7]. 
Определение. Величину pt=Nt/N, где Nt  - число логических закономерностей, содержащих признак xt, а N - общее число логических закономерностей, назовем мерой информативности (весом) признака xt. 
Если рассматривать аналогичное отношение логических закономерностей связанных с фиксированным классом, то можно оценить меру информативности данного признака для выделенного класса. 
Определение. Обозначим через N ij   число логических закономерностей, содержащих признаки с номерами  i  и  j . Величину rij = 1-N ij / min ( Ni ,Nj) назовем коэффициентом логической корреляции признаков xi и x j . 
Очевидно, rij (0, причем тем случаям, когда данные признаки встречаются в логических закономерностях только совместно, соответствуют нулевые значения коэффициента (признаки "дополняют друг друга" в обязательном порядке). Наоборот, если признаки являются зависимыми (т.е. в какой-либо мере заменяют друг друга), коэффициент корреляции будет близок к единице. В случаях равных или пропорциональных признаков (столбцов таблицы обучения), в силу свойств логических закономерностей N ij =0 (что непосредственно следует из алгоритма их поиска) и, следовательно, rij=1. 
Если  min (Ni ,Nj)=0, полагаем rij=0 (данный случай возникает , например, если x i ( S )( const ). 
Если найдено множество  логических закономерностей класса Kj, то кратчайшее и минимальное логические описания класса находятся как решения задач поиска покрытий множества эталонов класса соответствующими предикатам Pt(S) гиперпараллелепипедами. 
Пусть  ={ P1(S), P2(S),., PN(S) }.
 Рассмотрим задачу:  
                                                      	                  (10)
                                                         (11)
Тогда при  at  =1  единичные компоненты решения задачи (10-11) определяют кратчайшее логическое описание  класса Kj , а при  at  , равных числу переменных в Pt(S),  - минимальное логическое описание. 
Исходные множества  могут содержать равные или близкие элементы, мощность  может быть весьма велика (что однако является благоприятным в процедурах распознавания). Данные свойства множеств  существенно зависят от длины обучающей выборки и самого алгоритма их поиска. В то же время, кратчайшие и минимальные логические описания классов образуют уже неизбыточные подмножества , выражающие как основные свойства данных множеств, так и свойства самих классов. Поэтому вычисление  и  может рассматриваться как один из подходов к проблеме сортировки логических закономерностей классов. Входящие в  и  предикаты могут рассматриваться как наиболее компактные представления о классах, включающие как наиболее представительные знания (предикаты, покрывающие большое число эталонов), так и уникальные или редкие (предикаты, покрывающие малое число эталонов или отдельные из них). 
Определение. Логической сложностью (компактностью) классов называются величины: 
(1(Kj)=<число конъюнкций в >; 
(2(Kj)=<число переменных в >. 
Величина  ( ()=  , где  (  - некоторый критерий логической сложности класса, называется логической сложностью задачи. 
Естественно ожидать, что если некоторый класс является компактным, простым множеством объектов,  хорошо логически отделимым от других классов, то он имеет малое число переменных в минимальном логическом описании класса и/или малое число конъюнкций в кратчайшем. 





= 44 =



(

(

(

(

(

(

(

(


(

(

(

(

 St
 














