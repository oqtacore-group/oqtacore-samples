
Лекции по статистическим (байесовским) алгоритмам классификации (черновик) К. В. Воронцов 14 сентября 2006 г. Содержание 1 Байесовские алгоритмы классификации 2 1.1 Вероятностная постановка задачи классификации . . . . . . . . . . . . 2 1.1.1 Функционал среднего риска . . . . . . . . . . . . . . . . . . . . . 3 1.1.2 Оптимальное байесовское решающее правило . . . . . . . . . . . 4 1.1.3 Задача восстановления плотности распределения . . . . . . . . . 6 1.2 Нормальный дискриминантный анализ . . . . . . . . . . . . . . . . . . . 8 1.2.1 Линейные и квадратичные разделяющие поверхности . . . . . . 9 1.2.2 Подстановочный алгоритм . . . . . . . . . . . . . . . . . . . . . . 14 1.2.3 Линейный дискриминант Фишера . . . . . . . . . . . . . . . . . . 15 1.3 Разделение смеси распределений . . . . . . . . . . . . . . . . . . . . . . . 18 1.3.1 EM-алгоритм . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.3.2 Смеси многомерных нормальных распределений . . . . . . . . . 23 1.3.3 Сеть радиальных базисных функций . . . . . . . . . . . . . . . . 25 1.4 Непараметрические методы классификации . . . . . . . . . . . . . . . . 27 1.4.1 Локальные оценки плотности распределения . . . . . . . . . . . 27 1.4.2 Метод парзеновского окна . . . . . . . . . . . . . . . . . . . . . . 29 1.4.3 ?Наивный? байесовский классификатор . . . . . . . . . . . . . . 31 1.5 Выводы . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
- 2 - 1 Байесовские алгоритмы классификации Байесовский подход основан на предположении, что плотности распределения каждого из классов известны заранее. Это очень сильное предположение. Оно поз- воляет выписать искомый алгоритм в явном аналитическом виде. Более того, можно доказать, что этот алгоритм является оптимальным, то есть обладает минимальной вероятностью ошибочной классификации. На практике плотности распределения классов, как правило, неизвестны. Их приходится оценивать (восстанавливать) по имеющейся обучающей выборке. При этом байесовский алгоритм перестаёт быть оптимальным, так как плотность распре- деления можно восстановить по конечной выборке только с некоторой погрешностью. Тем не менее, во многих практических задачах байесовский подход позволяет стро- ить вполне работоспособные алгоритмы классификации. Далее будут рассмотрены три наиболее распространённых подхода к восстанов- лению плотностей распределения, и, соответственно, три типа байесовских алгорит- мов классификации. : Если функция плотности известна с точностью до параметров, то значения этих параметров можно оценить по выборке, исходя из принципа максимума правдоподобия. В этом случае говорят о параметрических методах оценивания плотности. Наиболее развита техника восстановления многомерных нормаль- ных распределений. : Если функцию плотности не удаётся смоделировать некоторым параметриче- ским распределением, можно попытаться описать её смесью нескольких рас- пределений. Смеси многомерных нормальных распределений позволяют при- ближать любые непрерывные плотности с заданной точностью. : Если подогнать параметрическую модель распределения под имеющиеся дан- ные не удаётся, то применяют непараметрические методы, основанные на ло- кальной аппроксимации плотности в каждой точке пространства X. Задача построения алгоритма классификации в условиях, когда фиксирован вид функций правдоподобия классов, либо вид разделяющей поверхности, называ- ется в математической статистике задачей дискриминантного анализа. :1.1 Вероятностная постановка задачи классификации Важно понимать, что элементы множества X это не сами объекты, а лишь их описания, содержащие доступную информацию об объектах. Полные описания объ- ектов практически никогда не бывают известны. Возможно ли полностью, во всех деталях, описать человека, предприятие, геологический район, экономическую ситу- ацию? На практике собирается только та часть информации об объектах, которая из общих соображений представляется полезной, и к тому же, требует разумных затрат на её получение и хранение. Итак, описания объектов неизбежно являются частичными. Поэтому не исклю- чены ситуации, когда объекты, относящиеся к разным классам, будут иметь одина- ковые описания. Но тогда определять классы как непересекающиеся подмножества
- 3 - Ky = cx ? X ??y?(x) = y? не вполне корректно. Более того, соответствие x > y?(x) в общем случае не является функцией, тем не менее, алгоритм a(x) обязан быть функцией. Далеко не все методы классификации ?замечают? это противоречие. Ча- ще всё же предполагается, что объекты взаимно однозначно соответствуют своим описаниям, а целевая зависимость y?(x) является функцией. Байесовский вероятностный подход обходит указанное противоречие более кор- ректным способом. Предполагается, что в произвольной точке x ? X каждый класс Ky имеет свою плотность вероятности p(x|Ky), y ? Y . Задача заключается в том, чтобы построить алгоритм a(x), минимизирующий вероятность ошибочной классификации. Реализация этой идеи опирается на следующие исходные гипотезы. Гипотеза 1.1. Множество прецедентов X ? Y является вероятностным простран- ством с вероятностной мерой P. Гипотеза 1.2. Классы это подмножества Ky ? X, y ? Y . Известны плотности распределения классов py(x) = p(x|Ky), называемые функциями правдоподобия. Гипотеза 1.3. Известны вероятности Py = P(Ky) появления объектов каждого из классов y ? Y , называемые априорными вероятностями классов. Предполагается также, что существуют, но не известны, условные вероятности P(Ky|x) принадлежности объекта x классам Ky, y ? Y . Их называют апостериор- ными вероятностями классов, подчёркивая, что они возникают a posteriori, то есть после того, как стал известен объект x. Значение P(Ky|x) можно рассматривать как оценку степени принадлежности объекта x классу y. Представляется естественным относить объект к тому классу, для которого эта величина максимальна. Нашей бли- жайшей целью будет формальное обоснование этого принципа. 1.1.1 Функционал среднего риска Функции правдоподобия позволяют находить вероятности событий вида ?x ? - при условии, что x принадлежит классу Ky?: P(-|Ky) = Z- py(x) dx. Рассмотрим произвольный алгоритм a : X > Y . Он разбивает множество X на непересекающиеся области: Ay = {x ? X | a(x) = y}, y ? Y. Вероятность того, что алгоритм a относит объект y-го класса к s-му классу, равна P(As ? Ky) = Py P(As|Ky). Если y = s, то это вероятность правильной класси- фикации. Если y 6= s, то это вероятность ошибочной классификации. В зависимости от конкретной задачи потери от ошибок разного рода могут быть различны. Каждой паре (y, s) поставим в соответствие величину потери ?ys, возникающей при отнесении объекта класса y к классу s. Обычно полагают ?yy = 0, и ?ys > 0 при y 6= s.
- 4 - Пример 1.1. В задаче радиолокационной разведки класс K0 самолёты против- ника, класс K1 ложные цели. Наибольшая потеря возникает в том случае, когда объект класса K0 принимается за объект класса K1. Это называется ошибкой I рода или ?пропуском цели?. Когда объект класса K1 принимается за объект класса K0, говорят об ошибке II рода или ?ложной тревоге?. В данном случае ?01 > ?10. Пример 1.2. В задаче обнаружения спама класс K0 нежелательные сообщения, подлежащие удалению. Класс K1 сообщения, которые следует сохранить. Здесь, наоборот, удаление нужного сообщения является более существенной потерей, чем пропуск спама, поэтому ?01 < ?10. Опр. 1.1. Функционал среднего риска есть ожидаемая величина потери при клас- сификации объектов алгоритмом a: R(a) =Xy?YXs?Y ?ysPy P(As|Ky). Если величина потерь одинакова для ошибок любого рода (положим для опре- делённости ?yy = 0 и ?ys = 1 при y 6= s), то средний риск R(a) совпадает с вероятно- стью ошибки алгоритма a. 1.1.2 Оптимальное байесовское решающее правило Знание функций правдоподобия позволяет выписать в явном виде алгоритм a, минимизирующий средний риск R(a). Теорема 1.1. Если известны априорные вероятности Py и функции правдоподобия py(x), то минимум среднего риска достигается алгоритмом a(x) = argmin s?Y Xy?Y ?ysPypy(x). Доказательство. По формуле полной вероятности для любых y и t из Y P(At|Ky) = 1 ? X s?Y \{t} P(As|Ky). Выделив произвольный t ? Y , распишем функционал полного риска: R(a) =Xy?YXs?Y ?ysPy P(As|Ky) = =Xy?Y ?ytPy P(At|Ky) +Xy?Y X s?Y \{t} ?ysPy P(As|Ky) = =Xy?Y ?ytPy | {z } const(a) +Xy?Y X s?Y \{t}(?ys ? ?yt)Py P(As|Ky) = = const(a) + X s?Y \{t} ZAsXy?Y (?ys ? ?yt)Pypy(x) dx = (1.1) = const(a) + X s?Y \{t} ZAs(gs(x) ? gt(x)) dx,
- 5 - где введено обозначение gs(x) =Xy?Y ?ysPypy(x). В выражении (1.1) от алгоритма a зависят только области As. Интеграл при- нимает наименьшее значение, когда область As состоит их тех и только тех точек, на которых подынтегральное выражение отрицательно. В силу произвольности t As = cx ? X ??gs(x) < gt(x), ?t 6= s? для всех s ? Y . Но это и означает, что алгоритм a принимает значение s на тех и только тех объек- тах x, для которых значение gs(x) минимально по s ? Y . ? Часто можно полагать, что величина потери зависит только от истинной клас- сификации объекта, но не от того, к какому классу он был ошибочно отнесён, то есть ?ys ? ?y для всех y, s ? Y . В этом случае алгоритм, доставляющий наименьшее зна- чение функционалу среднего риска, приобретает более простой вид. Теорема 1.2. Если известны априорные вероятности Py и функции правдоподобия py(x), и, кроме того, ?yy = 0 и ?ys ? ?y для всех y, s ? Y , то минимум среднего риска доставляется алгоритмом a(x) = argmax y?Y ?yPypy(x). (1.2) Доказательство. Рассмотрим выражение (1.1) из доказательства предыдущей теоремы. Посколь- ку ?ys не зависит от второго индекса, то для любых s, t ? Y справедливо Xy?Y (?ys ? ?yt)Py py(x) = ?tPtpt(x) ? ?sPsps(x) = gt(x) ? gs(x), где gs(x) = ?sPsps(x). Аналогично доказательству предыдущей теоремы отсюда вы- текает, что алгоритм a принимает значение s на тех и только тех объектах x, для которых значение gs(x) максимально по s ? Y . ? Замечание 1.1. Если максимум в (1.2) достигается при y = s и y = t одновременно, то объект x находится на разделяющей поверхности между классами Kt и Ks. Раз- деляющая поверхность определяется уравнением ?tPtpt(x) = ?sPsps(x). Алгоритм может относить такие объекты к любому из классов, это не повлияет на средний риск R(a). В некоторых задачах имеет смысл выдавать ?особый ответ? ? /? Y , озна- чающий отказ алгоритма от классификации объекта. Принцип максимума апостериорной вероятности. По определению условной вероятности py(x)Py = P(Ky|x)p(x), поэтому оптимальный алгоритм классифика- ции (1.2) можно также записать в виде a(x) = argmax y?Y ?y P(Ky|x). Если классы равнозначны (?y ? 1), то данное правило классификации называется методом максимума апостериорной вероятности. Если классы ещё и равноверо- ятны (Py ? 1/M), то объект x просто относится к классу с наибольшим значением плотности распределения в точке x: a(x) = argmax y?Y py(x).
- 6 - Выражение (1.2) называют байесовским решающим правилом. Оно непосред- ственно вытекает из формулы Байеса (точнее, из определения условной вероятно- сти), если в качестве исходного постулата принять принцип максимума апостериор- ной вероятности. Мы исходили из принципа минимизации среднего риска, что поз- волило обобщить решение на случай произвольной матрицы потерь (?ys)|Y |?|Y |. О тестировании методов обучения на модельных данных. Благодаря свой- ству оптимальности байесовское решающее правило удобно использовать в качестве эталона при тестировании различных методов обучения на этапе их разработки. Ме- тодика тестирования заключается в следующем. 1. С помощью заранее известных функций правдоподобия и априорных вероят- ностей классов генерируются модельные выборки: обучающая и контрольная. 2. По обучающей выборке тестируемым методом настраивается алгоритм a. 3. Вычисляется частота ошибок алгоритма a и частота ошибок байесовского ал- горитма на контрольной выборке. Эти величины являются несмещёнными эм- пирическими оценками функционала среднего риска. 4. Тестируемый метод обучения считается пригодным, если эмпирическая оценка риска для алгоритма a оказывается не сильно хуже байесовской. 1.1.3 Задача восстановления плотности распределения До сих пор предполагалось, что функции правдоподобия классов py(x) и апри- орные вероятности Py заданы. На практике такая информация, как правило, неиз- вестна. Обычно имеется конечная выборка данных, и можно лишь предполагать, что она сгенерирована некоторым неизвестным вероятностным распределением. Гипотеза 1.4 (о вероятностной природе данных). Прецеденты X? = (xi, yi)? i=1 появляются случайно и независимо согласно вероятностному распределению с плот- ностью p(x, y) = P(Ky)p(x|Ky) = Pypy(x). Возникает задача: имея выборку данных X?, найти плотность вероятностного распределения Pypy(x), её сгенерировавшего. Проще всего оценить априорные вероятности классов Py. Оценкой является частота появления объектов данного класса1: ? Py = ?y ? , где ?y =X? i=1 [yi = y], y ? Y. (1.3) Согласно закону больших чисел случайная величина ? Py сходится по вероят- ности к Py при ?y > ?. Иными словами, чем больше длина выборки, тем точнее выборочная оценка. 1 Здесь и далее символами с ?крышечкой? обозначаются оценки вероятностей, распределений или случайных величин, вычисленные по конечным выборкам. Такие оценки принято называть выборочными.
- 7 - Гораздо труднее восстановить по выборке плотности распределения классов. Эта задача является некорректно поставленной, поскольку многие распределения могли бы сгенерировать одну и ту же выборку. Для обеспечения единственности ре- шения приходится привлекать те или иные дополнительные предположения о плот- ностях распределения классов. Рассмотрим сначала классический параметрический подход к восстановлению плотности, тесно связанный с методом максимума правдоподобия. Гипотеза 1.5. Функции правдоподобия классов принадлежат заданному парамет- рическому семейству плотностей py(x) = ?(x; ?y), где ? фиксированная функция, ?y вектор параметров, свой для каждого класса y ? Y . Это довольно сильное предположение. Фактически, утверждается, что ?фор- ма? классов может быть приближённо описана (смоделирована) с помощью задан- ного семейства вероятностных распределений, одного и того же для всех классов. Метод максимума правдоподобия позволяет оценить неизвестные параметры плотности распределения ?(x; ?) по случайной, независимой, одинаково распределён- ной выборке2 наблюдений Xm = {x1, . . . , xm}. Метод состоит в том, чтобы найти зна- чение вектора параметров ?, при котором наблюдаемая выборка наиболее вероятна. В силу независимости наблюдений плотность распределения (функция правдоподо- бия) выборки есть произведение плотностей распределения отдельных наблюдений: p(Xm; ?) =Ym i=1 ?(xi; ?). Этот функционал необходимо максимизировать по вектору параметров ?. Техниче- ски более удобно максимизировать ln p(Xm; ?). Введение логарифма позволяет заме- нить произведение суммой, что существенно упрощает дальнейшие выкладки: L(Xm; ?) = ln p(Xm; ?) = Xm i=1 ln ?(xi; ?) > max ? . (1.4) Заметим, что функционал ?L(Xm; ?) имеет вид суммы потерь, которую нужно минимизировать по параметру ?. Величина потери ln ?(xi; ?) на отдельном объекте xi тем больше, чем меньше вероятность появления объекта xi согласно распределению ?(x; ?), то есть чем хуже данный объект ?подходит? под данное вероятностное рас- пределение. В некоторых случаях объекты имеют различную степень важности, которая за- даётся вектором неотрицательных весов объектов Gm = {g1, . . . , gm}. Тогда вводится функционал взвешенного правдоподобия: L(Xm,Gm; ?) = Xm i=1 gi ln ?(xi; ?) > max ? . (1.5) 2 Рассматривая вспомогательную задачу восстановления плотности, будем обозначать выборку через Xm (вместо X?), поскольку в основной задаче классификации плотность должна восстанав- ливаться отдельно для каждого класса.
- 8 - Итак, принцип максимума правдоподобия позволяет сводить задачи оценива- ния неизвестных параметров распределений к задачам оптимизации, для решения которых можно применять стандартные методы. Иногда решение удаётся выписать в явном виде, исходя из необходимого усло- вия минимума: ? ??L(Xm,Gm; ?) =Xm i=1 gi ? ?? ln ?(xi; ?) = 0, при этом предполагается, что функция ?(x; ?) достаточно гладкая по параметру ?. В частности, задача решается аналитически, когда ?(x; ?) многомерное нор- мальное распределение. Рассмотрим этот классический случай подробно. :1.2 Нормальный дискриминантный анализ Опр. 1.2. Вероятностное распределение с плотностью N(x; ?, :) = (2?)?n2 |:|?12 exp!?12 (x ? ?)T:?1(x ? ?)?, x ? Rn, называется n-мерным нормальным (гауссовским) распределением с вектором мато- жидания (центром) ? ? Rn и ковариационной матрицей : ? Rn?n. Предполагается, что матрица : симметричная, невырожденная и положительно определённая. Интегрируя по Rn, нетрудно убедиться в том, что параметры распределения ? и : оправдывают своё название: Ex = Z xN(x; ?, :)dx = ?; E(x ? ?)(x ? ?)T = Z (x ? ?)(x ? ?)TN(x; ?, :)dx = :. Геометрическая интерпретация нормальной плотности. Если признаки некор- релированы, : = diag(?21, . . . , ?2n), то линии уровня плотности распределения имеют форму эллипсоидов с центром ? и осями, параллельными линиям координат. Если признаки имеют одинаковые дисперсии, : = ?2In, то эллипсоиды являются сферами. Если признаки коррелированы, то матрица : не диагональна и линии уровня имеют форму эллипсоидов, оси которых повёрнуты относительно исходной системы координат. Действительно, как всякая симметричная матрица, : имеет спектральное разложение : = V SV T, где V = (v1, . . . , vn) ортогональные собственные векторы матрицы :, соответствующие собственным значениям ?1, . . . , ?n, матрица S диаго- нальна, S = diag(?1, . . . , ?n). Тогда :?1 = V S?1V T, следовательно, (x ? ?)T:?1(x ? ?) = (x ? ?)TV S?1V T(x ? ?) = (x' ? ?')TS?1(x' ? ?'). Это означает, что в результате ортогонального преобразования координат x' = V Tx оси эллипсоидов становятся параллельны линиям координат. В новых координатах ковариационная матрица S является диагональной. Поэтому линейное преобразо- вание V называется декоррелирующим. В исходных координатах оси эллипсоидов направлены вдоль собственных векторов матрицы :.
- 9 - 1.2.1 Линейные и квадратичные разделяющие поверхности Рассмотрим задачу классификации, в которой объекты описываются n- мерными векторами, X = Rn. Число классов |Y | произвольно, но конечно. Гипотеза 1.6. Классы имеют n-мерные нормальные плотности распределения с па- раметрами (?y,:y), y ? Y . Теорема 1.3. Если классы имеют нормальные функции правдоподобия, то байе- совское решающее правило строит квадратичную разделяющую поверхность. Квад- ратичная поверхность вырождается в линейную тогда и только тогда, когда ковари- ационные матрицы классов равны. Доказательство. Запишем уравнение поверхности, разделяющей классы s и t: ?sPsps(x) = ?tPtpt(x); ln ps(x) ? ln pt(x) ? |ln(?tP{tz/?sPs}) Cst=const(x) = 0. Разделяющая поверхность в общем случае квадратична, поскольку ln py(x) является квадратичной формой по x: ln py(x) = ?n2 ln 2? ? 12 ln |:y| ? 12 (x ? ?y)T:?1 y (x ? ?y). Если :s = :t ? :, то квадратичные члены сокращаются и уравнение поверхности вырождается в линейную форму: xT:?1(?s ? ?t) ? 12?Ts :?1?s + 12?Tt :?1?t ? Cst = 0; (x ? ?st)T:?1(?s ? ?t) ? Cst = 0; где ?st = 12 (?s + ?t) точка посередине между центрами классов. ? Геометрия разделяющих поверхностей. Простейший случай: классы равноверо- ятны и равнозначны, ковариационные матрицы равны, признаки некоррелированы и имеют одинаковые дисперсии. Это означает, что классы имеют одинаковую сфе- рическую форму. В этом случае разделяющая гиперплоскость проходит посередине между классами, ортогонально линии, соединяющей центры классов. Нормаль ги- перплоскости обладает оптимальным свойством: в одномерной проекции на нормаль классы разделяются наилучшим образом. Усложнение 1: признаки коррелированы. Тогда ортогональность исчезает, одна- ко разделяющая гиперплоскость по-прежнему проходит посередине между классами, касательно к линиям уровня обоих распределений. Усложнение 2: классы не равновероятны или не равнозначны. Тогда разделяю- щая гиперплоскость старается держаться подальше от более значимого класса. Усложнение 3: ковариационные матрицы общего вида (не диагональны) и не равны. Тогда разделяющая поверхность становится квадратичной и прогиба- ется так, чтобы менее плотный класс охватывал более плотный. Усложнение 4: Если число классов превышает 2, то разделяющая поверхность является кусочно-квадратичной, а при равных ковариационных матрицах кусочно- линейной.
- 10 - Расстояние Махаланобиса. Пусть классы равновероятны и равнозначны, ковари- ационные матрицы равны. Тогда уравнение разделяющей поверхности принимает вид (x ? ?s)T:?1(x ? ?s) = (x ? ?t)T:?1(x ? ?t); kx ? ?sk: = kx ? ?tk:; где ku ? vk: ? q(u ? v)T:?1(u ? v) метрика, называемая расстоянием Махалан- обиса. Разделяющая поверхность является геометрическим местом точек, равноуда- лённых от центров классов в смысле расстояния Махаланобиса. Если признаки независимы и имеют одинаковые дисперсии, то расстояние Ма- халанобиса совпадает с обычной евклидовой метрикой. В этом случае оптималь- ным (байесовским) решающим правилом является ?относить объект к классу с бли- жайшим центром?. Это алгоритм называют классификатором ближайшего среднего (nearest mean classifier). Выборочные оценки параметров нормального распределения. В случае гаус- совской плотности с параметрами ? ? (?, :) задача максимизации правдоподобия имеет аналитическое решение. Выведем его. Напомним, что производная скалярной функции f(A) по матрице A = (aij) определяется как матрица частных производных ? ?Af(A) = ! ? ?aij f(A)?. Через diagA обозначается матрица, диагональные элементы которой совпадают с соответствую- щими диагональными элементами матрицы A, остальные элементы равны нулю. Лемма 1.4. Если A квадратная n ? n-матрица, u вектор размерности n, то справедливы соотношения: если A произвольного вида: ? ?uuTAu = ATu + Au; ? ?A ln |A| = A?1T; ? ?AuTAu = uuT; если A симметричная: ? ?uuTAu = 2Au; ? ?A ln |A| = 2A?1 ? diagA?1; ? ?AuTAu = 2uuT ? diag uuT; Доказательство. 1. Распишем производную по вектору u покомпонентно: ? ?ui uTAu = ? ?uiXn s=1 Xn t=1 astusut = Xn s=1 Xn t=1 ast(?isut + ?itus) = Xn t=1 aitut + Xn s=1 asius, где ?ab = [a = b] символ Кронекера. Тогда в векторной записи ? ?uuTAu = ATu + Au. 2. Если матрица A симметричная (AT = A), то ? ?uuTAu = 2Au.
- 11 - 3. Теперь распишем производные по матрице A. Рассмотрим сначала случай, когда A произвольная матрица, на элементы которой не наложено никаких допол- нительных ограничений. Выпишем разложение определителя A по i-й строке: |A| =Xn s=1 aisAis, где Ais алгебраическое дополнение элемента ais. Нам понадобятся два свойства алгебраических дополнений. Во-первых, Ais не зависит от элементов i-й строки и s- го столбца матрицы A. Во-вторых, Ais связано с элементами обратной матрицы (bij)n?n = B = A?1 соотношением bij = Aji/|A|. Отсюда следует, что ? ?aij |A| = Aij = bji|A|, или в векторной записи ? ?A|A| = |A|A?1T. Теперь легко найти и производную от логарифма определителя: ? ?A ln |A| = 1 |A||A|A?1T = A?1T. 4. Наконец, производная квадратичной формы uTAu по A есть ? ?aij uTAu = ? ?aij Xn s=1 Xn t=1 astusut = uiuj , или в векторной записи ? ?AuTAu = uuT. 5. Если матрица A симметричная, всё немного усложняется, так как элементы матрицы теперь связаны дополнительными ограничениями aij = aji. Теперь любая функция от матрицы A имеет не n2 аргументов, а только n(n + 1)/2 аргументов aij , i 6 j. Выпишем разложение определителя симметричной матрицы A по i-й строке, пользуясь тем, что aij = aji и Aij = Aji: |A| =Xs<i aisAis +Xs>i aisAis + aiiAii = 2Xs<i aisAis + aiiAii. Отсюда следует ? ?aij |A| = (2Aij i < j; Aij i = j; или в векторной записи ? ?A|A| = |A|(2A?1 ? diagA?1); ? ?A ln |A| = 2A?1 ? diagA?1.
- 12 - 6. Наконец, для симметричной матрицы A uTAu =Xn s=1 Xn t=1 [s < t]astusut + Xn s=1 Xn t=1 [s > t]astusut + Xn s=1 assusus = = 2 Xn s=1 Xn t=1 [s 6 t]astusut ? Xn s=1 assu2s. Производная квадратичной формы uTAu по A есть ? ?aij uTAu = 2uiuj ? ?iju2i; i 6 j; ? ?AuTAu = 2uuT ? diag uuT. ? Теорема 1.5. Пусть задана случайная, независимая, одинаково распределённая выборка наблюдений Xm = {x1, . . . , xm} и вектор весов объектов Gm = {g1, . . . , gm} при условии нормировки Pmi=1 gi = 1. Тогда оценки параметров гауссовской плотно- сти ?(x; ?) ? N(x; ?, :), доставляющие максимум взвешенному функционалу прав- доподобия (1.5), имеют вид ?? = Xm i=1 gixi; ?:= Xm i=1 gi(xi ? ??)(xi ? ??)T. Доказательство. Запишем логарифм плотности нормального распределения. Нам будет удобно представить N как функцию от :?1, а не от самой ковариационной матрицы. Для этого воспользуемся тождеством |:?1| = |:|?1. lnN(x; ?,:?1) = const(?,:?1) + 12 ln |:?1| ? 12 (x ? ?)T:?1(x ? ?). Возьмём производные от lnN по вектору матожидания ? и матрице :?1: ? ?? lnN(xi; ?,:?1) = ?:?1(xi ? ?); ? ?:?1 lnN(xi; ?,:?1) = 12!2: ? diag :? ? ? 12!2(xi ? ?)(xi ? ?)T ? diag(xi ? ?)(xi ? ?)T?. Необходимые условия минимума функционала L(Xm,Gm; ?), см. (1.5): ?L ?? = Xm i=1 gi ? ?? lnN(xi; ?, :) = 0; ?L ?:?1 = Xm i=1 gi ? ?:?1 lnN(xi; ?, :) = 0;
- 13 - Подставляя сюда производную lnN по вектору ?, получим: Xm i=1 gi:?1(xi ? ?) = 0. Умножим это равенство слева на : и вынесем ? за знак суммирования: ? Xm i=1 gi = Xm i=1 gixi. С учётом нормировки отсюда следует первое из утверждаемых соотношений. Введём обозначения S(xi) = : ? (xi ? ?)(xi ? ?)T, S = Xm i=1 giS(xi). В этих обозначениях производная L по матрице :?1 примет вид ?L ?:?1 = 12 Xm i=1 gi!2S(xi) ? diag S(xi)? = S ? 12 diag S = 0. Последнее равенство выполняется тогда и только тогда, когда S = 0. Следовательно, : Xm i=1 gi = Xm i=1 gi(xi ? ?)(xi ? ?)T. откуда, с учётом нормировки, получаем второе соотношение. ? Следствие 1. В условиях предыдущей теоремы оценки параметров гауссовской плотности ?(x; ?) ? N(x; ?, :), доставляющие максимум (не взвешенному) функци- оналу правдоподобия (1.4), имеют вид ?? = 1m Xm i=1 xi; ?:= 1m Xm i=1 (xi ? ??)(xi ? ??)T. Поправка на смещение. Естественным требованием к оценке параметра распреде- ления является её несмещённость. Опр. 1.3. Пусть X? есть выборка случайных независимых наблюдений, полученная согласно распределению ?(x; ?) при фиксированном ? = ?0. Оценка ??(X?) парамет- ра ?, вычисленная по выборке X?, называется несмещённой, если EX? ??(X?) = ?0. Легко убедиться в том, что ?? является несмещённой оценкой матожидания ?: E?? = E1? X? i=1 xi = 1? X? i=1 Exi = Ex = ?.
- 14 - Аналогично можно показать, что E1?X? x=1(xi ? ?)(xi ? ?)T = ExxT ? ??T = :. Однако эта величина не равна E?:, ведь при вычислении ?:вместо неизвестного точ- ного значения матожидания ? подставляется его выборочная оценка ??. Аккуратный расчёт показывает, что ?:является смещённой (несколько заниженной) оценкой :: E?:= E1? X? x=1(xi ? ??)(xi ? ??)T = = Eu1? X? i=1 xixTi ? 2xi??T + ????T
 = Eu1? X? i=1 xixTi ? 1 ?2 X? i=1 X? j=1 xixTj 
 = = Eu1? X? i=1 xixTi ? 1 ?2 X? i=1 xixTi ? 1 ?2 X? i=1 X? j=1,j6=i xixTj 
 = = ExxT ? 1?ExxT ? 1 ?2 ?(? ? 1)ExExT = = ? ? 1 ? !ExxT ? ??T? = ? ? 1 ? :. Смещённость возникает из-за того, что при оценивании ковариационной матрицы вместо точного значения матожидания ? подставляется его выборочная оценка ??. Несмещённая оценка ковариационной матрицы имеет вид ?:= 1 ? ? 1 X? x=1(xi ? ??)(xi ? ??)T. 1.2.2 Подстановочный алгоритм В задачах классификации с гауссовскими классами (Гипотеза 1.6) параметры функций правдоподобия ??y и ?:y можно оценить по частям обучающей выборки X?y = {xi ? X? | yi = y}, y ? Y, для каждого класса y отдельно. Априорные вероятности классов Py оцениваются со- гласно (1.3). Полученные выборочные оценки непосредственно подставляются в фор- мулу (1.2). В результате получается алгоритм классификации, который называется подстановочным. В асимптотике ?y > ? оценки ??y и ?:y обладают рядом оптимальных свойств: они не смещены, состоятельны и эффективны. Однако в условиях конечных, зача- стую слишком коротких, выборок асимптотические свойства не гарантируют вы- сокого качества классификации. Приходится изобретать различные эвристические ?подпорки?, чтобы довести алгоритм до состояния практической пригодности.
- 15 - Недостатки подстановочного алгоритма. : Если длина выборки меньше размерности пространства, ?y < n, то матрица ?:y становится вырожденной, поскольку её ранг не может превышать ?y. В этом случае обратная матрица не существует и метод вообще неприменим. : Даже если длина выборки больше размерности пространства, ?y > n, матри- ца ?:y всё равно может оказаться вырожденной. Это происходит, когда призна- ки оказываются линейно зависимыми. Например, в базу данных по заёмщикам наряду с признаками ?доход заёмщика? и ?доход семьи? могли записывать признак ?суммарный доход?. Тривиальных ситуаций вроде этой легко избе- жать на этапе формирования данных. Однако на практике встречаются также скрытые линейные зависимости между большим числом признаков, которые практически невозможно обнаружить ?на глаз?. Это так называемая пробле- ма мультиколлинеарности. Нужны специальные численные методы, позволя- ющие выяснить, является ли матрица ?:y вырожденной. Более того, признаки могут оказаться почти линейно зависимыми. В этом случае матрица ?:y бу- дет невырождена, но близка к некоторой вырожденной матрице. Такие матри- цы называются плохо обусловленными и обладают рядом неприятных свойств. В результате их обращения получаются неустойчивые решения положение разделяющей гиперплоскости может непредсказуемо и сильно изменяться при незначительных вариациях обучающих данных. Если не предпринимать специ- альных мер против плохой обусловленности, построенные алгоритмы класси- фикации будут допускать слишком много ошибок. : Выборочные оценки чувствительны к нарушениям нормальности распределе- ний, в частности, к редким большим выбросам. Тьюки (1960) показал, что классическая оценка матожидания нормального распределения неустойчива от- носительно сколь угодно малого ?-загрязнения плотности даже в одномерном случае (загрязнённая плотность имеет вид (1??)N(x)+??(x), где N(x) плот- ность нормального распределения, ?(x) плотность загрязнения). Загрязнения с ?тяжёлым хвостом? приводят к появлению выбросов и значительному смеще- нию оценки матожидания. При увеличении размерности влияние загрязнений только усиливается. : Если функции правдоподобия классов существенно отличаются от гауссовских, то методы нормального дискриминантного анализа могут приводить к алго- ритмам низкого качества. В частности, когда имеются номинальные признаки, принимающие дискретные значения, или когда классы распадаются на изоли- рованные сгустки. Далее рассматриваются некоторые способы устранения перечисленных недостатков. 1.2.3 Линейный дискриминант Фишера В 1936 г. Фишер предложил простую эвристику, позволяющую увеличить число объектов, по которым оценивается ковариационная матрица, повысить её устойчи- вость и заодно упростить алгоритм обучения [5]. Эвристика заключается в том, чтобы считать ковариационные матрицы классов равными, даже если они на самом деле
- 16 - не равны. В таком случае достаточно оценить только одну ковариационную матри- цу ?:, задействовав для этого все ? обучающих объектов. При этом классы всегда разделяются линейными поверхностями. Коэффициенты поверхностей получаются непосредственно из (1.2): a(x) = argmax y?Y !?yPypy(x)? = = argmax y?Y !ln(?yPy) ? 12 ??Ty ?:?1??y | {z } ?y +xT ?:?1 | {z??}y ?y ? = = argmax y?Y !xT?y + ?y?. (1.6) Обучение сводится к оцениванию матожиданий ??y для всех y ? Y , вычислению общей ковариационной матрицы ?:и её обращению, см. Алгоритм 1.1. После обуче- ния классификация новых объектов производится по формуле (1.6). Этот алгоритм называется линейным дискриминантом Фишера (ЛДФ). Алгоритм 1.1. Обучение линейного дискриминанта Фишера Вход: выборка X?, предполагается ? > |Y |; величины потерь ?y, y ? Y ; Выход: коэффициенты линейных разделяющих поверхностей ?y ? Rn, ?y ? R, y ? Y ; 1: ?y := X? i=1 [yi = y], ? Py := ?y/?, ??y := 1 ?y X? i=1 [yi = y]xi, для всех y ? Y ; 2: ?::= 1 ? ? |Y | X? i=1 (xi ? ??yi)(xi ? ??yi)T; 3: ?y := ?:?1??y, ?y := ln(?y ? Py) ? ??Ty ?y 2 , для всех y ? Y ; Эвристика Фишера неплохо работает, когда формы классов близки к нормаль- ным и не слишком сильно различаются. В этом случае линейное решающее правило достаточно близко к оптимальному байесовскому, но существенно более устойчиво, чем квадратичное, и часто обладает лучшей обобщающей способностью. Регуляризация ковариационной матрицы. Общая ковариационная матрица классов ?:может оказаться плохо обусловленной (близкой к вырожденной), если дли- на выборки невелика по сравнению с числом признаков, или если среди признаков есть почти линейно зависимые. В этом случае некоторые собственные значения мат- рицы ?:будут близки к нулю, обратная матрица и разделяющая поверхность станут неустойчивыми. Вспомним, что линии уровня гауссовской плотности имеют форму эллипсоидов. Собственные векторы матрицы ?:задают направления осей эллипсоида. Собственные значения определяют ?толщину? эллипсоида вдоль соответствующих направлений. Существует простой способ увеличить все собственные значения матрицы ?:на одну
- 17 - и ту же величину ? , оставив неизменными собственные векторы. При этом ?форма? распределения немного искажается, зато матрица становится хорошо обусловленной. Пусть v собственный вектор матрицы ?:, соответствующий собственному зна- чению ?, ?:v = ?v. Тогда v является также собственным вектором матрицы ?:+ ? In с собственным значением ? + ? , где In единичная матрица размера n ? n: (?:+ ? In)v = ?v + ?v = (? + ? )v. Таким образом, проблема плохой обусловленности может быть решена путём обращение матрицы ?:+ ? In вместо ?:. Известны и другие рекомендации, например, пропорционально уменьшать недиагональные элементы вместо ?:брать матрицу (1 ? ? )?:+ ? diag ?:[3]. Можно занулять недиагональные элементы матрицы, соответствующие парам признаков, корреляции которых незначимо отличаются от нуля [1]. Матрица ста- новится разреженной, и для её обращения могут применяться специальные, более эффективные, алгоритмы. Можно разбивать множество признаков на группы и полагать, что призна- ки из разных групп не коррелированы. Тогда матрица ?:приобретает блочно- диагональный вид. Для обращения таких матриц также существуют специализи- рованные алгоритмы. Преобразование пространства признаков. Другой способ решения проблемы мультиколлинеарности заключается в том, чтобы отбросить некоторое количество наименее значимых признаков. Как правило, ими оказываются признаки, почти ли- нейно зависимые от оставшихся, более информативных, признаков. Различные ме- тоды отбора признаков (features selection) рассматриваются в разделе ??. Обратим внимание на кажущийся парадокс: информация отбрасывается, но решение получа- ется более высокого качества. Ещё один способ сокращения размерности заключается в том, чтобы из имею- щихся признаков построить меньшее количество более информативных признаков. Например, в классе линейных преобразований признаков таким свойством облада- ют собственные векторы ковариационной матрицы, соответствующие максимальным собственным значениям. Этот факт используется в методе главных компонент. Другие методы синтеза признаков (features extraction) рассматриваются в раз- деле ??. Робастные методы оценивания. Оценки, устойчивые относительно редких боль- ших выбросов, связанных с малыми загрязнениями плотности, называются робаст- ными (robust здравый). Простейший метод робастного оценивания параметра ? плотности ?(x; ?) по заданной выборке Xm основан на фильтрации выбросов и со- стоит из трёх шагов. 1. Оценка параметра ?? вычисляется по всей выборке Xm, исходя из принципа максимума правдоподобия. 2. Для каждого объекта xi ? Xm вычисляется правдоподобие ?i = ?(xi; ??). Если ?i < P0, то объект xi считается нетипичным (выбросом) и удаляется из выборки.
- 18 - 3. Оценка параметра ?? вычисляется вторично по отфильтрованной выборке. Пороговое значение P0 является параметром метода. Можно также задавать пороговое значение доли удаляемых объектов. В некоторых случаях удаление объектов не требует полного пересчёта оценки ??. Например, в случае нормального распределения можно воспользоваться оценками ??y, ?:, полученными на первом шаге. Поскольку эти оценки аддитивны по объектам выборки, достаточно просто вычесть из них слагаемые, соответствующие удаляемым объектам. Шаги 2 и 3 можно повторять итерационно, так как после уточнения оценки ?? некоторые объекты могут перейти в разряд нетипичных. В большинстве случаев итерационный процесс сходится очень быстро, 1-2 итераций бывает достаточно. Метод редукции. Где-то в эстетическом подсознании остаётся недоумение: почему мы должны оценивать 12n(n+1)+n|Y | параметров, чтобы получить всего (n+1)|Y | коэффициент ЛДФ? Альтернативный подход к уменьшению размерности простран- ства заключается в том, чтобы свести n-мерную задачу к последовательности дву- мерных [3]. Достоинствами метода являются: простота реализации, отсутствие необходимо- сти оценивать ковариационную матрицу, возможность отбросить неинформативные признаки. В некоторых прикладных задачах метод редукции показал превосходство над многими другими методами классификации. Недостатком является отсутствие строгого теоретического обоснования. :1.3 Разделение смеси распределений В тех случаях, когда ?форму? класса не удаётся описать каким-либо одним распределением, можно попробовать описать её смесью распределений. Гипотеза 1.7. Плотность распределения на X имеет вид смеси k распределений: p(x) =Xk j=1 wjpj(x), Xk j=1 wj = 1, где pj(x) функция правдоподобия j-й компоненты смеси, wj её априорная веро- ятность. Функции правдоподобия принадлежат параметрическому семейству рас- пределений ?(x; ?) и отличаются только значениями параметра, pj(x) = ?(x; ?j). Задача разделения смеси заключается в том, чтобы, зная выборку Xm, число k и семейство ?(x; ?), оценить вектор параметров ? = (w1, . . . ,wk, ?1, . . . , ?k). 1.3.1 EM-алгоритм К сожалению, попытка разделить смесь, используя принцип максимума правдо- подобия ?в лоб?, приводит к слишком громоздкой оптимизационной задаче. Обойти эту трудность позволяет алгоритм EM (expectation-maximization). Идея алгоритма заключается в следующем. Искусственно вводится вспомогательный вектор скрытых (hidden) переменных G, обладающий двумя замечательными свойствами. С одной
- 19 - стороны, он может быть вычислен, если известны значения вектора параметров ?. С другой стороны, поиск максимума правдоподобия сильно упрощается, если извест- ны значения скрытых переменных. EM-алгоритм состоит из итерационного повторения двух шагов. На E-шаге вы- числяется ожидаемое значение (expectation) вектора скрытых переменных G по те- кущему приближению вектора параметров ?. На М-шаге решается задача максими- зации правдоподобия (maximization) и находится следующее приближение вектора ? по текущим значениям векторов G и ?. Алгоритм 1.2. Общая идея EM-алгоритма 1: Вычислить начальное приближение вектора параметров ?; 2: повторять 3: G := EStep(?); 4: ? := MStep(?,G); 5: пока ? и G не стабилизируются. Этот общий алгоритм находит применение в задачах дискриминантного ана- лиза, кластеризации, восстановления пропусков в данных, обработки изображений. Здесь мы рассматриваем его как инструмент разделения смеси распределений. E-шаг (expectation). Обозначим через p(x&?j) совместную плотность вероятно- сти того, что получен объект x и этот объект сгенерирован j-й компонентой смеси. По формуле условной вероятности p(x&?j) = p(x) P(?j |x) = wjp(x|?j). Введём обозначение gij ? P(?j |xi). Это апостериорная вероятность того, что обуча- ющий объект xi был сгенерирован j-й компонентой смеси. Возьмём эти величины в качестве скрытых переменных. Обозначим G = (gij)m?k = (g1, . . . , gj), где gj j-й столбец матрицы G. Предполагается, что каждый объект может быть сгенерирован одной и только одной компонентной. Тогда, по формуле полной вероятности Xk j=1 gij = 1 для всех i. Зная параметры компонент wj , ?j , легко вычислить gij по формуле Байеса: gij = wjpj(xi) Pk s=1wsps(xi) для всех i, j. (1.7) В этом и заключается E-шаг алгоритма EM. M-шаг (maximization). Покажем, что знание значений скрытых переменных gij и принцип максимума правдоподобия приводят к оптимизационной задаче, допуска- ющей эффективное численное (или даже аналитическое) решение. Будем минимизи- ровать функционал Q(?) = ?ln Ym i=1 p(xi) = ? Xm i=1 ln p(xi).
- 20 - Для сходимости EM-процесса желательно, чтобы значение функционала Q(?) не уве- личивалось от итерации к итерации: ?Q ? Q(?) ? Q(?0) 6 0, где ?0 = (w01, . . . ,w0k, ?01, . . . , ?0k) вектор параметров с предыдущей итерации. По определению условной вероятности p0(xi)gij = w0j p0j(xi), следовательно, ?Q = ?Xm i=1 ln p(xi) p0(xi) = ? Xm i=1 ln Xk j=1 gij wjpj(xi) w0j p0j(xi) 6 6 ? Xm i=1 Xk j=1 gij ln wjpj(xi) w0j p0j(xi) , где неравенство следует из выпуклости функции f(z) = ?ln z. Введём обозначение eQ(?) = ? Xm i=1 Xk j=1 gij ln!wjpj(xi)?. Тогда ?Q 6 eQ(?) ? eQ(?0), следовательно, Q(?) 6 eQ(?) + Q(?0) ? eQ| {z (?0}) const(?) ? c(?). Функционал c(?) мажорирует Q(?) и совпадает с ним в точке ?0. Возьмём в качестве следующего приближения вектора ? точку минимума функционала eQ(?). Тогда гарантированно выполняется неравенство Q(?) 6 c(?0) = Q(?0). Сходимость алгоритма вытекает, главным образом, из этого факта. Более подробно условия схо- димости рассматриваются в работах [4, 9, 6]. Итак, на M-шаге решается задача минимизации функционала c(?) или, что то же самое, функционала eQ(?) при одном ограничении типа равенства: ?????? ????? eQ(?) = ? Xm i=1 Xk j=1 gij ln!wj?(xi; ?j)? > min ? ; Xk j=1 wj = 1; Представим логарифм произведения как сумму логарифмов и заметим, что каж- дое слагаемое зависит только от своего набора параметров. Происходит разделение переменных, и задача распадается на (k + 1) независимых подзадач оптимизации: Xm i=1 gij ln ?(xi; ?j) > max ?j ; j = 1, . . . , k. (1.8) Xk j=1 Xm i=1 gij lnwj > max w1,...,wk при ограничении Xk j=1 wj = 1; (1.9)
- 21 - Набор подзадач (1.8) представляет собой ни что иное, как модифицированный принцип максимума правдоподобия (1.4), применённый по-отдельности к каждой из k компонент смеси. Модификация заключается в том, что объекты выборки учи- тываются с весами gij , причём распределение весов своё для каждой компоненты. Подзадача (1.9) легко решается аналитически. Запишем лагранжиан этой оп- тимизационной задачи: L(w1, . . . ,wk; ?) =Xk j=1 Xm i=1 gij lnwj + ?u1 ? Xk j=1 wj
. Приравнивая нулю производные лагранжиана по параметрам wj , получаем: ? ?wju Xm i=1 gij lnwj ? ?wj
 = 0, j = 1, . . . , k, откуда вытекает ? = m и mwj = Pmi=1 gij . Следовательно, оценка максимума прав- доподобия для параметра wj имеет вид ? wj = 1m Xm i=1 gij , j = 1, . . . , k. (1.10) Критерий останова. Принцип максимума апостериорной информации (теоре- ма 1.2) позволяет отнести каждый объект xi к той или иной компоненте смеси. Обо- значим номер компоненты через J(xi): J(xi) = arg max j=1,...,k gij . Тогда критерием останова EM-алгоритма может быть стабилизация состава компонент, когда объекты перестают переcкакивать из одной компоненты в другую: J(xi) = J0(xi) для всех i, где J0(xi) номер компоненты, к которой xi был отнесён на предыдущей итерации. Обобщённый EM-алгоритм. Не обязательно добиваться высокой точности реше- ния оптимизационной задачи (1.8) на каждом шаге алгоритма. Достаточно лишь сместиться в направлении максимума, сделав одну или несколько итераций, и затем выполнить E-шаг. Этот алгоритм также обладает неплохой сходимостью и называ- ется обобщённым ЕМ-алгоритмом (generalized EM-algorithm, GEM) [4]. Проблема выбора начального приближения. Хотя алгоритм EM сходится при достаточно общих предположениях, скорость сходимости может существенно зави- сеть от ?удачности? начального приближения. Сходимость ухудшается в тех слу- чаях, когда делается попытка поместить несколько компонент в один фактический сгусток распределения, либо разместить компоненту посередине между сгустками. Стандартная (но далеко не самая лучшая) эвристика заключается в том, чтобы выбрать параметры компонент случайным образом. Более разумная идея найти в выборке k объектов, максимально удалённых друг от друга, и именно в этих точках разместить компоненты.
- 22 - Проблема выбора числа компонент k. До сих пор предполагалось, что число компонент k известно заранее. На практике это, как правило, не так. Иногда число компонент удаётся оценить визуально, спроецировав выборку на плоскость каким-либо способом и определив число сгустков точек на получен- ном графике. С этой целью можно применить метод главных компонент из ?? или многомерное шкалирование из ??. Однако визуальный подход обладает очевидными недостатками: проецирование искажает структуру выборки, а необходимость обра- щаться к эксперту исключает возможность автоматического анализа данных. Существует ещё один приём решить задачу несколько раз при последова- тельных значениях k, построить график зависимости правдоподобия выборки Q(?) от k, и выбрать наименьшее k, при котором график претерпевает резкий скачок правдоподобия. Это называется критерием ?крутого склона?. К сожалению, он так- же не лишён недостатков. Во-первых, существенно увеличиваются затраты времени. Во-вторых, если данные плохо описываются моделью компонент ?(x; ?), то ?крутой склон? может не наблюдаться. Наличие крутого склона свидетельствует о том, что модель компонент была выбрана довольно удачно. Последовательное добавление компонент позволяет решить обе проблемы сра- зу и с начальным приближением, и с определением числа компонент. Идея заклю- чается в следующем. Имея некоторый набор компонент, можно попытаться выделить ?плотный сгусток? объектов с низкими значениями правдоподобия и описать его ещё одной дополнительной компонентой. Так можно продолжать до тех пор, пока все объекты не окажутся покрыты компонентами. Реализация этой идеи представлена в Алгоритме 1.3. EM-алгоритм. Допустим, что в нашем распоряжении имеется процедура ML(?,U, g), вычисляющая оценку максимума правдоподобия ?? для параметра распределе- ния ?(x; ?) по выборке U ? Xm при заданном векторе весов объектов g ? Rm (если параметр g не указан, то веса предполагаются единичными): ML(?,U, g) = argmax ? Xxi?U gi ln ?(xi; ?). На шаге 1 Алгоритма 1.3 строится первая компонента, ?1 = ML(?,Xm) и по- лагается k = 1. Затем в цикле последовательно добавляется по одной компоненте. Если значение правдоподобия pj(xi) = ?(xi; ?j) меньше некоторого порога P0, значит объект xi плохо описывается j-ой компонентой. На шаге 3 формируется подвыбор- ка X(k) из объектов, которые не подходят ни к одной из компонент. Если длина этой подвыборки меньше порога ?0, то процесс добавления компонент на этом заканчи- вается, и оставшиеся объекты считаются выбросами. На шаге 6 снова применяется метод максимума правдоподобия ML для формирования новой компоненты, но теперь уже не по всей выборке, а только по подвыборке X(k). Веса компонент пересчитыва- ются таким образом, чтобы их сумма по-прежнему оставалась равной единице. Все предыдущие компоненты вместе с новой компонентой проходят через цикл итераций EM-алгоритма (шаги 7-11). На каждом E-шаге вычисляется матрица скрытых пере- менных G по формуле (1.7). На каждом M-шаге решается серия из k оптимизацион- ных задач (1.8). На шаге 10 определяется принадлежность объектов компонентам Ji,
- 23 - Алгоритм 1.3. Разделение смеси распределений с помощью EM-алгоритма Вход: выборка Xm = {x1, . . . , xm}; параметр P0 нижний порог правдоподобия объектов; параметр ?0 минимальная длина выборки для процедуры ML; функция ?(x; ?) модель компоненты смеси; Выход: число компонент смеси k; параметры и веса компонент ?k = (?1, . . . , ?k,w1, . . . ,wk); 1: начальное приближение одна компонента: ?1 := ML(?,Xm); w1 := 1; k := 1; 2: для всех k := 2, 3, . . . 3: выделить объекты с низким правдоподобием предыдущих компонент: X(k) := {xi ? Xm: ?(xi; ?j) < P0, j = 1, . . . , k ? 1}; 4: если |X(k)| < ?0 то 5: выход из цикла по k; 6: начальное приближение для k-й компоненты: ?k := ML(?,X(k)); wk := 1m|X(k)|; wj := wj(1 ? wk), для всех j = 1, . . . , k ? 1; 7: повторять 8: E-шаг (expectation): gij := wj?(xi; ?j) Pks=1 ws?(xi; ?s) , для всех i = 1, . . . ,m, j = 1, . . . , k; 9: M-шаг (maximization): ?j := ML(?,X(k), gj), wj := 1m mPi=1 gij , для всех j = 1, . . . , k; 10: J0 i := Ji, Ji := argmax j=1,...,k gij , для всех i = 1, . . . ,m; 11: пока ?i : Ji 6= J0 i ; 12: если Q(?k) ? Q(?k?1) то 13: выход из цикла по k; при стабилизации которой ЕМ-итерации останавливаются. На шаге 12 наращивание числа компонент k прекращается согласно критерию ?крутого склона?. 1.3.2 Смеси многомерных нормальных распределений Рассмотрим решение задачи M-шага в частном случае, когда компоненты име- ют нормальные (гауссовские) плотности. В этом случае функционал (1.8) является квадратичным и положительно определенным, поэтому решение выписывается в яв- ном аналитическом виде, а минимум функционала eQ(?) оказывается глобальным. Гауссовские смеси общего вида.
- 24 - Гипотеза 1.8. Компоненты смеси имеют n-мерные нормальные распределения ?(x; ?j) = N(x; ?j ,:j) с параметрами ?j = (?j ,:j), где ?j ? Rn вектор мато- жидания, :j ? Rn?n ковариационная матрица, j = 1, . . . , k. Теорема 1.6. Если справедливы Гипотезы 1.4 и 1.8, то стационарная точка опти- мизационной задачи (1.8) имеет вид ??j = 1 m? wjXm i=1 gijxi, j = 1, . . . , k; ?:j = 1 m? wj Xm i=1 gij(xi ? ??j)(xi ? ??j)T, j = 1, . . . , k. Данное утверждение непосредственно вытекает из Теоремы 1.5 и оценки (1.10). Таким образом, M-шаг сводится к вычислению выборочного среднего и вы- борочной ковариационной матрицы для каждой компоненты смеси. При этом для каждой компоненты используется своё распределение весов объектов. Вес i-го объ- екта для j-й компоненты равен gij оценке принадлежности данного объекта данной компоненте, вычисленной на E-шаге. Смеси многомерных нормальных распределений позволяют приближать любые непрерывные плотности вероятности. Они являются универсальными аппроксимато- рами плотностей, подобно тому, как полиномы являются универсальными аппрокси- маторами непрерывных функций. В практических задачах это позволяет восстанав- ливать функции правдоподобия классов даже в тех случаях, когда для выполнения Гипотезы 1.8 нет никаких содержательных оснований. Недостатком гауссовских смесей является необходимость обращать ковариаци- онные матрицы. Это трудоёмкая операция. Кроме того, ковариационные матрицы нередко оказываются вырожденными или плохо обусловленными. Тогда возникает проблема неустойчивости выборочных оценок плотности и самого классификатора. Стандартные приёмы (регуляризация, метод главных компонент) позволяют спра- виться с этой проблемой. Но есть и другой выход использовать для описания компонент более простые распределения, например, сферические. Гауссовские смеси c диагональными матрицами ковариации. Трудоёмкого об- ращения матриц можно избежать, если принять гипотезу, что в каждой компоненте смеси признаки некоррелированы. В этом случае гауссианы упрощаются, оставаясь, тем не менее, универсальными аппроксиматорами плотности. Можно было бы предположить, что компоненты имеют сферические плотно- сти, :j = ?2j In. Этот случай вынесен в качестве Упражнения 1.4. Однако такое пред- положение имеет очевидный недостаток: если признаки существенно различаются по порядку величины, то компоненты будут иметь сильно вытянутые формы, ко- торые придётся аппроксимировать большим количеством сферических гауссианов. Предположение о неравных дисперсиях признаков приводит к алгоритму классифи- кации, не чувствительному к различиям в масштабах измерения признаков. Гипотеза 1.9. Компоненты смеси имеют n-мерные нормальные распределения с параметрами (?j ,:j), где ?j = (?j1, . . . , ?jn), :j = diag(?2j1, . . . , ?2jn) диагональная
- 25 - матрица, j = 1, . . . , k: ?(x; ?j) = N(x; ?j ,:j) =Yn d=1 1 ?jdv2? expu?123?d ? ?jd ?jd ?2
, x = (?1, . . . , ?n). Отметим, что многомерная нормальная плотность с диагональной матрицей ко- вариации представима в виде произведения одномерных плотностей. Это означает, что предположения о некоррелированности и о независимости признаков в гауссов- ском случае равносильны. Теорема 1.7. Если справедлива Гипотеза 1.9, то стационарная точка оптимизаци- онной задачи (1.8) имеет вид ??jd = 1 m? wj Xm i=1 gijxid, d = 1, . . . , n; ??2jd = 1 m? wj Xm i=1 gij(xid ? ??jd)2, d = 1, . . . , n; где xi = (xi1, . . . , xin) объекты выборки Xm. Доказательство. Запишем производные нормальной плотности N(x; ?j ,:j) по параметрам ?jd, ?jd в точке xi = (xi1, . . . , xin): ? ??jd lnN(xi; ?j ,:j) = ??2 jd (xid ? ?jd); ? ??jd lnN(xi; ?j ,:j) = ???1 jd + ??3 jd (xid ? ?jd)2. Приравняем нулю производные взвешенного функционала правдоподобия по пара- метрам ?jd, ?jd: ???2 jd Xm i=1 gij(xid ? ?jd) = 0; ??3 jd Xm i=1 gij !?2jd ? (xid ? ?jd)2? = 0. Отсюда, вынося параметры wj , ?jd, ?jd за знак суммирования по i, получаем требу- емые соотношения. ? 1.3.3 Сеть радиальных базисных функций Выше мы рассматривали задачу разделения смеси распределений, забыв на вре- мя о том, что выборка состоит из объектов разных классов. Теперь вернёмся к задаче классификации. Пусть Y = {1, . . . ,M}, и каждый класс y ? Y имеет свою плотность распределения py(x) и свою часть выборки X?y = {xi ? X? | yi = y}. Тогда Гипотеза 1.9 может быть переформулирована следующим образом.
- 26 - Гипотеза 1.10. Функции правдоподобия классов py(x), y ? Y , представимы в виде смесей ky компонент. Каждая компонента имеет n-мерную гауссовскую плотность с параметрами ?yj = (?yj1, . . . , ?yjn), :yj = diag(?2yj1, . . . , ?2yjn), j = 1, . . . , ky: py(x) =Xky j=1 wyjpyj(x), pyj(x) = N(x; ?yj ,:yj), Xky j=1 wyj = 1, wyj > 0; Обратим внимание, что с каждым гауссианом pyj оказывается связана взвешен- ная евклидова метрика ?yj в n-мерном пространстве X: ?2yj(x, x') = Xn d=1 ??2 yjd |?d ? ?'d |2, x = (?1, . . . , ?n), x' = (?'1, . . . , ?'n). (1.11) Плотность каждой компоненты pyj(x) выражается через расстояние от объекта x до центра компоненты ?yj в этой метрике: pyj(x) = (2?)?n2 (?yj1 - - - ?yjn)?1 exp !?12?2yj(x, ?yj)? . Чем меньше расстояние ?yj(x, ?yj), тем выше значение плотности pyj(x). Функ- ции f(x), зависящие только от расстояния между x и фиксированной точкой про- странства X, принято называть радиальными. Алгоритм классификации. Запишем байесовское решающее правило (1.2): a(x) = argmax y?Y ?yPy Xky j=1 wyjpyj(x). Оно имеет вид суперпозиции, состоящей из трёх уровней (слоёв), Рис 1. Первый слой образован k1+- - -+kM гауссианами с параметрами ?yj , :yj . На входе они принимают описание объекта x, на выходе выдают значения плотностей компонент в точке x. Эти значения можно рассматривать как оценки принадлежности объекта x каждой из компонент pyj , либо как оценки близости объекта x к центрам ?yj . Второй слой состоит из M сумматоров, вычисляющих взвешенные средние этих оценок с веса- ми wyj . На выходе второго слоя появляются значения плотностей классов в точке x это оценки принадлежности объекта x каждому из классов. Третий слой состоит из единственного блока argmax, принимающего окончательное решение об отнесе- нии объекта x к одному из классов. Таким образом, при классификации объекта x оценивается его близость к каждому из центров ?yj по метрике ?yj , j = 1, . . . , ky. Объект относится к тому классу, к чьим центрам он располагается ближе. Описанный трёхслойный алгоритм классификации называется сетью радиаль- ных базисных функций или RBF-сетью (radial basis functions network). Это одна из разновидностей нейронных сетей. Обучение RBF-сети сводится к восстановлению плотности каждого из классов py(x) с помощью EM-алгоритма. Результатом обучения являются центры ?yj и дис- персии ?yj компонент j = 1, . . . , ky. Интересно отметить, что, оценивая дисперсии, мы
- 27 - 8?9>x:=;< - - - p11(x) - - - p1k1(x) pM1(x) - - - pMkM(x) PP arg max a(x) 55 22 ,, )) w11 YYYY ,,YYYY w1k1 gggg 33ggg wM1 WWWW ++WWW wMkM eee 22ee ?1P1 JJ JJ $$ JJ ?MPM tt tt ::t t // Рис. 1. Сеть радиальных базисных функций представляет собой трёхуровневую суперпозицию. фактически подбираем метрики ?yj , с помощью которых будут вычисляться рассто- яния до центров соответствующих компонент ?yj . При использовании Алгоритма 1.3 для каждого класса определяется оптимальное число компонент смеси. EM-алгоритм является достаточно эффективным способом настройки RBF- сетей. Он сильно выигрывает в производительности по сравнению с градиентными методами, которые чаще используются для настройки других разновидностей ней- ронных сетей, см. главу ??. :1.4 Непараметрические методы классификации Непараметрические методы классификации основаны на локальном оценива- нии плотностей распределения классов py(x) в окрестности классифицируемого объ- екта x ? X. Такой подход не требует знания функционального вида плотностей. Однако априорная информация всё равно привлекается. В случае метода парзенов- ского окна предполагается, что в пространстве X задана метрика ?(x, x'), адекватно оценивающая степень сходства объектов. В случае ?наивного? байесовского класси- фикатора предполагается, что признаки независимы. 1.4.1 Локальные оценки плотности распределения Одномерный случай. Пусть X = R и требуется восстановить одномерную плот- ность распределения p(x) по заданной конечной выборке Xm = {x1, . . . , xm}. Идея заключается в том, чтобы оценить, насколько много объектов выборки попадает в окрестность заданной точки x. Локальная эмпирическая оценка плотности ?ph(x) даётся формулой Парзена-Розенблатта [8, 7]: ?ph(x) = 1 mh Xm i=1 Kux ? xi h 
, (1.12) где K(z) функция ядра, h положительный параметр, называемый шириной оRкна. Ядро обязано быть чётной функцией и удовлетворять условию нормировки K(z) dz = 1. В этом случае R ?ph(x) dx = 1 при любом h, то есть функцию ?ph(x) действительно можно рассматривать как плотность вероятности. Если взять прямоугольное ядро K(z) = 12?|z| < 1¤, то выражение (1.12) опреде- ляет отношение доли точек выборки, попавших внутрь окна [x ? h, x + h], к ширине
- 28 - -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 -0.0 0.2 0.4 0.6 0.8 1.0 E Q T П G Рис. 2. Часто используемые ядра | прямоугольное; T треугольное; E Епанечникова; Q квадратическое; G гауссовское. -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 -1.0 -0.5 0.0 0.5 1.0 Рис. 3. Локальные оценки плотности (a) при заниженном h; (б) при завышенном h; (в) при оптимальном h = h¤; (г) при переменной ширине окна и k = k¤. окна 2h. Непрямоугольные ядра придают меньший вес точкам, более удалённым от x. Часто используемые ядра показаны на Рис. 2. Обоснованием формулы (1.12) является следующая теорема, утверждающая, что для широкого класса ядер парзеновская оценка ?ph(x) сходится к истинному зна- чению плотности p(x) при неограниченном увеличении длины выборки и одновре- менном уменьшении ширины окна. Теорема 1.8. Пусть функция K(z) нормирована, чётна, непрерывна, и её квадрат ограничен: RX K2(z) dz < ?. Пусть hm произвольная последовательность, удовле- творяющая условиям hm > 0 и mhm > ? при m > ?. Пусть Xm случайная, независимая выборка из плотности распределения p(x). Тогда ?phm(x) сходится к p(x) для почти всех x ? X при m > ?. Скорость сходимости имеет порядок O(m?2/5). Многомерный случай. Локальная оценка плотности (1.12) легко обобщается на многомерный случай, если в пространстве X задана функция расстояния ?(x, x'). ?ph(x) = 1 mV (h)Xm i=1 Ku?(x, xi) h 
, (1.13) где K ядро, h ширина окна, V (h) нормирующий множитель, гарантирующий, что ?ph(x) действительно является плотностью: V (h) = ZX Ku?(x, xi) h 
dx. Сходимость оценки (1.13) доказана при некоторых дополнительных ограниче- ниях на ядро K и метрику ?, и даже известны оценки скорости сходимости, не сильно отличающиеся от одномерного случая [2]. Замечание 1.2. Чтобы определение нормирующего множителя V (h) было кор- ректно, значение интеграла не должно зависеть от xi. Фактически, это требование
- 29 - однородности пространства X. Действительно, интеграл V (h) есть объём шара с цен- тром в точке xi и радиусом h, ?размытого? с помощью ядра K(?(x, xi)/h). Этот объ- ём не должен зависеть от того, в какую точку пространства помещён центр шара. Данное требование не является обременительным, поскольку меру на множестве X можно ввести как угодно (речь идёт не о вероятностной мере, а о некоторой есте- ственной мере, изначально присущей пространству X; вероятностная мера выража- ется через неё с помощью функции плотности распределения). В частности, числовое пространство Rn удовлетворяет требованию однородности. 1.4.2 Метод парзеновского окна Пусть в пространстве X задана метрика ?(x, x'). Запишем многомерную оценку плотности Парзена-Розенблатта (1.13) для каждого из классов y ? Y : ?py(x) = 1 ?yV (h)X? i=1 [yi = y]Ku?(x, xi) h 
, (1.14) где K ядро, h ширина окна, V (h) нормирующий множитель. В общем случае ширина окна может зависеть от самой классифицируемой точ- ки, h = h(x). Тогда нормирующий множитель становится функцией не только от h, но и от x. О пользе окон переменной ширины будет сказано ниже. Вычисление нормирующего множителя V (h, x) может оказаться сложной за- дачей, но, к счастью, его можно избежать. В байесовском решающем правиле (1.2) множители V (h, x) сокращаются, если V (h, x) не зависит от xi и y. Подставим оценку плотности (1.14) и оценку априорной вероятности классов ? Py = ?y/? в (1.2): a(x) = argmax y?Y ?y X? i=1 [yi = y]Ku?(x, xi) h(x) 
. (1.15) Это очень простой алгоритм. В нём мало параметров, подлежащих обучению. Если метрика ? фиксирована, то остаётся подобрать только ширину окна h и вид ядра K. Ширина окна h решающим образом влияет на качество восстановления плотности. При слишком узком окне (h > 0) плотность концентрируется вблизи обучающих объектов, и функция ?ph(x) претерпевает резкие скачки. При слишком широком окне плотность чрезмерно сглаживается и в пределе h > ? вырождается в константу, Рис. 3. Таким образом, должно существовать оптимальное значение ширины ок- на h?, при котором восстановленная плотность наиболее адекватна. Оптимальная ширина окна h? это компромисс между точностью описания конкретных данных и гладкостью эмпирической плотности ?ph(x). В качестве критерия для выбора h? чаще всего применяется оценка скользящего контроля с исключением объектов по одному (leave-one-out, LOO): LOO(h,X?) = X? i=1 ?a!xi;X?\{xi}, h? 6= yi¤ , где a(x;U, h) алгоритм классификации с параметром ширины окна h, построенный по обучающей выборке U ? X?.
- 30 - Обычно зависимость LOO от h имеет характерный минимум, соответствующий оптимальной ширине окна h?, см. Рис ??. Проблема локальных сгущений возникает в тех случаях, когда распределение объектов в пространстве X сильно неравномерно, и одно и то же значение ширины окна h приводит к чрезмерному сглаживанию плотности в одних местах, и недоста- точному сглаживанию в других. Проблему решают окна переменной ширины. Идея заключается в том, чтобы в каждой точке x ? X определить ширину окна как рас- стояние до k + 1-го соседа h(x) = ?(x, x(k+1)). Здесь через x(1), . . . , x(?) обозначаются обучающие объекты, отранжированные по возрастанию расстояний до объекта x. Ес- ли ядро K(r) имеет ограниченный носитель [?1,+1], то в оценке плотности ?ph(x) для любого x учитываются ровно k ближайших соседей объекта x. Чем выше локальная плотность объектов в окрестности x, тем меньшей будет ширина окна. Теперь целочисленный параметр k определяет компромисс между точностью описания данных и гладкостью функции плотности ?ph(x). Оптимальное значение k? также можно определять по критерию скользящего контроля: k? = argmin k LOO(k;X?). Замечание 1.3. Требование независимости V (h, x) от y означает, что в каждой точке x для всех классов должна использоваться одна и та же ширина окна h. По- этому для определения ширины окна в точке x должны учитываться все объекты вы- борки, независимо от их классовой принадлежности. В то же время, плотности ?py(x) оцениваются по подвыборкам X?y = cxi ? X? ??yi = y?, для каждого класса y ? Y в отдельности. Функция ядра K практически не влияет на точность восстановления плотности и на качество классификации. Часто используемые ядра показаны на Рис. 2 и в Таб- лице 1. В последней колонке приведены (для одномерного случая) численные оценки функционала качества восстановления плотности J(K) = Z +? ?? E!?ph(x) ? p(x)?2 dx. Минимальное значение J(K), равное J?, достигается для ядра Епанечникова E(r), которое является оптимальным. Другие ядра доставляют функционалу J(K) значе- ния, лишь немного худшие J?. Это и позволяет утверждать, что форма ядра прак- тически не влияет на качество восстановления плотности. В то же время, вид ядра определяющим образом влияет на степень гладкости функции ?ph(x), см. третью колонку Таблицы 1. Вид ядра может также влиять на эффективность вычислений. Гауссовское яд- ро G требует просмотра всей выборки для вычисления значения ?ph(x) в произвольной точке x. Ядра E, Q, T, |, имеют ограниченный носитель, и для них достаточно взять только те точки выборки, которые попадают в окрестность точки x радиуса h. Проблема ?проклятия размерности?. Если используемая метрика ?(x, x') осно- вана на суммировании различий по всем признакам, а число признаков очень велико,
- 31 - Ядро Формула степень гладкости J?/J(K) Епанечникова E(r) = 34 (1 ? r2)?|r| 6 1¤ ?p'h разрывна 1.000 Квадратическое Q(r) = 15 16 (1 ? r2)2?|r| 6 1¤ ?p''hразрывна 0.995 Треугольное T(r) = !1 ? |r|??|r| 6 1¤ ?p'h разрывна 0.989 Гауссовское G(r) = (2?)?1/2 exp(?r2/2) ? дифференцируема 0.961 Прямоугольное |(r) = 12?|r| 6 1¤ ?ph разрывна 0.943 Таблица 1. Часто используемые ядра. то все точки выборки могут оказаться практически одинаково далеки друг от друга. Тогда парзеновские оценки плотности становятся неадекватны. Это явление назы- вают проклятием размерности (curse of dimensionality). Выход заключается в по- нижении размерности с помощью преобразования пространства признаков (см. раз- дел ??), либо путём отбора информативных признаков (см. раздел ??). Можно стро- ить несколько альтернативных метрик в подпространствах меньшей размерности, и полученные по ним алгоритмы классификации объединять в композицию. На этой идее основаны алгоритмы вычисления оценок ??. 1.4.3 ?Наивный? байесовский классификатор Рассмотрим задачу классификации, в которой объекты x ? X описываются числовыми признаками f1(x), . . . , fn(x); число классов |Y | произвольно, но конечно. Будем обозначать через x = (?1, . . . , ?n), где ?j = fj(x), произвольный элемент пространства объектов X = Rn. Гипотеза 1.11. Для каждого класса y ? Y признаки ?j = fj(x) являются независи- мыми случайными величинами. Это означает, что функции правдоподобия классов представимы в виде py(x) = py1(?1) . . . pyn(?n), для всех y ? Y, (1.16) где pyj(?j) плотность распределения значений j-го признака для класса y. Предположение независимости признаков является чрезмерно сильным и прак- тически никогда не выполняется в реальных условиях. Поэтому методы, опирающи- еся на Гипотезу 1.11, называют ?наивными?. Тем более удивительно, что в ряде случаев эти методы неплохо работают! Предположение о независимости существенно упрощает решение задачи, так как оценивать n-мерную плотность распределения гораздо труднее, чем n одномер- ных плотностей. Преимуществами наивного байесовского классификатора является простота ре- ализации и низкие вычислительные затраты как при обучении, так и при класси- фикации. Недостатком является низкое качество классификации. Наивный байесов- ский классификатор часто используют как ?примитивный? эталон для сравнения обобщающей способности различных моделей алгоритмов. Другой вариант примене- ния в качестве базового алгоритма при построении алгоритмических композиций, см. Главу ??.
- 32 - Случай дискретных признаков. Пусть каждый из признаков fj принимает ко- нечное множество значений Dj . Построим по заданной выборке X? эмпирическую плотность распределения значений признака: ?pyj(?) = 1 ?yX? i=1 [yi = y][fj(xi) = ?], y ? Y, j = 1, . . . , n, ? ? Dj , (1.17) где ?y число обучающих объектов класса y. Заметим, что (1.17) совпадает с фор- мулой Парзена-Розенблатта, если взять ?точечное? ядро K(z) = [z = 0] и h = 1. Если все признаки дискретны, то байесовский классификатор легко выписыва- ется в явном виде. Для этого эмпирические плотности отдельных признаков ?pyj(?) подставляются в (1.16) вместо истинных плотностей pyj(?). Затем полученная эмпи- рическая плотность ?py(x) подставляется в (1.2) вместо истинной функции правдопо- добия py(x). Оценки априорных вероятностей классов ? Py вычисляются согласно (1.3). При ?y > ? эмпирические плотности сходятся по вероятности к истинным плотностям распределения согласно закону больших чисел. Таким образом, байесов- ский классификатор, построенный по эмпирическим плотностям, является асимпто- тически оптимальным (разумеется, при условии истинности Гипотезы 1.11). Случай бинарных признаков. Рассмотрим частный случай, когда классов только два, Y = {0, 1}, и все признаки бинарные, Dj = {0, 1}, j = 1, . . . , n. Обозначим через x = (?1, . . . , ?n) произвольный объект из X. Тогда, подставляя эмпирические плотности распределения, представим наивный байесовский класси- фикатор в следующем виде: a(x) = argmax y=0,1 ?yPypy(x) = -?1P1 ?p11(x) . . . ?p1n(x) ?0P0 ?p01(x) . . . ?p0n(x) > 1? = = -ln ?1P1 ?0P0 + ln ?p11(?1) ?p01(?1) + - - - + ln ?p1n(?n) ?p0n(?n) > 0? . Для любой бинарной величины ? и любой функции g справедливо тождество g(?) = ?g(1) + (1 ? ?)g(0). Заметим, что это линейная функция по ?. Отсюда следу- ет, что в бинарном случае наивный байесовский классификатор представим в виде линейной формы от вектора x = (?1, . . . , ?n): a(x) = " Xn j=1 ?j ln ?p1j(1)?p0j(0) ?p0j(1)?p1j(0) | {z } ?j +ln ?1P1 ?0P0 + Xn j=1 ln ?p1j(0) ?p0j(0) | {z } ?0 > 0# = = ??1?1 + - - - + ?n?n + ?0 > 0¤. Обучение наивного байесовского классификатора в данном случае сводится к вычислению коэффициентов ?0, ?j по явным формулам, см. Алгоритм 1.4. Случай вещественных признаков. Если Dj = R, то эмпирическая оценка плотно- сти pyj(?) даётся формулой Парзена-Розенблатта: ?pyj(?; h) = 1 ?yh X? i=1 [yi = y] Kufj(xi) ? ? h 
,
- 33 - Алгоритм 1.4. Обучение наивного байесовского классификатора в бинарном случае Вход: выборка X?; величины потерь ?y, y ? Y ; Выход: коэффициенты ?j , j = 0, . . . , n; 1: ?y :=X? i=1 [yi = y], ? Py := ?y/?, для всех y = 0, 1; 2: ?pyj(?) := 1 ?y X? i=1 [yi = y][fj(xi) = ?], для всех j = 1, . . . , n, y = 0, 1, ? = 0, 1; 3: ?j := ln ?p1j(1)?p0j(0) ?p0j(1)?p1j(0) , для всех j = 1, . . . , n; 4: ?0 := ln ?1 ? P1 ?0 ? P0 + Xn j=1 ln ?p1j(0) ?p0j(0) ; где K(z) функция ядра, h ширина окна, ?y = P?i=1[yi = y]. Байесовский классификатор в этом случае также выписывается в явном виде. Эмпирические плотности отдельных признаков ?pyj(?) подставляются в (1.16) вместо истинных плотностей pyj(?). Затем полученная эмпирическая плотность ?py(x) под- ставляется в (1.2) вместо истинной функции правдоподобия py(x). Оценки априорных вероятностей классов ? Py вычисляются согласно (1.3). При ?y > ? эмпирические плотности сходятся по вероятности к истинным плотностям распределения согласно Теореме 1.8. Таким образом, ?наивный? байе- совский классификатор, построенный по парзеновским эмпирическим плотностям, стремится к оптимальному с ростом длины выборки и уменьшением ширины окна (разумеется, при условии истинности Гипотезы 1.11). :1.5 Выводы О связи параметрических и непараметрических методов. Итак, байесовское решающее правило (1.2) является оптимальным классификатором, но требует знания функций правдоподобия (плотностей распределения) классов. Мы рассмотрели три подхода к восстановлению плотности по выборке. Первый подход предполагает, что плотности классов принадлежат заданному параметрическому семейству распределений ?(x; ?) и отличаются только вектором параметров ?y, y ? Y : py(x) = ?(x; ?y). Второй подход является обобщением первого. Когда ?форму? класса не удаёт- ся описать одним параметрическим распределением, делается попытка описать его смесью распределений: py(x) = Xky j=1 wyj?(x; ?yj), Xky j=1 wyj = 1,
- 34 - где ky число компонент в смеси. К форме компонент предъявляются не столь жёсткие требования, как при первом подходе, поскольку функция ?(x; ?) уже не пре- тендует на роль адекватной модели исходных данных. В качестве компонент можно брать произвольные универсальные аппроксиматоры плотностей, например, гаусси- аны с некоррелированными признаками. Третий подход основан на локальном оценивании плотностей классов в окрест- ности классифицируемого объекта. Этот подход называют непараметрическим, так как он вообще не предполагает знания функционального вида плотностей. Его также правомерно называть метрическим подходом в тех случаях, когда локальная оценка плотности опирается на метрику или функцию расстояния ?(x, x'): py(x) = 1 ?yV (h, x)X? i=1 [yi = y]Ku?(x, xi) h(x) 
, где h ширина окна, V (h, x) нормировочный множитель, от которого байесовское решающее правило не зависит. Сопоставление двух последних формул показывает, что третий подход является предельным частным случаем второго. Действительно, ядро pi(x) = 1 V (h,x)K!?(x,xi) h(x) ? удовлетворяет условию нормировки, и его можно рассматривать как плотность рас- пределения. Фактически, для каждого обучающего объекта xi строится ровно одна компонента смеси с априорной вероятностью wyj = 1/?y и сферической плотностью, центр которой помещается в точку xi. В третьем подходе происходит дальнейшее ослабление требований к форме компонент, и от вида функции K уже мало что зависит. Три подхода отличаются, главным образом, количественно числом компонент смеси: 1, ky, или ?y соответственно. Однако это количественное различие приводит к качественному различию в методах обучения. Преимущества байесовского подхода. : Байесовское решающее правило оптимально, выписывается в явном аналити- ческом виде, легко реализуется программно. : Байесовское решающее правило удобно использовать в качестве эталона при тестировании алгоритмов классификации на модельных данных. Недостатки байесовского подхода. : На практике функции правдоподобия классов, как правило, неизвестны. Их приходится приближённо оценивать (восстанавливать) по конечным выбор- кам данных. Решающее правило, полученное путём подстановки восстановлен- ной плотности в формулу (1.2), уже не является оптимальным. : Методов восстановления плотности известно довольно много. Однако ни один из них не является безусловно лучшим для всех задач. В каждой конкретной задаче метод восстановления приходится подбирать экспериментальным путём.
- 35 - Упражнения Упр. 1.1. Пусть X = R, Y = {0, 1}, ?0P0 = C, ?1P1 = 1, функции правдоподобия классов име- ют вид py(x) = ??1/2 exp!?(x ? y)2?. Выписать байесовский алгоритм классификации. Что собой представляет разделяющая поверхность при C = 1, при C = e? Упр. 1.2. Пусть X = R2, Y = {0, 1}, ln ?iPi = Ci, функции правдоподобия классов гауссовские, ?0 = !ab ?, ?1 = ! ?a ?b ?, с одинаковыми матрицами ковариации : = ! 1 0 0 S ?. Выписать байесовский алгоритм классификации и уравнение разделяющей поверхности. Доказать, что разделяющая по- верхность касается линий уровня плотностей обоих классов. Упр. 1.3. Выписать алгоритм обучения линейного дискриминанта Фишера при ?наивном? пред- положении о независимости признаков. Указание: ковариационная матрица диагональна. Упр. 1.4. Найти стационарную точку оптимизационной задачи (1.8) для случая, когда компонен- ты смеси имеют n-мерные сферические нормальные распределения с параметрами uj = (?j , ?j), где ?j n-мерный вектор, ?j скаляр: pj(x) = (?jv2?)?n exp !?12??2 j kx ? ?jk2? , j = 1, . . . , k. Список литературы [1] Айвазян С. А., Енюков И. С., Мешалкин Л. Д. Прикладная статистика: исследо- вание зависимостей.М.: Финансы и статистика, 1985. [2] Орлов А. И. Нечисловая статистика.М.: МЗ-Пресс, 2004. [3] Шурыгин А. М. Прикладная стохастика: робастность, оценивание, прогноз.М.: Финансы и статистика, 2000. [4] Dempster A. P., Laird N. M., Rubin D. B. Maximum likelihood from incomplete data via the EM algorithm //J. of the Royal Statistical Society, Series B.1977.no. 34.Pp. 1-38. [5] Fisher R. A. The use of multiple measurements in taxonomic problem //Ann. Eugen.1936.no. 7.Pp. 179-188. [6] Jordan M. I., Xu L. Convergence results for the EM algorithm to mixtures of experts architectures: Tech. Rep. A.I. Memo No. 1458: MIT, Cambridge, MA, 1993. [7] Parzen E. On the estimation of a probability density function and mode //Annals of Mathematical Statistics.1962.Vol. 33.Pp. 1065-1076. http://citeseer.ist.psu.edu/parzen62estimation.html. [8] Rosenblatt M. Remarks on some nonparametric estimates of a density function //Annals of Mathematical Statistics.1956.Vol. 27, no. 3.Pp. 832-837. [9] Wu C. F. G. On the convergence properties of the EM algorithm //The Annals of Statistics.1983.no. 11.Pp. 95-103.