 Глава 8.

Нейронные сети ассоциативной памяти

Е.М.Миркес

Вычислительный центр СО РАН в г. Красноярске

Рассматриваются нейронные сети ассоциативной памяти, восстанавливающие по искаженному и/или зашумленному образу ближайший к нему эталонный. Исследована информационная емкость сетей и предложено несколько путей ее повышения, в том числе - ортогональные тензорные (многочастичные) сети. Построены способы предобработки, позволяющие конструировать нейронные сети ассоциативной памяти для обработки образов, инвариантной относительно групп преобразований. Описан численный эксперимент по использованию нейронных сетей для декодирования различных кодов.

1. Введение
Прежде чем заниматься конструированием сетей ассоциативной памяти необходимо ответить на следующие два вопроса: "Как устроена ассоциативная память?" и "Какие задачи она решает?".  Когда мы задаем эти вопросы, имеется в виду не устройство отделов мозга, отвечающих за ассоциативную память, а наше представление о макропроцессах, происходящих при проявлении ассоциативной памяти.
Принято говорить, что у человека возникла ассоциация, если при получении некоторой неполной информации он может подробно описать объект, к которому по его мнению относится эта информация. Достаточно хорошим примером может служить описание малознакомого человека. К примеру, при высказывании: "Слушай, а что за парень, с которым ты вчера разговаривал на вечеринке, такой высокий блондин?"- у собеседника возникает образ вчерашнего собеседника, не ограничивающийся ростом и цветом волос. В ответ на заданный вопрос он может рассказать об этом человеке довольно много. При этом следует заметить, что содержащейся в вопросе информации явно недостаточно для точной идентификации собеседника. Более того, если вчерашний собеседник был случайным, то без дополнительной информации его и не вспомнят.
В качестве другого примера можно рассмотреть ситуацию, когда ваша однокурсница появляется в институте с совершенно новой прической и в незнакомой вам одежде. При этом вы, тем не менее, чаще всего ее узнаете и сможете определить чем ее новый образ отличается от привычного. Можно предположить, что это происходит следующим образом. При виде ее нового облика в вашей памяти возникает ассоциация с привычным для вас. А далее сравнивая эти два облика вы можете определить отличия.
Исходя из рассмотренных примеров можно сказать, что ассоциативная память позволяет по неполной и даже частично недостоверной информации восстановить достаточно полное описание знакомого объекта. Слово знакомого является очень важным, поскольку невозможно вызвать ассоциации с незнакомыми объектами. При этом объект должен быть знаком тому, у кого возникают ассоциации.
Одновременно рассмотренные примеры позволяют сформулировать решаемые ассоциативной памятью задачи:
Соотнести входную информацию со знакомыми объектами, и дополнить ее до точного описания объекта.
Отфильтровать из входной информации недостоверную, а на основании оставшейся решить первую задачу.
Очевидно, что под точным описанием объекта следует понимать всю информацию, которая доступна ассоциативной памяти. Вторая задача решается не поэтапно, а одновременно происходит соотнесение полученной информации с известными образцами и отсев недостоверной информации.

2. Постановка задачи
Пусть задан набор из  эталонов - -мерных векторов . Требуется построить сеть, которая при предъявлении на вход произвольного образа - вектора  - давала бы на выходе "наиболее похожий" эталон.
Всюду далее образы и, в том числе, эталоны  -мерные векторы с координатами .  Эталон, "наиболее похожий" на x  ближайший к x вектор . Легко заметить, что это требование эквивалентно требованию максимальности скалярного произведения векторов  и :
.
Первые два слагаемых в правой части совпадают для любых образов   и , так как длины всех векторов-образов равны . Таким образом, задача поиска ближайшего образа сводится к поиску образа, скалярное произведение с которым максимально. Этот простой факт приводит к тому, что сравнивать придется линейные функции от образов, тогда как расстояние является квадратичной функцией.

3. Сети Хопфилда
Наиболее известной сетью ассоциативной памяти является сеть Хопфилда [1]. В основе сети Хопфилда лежит следующая идея - запишем систему дифференциальных уравнений для градиентной минимизации "энергии" H (функции Ляпунова). Точки равновесия такой системы находятся в точках минимума энергии. Функцию энергии будем строить из следующих соображений:
Каждый эталон должен быть точкой минимума.
В точке минимума все координаты образа должны иметь значения .
Функция
				
не удовлетворяет этим требованиям строго, но можно предполагать, что первое слагаемое обеспечит притяжение к эталонам (для вектора x фиксированной длины максимум квадрата скалярного произведения  достигается при x=xi ), а второе слагаемое   - приблизит к единице абсолютные величины всех координат точки минимума. Величина  (  характеризует соотношение между этими двумя требованиями и может меняться со временем.
Используя выражение для энергии, можно записать систему уравнений, описывающих функционирование сети Хопфилда:
. 			(1)
Сеть Хопфилда в виде (1) является сетью с непрерывным временем. Это, быть может, и удобно для некоторых вариантов аналоговой реализации, но для цифровых компьютеров лучше воспользоваться сетями, функционирующими в дискретном  времени  шаг за шагом.
Построим сеть Хопфилда с дискретным временем. Сеть должна осуществлять преобразование входного вектора  так, чтобы выходной вектор  был ближе к тому эталону, который является правильным ответом. Преобразование сети будем искать в следующем виде:
, 					(2)
где  - вес -го эталона, характеризующий его близость к вектору , Sign - нелинейный оператор, переводящий вектор с координатами yi  в вектор с координатами sign yi .

Функционирование сети. Сеть работает следующим образом:
На вход сети подается образ , а на выходе снимается образ .
Если , то полагаем  и возвращаемся к шагу 1.
Полученный вектор  является ответом.
Таким образом, ответ всегда является неподвижной точкой преобразования сети (2) и именно это условие (неизменность при обработке образа сетью) и является условием остановки.
Пусть - номер эталона, ближайшего к образу . Тогда, если выбрать веса пропорционально близости эталонов к исходному образу , то следует ожидать, что образ   будет ближе к эталону , чем , а после нескольких итераций он станет совпадать с эталоном .
Наиболее простой сетью вида (2) является дискретный вариант сети Хопфилда с весами равными скалярному произведению эталонов на предъявляемый образ:
.					(3)

Рис. 1. а, б, с - эталоны, 
г - ответ сети на предъявление любого эталона
О сетях Хопфилда известно, что они способны запомнить и точно воспроизвести "порядка  слабо скоррелированных образов". В этом высказывании содержится два ограничения:
 число эталонов  не превосходит .
 эталоны слабо скоррелированны.
Наиболее существенным является второе ограничение, поскольку образы, которые сеть должна обрабатывать, часто очень похожи. Примером могут служить буквы латинского алфавита. При обучении сети Хопфилда распознаванию трех первых букв (см. рис. 1 а, б, в), при предъявлении на вход сети любого их эталонов в качестве ответа получается образ, приведенный на рис. 1 г.
В связи с такими примерами первый вопрос о качестве работы сети ассоциативной памяти звучит тривиально: будет ли сеть правильно обрабатывать сами эталонные образы (т.е. не искажает их)?
Зависимость работы сети Хопфилда от степени скоррелированности образов можно легко продемонстрировать на следующем примере. Пусть даны три эталона  таких, что 
  				 (4)
Для любой координаты существует одна из четырех возможностей:
				
В первом случае при предъявлении сети -го эталона в силу формулы (3) получаем , так как все скалярные произведения положительны по условию (4). Аналогично получаем в четвертом случае .
Во втором случае рассмотрим отдельно три варианта

так как скалярный квадрат любого образа равен , а сумма двух любых скалярных произведений эталонов больше , по условию (4). Таким образом, независимо от предъявленного эталона получаем . Аналогично в третьем случае получаем .
Окончательный вывод таков: если эталоны удовлетворяют условиям (4), то при предъявлении любого эталона на выходе всегда будет один образ. Этот образ может быть эталоном или "химерой", составленной, чаще всего, из узнаваемых фрагментов различных эталонов (примером "химеры" может служить образ, приведенный на рис. 1 г). Рассмотренный ранее пример с буквами детально иллюстрирует такую ситуацию.
Приведенные выше соображения позволяют сформулировать требование, детализирующие понятие "слабо скоррелированных образов". Для правильного распознавания всех эталонов достаточно (но не необходимо) потребовать, чтобы выполнялось следующее неравенство . Более простое и наглядное, хотя и более сильное условие можно записать в виде. Из этих условий видно, что чем больше задано эталонов, тем более жесткие требования предъявляются к степени их скоррелированности, тем ближе они должны быть к ортогональным.
Рассмотрим преобразование (3) как суперпозицию двух преобразований:
				(5)
Обозначим через  - линейное пространство, натянутое на множество эталонов. Тогда первое преобразование в (5) переводит векторы из  в . Второе преобразование в (5) переводит результат первого преобразования  в одну из вершин гиперкуба образов. Легко показать, что второе преобразование в (5) переводит точку  в ближайшую вершину гиперкуба. Действительно, пусть  и  две различные вершины гиперкуба такие, что  - ближайшая к , а . Из того, что  и  различны следует, что существует множество индексов, в которых координаты векторов   и  различны. Обозначим это множество через . Из второго преобразования в (5) и того, что , следует, что знаки координат вектора   всегда совпадают со знаками соответствующих координат вектора . Учитывая различие знаков -х координат  векторов  и  при  можно записать . Совпадение знаков -х координат  векторов  и  при  позволяет записать следующее неравенство . Сравним расстояния от вершин  и  до точки 

Полученное неравенство  противоречит тому, что  - ближайшая к . Таким образом доказано, что второе преобразование в (5) переводит точку  в ближайшую вершину гиперкуба образов.

4. Ортогональные сети
Для обеспечения правильного воспроизведения эталонов достаточно потребовать, чтобы первое преобразование в (5) было таким, что . Очевидно, что если проектор является ортогональным, то это требование выполняется, поскольку  при , а  по определению множества .
Для обеспечения ортогональности проектора воспользуемся дуальным множеством векторов. Множество векторов  называется дуальным к множеству векторов  если все вектора этого множества  удовлетворяют следующим требованиям:

.
Преобразование  является ортогональным проектором на линейное пространство . 
Ортогональная сеть ассоциативной памяти преобразует образы по формуле
.					(6)
Дуальное множество векторов существует тогда и только тогда, когда множество векторов  линейно независимо. Если множество эталонов  линейно зависимо, то исключим из него линейно зависимые образы и будем рассматривать полученное усеченное множество эталонов как основу для построения дуального множества и преобразования (6). Образы, исключенные из исходного множества эталонов, будут по-прежнему сохраняться сетью в исходном виде (преобразовываться в самих себя). Действительно, пусть эталон  является линейно зависимым от остальных    эталонов. Тогда его можно представить в виде . Подставив полученное выражение в преобразование (6) и учитывая свойства дуального множества получим:
		(7)
Рассмотрим свойства сети (6) [2]. Во-первых, количество запоминаемых и точно воспроизводимых эталонов не зависит от степени их скоррелированности. Во-вторых, формально сеть способна работать без искажений при любом возможном числе эталонов (всего их может быть до ). Однако, если число линейно независимых эталонов (т.е. ранг множества эталонов) равно , сеть становится прозрачной - какой бы образ не предъявили на ее вход, на выходе окажется тот же образ. Действительно, как было показано в (7), все образы, линейно зависимые от эталонов, преобразуются проективной частью преобразования (6) сами в себя. Значит, если в множестве эталонов есть  линейно независимых, то любой образ можно представить в виде линейной комбинации эталонов (точнее  линейно независимых эталонов), а проективная часть преобразования (6) в силу формулы (7) переводит любую линейную комбинацию эталонов в саму себя.
Если число линейно независимых эталонов меньше n, то сеть преобразует поступающий образ, отфильтровывая помехи, ортогональные всем эталонам.
Отметим, что результаты работы сетей (3) и (6) эквивалентны, если все эталоны попарно ортогональны.
Остановимся несколько подробнее на алгоритме вычисления дуального множества векторов. Обозначим через  матрицу Грама множества векторов . Элементы матрицы Грама имеют вид  (-ый элемент матрицы Грама равен скалярному произведению -го эталона на -ый). Известно, что векторы дуального множества можно записать в следующем виде:
,					(8)
где  - элемент матрицы . Поскольку определитель матрицы Грама равен нулю, если множество векторов линейно зависимо, то матрица, обратная к матрице Грама, а следовательно и дуальное множество векторов существует только тогда, когда множество эталонов линейно независимо.
Для работ сети (6) необходимо хранить эталоны и матрицу . 
Рассмотрим процедуру добавления нового эталона к сети (6). Эта операция часто называется дообучением сети. Важным критерием оценки алгоритма формирования сети является соотношение вычислительных затрат на обучение и дообучение. Затраты на дообучение не должны зависеть от числа освоенных ранее эталонов.
Для сетей Хопфилда это, очевидно, выполняется  добавление еще одного эталона сводится к прибавлению к функции H одного слагаемого , а модификация связей в сети  состоит в прибавлении к весу ij-й связи числа    всего  операций.
Для рассматриваемых сетей с ортогональным проектированием также возможно простое дообучение. На первый взгляд, это может показаться странным  если добавляемый эталон линейно независим от старых эталонов, то вообще говоря необходимо пересчитать матрицу Грама и обратить ее. Однако симметричность матрицы Грама позволяет не производить заново процедуру обращения всей матрицы. Действительно, обозначим через  - матрицу Грама для множества из  векторов ; через  - единичную матрицу размерности . При обращении матриц методом Гаусса используется следующая процедура:
Запишем матрицу размерности  следующего вида: .
Используя операции сложения строк и умножения строки на ненулевое число преобразуем левую квадратную подматрицу к единичной. В результате получим .
Пусть известна  - обратная к матрице Грама для множества из m векторов . Добавим к этому множеству вектор . Тогда матрица для обращения матрицы  методом Гаусса будет иметь вид:
.
После приведения к единичной матрице главного минора ранга m получится следующая матрица:
,
где  - неизвестные величины, полученные в ходе приведения главного минора к единичной матрице. Для завершения обращения матрицы  необходимо привести к нулевому виду первые m элементов последней строки и -о столбца. Для обращения в ноль i-о элемента последней строки необходимо умножить i-ю строку на  и вычесть из последней строки. После проведения этого преобразования получим 
,
где , .  только если новый эталон является линейной комбинацией первых m эталонов. Следовательно . Для завершения обращения необходимо разделить последнюю строку на  и затем вычесть из всех предыдущих строк последнюю, умноженную на соответствующее номеру строки . В результате получим следующую матрицу
,
где . Поскольку матрица, обратная к симметричной, всегда симметрична получаем  при всех i. Так как  следовательно . 
Обозначим через  вектор , через  - вектор . Используя эти обозначения можно записать . Матрица  записывается в виде
.
Таким образом, при добавлении нового эталона требуется произвести следующие операции:
Вычислить вектор  ( скалярных произведений -  операций, ).
Вычислить вектор  (умножение вектора на матрицу -  операций).
Вычислить  (два скалярных произведения  -  операций).
Умножить матрицу на число и добавить тензорное произведение вектора  на себя ( операций).
Записать .
Таким образом, эта процедура требует  операций. Тогда как стандартная схема полного пересчета потребует:
Вычислить всю матрицу Грама ( операций).
Методом Гаусса привести левую квадратную матрицу к единичному виду ( операций).
Записать .
Всего  операций, что  в  раз больше.
Используя ортогональную сеть (6), удалось добиться независимости способности сети к запоминанию и точному воспроизведению эталонов от степени скоррелированности эталонов. Так, например, ортогональная сеть смогла правильно воспроизвести все буквы латинского алфавита в написании, приведенном на рис. 1.
У сети (6) можно выделить два основных недостатка:
 Число линейно независимых эталонов должно быть меньше размерности системы .
 Неинвариантностью  если два визуальных образа отличаются только своим положением в рамке, то в большинстве задач желательно объединять их в один эталон.
Оба этих недостатка можно устранить, изменив выбор весовых коэффициентов в (2).
5. Тензорные сети
Для увеличения числа линейно независимых эталонов, не приводящих к прозрачности сети, используется прием перехода к тензорным или многочастичным сетям [3-7].
Тензорным произведением  -мерных векторов  называется -индексная величина , у которой все индексы независимо пробегают весь набор значений от единицы до , а . -ой тензорной степенью вектора  будем называть вектор , полученный как тензорное произведение  векторов . Вектор  является -мерным вектором. Однако пространство  имеет размерность, не превышающую величину , где  - число сочетаний из  по . 
	2			3	4			4	7	8			5	11	15	16			6	16	26	31	32			7	22	42	57	63	64			8	29	64	99	120	127	128			9	37	93	163	219	247	255	256			10	46	130	256	382	466	502	511	512													Рис. 1. "Тензорный" треугольник Паскаля		Теорема. При  в ранг  множества  равен: .
Небольшая модернизация треугольника Паскаля, позволяет легко вычислять эту величину. На рис. 1 приведен "тензорный" треугольник Паскаля. При его построении использованы следующие правила:
1.  Первая строка содержит двойку, поскольку при n=2 в множестве X всего два неколлинеарных вектора.
Таблица 1.
n	k					5	2	25	15	11			3	125	35	15		10	3	1 000	220	130			6	1 000 000	5005	466			8	100 000 000	24310	511		2. При переходе к новой строке, первый элемент получается добавлением единицы к первому элементу предыдущей строки, второй - как сумма первого и второго элементов предыдущей строки, третий - как сумма второго и третьего элементов и т.д. Последний элемент получается удвоением последнего элемента предыдущей строки.
В табл. 1 приведено сравнение трех оценок информационной емкости тензорных сетей для некоторых значений n и k. Первая оценка    заведомо завышена, вторая     дается формулой Эйлера для размерности пространства симметричных тензоров и третья  точное значение  
Как легко видеть из таблицы, уточнение при переходе к оценке  является весьма существенным. С другой стороны, предельная информационная емкость тензорной сети (число правильно воспроизводимых образов) может существенно превышать число нейронов, например, для 10 нейронов тензорная сеть валентности 8 имеет предельную информационную емкость 511.
Легко показать, что если множество векторов  не содержит взаимно обратных, то размерность пространства  равна числу векторов в множестве . Сеть (2) для случая тензорных сетей имеет вид
,		(9)
а ортогональная тензорная сеть
,	(10)
где  - элемент матрицы . Сеть (9) хорошо работает на слабо скоррелированных эталонах, а сеть (10) не чувствительна к степени скоррелированности эталонов.

6. Сети для инвариантной обработки изображений
Для того, чтобы при обработке переводить визуальные образов, отличающиеся только положением в рамке изображения, в один эталон, применяется следующий прием [7]. Преобразуем исходное изображение в некоторый вектор величин, не изменяющихся при сдвиге (вектор инвариантов).  Простейший набор инвариантов дают автокорреляторы  скалярные произведения образа на сдвинутый образ, рассматриваемые как функции вектора сдвига.
В качестве примера рассмотрим вычисление сдвигового автокоррелятора для черно-белых изображений. Пусть дан двумерный образ  размером . Обозначим точки образа как . Элементами автокоррелятора  будут величины , где  при выполнении любого из неравенств . Легко проверить, что автокорреляторы любых двух образов, отличающихся только расположением в рамке, совпадают. Отметим, что  при всех ,  и  при выполнении любого из неравенств . Таким образом, можно считать, что размер автокоррелятора равен .
Автокорреляторная сеть имеет вид
.				(11)
Сеть (11) позволяет обрабатывать различные визуальные образы, отличающиеся только положением в рамке, как один образ.
Подводя итоги, можно сказать, что все сети ассоциативной памяти типа (2) можно получить, комбинируя следующие преобразования:
Произвольное преобразование. Например, переход к автокорреляторам, позволяющий объединять в один выходной образ все образы, отличающиеся только положением в рамке.
Тензорное преобразование, позволяющее сильно увеличить способность сети запоминать и точно воспроизводить эталоны.
Переход к ортогональному проектору, снимающий зависимость надежности работы сети от степени скоррелированности образов.
Наиболее сложная сеть будет иметь вид:
,			(12)
где  - элементы матрицы, обратной матрице Грама системы векторов ,  - произвольное преобразование.

7. Численный эксперимент
Работа ортогональных тензорных сетей при наличии помех сравнивалась с возможностями линейных кодов, исправляющих ошибки. Линейным кодом, исправляющим k ошибок, называется линейное подпространство в n-мерном пространстве над GF2, все вектора которого удалены друг от друга не менее чем на 2k+1 (см., например, [8]). Линейный код называется совершенным, если для любого вектора n-мерного пространства существует кодовый вектор, удаленный от данного не более, чем на k. Тензорной сети в качестве эталонов подавались все кодовые векторы избранного для сравнения кода. Численные эксперименты с совершенными кодами показали, что тензорная сеть минимально необходимой валентности правильно декодирует все векторы. Для несовершенных кодов картина оказалась хуже - среди устойчивых образов тензорной сети появились "химеры" - векторы, не принадлежащие множеству эталонов.
В случае n=10, k=1 (см. табл. 2 и 3, строка 1) при валентностях 3 и 5 тензорная сеть работала как единичный оператор - все входные вектора передавались на выход сети без изменений. Однако уже при валентности 7 число химер резко сократилось и сеть правильно декодировала более 60% сигналов. При этом были правильно декодированы все векторы, удаленные от ближайшего эталона на расстояние 2, а часть векторов, удаленных от ближайшего эталона на расстояние 1, остались химерами. В случае n=10, k=2 (см. табл. 2 и 3, строки 3, 4, 5) наблюдалось уменьшение числа химер с ростом валентности, однако часть химер, удаленных от ближайшего эталона на расстояние 2 сохранялась. Сеть правильно декодировала более 50% сигналов. Таким образом при малых размерностях и кодах, далеких от совершенных, тензорная сеть работает довольно плохо. Однако, уже при n=15, k=3 и валентности, большей 3 (см. табл. 2 и 3, строки 6, 7), сеть правильно декодировала все сигналы с тремя ошибками. В большинстве экспериментов число эталонов было больше числа нейронов.
Подводя итог, можно сказать, что качество работы сети возрастает с ростом размерности пространства и валентности и по эффективности устранения ошибок сеть приближается к коду, гарантированно исправляющему ошибки.
Работа выполнена при поддержке Красноярского краевого фонда науки, грант 6F0124.

Литература
Hopfield J.J. Neural networks and physical systems with emergent collective computational abilities // Proc. Nat. Acad. Sci. USA. 1982. Vol. 79. P.2554-2558.
Горбань А.Н. Обучение нейронных сетей. М.: изд-во СССР-США СП "ParaGraph", 1990. 160 с.
Коноплев В.А., Синицын Е.В. "Моделирование нейронных сетей с максимально высокой информационной емкостью" // Тез. докл. III Всероссийского семинара "Нейроинформатика и ее приложения". Красноярск: изд. КГТУ, 1995 г., с. 66.
Golub D.N. Gorban A.N.   Multi-Particle   Networks   for  Associative  Memory //  Proc.  of  the  World  Congress  on Neural  Networks, Sept.  15-18, 1996,  San Diego,  CA, Lawrence  Erlbaum  Associates, 1996. P. 772-775.
Горбань А.Н., Миркес Е.М. Информационная емкость тензорных сетей // Тез. докл. IV Всероссийского семинара "Нейроинформатика и ее приложения". Красноярск: изд. КГТУ, 1996 г., с. 22-23.
Горбань А.Н., Миркес Е.М. Помехоустойчивость тензорных сетей // Тез. докл. IV Всероссийского семинара "Нейроинформатика и ее приложения". Красноярск: изд. КГТУ, 1996 г., с. 24-25.
Горбань А.Н., Россиев Д.А. Нейронные сети на персональном компьютере. Новосибирск: Наука, 1996. 
Блейхут Р. Теория и практика кодов, контролирующих ошибки. М.: Мир. 1986. 576 с.


 660036,  Красноярск-36,  ВЦК СО РАН E-mail: amse@cckr.krasnoyarsk.su







