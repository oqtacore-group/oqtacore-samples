 5. Нелинейный классификатор. Многослойный персептрон
5.1. Задача исключающего ИЛИ
	Рассмотрим булеву функцию  как некий классификатор. Вектор признаков имеет вид . В данном случае имеется четыре прецедента и два класса. Напомним таблицу значений функции .
№ прецедента				Класс																										Как видно из рисунка тут нельзя построить разделяющую прямую, поскольку выпуклые оболочки точек, относящихся к первому классу и ко второму классу, пересекаются. Следовательно, и линейный классификатор построить нельзя. Попытаемся построить необходимый нелинейный классификатор как суперпозицию несколько линейных. 
Рассмотрим две вспомогательные булевы функции  и . Напомним таблицы значений этих функций:
№ прецедента																															
5.1.1. Построение линейного классификатора функции . Очевидно, что разделяющей прямой является линия:

Соответствующий персептрон имеет вид : 

	
5.1.2.  Построение линейного классификатора функции .  Здесь также можно построить разделяющую прямую: 

Соответствующий персептрон имеет вид : 

	5.1.3. Построение нелинейного классификатора функции .  Пусть на выходе персептрона для функции  - , а на выходе персептрона для функции  - . Посмотрим, какие значения принимает вектор .
Исходные вектора	 OR 	 AND 	 XOR	 							Класс						 1	 						 0	 						 0	 						 1	 		Обозначив классы как показано в таблице, получаем разделяющую прямую, изображенную на рисунке и соответствующий линейный классификатор:

Учитывая вышеизложенное, получаем нелинейный классификатор, который задается через два линейных классификатора, как показано на рисунке слева:
 и 
Соответствующий двухслойный персептрон изображен на рисунке справа.
 
 5.2. Классификационные способности двухслойного персептрона
Рассмотрим общий случай двухслойного персептрона. Пусть  и в скрытом слое  нейронов. Скрытый слой нейронов отображает  в , где  - гиперкуб. Другими словами, каждый нейрон задает гиперплоскость, которая разделяет пространство пополам, т.е.  скрытый слой нейронов делит пространство  на полиэдры. Все вектора из каждого полиэдра отображаются в вершину -мерного единичного куба. Выходной нейрон разделяет вектора в классах, описанных полиэдрами, т.е. производит сечение гиперкуба, полученного в скрытом слое.
Пример. Рассмотрим нейронную сеть с двумя входами () и тремя нейронами (). Тогда пространство . Пусть первый слой нейронов задает разбиение признакового пространства (плоскости) как показано на рисунке. В каждом многоугольнике (возможно, бесконечном) все точки соответствуют одному классу ( A  или  B ). При этом в каждом многоугольнике знаки линейных функционалов  g 1,  g 2,  g 3 остаются постоянными. Следовательно, с каждым многоугольником связано определенное значение вектора выходов нейронов первого слоя, причем для разных многоугольников эти значения различны. Поскольку значениями компонент этого вектора являются 0 либо 1, получаем, что каждому многоугольнику соответствует некоторая вершина единичного куба  в пространстве . При этом каждой вершине куба сопоставлен один класс  A  или  B . На рисунке изображен единичный куб , у которого закрашенные вершины относятся к классу , а не закрашенные - к классу . Задача нейрона второго слоя состоит в разделении вершин этого куба. Нетрудно видеть, что в нашем примере плоскость  является разделяющей для куба . Она и задает параметры нейрона второго слоя. Заметим, что вершина  в кубе не загружена, т.е. в нее не отображается ни один многоугольник. 	
5.3. Трехслойный персептрон
	Внешний (выходной) нейрон реализует лишь одну гиперплоскость. Очевидно, что одна разделяющая гиперплоскость не всегда может обеспечить желаемое разделение вершин гиперкуба. Например, если два конца одной его главной диагонали относятся к классу  A , а два конца другой диагонали - к классу  B . С аналогичной ситуацией мы уже сталкивались в задаче исключающего или. Попробуем ввести еще один слой нейронов.
	Утверждение. Трехслойная нейронная сеть позволяет описать любые разделения объединений полиэдров.
	Доказательство. Рассмотрим первый слой из  p  нейронов. На первом формируются гиперплоскости, т.к. строится полиэдральное разбиение пространства гиперплоскостями. Очевидно, что для заданного конечного множества прецедентов всегда можно построить разбиение пространства признаков на полиэдры такое, что ни в каком полиэдре не окажется пары точек из разных классов. Как было показано выше, первый слой отображает полиэдры в вершины  p -мерного единичного гиперкуба. Поскольку с каждым полиэдром связаны образы одного класса, то и с каждой вершиной гиперкуба связан лишь один класс.
	Каждый нейрон второго слоя описывает сечение полученного гиперкуба. Выберем в качестве таких сечений гиперплоскости, отсекающие ровно одну вершину гиперкуба. Поскольку число вершин в гиперкубе равно 2 p , число нейронов второго слоя также равно 2 p . Таким образом, выход нейронов второго слоя имеет следующий вид. Это вектор размерности 2 p , у которого всегда лишь одно значение равно 1, а остальные равны нулю. Назовем нейроны второго слоя нейронами класса  A  или  B  в соответствии с классом вершины гиперкуба, которую отсекает этот нейрон. Теперь становится понятно, каким образом строить третий слой нейронной сети. Нужно в выходном нейроне третьего слоя реализовать оператор логического сложения выходов нейронов второго слоя, относящихся к классу  A . Таким образом разделяющая гиперплоскость выходного нейрона задается уравнением:
, где  k =2 p , а 

Таким образом, можно построить трехслойный персептрон следующим образом. Нейроны первого слоя разделяют пространство признаков на полиэдры одного класса и отображают их в вершины гиперкуба. Нейроны второго слоя отсекают вершины гиперкуба. Нейрон третьего слоя собственно осуществляет классификацию через оператор логического сложения. Тем самым утверждение доказано. 
ч.т.д.
Рассмотрим, как строится уравнение гиперплоскости, отсекающей вершину -мерного единичного гиперкуба. Диагональ куба имеет длину . Длины диагоналей -мерных единичных гиперкубов, являющихся боковыми гранями -мерного куба, равны .  Центр куба находится в точке . Расстояние от центра куба до любой вершины равно . Плоскость проводим перпендикулярно главной диагонали куба, инцидентной вершине, которую надо отсечь,  так, чтобы расстояние от этой вершины до секущей плоскости было равно , причем данная точка должна находиться на диагонали куба, проведенной к отсекаемой вершине.
	Пусть  - отделяемая вершина,  - диагонально противоположная вершина (, где  E  обозначает -мерный вектор, состоящий из единиц). Следовательно,  - направляющий вектор разделяющей гиперплоскости. Тогда гиперплоскость проходит через точку:

Обозначим : 

Тогда 
  
и уравнение гиперплоскости запишется в виде: .
5.4. Построение нейронной сети 
	Существует два подхода к задаче построения нейронной сети-классификатора. Первый подход заключается в построении сети, варьируя архитектуру. Данный метод основан на точной классификации прецедентов. Второй подход состоит в подборке параметров (весов и порогов) для сети с заданной архитектурой.
	5.4.1 Алгоритм, основанные на точной классификации множества прецедентов. Опишем общую идею метода. За основу берется один нейрон. Далее наращиваем нейрон, пока не получим правильную классификацию всех прецедентов.

	Рассмотрим более подробно алгоритм. Начинаем с одного нейрона , называемого мастером. После его тренировки получаем разделение множества  на  и . Если  содержит вектора из двух классов, то вводим новый узел , называемый последователем.
	Таким образом, на первом слое нейронов находится один мастер и несколько последователей. Никакие вектора из разных классов не имеют одинакового выхода из первого слоя.
  ,
где  - отображение, задаваемое первым слоем.
	Аналогичным образом строим второй слой, третий слой и т.д.
	Утверждение. При правильном выборе весов каждый очередной слой правильно классифицирует все вектора, которые правильно классифицировал мастер и еще хотя бы один вектор.
	Таким образом, получаем архитектуру, имеющую конечное число слоев, правильно классифицирующие все прецеденты.
	4.1.1. Алгоритм ближайших соседей.  Нейроны первого слоя - это биссекторы, разделяющие пары. Второй слой - нейроны , определяющие полиэдры. Третий слой - нейроны , определяющие классы.
	Основным недостатком данного метода является слишком большое количество нейронов. Уменьшить количество нейронов можно путем удаления внутренних ячеек:
.
	4.2. Алгоритм, основанный на подборе весов для сети с заданной архитектурой. Идея данного метода состоит в том, чтобы ввести критерий в виде функции стоимости, которую необходимо минимизировать.
	Пусть 
 - число слоев в сети; 
 - число нейронов в слое , где ; 
 - число выходных нейронов; 
 - размер входа;
   - входной вектор признаков;
 а - выходной вектор, который должен быть правильно классифицирован.
	 Текущем состоянии сеть при обучении дает результат  не совпадающий с . Обозначим:
  ,
где  - число прецедентов;  - ошибка на -ом прецеденте;
,
где .  - функция всех синоптических весов и порогов. Таким образом, целью обучения является решение оптимизационной задачи:
,
где  - множество синоптических весов.

	Пусть  - выход -ого нейрона -ого слоя;  - весовой вектор (включая порог) -ого нейрона в -ом слое, т.е. , где  а - число нейронов в -ом слое. Таким образом,  - разрывная функция  переменных, где

 разрывна, т.к. разрывна функция активации :

 	 4.2.1 Алгоритм обратной волны. Суть - аппроксимация  непрерывной дифференцируемой функцией за счет замены функции активации "сигмовидной" функцией:

Вычислим производную функции : 

	При данном чисто формальном приеме вектора признаков уже могут отображаться не только в вершины, но и внутрь гиперкуба. Необходимо решить задачу минимизации : 

4.2.1.1. Метод градиентного спуска решения задачи минимизации. Пусть . Тогда метод градиентного спуска выглядит так:
,
где  а - шаг градиентного спуска. Очевидно, для его реализации необходимо уметь градиент .
	4.2.1.2. Вычисление градиента. Аргумент функции активации -ого нейрона -ого слоя

принимает различные значения в зависимости от индекса прецедента. В данном случае .
Во входном слое, при  , . В выходном слое, при  , .
	Рассмотрим выходной слой .


 - не зависит от -ого номера нейрона в слое, т.е. имеем одинаковый вектор производных для всех нейронов -ого слоя.

Следовательно, для последнего слоя 
	Рассмотрим скрытый слой . Имеется зависимость: 


,
но , следовательно : 


Сумма, заключенная в квадратных скобках, известна из предыдущего шага.
	4.2.1.3. Описание алгоритма.
0. Начальное приближение. Случайно выбираются веса небольших значений:   , , , .
1. Прямой проход. Для каждого вектора прецедента ,  вычисляются все , , , . Вычисляется текущее значение ценовой функции :
Цикл по  (по прецедентам):
	Вычислить: 
  , .
	.
	Цикл по  (по слоям):
		Цикл по  (по нейронам в слое):
			 
			
		 Конец цикла по .
	Конец цикла по .
Конец цикла по .

2. Обратный проход. Для каждого значения  и  вычисляется . Затем последовательно необходимо вычислить  для всех  и :
Цикл по  (по нейронам в слое):
	Вычислить : 
	
	
	Цикл по  (по слоям):
		Цикл по  (по нейронам в слое):
			
			
		Конец цикла по .
	Конец цикла по .
Конец цикла по .
3. Пересчет весов. Для всех  и  , где .
Останов алгоритма может происходить по двум критериям: либо  стала меньше порога, либо градиент стал очень мал.
От выбора  зависит скорость сходимости. Если  мало, то скорость сходимости также мала. Если  велико, то и скорость сходимости высока, но при такой скорости можно пропустить .
В силу много экстремальности существует возможность спустить в локальный минимум. Если данный минимум по каким-то причинам не подходит, надо начинать алгоритм с другой случайной точки.
Данный алгоритм быстрее, чем алгоритм с обучением.
