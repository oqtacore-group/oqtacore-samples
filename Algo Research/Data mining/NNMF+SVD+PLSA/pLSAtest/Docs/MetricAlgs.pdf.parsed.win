
Лекции по метрическим алгоритмам: классификация, кластеризация, регрессия, многомерное шкалирование К. В. Воронцов 14 сентября 2006 г. Содержание 1 Метрические алгоритмы 2 1.1 Метрические алгоритмы классификации . . . . . . . . . . . . . . . . . . 3 1.1.1 Метод ближайших соседей и его обобщения . . . . . . . . . . . . 3 1.1.2 Метод потенциальных функций . . . . . . . . . . . . . . . . . . . 6 1.1.3 Отбор эталонных объектов . . . . . . . . . . . . . . . . . . . . . . 6 1.1.4 Взвешенный kNN . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.1.5 Быстрый поиск ближайших соседей. . . . . . . . . . . . . . . . . 9 1.1.6 Сравнение метрических методов классификации . . . . . . . . . 9 1.2 Алгоритмы кластеризации . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.2.1 Эвристические графовые алгоритмы . . . . . . . . . . . . . . . . 11 1.2.2 Функционалы качества кластеризации . . . . . . . . . . . . . . . 13 1.2.3 Статистические алгоритмы . . . . . . . . . . . . . . . . . . . . . . 14 1.2.4 Иерархическая кластеризация . . . . . . . . . . . . . . . . . . . . 17
- 2 - 1 Метрические алгоритмы Во многих прикладных задачах измерять степень сходства объектов существен- но проще, чем формировать признаковые описания. Например, легче посмотреть на две фотографии (две подписи, два отпечатка пальца) и сказать, что они при- надлежат одному и тому же человеку, чем понять, на основании каких признаков они схожи. Такие ситуации часто возникают при распознавании изображений, вре- менных рядов или символьных последовательностей. Они характеризуются тем, что исходные ?сырые? данные не годятся в качестве признаковых описаний, но в то же время, существуют эффективные и содержательно обоснованные способы оценить степень сходства любой пары ?сырых? описаний. Есть и вторая, более существенная, характерная особенность этих задач. Ес- ли мера сходства введена достаточно удачно, то оказывается, что схожим объектам, как правило, соответствуют схожие ответы. В задачах восстановления регрессии это означает, что целевая функция y?(x) является достаточно гладкой. В случае класси- фикации это означает, что схожие объекты гораздо чаще лежат в одном классе, чем в разных. Если задача в принципе поддаётся решению, то граница между классами не может проходить везде, классы должны образовывать компактно локализованные подмножества в пространстве объектов. Это предположение принято называть гипо- тезой компактности1. Фактически, это предположение о том, что задача является ?достаточно хорошей?. Для формализации понятия ?сходства? вводится функция расстояния или мет- рика ?(x, x') в пространстве объектов X. Алгоритмы, основанные на анализе сход- ства объектов, называют метрическими. В этой главе будут рассмотрены различные типы задач, в которых основную роль играет информация о сходстве объектов. В :1.1 рассматриваются метрические алгоритмы классификации. Задачи кластеризации или обучения без учителя отличаются тем, что обуча- ющие объекты изначально не приписаны классам, и классификация производится только на основе сходства объектов друг с другом. В :1.2 рассматриваются стати- стические, иерархические и графовые алгоритмы кластеризации. К метрическим алгоритмам восстановления регрессии относятся алгоритмы ядерного сглаживания, рассматриваемые в ??. В ?? рассматриваются методы многомерного шкалирования, позволяющие вос- станавливать признаковые описания объектов по матрице попарных расстояний меж- ду ними. В ?? рассматриваются проблемы, общие для многих метрических алгорит- мов анализа данных. Во-первых, проблема эффективности. Анализ всех попарных расстояний между ? объектами требует затрат времени и памяти не менее O(?2). На сверхбольших данных (более нескольких тысяч объектов) эти затраты становятся неприемлемыми. Эффективные алгоритмы с неизбежностью должны быть субквад- ратичными и использовать лишь небольшую долю попарных расстояний. Во мно- гих эффективных метрических алгоритмах применяются быстрые алгоритмы поиска ближайших объектов. Вторая проблема как строить ?хорошие? метрики, адекват- ные решаемой задаче. 1 В математическом анализе компактными называются ограниченные замкнутые множества. В данном случае термин компактность не имеет ничего общего с этим определением, и должен пониматься скорее в ?бытовом? смысле этого слова.
- 3 - :1.1 Метрические алгоритмы классификации 1.1.1 Метод ближайших соседей и его обобщения Для произвольного объекта u ? X расположим элементы обучающей выбор- ки X? = {x1, . . . , x?} в порядке возрастания расстояний до u: ?(u, x1,u) 6 ?(u, x2,u) 6 . . . 6 ?(u, x?,u), где через xi,u обозначается i-й сосед объекта u. Аналогичное обозначение введём и для ответа на i-м соседе: yi,u = y?(xi,u). Каждый объект u ? X порождает свою перенумерацию выборки X? = {x1,u, . . . , x?,u}. Алгоритм ближайшего соседа (nearest neighbor, NN) является самым простым алгоритмом классификации. Он относит классифицируемый объект u ? X? к тому классу, которому принадлежит ближайший обучающий объект: a(u;X?) = y1,u. Обучение NN сводится к элементарному запоминанию выборки X?. Других пара- метров этот алгоритм не имеет. Качество классификации целиком определяется тем, насколько удачно выбрана метрика ?. Алгоритм k ближайших соседей (k nearest neighbors, kNN). При наличии по- грешностей в данных принимать решения на основании единственного соседа весьма опрометчиво. На практике погрешности есть всегда. Даже если данные измерены абсолютно точно (изредка встречаются и такие задачи), остаются погрешности, свя- занные с недостаточной адекватностью самой метрики ?, которая, являясь лишь некоторой моделью сходства объектов, не может быть идеальной. Идея усреднения погрешностей путём голосования приводит к правилу k бли- жайших соседей. Каждый из соседей xi,u, i = 1, . . . , k голосует за отнесение объекта u к классу yi,u. В результате объект u относится к тому классу, которому принадлежит большинство из k ближайших к нему объектов обучающей выборки: a(u;X?, k) = argmax y?Y Xk i=1 [yi,u = y]. Алгоритм имеет параметр k, который либо задаётся априори, либо оптимизи- руется по критерию скользящего контроля. Чаще всего используется контроль с ис- ключением объектов по одному (leave-one-out, LOO): для каждого объекта xi ? X? проверяется, правильно ли он классифицируется по своим k ближайшим соседям. Выбирается то значение k, при котором число ошибок классификации минимально: LOO(k,X?) = X? i=1 ?a!xi;X?\{xi}, k? 6= yi¤ > min k . Аналогичный функционал применялся в разделе ?? для оптимизации ширины окна при непараметрическом восстановлении плотности.
- 4 - Недостаток kNN в том, что максимальная сумма голосов может достигаться на нескольких классах одновременно. В задачах с двумя классами этого можно избе- жать, если брать только нечётные значения k. Более общая тактика, которая годится и для случая многих классов ввести строго убывающую последовательность ве- щественных весов wi, задающих вклад i-го соседа в классификацию: a(u;X?, k) = argmax y?Y Xk i=1 [yi,u = y]wi. Выбор последовательности wi является эвристикой. Если взять веса, убывающие линейно wi = 1 ? 1k (i ? 1), то неоднозначности также могут возникать, хотя и ре- же (пример: классов два; первый и четвёртый сосед голосуют за один класс, вто- рой и третий за другой; суммы голосов совпадают). Неоднозначность устраняется окончательно, если взять нелинейную последовательность, скажем, геометрическую прогрессию: wi = !1 ? 1k ?i?1. Другой вариант задать веса wi не как функцию от ранга соседа i, а как функцию от расстояния ?(u, xi,u). Это приводит к методу парзеновского окна. Метод парзеновского окна. Введём функцию ядра K(z), убывающую на [0,?), и рассмотрим алгоритм a(u;X?, h,K) = argmax y?Y X? i=1 [yi,u = y]Ku?(u, xi,u) h 
. Это ни что иное, как байесовский алгоритм классификации, основанный на непа- раметрических оценках плотности Парзена-Розенблатта с шириной окна h, и опи- санный в разделе ??. Параметр h можно либо задавать априори, либо определять по скользящему контролю. Ещё лучше ввести переменную ширину окна. Возьмём функцию ядра K, убывающую на отрезке [0, 1], и равную нулю вне его. Определим h как наибольшее число, при котором ровно k ближайших соседей объекта u получают ненулевые веса. Тогда h(u) = ?(u, xk+1,u), a(u;X?, k,K) = argmax y?Y Xk i=1 [yi,u = y]Ku ?(u, xi,u) ?(u, xk+1,u)
. Это байесовский алгоритм с парзеновской оценкой плотности по окну переменной ширины h(u). Параметрами алгоритма являются число соседей k и вид ядра K. Выбор этих параметров по критерию скользящего контроля обсуждался в ??. Таким образом, метрические и статистические алгоритмы классификации тес- но связаны. С одной стороны, при парзеновском оценивании плотности байесовский классификатор можно рассматривать как одно из обобщений алгоритма ближайших соседей. С другой стороны, метрические алгоритмы получают дополнительное обос- нование через байесовский классификатор, который, как известно, является опти- мальным. Правда, для оптимальности необходимо, чтобы непараметрические оцен- ки плотности точно описывали функции правдоподобия (плотности распределения) каждого из классов. Это очень сильное требование, которое опять-таки упирается в вопрос об адекватности выбранной метрики ?. На практике вряд ли возможно по- строить метрику, которая была бы идеальной моделью сходства реальных объектов.
- 5 - Обобщенный метрический классификатор. Описанные выше алгоритмы клас- сификации являются частными случаями одной общей формулы. Пусть задана весовая функция w(i, u), которая оценивает степень важности i-го соседа для классификации объекта u. Естественно полагать, что эта функция неотрицательна и не возрастает по i. Согласно гипотезе компактности чем меньше i, тем ближе объекты u и xi,u, тем выше шансы, что они принадлежат одному классу. Опр. 1.1. Метрическим алгоритмом классификации с обучающей выборкой X? будем называть отображение вида a(u;X?) = argmax y?Y !y(u,X?), (1.1) где !y(u,X?) суммарный вес обучающих объектов класса y ? Y в окрестности классифицируемого объекта u ? X: !y(u,X?) =X? i=1 [yi,u = y]w(i, u). Таким образом, алгоритм a относит объект u к тому классу, элементы которого преобладают в окрестности u. Выбирая весовую функцию w(i, u), можно получать различные типы метриче- ских алгоритмов классификации: w(i, u) = [i = 1]; ближайший сосед; w(i, u) = [i 6 k]; k ближайших соседей; w(i, u) = [i 6 k] (1 ? 1/k)i?1 ; k взвешенных ближайших соседей; w(i, u) = K!?(u, xi,u)/h?; парзеновское окно ширины h; w(i, u) = Ku ?(u, xi,u) ?(u, xk+1,u)
; парзеновское окно переменной ширины. Обучающая выборка X? играет роль параметра алгоритма a. Его настройка, как правило, сводится к тривиальному копированию выборки. В некоторых случа- ях может производиться отсев неинформативных объектов (см. далее), однако сами объекты не подвергаются обработке и сохраняются ?как есть?. По этой причине метрические алгоритмы называют рассуждением на основе прецедентов (case-based reasoning, CBR). Здесь действительно можно говорить о ?рассуждениях?, так как на вопрос ?почему объект u был отнесён к классу y?? алгоритм может дать чёткий ответ: ?потому, что имеются прецеденты схожие с ним объекты, принадлежащие классу y?, и предъявить список этих прецедентов. Достоинства простейших метрических алгоритмов типа kNN. : Простота реализации и возможность введения различных модификаций. : Возможность интерпретировать классификацию объекта путём предъявления пользователю ближайшего объекта или нескольких. ?Прецедентная? логика работы алгоритма хорошо понятна экспертам в таких предметных областях, как медицина, биометрия, юриспруденция, и др.
- 6 - Недостатки простейших метрических алгоритмов. : Приходится хранить обучающую выборку целиком. Это приводит к неэффек- тивному расходу памяти и чрезмерному усложнению решающего правила. При наличии погрешностей (как в исходных данных, так и в модели сходства ?), это может приводить к понижению точности классификации вблизи границы классов. Имеет смысл выбирать минимальное подмножество опорных объектов, действительно необходимых для классификации. : Поиск ближайшего соседа ?в лоб? требует сравнения классифицируемого объ- екта со всеми объектами выборки за O(?) операций. Для задач с большими выборками это может оказаться накладно. Проблема решается с помощью эф- фективных алгоритмов поиска ближайших соседей, которые осуществляют по- иск в среднем за O(ln ?) операций. : В исходном виде модель алгоритмов kNN крайне бедна. Она имеет только сво- бодный параметр k, да и тот дискретный с небольшим числом разумных аль- тернатив. Для обогащения модели необходимо вводить веса объектов и/или параметризовать способ вычисления метрики. Продолжим рассмотрение различных видов метрических алгоритмов, в кото- рых делается попытка устранить обозначенные выше недостатки. 1.1.2 Метод потенциальных функций 1.1.3 Отбор эталонных объектов Обычно объекты обучения не являются равноценными. Среди них могут нахо- диться типичные представители классов эталоны. Близость классифицируемого объекта к эталону является веским доводом в пользу его отнесения к данному классу. Кроме того, выборка может содержать большое количество объектов, плотно окру- жённых другими объектами того же класса. Если их удалить из выборки, это прак- тически не отразится на работе метрического классификатора. Наконец, в выборку может попасть некоторое количество шумовых выбросов объектов, находящих- ся ?в гуще? чужого класса. Как правило, их удаление только улучшает качество классификации. Эти соображения приводят к идее исключить из выборки шумовые и неинфор- мативные объекты, оставив только минимальное достаточное количество эталонов. Это позволит повысить качество классификации, сократить объём хранимых данных и уменьшить время классификации. Идея отбора эталонов реализована в алгоритме STOLP [1]. Мы рассмотрим его обобщенный вариант с произвольной весовой функцией w(i, u), см. Алгоритм 1.1. Множество эталонов - ? X? формируется путём последовательного ?жадного? добавления объектов. Сначала в - заносится по одному наиболее типичному предста- вителю от каждого класса (шаг 4). Затем начинается процесс добавлений, на каждом шаге которого к - присоединяется объект xi, имеющий минимальное отрицательное значение отступа (margin) M(xi, -) = !yi(xi, -) ? max y?-\{yi} !y(xi, -).
- 7 - Алгоритм 1.1. Отбор эталонных объектов STOLP Вход: X? обучающая выборка; ? порог фильтрации выбросов; ? допустимая доля ошибок; Выход: Множество опорных объектов - ? X?; 1: для всех xi ? X? фильтрация выбросов: 2: если M(xi,X?) < ?? то 3: X??1 := X? \ {xi}; ? := ? ? 1; 4: Инициализация: взять по одному представителю от каждого класса: - := narg max xi?X? !y(xi,X?)??y ? Y o; 5: пока - 6= X?; 6: Выделить множество напряжённых объектов: E := {xj ? X? \ - : M(xj , -) < 0}; 7: если |E| < ?? то 8: выход; 9: Найти наиболее напряжённый объект и занести его в -: xi := arg min xj?X?\-M(xj , -); - := - ? {xi}; Значение отступа можно рассматривать как расстояние от объекта xi до поверх- ности, разделяющей классы. Отрицательный отступ означает, что алгоритм a(xi; -) ошибается на объекте xi. Абсолютное значение отрицательного отступа как раз и ха- рактеризует величину ошибки или ?напряжённость? данного объекта. Процесс добавлений продолжается до тех пор, пока в выборке остаётся значи- тельная доля объектов с отрицательным отступом. Этот алгоритм имеет два недостатка. Первый недостаток чувствительность к выбросам. Выбросы это объекты, у которых большинство соседей принадлежат другим классам. Они могут появляться из-за ошибок в исходных данных, либо по причине недостаточной адекватности мет- рики. В любом случае лучшее, что можно сделать с выбросом удалить его из вы- борки, так как он скорее мешает настраивать алгоритм, чем помогает. Однако в на- шем случае выбросы обладают наибольшей напряженностью, и на первых же шагах алгоритма попадают в -. Это отрицательно сказывается на качестве классификации. Кроме того, для нейтрализации влияния выброса на следующих шагах приходится включать в число эталонов и некоторое количество его соседей. Что приводит к из- быточному разрастанию множества -. Проблема решается путём предварительной фильтрации выбросов (шаги 1-3). Перед началом процесса из выборки X? исключа- ются все объекты xi с отступом M(xi,X?), меньшим заданного порога ?. Как вариант, можно исключать заданную долю объектов с наименьшими значениями отступа. Вторая проблема относительно низкая эффективность алгоритма. Для выбо- ра наиболее напряжённого объекта необходимо перебрать множество объектов X?\-,
- 8 - и для каждого вычислить отступ. Общее число операций составляет O(|-|?k), в худ- шем случае O(?2k). Для ускорения алгоритма можно добавлять сразу по несколько наиболее напряжённых объектов, выбирая их достаточно далеко друг от друга, что- бы добавление любого из них не влияло на принятие решения о добавлении любого другого. Реализация этой идеи не включена в Алгоритм 1.1, чтобы не загромождать его техническими подробностями. Результатом работы алгоритма STOLP является разбиение обучающих объ- ектов на три категории: шумовые, эталонные и неинформативные. Если гипотеза компактности верна и выборка достаточно велика, то основная масса обучающих объектов окажется неинформативной и будет отброшена. Фактически, произойдёт сжатие исходных данных. 1.1.4 Взвешенный kNN Альтернативный подход заключается в том, чтобы не отбирать объекты жёстко, а присвоить каждому обучающему объекту xi некоторый неотрицательный вес ?(xi). Чем более информативен объект, тем больше должен быть его вес. Определим весовую функцию w(i, u) так, чтобы каждый объект xi учитывался со своим весом ?(xi): w(i, u) = ?(xi,u) ew(i, u), где функция ew(i, u) неотрицательна и не возрастает по i. В простейшем случае ew(i, u) = [i 6 k], и тогда имеем алгоритм kNN, в котором вес i-го соседа xi,u за- висит только от самого этого соседа, но ни от номера i, ни от расстояния ?(u, xi,u). Чтобы настроить значения параметров ?(xi) по обучающей выборке, выпишем условие корректности алгоритма на объектах обучения: a(xi) = yi, i = 1, . . . , ?. Эта система из ? равенств равносильна системе из ?!|Y | ? 1? линейных нера- венств относительно весов ?(xi): X? s=2 ?(xs,xi) ew(s, xi)![ys,xi = yi] ? [ys,xi = y]? > 0, i = 1, . . . , ?, y ? Y \ {yi}. Здесь суммирование начинается с 2, чтобы сам объект xi не попадал в число своих ближайших соседей. Запишем систему неравенств в матричном виде. Введём вектор-столбец весов ? = (?(x1), . . . , ?(x?))T. Заполним ? матриц Bi = (biyj! ), i = 1, . . . , ?, имеющих размер |Y |?1???, следующим образом. Для каждой тройки индексов (i, y, s), пробегающих значения i = 1, . . . , ?, y ? Y \ {yi}, s = 2, . . . , ?, найдём объект xj = xs,xi , который является s-м соседом объекта xi. Положим biyj = ??? ?? ew(s, xi), если yj = yi; ?ew(s, xi), если yj = y; 0, в противном случае.
- 9 - В матричном представлении система неравенств относительно ? примет вид (Bi? > 0, i = 1, . . . , ?; ? > 0. В общем случае эта система может оказаться несовместной. Нарушение нера- венства, соответствующего паре индексов (i, y) означает, что алгоритм a(u) допус- кает ошибку на обучающем объекте xi, выдавая ответ y вместо yi. Таким образом, минимизация числа ошибок на обучающей выборке сводится к задаче поиска мак- симальной совместной подсистемы в системе Bi? > 0 при обязательном выполнении ограничений ? > 0. Для решения данной задачи применимы методы построения линейной разделяющей поверхности с неотрицательными коэффициентами, в част- ности, модификация метода опорных векторов Non-Negative SVM, описанная в раз- деле ??.??. В результате решения системы неравенств веса некоторых объектов могут при- нять нулевые значения. Обычно ими оказываются шумовые выбросы, только ухуд- шающие качество классификации. Имеет смысл вообще исключить их из обучающей выборки X? и не учитывать при классификации. 1.1.5 Быстрый поиск ближайших соседей. 1.1.6 Сравнение метрических методов классификации :1.2 Алгоритмы кластеризации Задача кластеризации (или обучения без учителя) заключается в следующем. Имеется обучающая выборка X? = {x1, . . . , x?} ? X и функция расстояния между объектами ?(x, x'). Требуется разбить выборку на непересекающиеся подмножества, называемые кластерами, так, чтобы каждый кластер состоял из схожих объектов, а объекты разных кластеров существенно отличались. При этом каждому объек- ту xi ? X? приписывается метка (номер) кластера yi. Алгоритм кластеризации это функция a: X > Y , которое любому объек- ту x ? X ставит в соответствие метку кластера y ? Y . Множество меток Y в неко- торых случаях известно заранее, однако чаще ставится задача найти минимальное число кластеров, при котором удовлетворяется некоторый критерий качества кла- стеризации. Решение задачи кластеризации принципиально неоднозначно, и тому есть несколько причин. Во-первых, результат кластеризации существенно зависит от мет- рики ?, выбор которой, как правило, субъективен и определяется экспертом. Во- вторых, не существует однозначно наилучшего критерия качества кластеризации. Известен целый ряд достаточно разумных критериев, а также ряд алгоритмов, не имеющих чётко выраженного критерия, но осуществляющих достаточно разум- ную кластеризацию ?по построению?. Все они приводят к немного разным класте- ризациям. В-третьих, число кластеров, как правило, неизвестно заранее и устанав- ливается в соответствии с некоторым субъективным критерием.
- 10 - Кластеризация (обучение без учителя) отличается от классификации (обучения с учителем) тем, что метки исходных объектов yi изначально не заданы, и даже может быть неизвестно само множество Y . В этом смысле задача кластеризации ещё в большей степени некорректно поставленная, чем задача классификации. Цели кластеризации могут быть различными в зависимости от особенностей кон- кретной прикладной задачи: : Понять структуру множества объектов X?, разбив его на группы схожих объ- ектов. Упростить дальнейшую обработку данных и принятия решений, обраба- тывая каждый кластер индивидуально (стратегия ?разделяй и властвуй?). : В случае сверхбольшой выборки X? сократить объём хранимых данных, оста- вив по одному наиболее типичному представителю от каждого кластера. : Выделить нетипичные объекты, которые не присоединяются ни к одному из кластеров. Эту задачу называют одноклассовой классификацией, обнару- жением нетипичности или обнаружением новизны (novelty detection). В первом случае число кластеров стараются сделать поменьше. Во втором слу- чае важнее обеспечить высокую степень сходства объектов внутри каждого класте- ра, а кластеров может быть сколько угодно. В третьем случае наибольший интерес представляют кластеры из одного-двух объектов. Во всех этих случаях может применяться иерархическая кластеризация, когда крупные кластеры дробятся на более мелкие, те в свою очередь дробятся ещё мель- че, и т. д. Такие задачи называются задачами таксономии (taxonomy). Результатом таксономии является не простое разбиение множества объектов на кластеры, а дре- вообразная иерархическая структура. Вместо номера кластера объект характеризу- ется перечислением всех кластеров, которым он принадлежит, обычно от крупного к мелкому. Классическим примером таксономии на основе сходства является систе- матизация живых существ, предложенная Карлом Линнеем ещё в середине XVIII века. В современном представлении биологическая иерархия имеет около 30 уров- ней, основные из них: царство, тип, класс, отряд, семейство, колено (триба), род, вид, секция. Такого рода систематизации приходится строить во многих областях знания, чтобы упорядочить информацию о большом количестве объектов. Типы кластерных структур. Попытаемся составить реестр различных типов кла- стерных структур, которые могут возникать в задачах кластеризации. Различные алгоритмы кластеризации могут быть более или менее успешны в этих ситуациях. Простые алгоритмы, как правило, узко специализированы и да- ют адекватные результаты только в одной-двух ситуациях. Более сложные алгорит- мы, такие как FOREL или агломеративная процедура Ланса-Вильямса, справляются с несколькими типами ситуаций. Однако создание алгоритма, успешно работающего во всех ситуациях без исключения, представляется трудной и едва ли разрешимой задачей. Современный обзор по методам кластеризации [6].
- 11 - 1.2.1 Эвристические графовые алгоритмы Обширный класс алгоритмов кластеризации основан на представлении выбор- ки в виде графа. Вершинам графа соответствуют объекты выборки, а рёбрам попарные расстояния между объектами ?ij = ?(xi, xj). Достоинством графовых алгоритмов кластеризации является наглядность, от- носительная простота реализации, возможность вносить различные усовершенство- вания, опираясь на простые геометрические соображения. Алгоритм выделения связных компонент. Задаётся параметр R и в графе удаля- ются все рёбра (i, j), для которых ?ij > R. Соединёнными остаются только наиболее близкие пары объектов. Идея алгоритма заключается в том, чтобы подобрать такое значение R ? [min ?ij ,max ?ij ], при котором граф развалится на несколько связных компонент. Найденные связные компоненты и есть кластеры. Связной компонентой графа называется подмножество его вершин, в котором любые две вершины можно соединить путём, целиком лежащим в этом подмноже- стве [cite?]. Для поиска связных компонент можно использовать стандартные алго- ритмы поиска в ширину (алгоритм Дейкстры) или поиска в глубину [cite?]. Для подбора параметра R обычно рекомендуется построить гистограмму рас- пределения попарных расстояний ?ij . В задачах с выраженной кластерной струк- турой эта гистограмма имеет два чётких пика: зона небольших внутриклассовых расстояний и зона больших межклассовых расстояний. Параметр R задаётся как расстояние, соответствующее точке минимума между этими пиками [3]. Отметим два недостатка этого алгоритма. Ограниченная применимость. Алгоритм выделения связных компонент наи- более подходит для выделения кластеров типа сгущений или лент. Наличие разре- женного фона или ?узких перемычек? между кластерами приводит к неадекватной кластеризации. Плохая управляемость числом кластеров. Для многих приложений удобнее задавать не параметр R, а число кластеров или некоторый порог ?чёткости класте- ризации?. Управлять числом кластеров с помощью параметра R довольно затрудни- тельно. Приходится несколько раз решать задачу при разных R, что отрицательно сказывается на временных затратах. Алгоритм кратчайшего незамкнутого пути строит граф из ??1 рёбер так, чтобы они соединяли все ? точек и обладали минимальной суммарной длиной. Такой граф называется кратчайшим незамкнутым путём (КНП), минимальным покрывающим деревом или каркасом. Доказано [cite? Прим, 1967], что этот граф строится с помо- щью несложной процедуры, соответствующей шагам 1-4 Алгоритма 1.2. На шаге 5 удаляются K?1 самых длинных рёбер, и связный граф распадается на K кластеров. В отличие от предыдущего алгоритма, число кластеров K задаётся как входной параметр. Его можно также определять графически, если упорядочить все рассто- яния, образующие каркас, в порядке убывания и отложить их на графике. Резкий скачок вниз где-то на начальном (левом) участке графика покажет количество наи- более чётко выделяемых кластеров. Этот алгоритм, как и предыдущий, очень прост и также имеет ограниченную применимость. Наличие разреженного фона или ?узких перемычкек? между класте-
- 12 - Алгоритм 1.2. Алгоритм кратчайшего незамкнутого пути (КНП) 1: Найти пару точек (i, j) с наименьшим ?ij и соединить их ребром; 2: пока в выборке остаются изолированные точки 3: найти изолированную точку, ближайшую к некоторой неизолированной; 4: соединить эти две точки ребром; 5: удалить K ? 1 самых длинных рёбер; рами приводит к неадекватной кластеризации. Другим недостатком КНП является высокая трудоёмкость для построения кратчайшего незамкнутого пути требуется O(?3) операций. Алгоритм FOREL (ФОРмальный ЭЛемент) предложен Загоруйко и Ёлкиной в 1967 при решении одной прикладной задачи в области палеонтологии. Алгоритм имеет много вариаций, подробно описанных в [2, 1]. В основе всех этих вариаций лежит следующая базовая процедура. Пусть задана некоторая точка x0 ? X и параметр R. Выделяются все точки выборки xi ? X?, попадающие внутрь сферы ?(xi, x0) 6 R, и точка x0 переносится в центр тяжести выделенных точек. Эта процедура повторяется до тех пор, пока состав выделенных точек, а значит и положение центра, не перестанет меняться. Доказано, что эта процедура сходится за конечное число шагов. При этом сфера перемещается в место локального сгущения точек. Центр сферы x0 в общем случае не является объектом выборки, потому и называется формальным элементом. Для вычисления центра необходимо, чтобы множество объектов X было не только метрическим, но и линейным векторным пространством. Это требование естественным образом выполняется, когда объекты описываются числовыми при- знаками. Однако существуют задачи, в которых изначально задана только метрика, а сложение и умножение на число не определены на X. Тогда в качестве центра сфе- ры можно взять тот объект обучающей выборки, для которого среднее расстояние до других объектов кластера минимально. Соответственно, шаг 6 заменяется на x0 := argmin x?K0 Xx'?K0 ?(x, x'). При этом заметно увеличивается трудоёмкость алгоритма. Если в линейном про- странстве для вычисления центра требуется O({) операций, то в метрическом O({2), где { чисто точек в кластере. Можно немного сэкономить, если заметить, что пересчёт центра при добавлении/удалении отдельной точки кластера требует лишь O({) операций. Различные варианты алгоритма FOREL отличаются способами объединения сфер в кластеры, способами варьирования параметра R, способами выбора началь- ного приближения для точек x0. В Алгоритме 1.3 представлен один из вариантов, в котором сферы строятся последовательно. На шаге 9 к центрам этих сфер применя- ется алгоритм КНП. С одной стороны, это решает проблему низкой эффективности КНП, так как сфер гораздо меньше, чем исходных объектов. С другой стороны, мы получаем более тонкую, двухуровневую, структуру кластеров: каждый кластер верхнего уровня распадается на более мелкие подкластеры нижнего уровня.
- 13 - Алгоритм 1.3. Алгоритм FOREL 1: Инициализировать множество некластеризованных точек: U := X?; 2: пока в выборке есть некластеризованные точки, U 6= ?: 3: взять произвольную точку x0 ? U случайным образом; 4: повторять 5: образовать кластер сферу с центром в x0 и радиусом R: K0 := {xi ? U | ?(xi, x0) 6 R}; 6: поместить центр сферы в центр масс кластера: x0 := 1 |K0| Pxi?K0 xi; 7: пока центр x0 не стабилизируется; 8: пометить все точки K0 как кластеризованные: U := U \ K0; 9: применить алгоритм КПН к множеству центров всех найденных кластеров; 10: каждый объект xi ? X? приписать кластеру с ближайшим центром; Другое преимущество этого алгоритма возможность описывать кластеры произвольной геометрической формы. Варьируя параметр R, можно получать кла- стеризации различной степени детальности. Если кластеры близки по форме к ша- рам, можно сделать R достаточно большим. Для описания кластеров более сложной формы следует уменьшать R. Алгоритм 1.3 довольно чувствителен к выбору начального положения точки x0 для каждого нового кластера. Для устранения этого недостатка в [1] предлагается генерировать несколько (порядка 10..20) кластеризаций. Поскольку начальное поло- жение центров выбирается случайным образом, эти кластеризации будут довольно сильно отличаться. Окончательно выбирается та кластеризация, которая доставляет наилучшее значение заданному функционалу качества. Различные виды функционалов качества рассматриваются далее. 1.2.2 Функционалы качества кластеризации Задачу кластеризации можно ставить как задачу дискретной оптимизации: необходимо так приписать номера кластеров yi объектам xi, чтобы значение вы- бранного функционала качества приняло наилучшее значение. Существует много разновидностей функционалов качества кластеризации, но нет ?самого правильно- го? функционала. По сути дела, каждый метод кластеризации можно рассматривать как точный или приближённый алгоритм поиска оптимума некоторого функционала. Среднее внутрикластерное расстояние должно быть как можно меньше: F0 = Pi<j [yi = yj ]?(xi, xj) Pi<j [yi = yj ] > min . Среднее межкластерное расстояние должно быть как можно больше: F1 = Pi<j [yi 6= yj ]?(xi, xj) Pi<j [yi 6= yj ] > max .
- 14 - Если алгоритм кластеризации вычисляет центры кластеров ?y, y ? Y , то можно определить функционалы, вычислительно более эффективные. Сумма средних внутрикластерных расстояний должна быть как можно меньше: c0 =Xy?Y 1 |Ky| Xxi?Ky ?2(xi, ?y) > min, где Ky = {xi ? X? | yi = y} кластер с номером y. В этой формуле можно было бы взять не квадраты расстояний, а сами расстояния. Однако, если ? евклидова мет- рика, то внутренняя сумма в c0 приобретает физический смысл момента инерции кластера Ky относительно его центра масс, если рассматривать кластер как матери- альное тело, состоящее из |Ky| точек одинаковой массы. Сумма межкластерных расстояний должна быть как можно больше: c1 =Xy?Y ?2(?y, ?) > max, где ? центр масс всей выборки. На практике вычисляют отношение пары функционалов, чтобы учесть как межкластерные, так и внутрикластерные расстояния: F0/F1 > min либо c0/c1 > min . 1.2.3 Статистические алгоритмы Статистические алгоритмы основаны на предположении, что кластеры непло- хо описываются некоторым семейством вероятностных распределений. Тогда задача кластеризации сводится к разделению смеси распределений по конечной выборке. Снова EM-алгоритм. Напомним основные гипотезы байесовского подхода к разде- лению смесей вероятностных распределений, см. ??. Гипотеза 1.1 (о вероятностной природе данных). Объекты выборки X? по- являются случайно, независимо, согласно вероятностному распределению, представ- ляющему собой смесь распределений p(x) =Xy?Y wypy(x), Xy?Y wy = 1, где py(x) функция плотности распределения кластера y, wy неизвестная апри- орная вероятность появления объектов из кластера y. Конкретизируя вид распределений py(x), чаще всего берут сферические гаус- совские плотности. Это обычная практика представлять кластеры в виде шаров. Мы немного обобщим это представление и будем предполагать, что кластеры скорее похожи на эллипсоиды, оси которых направлены вдоль осей координат. Преимуще- ство эллиптических гауссианов в том, что они обходят проблему выбора нормировки признаков. Нормировать можно немного по-разному, но пока не произведена класте- ризация, трудно понять, какая нормировка лучше. При использовании эллиптиче- ских гауссианов оптимальная нормировка подбирается самим алгоритмом кластери- зации, причём индивидуально для каждого кластера.
- 15 - Алгоритм 1.4. Кластеризация с помощью EM-алгоритма 1: начальное приближение для всех кластеров y ? Y : wy := 1/|Y |; ?y := случайный объект выборки; ?2yj := 1 ?|Y | P?i=1(fj(xi) ? ?yj)2, j = 1, . . . , n; 2: повторять 3: E-шаг (expectation): giy := Pwypy(xi) z?Y wzpz(xi) , y ? Y , i = 1, . . . , ?; 4: M-шаг (maximization): wy := 1? P? i=1 giy, y ? Y ; ?yj := 1 ?wy P? i=1 giyfj(xi), y ? Y , j = 1, . . . , n; ?2yj := 1 ?wy P? i=1 giy(fj(xi) ? ?yj)2, y ? Y , j = 1, . . . , n; 5: Отнести объекты к кластерам по байесовскому решающему правилу: yi := argmax y?Y giy, i = 1, . . . , ?; 6: пока yi не перестанут изменяться; Гипотеза 1.2 (о форме кластеров). Объекты описываются n числовыми при- знаками f1(x), . . . , fn(x), X = Rn. Каждый кластер y ? Y описывается n-мерной гауссовской плотностью с центром ?y = (?y1, . . . , ?yn) и диагональной ковариацион- ной матрицей :y = diag(?2y1, . . . , ?2yn). При этих предположениях задача кластеризации совпадает с задачей разделе- ния смеси вероятностных распределений, и для её решения можно применить EM- алгоритм ??. Для оценивания параметров кластеров воспользуемся формулами, по- лученными в Теореме ?? как раз для случая эллиптических гауссианов. Реализация этой идеи представлена в Алгоритме 1.4. Напомним, что EM-алгоритм заключается в итерационном повторении двух шагов. На E-шаге по формуле Байеса вычисляются скрытые переменные giy. Значе- ние giy равно вероятности того, что объект xi ? X? принадлежит кластеру y ? Y . На M-шаге уточняются параметры каждого кластера (?y,:y), при этом существенно используются скрытые переменные giy. В Алгоритме 1.4 для простоты предполагается, что число кластеров известно заранее. Однако в большинстве практических случаев его лучше определять авто- матически, как это было сделано в Алгоритме ??. Метод k-средних, представленный в Алгоритме 1.5, является упрощением EM-ал- горитма. Главное отличие в том, что в EM-алгоритме каждый объект xi распреде- ляется по всем кластерам с вероятностями giy = P{yi = y}. В алгоритме k-средних (k-means) каждый объект жёстко приписывается только одному кластеру. Второе отличие в том, что в k-means форма кластеров не настраивается. Однако это отличие не столь принципиально. Можно предложить упрощённый вариант EM, в котором форма кластеров также не будет настраиваться для этого достаточно
- 16 - Алгоритм 1.5. Кластеризация с помощью алгоритма k-средних 1: сформировать начальное приближение центров всех кластеров y ? Y : ?y наиболее удалённые друг от друга объекты выборки; 2: повторять 3: отнести каждый объект к ближайшему центру (аналог E-шага): yi := argmin y?Y ?(xi, ?y), i = 1, . . . , ?; 4: вычислить новое положение центров (аналог M-шага): ?yj := P?i=1[yi = y]fj(xi) P?i=1[yi = y] , y ? Y , j = 1, . . . , n; 5: пока yi не перестанут изменяться; зафиксировать ковариационную матрицу :y. С другой стороны, возможен и обоб- щённый вариант k-means, в котором будут определяться дисперсии кластеров вдоль координатных осей. Для этого в Алгоритме 1.4 достаточно заменить E-шаг жёстким приписыванием объектов кластерам (и убрать шаг 5 за ненадобностью): yi := argmin y?Y ?(xi, ?y), j = 1, . . . , n; giy := [yi = y], j = 1, . . . , n, y ? Y ; где ?2(x, x') = Pnj=1 ??2 j !fj(x) ? fj(x')?2 евклидово расстояние с весами ??2 j . Та- ким образом, EM и k-means довольно плавно ?перетекают? друг в друга, позволяя строить различные ?промежуточные? варианты алгоритмов. Заметим, что k-means также очень похож на базовую процедуру поиска центра кластера в алгоритме FOREL. Отличие в том, что в FOREL кластер это шар за- данного радиуса R, тогда как в k-means объекты относятся к кластерам по принципу ближайшего соседа. Алгоритм k-means имеет, как минимум, два варианта. Вариант Болла-Холла [4, стр. 110] представлен в Алгоритме 1.5. Вариант МакКина [4, стр. 98] отличается тем, что шаги 3 и 4 выполняются внутри одного цикла по объектам выборки. Когда находится объект, переходящий из одного кластера в другой, центры обоих кластеров пересчитываются. МакКин показал в 1967, что этот вариант алгоритма приводит к локальному минимуму функционала c0. Алгоритм k-means крайне чувствителен к выбору начальных приближений цен- тров. Случайная инициализация центров на шаге 1 может приводить к сколь угодно плохим кластеризациям. Для формирования начального приближения лучше выде- лить k наиболее удалённых точек выборки: первые две точки выделяются по макси- муму всех попарных расстояний; каждая следующая точка выбирается так, чтобы расстояние от неё до ближайшей из уже выделенных точек было максимально. Кластеризация может оказаться неадекватной и в том случае, если изначаль- но будет неверно угадано число кластеров. Стандартная рекомендация провести кластеризацию при различных значениях k и выбрать то, при котором достигается резкое улучшение качества кластеризации Кластеризация с частичным обучением. Алгоритмы EM и k-means легко приспо- собить для решения задач кластеризации с частичным обучением (semi-supervised
- 17 - learning), когда для некоторых объектов xi известны правильные классифика- ции y?(xi). Обозначим через U подмножество таких объектов, U ? X?. Модифи- кация обоих алгоритмов очень проста: на E-шаге (шаг 3) для всех xi ? U полагаем giy := [y = y?(xi)], для всех остальных объектов xi скрытые переменные giy вычисля- ются как прежде. В некоторых случаях частичная классификация даже небольшого количества объектов существенно улучшает качество кластеризации [cite?]. 1.2.4 Иерархическая кластеризация Иерархические алгоритмы кластеризации, называемые также алгоритмами таксономии, строят не одно разбиение выборки на непересекающиеся классы, а си- стему вложенных разбиений. Результат таксономии обычно представляется в виде таксономического дерева дендрограммы. Классическим примером такого дерева является иерархическая классификация животных и растений. Среди алгоритмов иерархической кластеризации различаются два основных ти- па. Дивизимные или нисходящие алгоритмы разбивают выборку на всё более и более мелкие кластеры. Более распространены агломеративные или восходящие алгорит- мы, в которых объекты объединяются во всё более и более крупные кластеры. Реа- лизация этой идеи представлена в Алгоритме 1.6. Сначала каждый объект считается отдельным кластером. Для одноэлементных кластеров естественным образом определяется функция расстояния R({x}, {x'}) = ?(x, x'). Затем запускается процесс слияний. На каждой итерации вместо пары самых близких кластеров U и V образуется новый кластер W = U ?V . Расстояние от нового кластера W до любого другого кластера S вычисляется по расстояниям R(U, V ), R(U, S) и R(V, S), которые к этому моменту уже должны быть известны: R(U ? V, S) = ?UR(U, S) + ?V R(V, S) + ?R(U, V ) + ?|R(U, S) ? R(V, S)|, где ?U, ?U, ?, ? числовые параметры. Эта универсальная формула обобщает прак- тически все разумные способы определить расстояние между кластерами. Она была предложена Лансом и Уильямсом в 1967 году [7, 5]. На практике используются следующие способы вычисления расстояний R(W, S) между кластерами W и S. Для каждого из них доказано соответствие формуле Ланса-Вильямса при определённых сочетаниях параметров [4]: Расстояние ближнего соседа: Rб(W, S) = min w?W,s?S ?(w, s); ?U = ?V = 12 , ? = 0, ? = ?12 . Расстояние дальнего соседа: Rд(W, S) = max w?W,s?S ?(w, s); ?U = ?V = 12 , ? = 0, ? = 12 . Среднее расстояние: Rс(W, S) = 1 |W||S| P w?W Ps?S ?(w, s); ?U = |U| |W| , ?V = |V | |W| , ? = ? = 0. Расстояние между центрами: Rц(W, S) = ?23 P w?W w |W| , Ps?S s |S|?; ?U = |U| |W| , ?V = |V | |W| , ? = ??U?V , ? = 0.
- 18 - Алгоритм 1.6. Агломеративная кластеризация Ланса-Уильямса 1: инициализировать множество кластеров C1: t := 1; Ct = c{x1}, . . . , {x?}?; 2: для всех t = 2, . . . , ? (t номер итерации): 3: найти в Ct?1 два ближайших кластера: (U, V ) := argmin U6=V R(U, V ); Rt := R(U, V ); 4: изъять кластеры U и V , добавить слитый кластер W = U ? V : Ct := Ct?1 ? {W} \ {U, V }; 5: для всех S ? Ct 6: вычислить расстояние R(W, S) по формуле Ланса-Уильямса; Расстояние Уорда: Rу(W, S) = |S||W| |S|+|W|?23 P w?W w |W| , Ps?S s |S|?; ?U = |S|+|U| |S|+|W| , ?V = |S|+|V | |S|+|W| , ? = ? |S| |S|+|W| , ? = 0. Возможных вариантов слишком много, и на первый взгляд все они кажутся достаточно разумными. Возникает вопрос: какой из них предпочесть? Рассмотрим несколько дополнительных свойств, характеризующих качество кластеризации. Свойство монотонности. Обозначим через Rt расстояние между ближайшими кла- стерами, выбранными на t-м шаге для слияния. Говорят, что функция расстояния R обладает свойством монотонности, если при каждом слиянии расстояние между объединяемыми кластерами только увеличивается: R2 6 R3 6 . . . 6 R?. Свойство монотонности позволяет изобразить процесс кластеризации в виде специального графика, называемого дендрограммой. По вертикальной оси отклады- ваются объекты, по горизонтальной расстояния Rt. Можно доказать, что если кластеризация обладает свойством монотонности, то дендрограмму можно постро- ить так, чтобы она не имела самопересечений. При этом любой кластер из множе- ства Ct представляется сплошной последовательностью точек на вертикальной оси. Если же кластеризация не монотонна, дендрограмма представляет собой запутанный клубок линий, на котором трудно что-либо разобрать. Дендрограмма позволяет представить кластерную структуру в виде плоского графика независимо от того, какова размерность исходного пространства. Существу- ют и другие способы визуализации многомерных данных, такие как многомерное шкалирование или карты Кохонена, но они привносят в картину искусственные ис- кажения, влияние которых довольно трудно оценить. Оказывается, не любое сочетание коэффициентов в формуле Ланса-Вильямса приводит к монотонной кластеризации. Теорема 1.1 (Миллиган, 1979). Если выполняются следующие три условия, то кластеризация является монотонной: 1) ?U > 0, ?V > 0; 2) ?U + ?V + ? > 1; 3) min{?U, ?V } + ? > 0.
- 19 - Из перечисленных выше расстояний только Rц не является монотонным. Рас- стояние Уорда отличается от него мультипликативной поправкой, которая и делает его монотонным. Свойства растяжения и сжатия. Некоторые расстояния обладают свойством рас- тяжения. По мере того, как кластер растёт, расстояния от него до других кластеров увеличиваются, как будто пространство вокруг кластера растягивается. Свойство растяжения считается желательным, так как оно способствует более чёткому отде- лению кластеров. С другой стороны, при слишком сильном растяжении возможно найти кластеры там, где их изначально не было. Растягивающими являются рассто- яния Rд и Rу. Некоторые расстояния, наоборот, обладают свойством сжатия. По мере роста кластера расстояния от него до других кластеров уменьшается, и кажется, что про- странство вокруг кластера сжимается. Естественная кластеризация при этом ?раз- мазывается?. Расстояние ближнего соседа Rб является сильно сжимающим. Свойства сжатия и растяжения определяются через отношение Rt/?(?U, ?V ), где Rt = R(U, V ) расстояние между ближайшими кластерами, объединяемыми на t-м шаге, ?U и ?V центры этих кластеров. Если это отношение на каждом шаге больше единицы, то расстояние R является растягивающим; если оно всегда меньше единицы, то сжимающим. Есть и такие расстояния, которые не являются ни сжимающими, ни растягивающими, например, Rс и Rц. О них говорят, что они сохраняют метрику пространства. На практике часто применяют гибкое расстояние, которое представляет собой компромисс между методами ближнего соседа, дальнего соседа и среднего расстоя- ния. Оно определяется одним параметром ? вместо четырёх: ?U = ?V = (1 ? ?)/2, ? = 0, ? < 1. Гибкое расстояние является сжимающим при ? > 0 и растягивающим при ? < 0. Стандартная рекомендация: ? = ?0,25 [5]. Свойство редуктивности. Самой трудоёмкой операцией в Алгоритме 1.6 являет- ся поиск пары ближайших кластеров на шаге 3. Он требует O(?2) операций внутри основного цикла. Соответственно, построение всего таксономического дерева требу- ет O(?3) операций. Это ограничивает применимость алгоритма выборками длины в несколько сотен объектов. Идея ускорения алгоритма заключается в том, чтобы перебирать лишь наи- более близкие пары. Задаётся параметр ?, и перебор ограничивается сокращённым множеством пар c(U, V ) : R(U, V ) 6 ??. Когда все такие пары будут исчерпаны, па- раметр ? увеличивается, и формируется новое сокращённое множество пар. И так далее, до полного слияния всех объектов в один кластер. Реализация представлена в Алгоритме 1.7. Нетрудно доказать, что этот алгоритм приводит к той же кластеризации, что и Алгоритм 1.6, если расстояние R обладает свойством редуктивности: Опр. 1.2 (Брюинош, 1978). Расстояние R называется редуктивным, если для любого ? > 0 и любых ?-близких кластеров U и V объединение ?-окрестностей U и V
- 20 - Алгоритм 1.7. Быстрая агломеративная кластеризация на основе редуктивности 1: инициализировать множество кластеров C1: t := 1; Ct = c{x1}, . . . , {x?}?; 2: выбрать начальное значение параметра ?; 3: P(?) := c(U, V )??U, V ? Ct, R(U, V ) 6 ??; 4: для всех t = 2, . . . , ? (t номер итерации): 5: пока P(?) = ? 6: увеличить ?; 7: найти в P(?) пару ближайших кластеров: (U, V ) := argmin (U,V )?P(?)R(U, V ); Rt := R(U, V ); 8: изъять кластеры U и V , добавить слитый кластер W = U ? V : Ct := Ct?1 ? {W} \ {U, V }; 9: для всех S ? Ct 10: вычислить расстояние R(W, S) по формуле Ланса-Уильямса; 11: если R(W, S) 6 ? то 12: P(?) := P(?) ? c(W, S)?; содержит в себе ?-окрестность кластера W = U ? V : cS ??R(U?V, S) < ?, R(U, V ) 6 ?? ? cS ??R(S,U) < ? или R(S, V ) < ??. Теорема 1.2 (Диде и Моро, 1984). Если выполняются следующие три условия, то расстояние R является редуктивным: 1) ?U > 0, ?V > 0; 2) ?U + ?V + min{?, 0} > 1; 3) min{?U, ?V } + ? > 0. Сравнение условий теорем 1.2 и 1.1, говорит, что всякое редуктивное расстояние является монотонным, следовательно, позволяет отобразить процесс кластеризации в виде дендрограммы. Из перечисленных выше расстояний только Rц не является редуктивным. В Алгоритме 1.7 возможны различные эвристические стратегии выбора па- раметра ? на шагах 2 и 6. Общие соображения таковы: если ? настолько велико, что P(?) содержит практически все пары кластеров, то мы фактически возвращаем- ся к неэффективному Алгоритму 1.6; если же ? мал?о, то приходится слишком часто формировать множество P(?). На практике поступают следующим образом. Если число кластеров в Ct не превышает порог n1, то в качестве P(?) берут множество всех возможных пар (U, V ) из Ct. В противном случае выбирается случайным обра- зом n2 расстояний R(U, V ), и ? полагается равным наименьшему из них. В случае редуктивности параметры алгоритма n1 и n2 влияют только на время выполнения алгоритма, но не на результат кластеризации. Оптимальные значения для них могут подбираться в результате калибровочных тестов и, вообще говоря, зависят от ком- пьютера. В качестве начального выбора можно предложить n1 = n2 = 20.
- 21 - Определение числа кластеров проще всего производить путём отсечения финаль- ного участка дендрограммы. На горизонтальной оси находится интервал максималь- ной длины |Rt+1?Rt|, и в качестве результирующей кластеризации выдаётся множе- ство кластеров Ct. Число кластеров равно K = ? ? t + 1. При необходимости можно задать ограничение на минимальное и максимальное число кластеров K0 6 K 6 K1 и выбирать t, удовлетворяющие ограничениям ? ? K1 + 1 6 t 6 ? ? K0 + 1. Во многих прикладных задачах интерес представляет таксономическое дерево, и определять оптимальное число кластеров не имеет особого смысла. Достоинства и недостатки агломеративной кластеризации. Точного ответа на во- прос, какой алгоритм кластеризации лучше, не существует. Каждое из расстояний, перечисленных выше, имеет свои недостатки и подходит не для всех задач. Метод ближнего соседа обладает цепочечным эффектом, когда независимо от формы кластера к нему присоединяются ближайшие к границе объекты. В неко- торых случаях это приводит к тому, что кластеры ?отращивают щупальца?. В зави- симости от задачи это свойство может быть как полезным, так и мешающим. Метод ближнего соседа хорошо подходит для выделения кластеров ленточной формы. Метод дальнего соседа цепочечного эффекта не имеет, но на раннем этапе мо- жет объединять довольно несхожие группы. Расстояние между центрами масс не является ни монотонным, ни редуктив- ным, поэтому крайне редко используется на практике, несмотря на интуитивную очевидность. Метод Уорда оказался наилучшим по результатам экспериментального срав- нения на представительном наборе модельных задач [4]. Он чаще других методов восстанавливает интуитивно наилучшую кластеризацию. Список литературы [1] Загоруйко Н. Г. Прикладные методы анализа данных и знаний.Новосибирск: ИМ СО РАН, 1999. [2] Загоруйко Н. Г., Ёлкина В. Н., Лбов Г. С. Алгоритмы обнаружения эмпирических закономерностей. Новосибирск: Наука, 1985. [3] Лагутин М. Б. Наглядная математическая статистика.М.: П-центр, 2003. [4] Мандель И. Д. Кластерный анализ.М.: Финансы и Статистика, 1988. [5] Уиллиамс У. Т., Ланс Д. Н. Методы иерархической классификации //Стати- стические методы для ЭВМ /Под ред. М. Б. Малютов.М.: Наука, 1986.С. 269-301. [6] Jain A., Murty M., Flynn P. Data clustering: A review //ACM Computing Surveys.1999.Vol. 31, no. 3.Pp. 264-323. http://citeseer.ifi.unizh.ch/jain99data.html. [7] Lance G. N., Willams W. T. A general theory of classification sorting strategies. 1. hierarchical systems //Comp. J.1967.no. 9.Pp. 373-380.