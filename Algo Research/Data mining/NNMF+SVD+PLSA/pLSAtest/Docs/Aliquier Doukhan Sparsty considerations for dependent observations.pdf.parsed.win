
arXiv:1102.1615v3 [math.ST] 24 Feb 2011 Sparsity considerations for dependent observations Pierre Alquier(1,2) & Paul Doukhan(3) (1) Universite Paris 7, LPMA (2) CREST-ENSAE, Laboratoire de Statistique (3) Universite de Cergy Abstract The aim of this paper is to provide a comprehensive introduction for the study of ?1-penalized estimators in the context of dependent obser- vations. We define a general ?1-penalized estimator for solving problems of stochastic optimization. This estimator turns out to be the LASSO [Tib96] in the regression estimation setting. Powerful theoretical guaran- tees on the statistical performances of the LASSO were provided in recent papers, however, they usually only deal with the iid case. Here, we study our estimator under various dependence assumptions. Contents 1 Introduction 2 1.1 Sparsity in high dimensional estimation problems . . . . . . . . . 2 1.2 General setting and ?1-penalized estimator . . . . . . . . . . . . . 3 1.3 Overview of the paper . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Main result 4 2.1 Assumptions and result . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Remarks on the density and regression estimation setting . . . . 6 3 Models fitting conditions of Theorem 2.1 7 3.1 Weak dependence (? = 0) . . . . . . . . . . . . . . . . . . . . . . 7 3.1.1 Moment inequalities . . . . . . . . . . . . . . . . . . . . . 8 3.1.2 Exponential inequalities . . . . . . . . . . . . . . . . . . . 8 3.2 Long range dependence (? ?]0, 12 [) . . . . . . . . . . . . . . . . . 9 3.2.1 Power decays . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2.2 Gaussian case . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2.3 Non subGaussian tails . . . . . . . . . . . . . . . . . . . . 9 4 Application to regression estimation 10 4.1 Regression in the iid case . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Regression estimation in the dependent case . . . . . . . . . . . . 11 4.2.1 Marcinkiewicz-Zygmund type inequalities . . . . . . . . . 11 1
4.2.2 Exponential inequalities . . . . . . . . . . . . . . . . . . . 12 4.3 Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 5 Application to density estimation 15 5.1 Density estimation in the iid case . . . . . . . . . . . . . . . . . . 15 5.2 Density estimation in the dependent case . . . . . . . . . . . . . 16 6 Conclusion 17 7 Proofs 17 1 Introduction 1.1 Sparsity in high dimensional estimation problems In the last few years, statistical problems in large dimension received a lot of attention. That is, estimation problems where the dimension of the parameter to be estimated, say p, is larger than the size of the sample, usually denoted by n. This setting is motivated by modern applications such as genomics, where we often have n ? 100 the number of patients with a very rare desease, and p of the order of 105 or even 106 (CGH arrays), see [RBV08] and the references therein for example. Other examples appear in econometrics, see Belloni and Chernozhukov [BC09, BC10] for example. Probably the most famous example is high dimensional regression estimation: one observes pairs (x1, yi) for 1 ? i ? n with yi ? R, xi ? Rp and one wants to find a ? ? Rp such that for a new pair (x, y), ?'x would be a good prediction for y. If p ? n, it is well known that a good estimation cannot be performed unless we make an additional assumption. Very often, it is quite natural to assume that most one can find such a ? that is sparse: most of its coordinates are equal to 0. If we let k?k0 denote the number of non-zero coordinates in ?, this means that k?k0 ? p. In the genomics example, it means that only a few genes are relevant to explain the desease. The first estimators to deal with this kind of problems where the now famous AIC [Aka73] and BIC [Sch78]. Both can be written arg min ??Rp(Xn i=1 (yi ? ?'xi)2 + ?nk?k0) (1) where ?n > 0 differs in AIC and BIC. These estimators were proved to have satisfying statistical properties, see for example [BTW07] where a nice oracle inequality for BIC is provided. The main problem with this so-called ?0 penalization approach is that the problem of the effective computation of the estimators defined in (1) is very time consuming. In practice, these estimators cannot be used for p more than a few tens. This motivated the study of the LASSO introduced by Tibshirani [Tib96]. This estimator is defined by arg min ??Rp ( Xn i=1 (yi ? ?'xi)2 + ?nk?k1). (2) The convexity of this minimization problem ensures that the estimator can be computed for very large p, see Efron et al. [EHJT04] for example. This 2
motivated a lot of theoretical studies on the statistical performances of this estimator. The results with the weakest hypothesis can be found in the work of Bickel et al. [BRT09] or Koltchinksii [Kol]. See also very nice reviews in the paper by Van de Geer and Buhlmann [vdGB09] or in the PhD Thesis of Hebiri [Heb09]. Also note that a quantity of variants of the idea of ?1-penalization were studied simultaneously to the LASSO: among others the basis pursuit [Che95, CDS01], the Dantzig Selector [CT07], the Elastic Net [ZH05]... Another problem of estimation in high dimension is the so-called problem of sparse density estimation. In this setting, we observe n random variables with (unknown) density f and the purpose is to estimate f as a linear combination of some functions ?1, . . . , ?p. If p ? n and f(-) ?Xp j=1 ?j?j (-) we can use the SPADES (for "sparse density estimator") by Bunea et al. [BWT07] or the iterative feature selection procedure in [Alq08]. One of the common feature of all the theoretical studies of sparse estimators is that they focus only on the case where the observations are independent. For example, for the density estimation case, in [BWT07] and [Alq08] the observations are assumed to be iid. The purpose of this paper is to propose a unified framework. Namely, we define a general stochastic optimization problem that contains as a special case regression and density estimation. We then define a general ?1-penalized estimator for this problem, in the special case of regression estimation this estimator is actually the LASSO and in the case of density estimation it is SPADES. Finally, we provide guarantees on the statistical performances of this estimator in the spirit of [BRT09], but we do not only consider independent observations: we want to study the case of dependent observations, and prove that we can still recover the target ? in this case, under various hypothesis. 1.2 General setting and ?1-penalized estimator We now give the general setting and notations of our paper. Note that the cases of regression and density estimation will appear as particular cases. We observe n random variables in Z : Z1, . . . ,Zn. Let P be the distribution of (Z1, . . . ,Zn). We have a function Q : Z ? Rp > R such that for any z ? Z, ? ? Rp 7> Q(z, ?) is a quadratic function. The objective is the estimation of a value ? that minimizes the following expression which only depends on n and ?: R(?) = 1n Xn i=1 EQ(Zi, ?) = ZZn 1n Xn i=1 Q(zi, ?)dP(z1, . . . , zn). All the results that will follow are intended to be interesting in the case p > n on the condition that k?k0 := card{j : ?j 6= 0} is small. We use the following estimator: arg min ??Rp " 1n Xn i=1 Q(Zi, ?) + ?k?k1# 3
and ??? denotes any solution of this minimization problem. We now detail the notations in the two examples of interest: 1. in the regression example, Zi = (Xi, Yi) with the Xi ? Rp deterministic, and Yi = X' i? + ?i (3) where E(?i) = 0 (the ?i are not necessarily iid, they may be dependent and have different distribution). Here we take Q((x, y), ?) = (y ? x'?)2. In this example, ??? is known as the LASSO estimator [Tib96]. 2. in the density estimation case, Zi ? R have the same density wrt Lebesgue measure (but they are not necessarily independent) and we have a family of functions (?i)pi=1 and we want to estimate the density f of Zi by functions of the form f?(-) =Xp i=1 ?i?i(-). In this case we take Q(z, ?) = Z f2 ? (?)d? ? 2f?(z) and note that this leads to R(?) = Z (f?(x) ? f(x))2 dx ? Z f2(x)dx = Z (f?(x) ? f(x))2 dx ? cst. Then ??? is the estimator known as SPADES [BWT07]. 1.3 Overview of the paper In Section 2 we provide a sparsity inequality that extend the one of Bickel et al. [BRT09] to the case of non iid variables. This result involves two assumptions: one of them is about the function Q and is already needed in the iid case. The other one is more involved, it is specific to the non iid case. In Section 3, we provide examples of classical assumptions on the observations that can ensure that this hypothesis is satisfied. These assumptions are expressed in terms of dependence coefficients. We apply the results of Sections 2 and 3 to regression estimation in Section 4 and to density estimation in Section 5. Finally the proofs are given Section 7. 2 Main result 2.1 Assumptions and result First, we need an assumption on the quadratic form R(-). Assumption A(?) with ? > 0. As Q(z, -) is a quadratic form, we have the matrix M = ?2 ??2 1n Xn i=1 Q(Zi, ?) 4
that does not depend on ?, and we assume that the matrix M has only 1 on its diagonal (actually, this just means that we renormalize the observations Xi in the regression case, or the function ?j in the density estimation case), that it is non-random (here again, this is easily checked in the two examples) and that it satisfies ? ? inf v ? Rp, J ? {1, . . . , p} Pj /?J |vj | ? 3Pj?J |vj | |J| < k?k0 v' P Mv j?J v2j . Note that this condition is already required in the iid setting, see [BRT09] and [vdGB09] for example. In these paper it is also discussed why we cannot hope to get rid of this hypothesis. We set for simplicity W(j) i = 12 ?Q(Zi, ?) ??j , i ? {1, . . . , n}, j ? {1, . . . , p} (4) Recall that as Q(z, ?) is a quadratic function it may be written as Q(z, ?) = ?'A(z)? +b(z)'? +c(z) for a p?p-matrix valued function A on Rp and a vector function b : Rp > Rp so that W(j) i = (A(Zi)?)j + 12(b(Zi))j Theorem 2.1 Let us assume that Assumption A(?) is satisfied. Let us assume that the distribution P of (Z1, . . . ,Zn) is such that there is a constant ? ? [0, 12 ] and a decreasing continuous function ?(-) with ?j ? {1, . . . , p}, P 1n Xn i=1 W(j) i ? n?12+?t! ? ?(t). (5) Let us put ? ? ?? := 4n??12 ??1 ?p. Then P????????? ???????? R(???) ? R(?) ? 4?2k?k0 ? and, simultaneously k??? ? ?k1 ? 8?k?k0 ? ????????? ???????? ? 1 ? ?. The arguments of the proof of Theorem 2.1 are taken from [BRT09]. The proof is given in Section 7, page 17. Note that the hypotheses of this theorem heavily depend on the distribution of the variables Z1, . . . , Zn, and particulary on their type of dependence. 5
Also note that the upper bound in the inequality is minimized if we make the choice ? = ??. Then P?????????? ????????? R(???) ? R(?) ? 64 ? k?k0 ??1 (?/p)2 n1?2? and k??? ? ?k1 ? 32 ? k?k0 ??1 (?/p)2 n12?? ?????????? ????????? ? 1 ? ?. It is important to remark that the choice ? = 4n??12 ??1 ?pmay be impossible in practice, as the practitionner may ignore ? and ?(-). Moreover, this choice is not necessarily the best one in practice: in the regression case with iid noise N(0, ?2), we will see that this choice leads to ? = 4?p2n log(p/?). This choice requires the knowledge of ?. Moreover it is not usually the best choice in practice, see for example the simulations in [Heb09]. Even in the iid case, the choice of a good ? in practice is still an open problem. However, note that 1. the question is in some sense meaningless. For example the value of ? that minimizes the quadratic risk R(???) is not the same than the value of ? that may ensure, under some supplementary hypothesis, that ??? identifies correctly the non-zero coordinates in ?, see for example Leeb and Potscher [LP05] on that topic. One has to be careful to what one means when one say "a good choice for ?". 2. some popular methods like cross-validation seem to give good results for the quadratic risk, at least in the iid case. An interesting open question is to know if one can prove theoretical results for cross validation in the non iid setting. 3. the LARS algorithm [EHJT04] compute ??? for any ? > 0 in a very short time. Our point of view here is that we should use Theorem 2.1 in the following way: we fix ? large enough (depending on what we know about the data, for example, ? = Ln?12 log(p/?) with L, a large enough constant). Then the theorem ensures the sparsity oracle inequality as soon as the data satisfy some suitable conditions. These conditions are discussed below. 2.2 Remarks on the density and regression estimation setting First, note that in the regression setting (Equation 3), for any i ? {1, . . . , n} and j ? {1, . . . , p} we have W(j) i = (Xj)i(Yi ? X' i?) = (Xj)i?i. (6) Then, in the density estimation context, W(j) i = Z ?j (x)f?(x)dx ? ?j(Zi) = Z ?j(x)f(x)dx ? ?j(Zi) = E[?j (Z1)] ? ?j(Zi). (7) 6
So, in both cases, the assumption given by Equation 5 is satisfied if we have a control of the deviation of empirical means to their expectation. In the next sections, we discuss some conditions to obtain such controls with dependent variables. 3 Models fitting conditions of Theorem 2.1 In this section, we give some results that allow to control the deviation of empirical means to their expectations for general (non iid) obsrevations. The idea will be, in the next sections, to apply these results to the processes W(j) = (W(j) i )1?i?n for 1 ? j ? p. For the sake of simplicity, in this section, we deal with a generic process V = (Vi)i?Z and the applications are given in the next sections. Various examples of pairs (?, ?) are given. 3.1 Weak dependence (? = 0) Definition 3.1 We put, for any process (Vi)i?Z, cV,m(r) = max 1?l<m sup t1 ? - - - ? tm tl+1 ? tl ? r cov ??Vt1 - - - Vtl , Vtl+1 - - - Vtm(8) We precise in :-3.1.1 and in :-3.1.2 that suitable decays of those coefficients yield the conditions in Theorem 2.1. Those two sections will provide quite different forms of the function ?. Cases of bounded or non-bounded random variables are to be considered separately. Eg. weak dependence conditions from Dedecker et al. [DDL+07] imply that the coefficients (8) are bounded by a constant only depending on m multipled by supi kVikm? ?(r). Set M = supi kVik?, then for example if ?, or ? weak dependence or if strong mixing hold then cV,m(r) ? mMm?V (r) (9) ? m2Mm?1?V (r) ? Mm?V (r). Now the previous strong mixing condition is completely hereditary through measurable images but this is not the case for the weak dependence conditions. If the random process (Vi) satisfies a weak dependence condition then f(Vi) too if for some L > 0, a > 1, ?t, z : |f(z + t) ? f(z)| ? L(|z|a?1 ? 1)|t| (10) and K = supj kVjks < ? for some s > a then looking more precisely at the proofs in [DDL+07] page 13 yields ?f(V )(r) ? 5LK?V (r) s?a s?1 (11) ?f(V )(r) ? 5LK?V (r) s?a s+a?2 . 7
3.1.1 Moment inequalities In Doukhan and Louhichi [DL99] it is proved that if for an even integer 2q we have ?C ? 1 such that: cV,2q(r) ? C(r + 1)?q, ?j ? [1, p], ?r ? 0 (12) then Marcinkiewicz-Zygmund inequality follows: E??(V1 + - - - + Vn)2q= O(nq) and thus ? = 0 and ?(t) is of the order of 1/t2q. However, explicit constants are needed in Theorem 2.1. We actually have the following result. Proposition 3.1 Assume that coefficients (8) fit the relation (12) for some integer q ? 1, then Marcinkiewicz-Zygmund inequality follows E(V1 + - - - + Vn)2q? Cqd2q(2q)!nq (13) where dm ? 1m (2m? 2)! ((m ? 1)!)2 , m = 2, 3, . . . The proof follows [DL99], it is given in Section 7. Remark 3.1 Sharper constants a2q are also derived in the proof (Equation 26, page 20), one may replace the constants 2d2, 24d4, 720d6 by 1, 4 and 17) and using the recursion (27) also improves the above mentioned bounds. Various inequalities of this type where derived for alternative dependences (see Doukhan [Dou94], Rio [Rio00] and Dedecker et al. [DDL+07] for an extensive bibliography which also covers the case of non integer exponents. 3.1.2 Exponential inequalities Using the previous inequality, Doukhan and Louhichi [DL99] proved exponential inequalities that would lead to ?(t) in exp(?vt). Doukhan and Neumann [DN07] use alternative cumulant techniques to get ?(t) in exp(?t2) for suitable bounds of the previous covariances (8). Theorem 3.2 [DN07] Let : N2 > N be one of the following functions: (a) (u, v) = 2v, (b) (u, v) = u + v, (c) (u, v) = uv, (d) (u, v) = ?(u + v) + (1 ? ?)uv, for some ? ? (0, 1). We assume that there exist constants K,L1,L2 < ?, ? ? 0, and a nonincreasing sequence of real coefficients (?(n))n?0 such that, for all u-tuples (s1, . . . , su) and all v-tuples (t1, . . . , tv) with 1 ? s1 ? - - - ? su ? t1 ? - - - ? tv ? n the following inequality is fulfilled: |cov (Xs1 - - -Xsu,Xt1 - - -Xtv )| ? K2Mu+v?2(u, v)?(t1 ? su), (14) 8
where ?Xs=0(s + 1)k?(s) ? L1Lk2(k!)? ?k ? 0. (15) Then P(Sn ? t) ? exp ? t2/2 An + B1/(?+2) n t(2?+3)/(?+2)!, (16) where An can be chosen as any number greater than or equal to ?2n:= V ar(V1 + - - - + Vn) and Bn = 2(K ?M)L2 24+?nK2L1 An ? 1. This result yields convienient bounds for the function ?. A review paper by Olivier Wintenberger [Win09] is also of interest: it directly yields alternative results from our main result. In this paper, we do not intend to provide the reader with encyclopedic references but mainly to precise some ideas and techniques so that this will be developed in further papers. 3.2 Long range dependence (? ?]0, 12 [) 3.2.1 Power decays Assume that a centered series (Vi) satisfies Pi supk |cov(Vk, Vk+i)| = ? then ? > 0 may occur, eg. if r(i) ? sup k |cov(Vk, Vk+i)| ? i?? for ? ?]0, 1] then var(Sn) ? n2??; then ? = (1 ? ?)/2 holds. 3.2.2 Gaussian case In the special case of Gaussian processes (Vi)i, tails of S(j) n are classically described because Sn ? N(0, ?2n) and here ?(t) = exp(?t2). We thus may obtain simultaneously subGaussian tails and ? = (1 ? ?)/2 > 0. 3.2.3 Non subGaussian tails Assume that that for each i, j Gi ? N(0, 1) and (Gi)i is a stationary Gaussian processes with r(i) = cov(Gk,Gk+i) ? ci?? Let Vi = P(Gi) for a function with Hermite rank m ? 1, and since cov(Hm(G0),Hm(Gi)) = m! (r(i))m their covariance series is non m-th summable in case ? ?] 1m, 1[. The case P(x) = x2 ? 1 and ? ?] 12 , 1[ is investigated by using the following 9
expansion in the seminal work by Rosenblatt [Ros61]. Set Rn for the covariance matrix of the Gaussian random vector (G1, . . . ,Gn): Eetn??1Sn = e?tn?det?12 ??In ? 2tn??1Rn= exp 12 ?Xk=2 1k (2tn??1)k trace (Rn)k! Quoting that nk(??1) trace (Rn)k >n>? ck > 0 with ck = ck Z 1 0 - - - Z 1 0 |x1 ?x2|??|x2 ?x3|?? - - - |xk?1 ?xk|??|xk ?x1|??dx1 - - - dxk this is thus clear that for small enough |t| < ? = 12 supk?2,j?1 c(j) k 1k , Eetn??1S(j) n >n>? exp 12 ?Xk=2(2t)k ck k ! Here the conditions in the main theorem hold with ?(t) = e?t and ? = 12?? > 0 for any M > 1/?. Extension to higher order Gaussian subordinated models. Analogously if Vi = P(Gi) for a function with Hermite rank m ? 3, and for Gaussian processes (Gi)i with non m-th summable covariance series. The above mentioned arguments from analytic functions theory however don't work anymore in this case; indeed the corresponding Laplace transforms are it is no more an analytic function around zero in this case. Anyway we guess that here ?(t) = exp(?t 2m ). 4 Application to regression estimation In this section we apply Theorem 2.1 and the various examples of Section 3 to obtain results for regression estimation. Note that the results in the iid setting are already known, they are only given here for the sake of completeness, in order to provide comparison with the other cases. Let us remind that in the regression case, we want to apply the results of Section 3 to W(j) i = (Xj)i?i. For the sake of simplicity, in this whole session dedicated to regression, let us put max(X) := max 1?i?n max 1?j?p |(Xi)j |. 4.1 Regression in the iid case Under the usual assumption that the ?i are iid and subGaussian, ?s, E[exp(s?2i)] ? exps2 2?210
for some known ?2, then we have P 1nXn i=1 W(j) i ? t vn! ? ?(t) = exp(? t2 2?2 ). So we can apply Theorem 2.1 in order to obtain the following well known result: Corollary 4.1 ([BRT09]) In the context of Equation 3, under Assumption A(?), if the (?i) are iid and subGaussian with variance upper bounded by ?2, the choice ? = 4?p2 log(p/?)/n leads to P R(???) ? R(?) ? 128?2 ? k?k0 log p? n ! ? 1 ? ?. 4.2 Regression estimation in the dependent case 4.2.1 Marcinkiewicz-Zygmund type inequalities Let us remark that, for any 1 ? j ? p, cW(j),m(r) ? c?,m(r)max i,j |(Xj)i|m = max(X)mc?,m(r). Thus, we apply Theorem 2.1 and Proposition 3.1 to obtain the following result. Corollary 4.2 In the context of Equation 3, under Assumption A(?), if the (?i) satisfy, for some even integer 2q, ?C ? 1 such that: ?r ? 0, c?,2q(r) ? C(r + 1)?q, the choice ? = 4C 12 max(X)q vn d2qq!p ? 1 2q leads to P R(???) ? R(?) ? 64C max(X)2q(d2qq!) 1q ? k?k0p1q ? 1q n ! ? 1 ? ?. Remark 4.1 This result aims at filling a gap for non subGaussian and non iid random variables. The result still allows to deal with the sparse case p > n in case q > 1. In this case we deal with the case p = nq/2 and we get a rate of convergence in probability O(1/vn). If q = 1 and pn > 0 the least squares methods apply which make such sparsity algorithms less relevant. Moreover if q < 1 the present method is definitely not efficient. Hence the case of heavy tails such as considered in the paper by Bartkiewicz et al. [BJMW10] should neither use our result. Anyway, using least squares for heavy tailed models (without second order moments) does not look to be a good idea! 11
4.2.2 Exponential inequalities Using Theorem 2.1 and Theorem 3.2 we prove the following result. Corollary 4.3 Let us assume that the (?i) satisfy the hypothesis of Theorem 3.2: let : N2 > N be one of the functions of Theorem 3.2, we assume that there are constants K,L1,L2 < ?, ? ? 0, and a nonincreasing sequence of real coefficients (?(n))n?0 such that, for all u-tuples (s1, . . . , su) and all v-tuples (t1, . . . , tv) with 1 ? s1 ? - - - ? su ? t1 ? - - - ? tv ? n the following inequality is fulfilled:|cov (?s1 - - - ?su, ?t1 - - - ?tv )| ? K2Mu+v?2(u, v)?(t1 ? su), where ?Xs=0(s + 1)k?(s) ? L1Lk2(k!)? ?k ? 0. Let c be a positive constant and let us put C := 4K2max(X)2(1, 1)L1 + c2L2max(X)(K ?M)2?+3 (1, 1) ? 1. If we assume that Let us assume that ? > 0, p and n are such that p ? ?2 exp c2n 1 ?+2 C ! then for ? = 4sC log ?? 2p ? n we have P(R(???) ? R(?) ? 64C ? k?k0 log ?? 2p ? n ) ? 1 ? ?. So the rate is the same than in the iid case. The only difference is in the constant, and a restriction for very large values of p. Proof: For the sake of shorteness, let us put C1 = 4K2max(X)2(1, 1)L1 and C2 = 2L2max(X)(K ?M)2?+3 (1, 1) ? 1and note that C = C1 + cC2. First, note that for any j ? {1, . . . , p}, cov W(j) s1 - - -W(j) su ,W(j) t1 - - -W(j) tv ? sup i X(j) i u+v K2Mu+v?2(u, v)?(t1 ? su) ? ?K2 ?Mu+v?2(u, v)?(t1 ? su) 12
if we put ?K= max(X)K and ?M= max(X)M. Using Theorem 3.2, we obtain for any j, P Xi W(j) i ? t! ? 2 exp ? t2/2 An + B 1 ?+2 n t 2?+3 ?+2 ! where An = ?2n ? 2n ?K2(1, 1)L1 and Bn = 2(K ?M)L2 24+?nK2L1 An ? 1, in other words:P Xi W(j) i ? t! ? 2 exp ? t2/2 C1n + C2t 2?+3 ?+2 !. Now, let us put u = t/vn, we obtain P 1nXi W(j) i ? un?12! ? 2 exp ? nu2/2 C1n + C2n2?+3 2?+4 u2?+3 ?+2 ! ? 2 exp ? u2/2 C1 + C2n? 1 2?+4 u2?+3 ?+2 !. Remark that we cannot in general compute explicitely the inverse of this function but we decide to upper-bound the range for u: u ? c - n 1 2?+4 In this case, P 1nXi W(j) i ? un?12! ? 2 exp? u2/2 C1 + C2c= 2 exp? u2 2 C=: ?(u) and so ??1(y) = sC log 2y. So we can take, following Theorem 2.1, ? = 4n?12 ??1 ?p= 4n?12sC log2p ? as soon as ??1(?/p) < n1/(2?+4). For example, for a fixed number of observations n and a fixed confidence level ?, we have the restriction: p ? ?2 exp cn 1 ?+2 C !. 13
Under this condition we have, by Theorem 2.1, P?????????? ????????? R(???) ? R(?) ? 64 C ? k?k0 log ?? 2p ? n and, k??? ? ?k1 ? 32 C ? k?k0 log ?? 2p ? n12 ?????????? ????????? ? 1 ? ?, this ends the proof. 4.3 Simulations In order to illustrate the results, we propose a very short simulation study. The purpose of this study is not to show the good performances of the estimator in practice or to give recipes for the choice of ?. The aim is more to show that the performances of the iid setting are likely to be obtained in the dependent setting if the dependence coefficients are small. We use the following model: Yi = ?'Xi + ?i, 1 ? i ? n = 30 where the Xi's will be treated as a random design, but in practice will be iid vectors in Rp with p = 50, with distribution Np(0,) where is given by i,j = 0.5|i?j|. Figure 1: results of the experiments. The x-axis gives the value g where ? = gplog(p)/n. The y-axis gives Pni=1(??'Xi ? ?'Xi)2 the error of reconstruction of the signal. The color code is the following: ? = ?0.95: black, ? = ?0.75: red, ? = ?0.5: green, ? = 0: blue, ? = 0.5: cyan, ? = 0.75: violet, ? = 0.95: yellow. 14
The parameter is given by ? = (3, 1.5, 0, 0, 2, 0, 0, . . .)' ? Rp. This is the toy example used by Tibshirani [Tib96]. Let ? ?] ? 1, 1[. The noise satisfies ?i = ??i?1 + ?i, for i ? 2, where the ?i are iid N(0, 1 ? ?2) and ?1 ? N(0, 1). Note that this ensure that E(?2i) = 1 for any i, so the noise level does not depent on ?. In the experiments, ? ? {?0.95,?0.75,?0.5, 0, 0.5, 0.75, 0.95}. We fixed a grid of values G ?]0, 1.5[ and we computed, for every experiment, the LASSO estimator with ? = gplog(p)/n for all g ? G. We have repeated the experiment 25 times for every value of ? and report the results in Figure 1. We can remark that all the curves are very similar. The minimum reconstruction error is obtained for g ? 0.2, that corresponds to ? ? 0.072. Note that in the iid case, it is smaller than the theoretical value given by Theorem 2.1, ? = 4?p2 log(p/?)/n ? 2.56 for ? = 1/10, that would correspond to g ? 7.10, a value that would not event stand in the figure! 5 Application to density estimation Here we apply Theorem 2.1 and Section 3 to the context of density estimation. Let us remind that in this setting, W(j) i = E[?j (Z1)] ? ?j(Zi). 5.1 Density estimation in the iid case If the Zi are iid with density f and if k?jk? < B for any j ? {1, . . . , p} then we can apply Hoeffding inequality [Hoe63] to upper bound Z ?j(x)f(x)dx ? 1nXn i=1 ?j (Zi). We obtain P 1n Xn i=1 W(j) i ? t vn! ? ?(t) = 2 exp(? t2 2B2 ). So we can apply Theorem 2.1. Corollary 5.1 In the context of density estimation, under Assumption A(?), if the Zi are iid with density f and if k?jk? < B for any j ? {1, . . . , p}, the choice ? = 4Bp2n log(2p/?) leads to P Z f???(x) ? f?(x)2 dx ? 128B2 ? k?k0 log 2p ? n ! ? 1 ? ?. This result is essentially known, see [BWT07]. 15
5.2 Density estimation in the dependent case Note that if as previously we work with bounded ?j (-), we automatically have moments of any order. So we will only state a result based on exponential inequality. So, using Theorem 2.1 and Theorem 3.2 we obtain: Corollary 5.2 Let us assume that there are L > 0 and B ? 1 such that ?j (-) is L-Lipschitz and k?jk? < B for any j ? {1, . . . , p}. Let us assume that Z1, . . . , Zn satisfy ?k ? 0, ?Xs=0(s + 1)k?Z(s) ? L1Lk2(k!)? for some L1,L2, ? > 0. Let us put a c > 0, define C := 4BLL1 + (23+?BL1)1/(?+2)c and assume that p, n and the confidence level ? are such that p ? ?2 exp c2n 1 ?+2 C !. Then P Z f???(x) ? f?(x)2 dx ? 64 C ? k?k0 log ?? 2p ? n ! ? 1 ? ?. Proof: As ?(j) is K-Lipschitz, we have: ??j (Z)(r) ? L?Z(r). So we have ?k ? 0, ?Xk=1(s + 1)k??j (Z)(r) ? LL1Lk2(k!)?. Moreover, following Remark 9 page 9 in Doukhan and Neumann [DN07], |cov (?j (Zs1) - - - ?j (Zsu), ?j (Zt1) - - - ?j(Ztv ))| ? Bu+v?1(u + v)L - ?Z(r). So we can apply Theorem 3.2 with (u, v) = u + v and we obtain P Xn i=1 W(j) i > t! ? 2 exp  ?t2/2 An + B 1 ?+2 n t 2?+3 ?+2 ! with An = 4nBLL1 and Bn = 23+?BL1, in other words P Xn i=1 W(j) i > t! ? 2 exp  ?t2/2 4nBLL1 + (23+?BL1) 1 ?+2 t 2?+3 ?+2 !. 16
We then put u = tvn to obtain P 1nXn i=1 W(j) i > u vn!? 2 exp  ?nu2/2 4BLL1 + (23+?BL1) 1 ?+2 n? 1 2?+4 u2?+3 ?+2 !. Here again, if we have u ? cn1/(2?+4) then P 1n Xn i=1 W(j) i > u vn! ? 2 exp  ?nu2/2 4BLL1 + (23+?BL1) 1 ?+2 c!2 exp?nu2 2 C =: ?(u). So we take, following Theorem 2.1, ? = 4 vn??1 ?p= 4sC log ?? 2p ? n and we obtain, with probability at least 1 ? ?, Z f???(x) ? f?(x)2 dx ? 64 C ? k?k0 log ?? 2p ? n . 6 Conclusion In this paper, we showed how the LASSO and other ?1-penalized methods can be extended to the case of dependent random variables. An open and ambitious question to be adressed later is to find a good datadriven way to calibrate the regularization parameter ? when we don't know in advance the dependence coefficients of our observations. Anyway this first step with sparsity in the dependent setting is done for accurate applications and our brief simulations let us think that such techniques are reasonable for time series. Here again extensions to random fields or to dependent point processes seem plausible. 7 Proofs Proof of Theorem 2.1: By definition, 1n Xn i=1 Q(Zi, ???) + ?k???k1 ? 1n Xn i=1 Q(Zi, ?) + ?k?k1 17
and so R(???) ? R(?) ? ZZn 1n (Xn i=1 hQ(zi, ???) ? Q(zi, ?)i)dP(z1, . . . , zn) ? 1n Xn i=1 hQ(Zi, ???) ? Q(Zi, ?)i + ? k?k1 ? k???k1. (17) Now, as Q is quadratic wrt ? we have Q(z, ???) = Q(z, ?) + (??? ? ?)' ?Q(z, ?) ?? + 12(??? ? ?)'M(??? ? ?). (18) Moreover, as ? is the minimizer of R(.), we have the relation ?R(?) ?? = ZZn 1n Xn i=1 ?Q(Zi, ?) ?? dP(z1, . . . , zn) = 0. (19) Pluging (18) and (19) into (17) leads to R(???) ? R(?) ? (??? ? ?)' 1n Xn i=1 ?Q(z, ?) ?? + ? k?k1 ? k???k1and then R(???) ? R(?) ? k??? ? ?k1 sup 1?j?p 1n Xn i=1 ?Q(z, ?) ??j + ? k?k1 ? k???k1. (20) Now, we remind that we have the hypothesis ?j ? {1, . . . , p}, P 1n Xn i=1 12 ?Q(Zi, ?) ??j ? n??12 t! ? ?(t) that becomes, with a simple union bound argument, P  sup 1?j?p 1n Xn i=1 12 ?Q(Zi, ?) ??j ? n??12 t! ? p?(t) and so, if we put t = ??1(?/p), P  sup 1?j?p 1n Xn i=1 12 ?Q(Zi, ?) ??j ? n??12 ??1 ?p! ? ?. Also remark that n??1/2??1(?/p) = ??/4 ? ?/4. So until the end of the proof, we will work on the event (? ? : sup 1?j?p 1n Xn i=1 12 ?Q(Zi(?), ?) ??j ? ?4) true with probability at least 1 ? ?. Going back to 20, we have R(???) ? R(?) ? ?2 k??? ? ?k1 + ? k?k1 ? k???k118
and then R(???) ? R(?) + ?2 k??? ? ?k1 ? ? k??? ? ?k1 + k?k1 ? k???k1= ???Xp j=1 |(???)j ? ?j | + Xp j=1(|?j | ? |(???)j |)?? = ??? X j:?j 6=0 |(???)j ? ?j | + X j:?j 6=0(|?j | ? |(???)j |)?? that leads to the following inequality that will play a central role in the end of the proof: R(???) ? R(?) + ?2 k??? ? ?k1 ? 2? X j:?j 6=0 |(???)j ? ?j |. (21) First, if we remind that R(???) ? R(?) ? 0, (21) leads to k??? ? ?k1 ? 4 X j:?j 6=0 |(???)j ? ?j | and so X j:?j=0 |(???)j ? ?j | ? 3 X j:?j 6=0 |(???)j ? ?j |. So we can take v := ??? ? ? in Assumption A(?). So, (21) leads to R(???) ? R(?) + ?2 k??? ? ?k1 ? 2? X j:?j 6=0 |(???)j ? ?j | (22) ? 2???k?k0 X j:?j 6=0[(???)j ? ?j ]2??12 ? 2?k?k0 ? (??? ? ?)'M2 (??? ? ?)12 = 2?k?k0 ? hR(???) ? R(?)i12 . (23) We conclude that R(???) ? R(?) ? 4?2k?k0 ? . Remark that pluging this result into (22) to (23) gives k??? ? ?k1 ? 8?k?k0 ? . This ends the proof. 19
Proof of Proposition 3.1: First E(W(j) 1 + - - - +W(j) n )?? ?!A(j) ?,n ? ?! X 1?k1,...,k??nEW(j) k1 - - -W(j) k? . The same combinatorial arguments as in [DL99] yield for p ? 2q A(j) ?,n ? C(j) q,?,n +X??2 m=2A(j) m,nA(j) ??m,n, where (24) C(j) q,?,n ? (p ? 1)n nX?1 r=0(r + 1)??2cW(j),2q(r). (25) Let us now assume the condition (12) then C(j) q,?,n ? C(? ? 1)n nX?1 r=0(r + 1)??2?q ? C(? ? 1)n Z n+1 2 x??2?q dx, if ? ? q + 2 ? C(q + 1)(n + 1)2, if ? = q + 2 ? C ? ? 1 ? ? q ? 2 (n + 1)??q, if ? ? q + 2 ? C(? ? 1)n Z n 1 x??2?q dx, if ? < q + 2 ? C ? ? 1 q + 2 ? ?n, A rough bound is thus C(j) q,?,n ? C(? ? 1)n(??q)?1 and we thus derive A(j) 2,n ? Cn, A(j) 3,n ? 2Cn, A(j) 4,n ? 4Cn2 A(j) 5,n ? 8Cn2, A(j) 6,n ? 17Cn3. (26) Now using precisely condition (12) with the relation (24) we see that if a2 = 1, and a3 = 2 then the sequence recursively defined as am = m ? 1 + mX?2 k=2 akam?k (27) satisfies A(j) m ? amC[m2 ]n[m2 ]. Remember that dm ? 1m (2m ? 2)! ((m ? 1)!)2 , m = 2, 3, . . . (28) hence as in [DL99] we quote that am ? dm is less that the m-th Catalan number, dm and this ends the proof. 20
References [Aka73] H. Akaike. Information theory and an extension of the maximum likelihood principle. In B. N. Petrov and F. Csaki, editors, 2nd International Symposium on Information Theory, pages 267-281. Budapest: Akademia Kiado, 1973. [Alq08] P. Alquier. Density estimation with quadratic loss, a confidence intervals method. ESAIM: P&S, 12:438-463, 2008. [BC09] A. Belloni and V. Chernozhukov. ?1-penalized quantile regression in high-dimensional sparse models. preprint arXiv:0904.2931 to appear in the Annals of Statistics, 2009. [BC10] A. Belloni and V. Chernozhukov. High dimensional sparse econometric models: An introduction. to appear in the proceedings of "Stats in the Chateau", 2010. [BJMW10] K. Bartkiewicz, A. Jakubowskin, T. Mikosch, and O.Wintenberger. Infinite variances stable limits for sums of dependent random variables. Proba. Theory and Related Fields, 2010. [BRT09] P. J. Bickel, Y. Ritov, and A. Tsybakov. Simultaneous analysis of lasso and Dantzig selector. The Annals of Statistics, 37(4):1705- 1732, 2009. [BTW07] F. Bunea, A.B. Tsybakov, and M.H. Wegkamp. Aggregation for Gaussian regression. Annals of Statistics, 35:1674-1697, 2007. [BWT07] F. Bunea, M. Wegkamp, and A. Tsybakov. Sparse density estimation with ?1 penalties. Proceedings of 20th Annual Conference on Learning Theory (COLT 2007) -Springer, pages 530-543, 2007. [CDS01] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Review, 43(1):129-159, 2001. [Che95] S. S. Chen. Basis pursuit, 1995. Stanford University. [CT07] E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35, 2007. [DDL+07] J. Dedecker, P. Doukhan, G. Lang, S. Leon R., J. R.and Louhichi, and C. Prieur. Weak dependence: with examples and applications, volume 190 of Lecture Notes in Statistics. Springer, New York, 2007. [DL99] P. Doukhan and S. Louhichi. A new weak dependence condition and applications to moment inequalities. Stochastic Process. Appl., 84(2):313-342, 1999. [DN07] P. Doukhan and M. H. Neumann. Probability and moment inequalities for sums of weakly dependent random variables, with applications. Stochastic Process. Appl., 117(7):878-903, 2007. [Dou94] P. Doukhan. Mixing, volume 85 of Lecture Notes in Statistics. Springer-Verlag, New York, 1994. Properties and examples. 21
[EHJT04] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of Statistics, 32(2):407-499, 2004. [Heb09] M. Hebiri. Quelques questions de selection de variables autour de l'estimateur lasso, 2009. PhD Thesis, Universite Paris 7 (in english). [Hoe63] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13-30, 1963. [Kol] V. Koltchinskii. Sparsity in empirical risk minimization. Annales de l'Institut Henri Poincare, Probability and Statistics (to appear). [LP05] H. Leeb and B. M. Potscher. Sparse estimators and the oracle property, or the return of hodges' estimator. Cowles Foundation Discussion Papers 1500, Cowles Foundation, Yale University, 2005. [RBV08] F. Rapaport, E. Barillot, and J.-P. Vert. Classification of array-CGH data using fused SVM. Bioinformatics, 24(13):1375,1382, 2008. [Rio00] E. Rio. Theorie asymptotique pour des processus aleatoire faiblement dependants. SMAI, Mathematiques et Applications 31, Springer, 2000. [Ros61] M. Rosenblatt. Independence and dependence. Proceeding 4th. Berkeley Symp. Math. Stat. Prob. Berkeley University Press, pages 411-443, 1961. [Sch78] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6:461-464, 1978. [Tib96] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society B, 58(1):267-288, 1996. [vdGB09] S. A. van de Geer and P. Buhlmann. On the conditions used to prove oracle results for the lasso. Electronic Journal of Statistics, 3:1360-1392, 2009. [Win09] O. Wintenberger. Deviation inequalities for sums of weakly dependent time series. preprint arXiv:0911.1682, 2009. [ZH05] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society B, 67(2):301-320, 2005. 22