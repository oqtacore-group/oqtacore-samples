
A MARTINGALE APPROACH TO SCAN STATISTICS VLADIMIR POZDNYAKOV, JOSEPH GLAZ, MARTIN KULLDORFF, AND J. MICHAEL STEELE Abstract. Scan statistics are commonly used in biology, medicine, engineering and other fields where interest is in the probability of observing clusters of events in a window at an unknown location. Due to the dependent nature of the number of events in a large number of overlapping window locations, even approximate solutions for the simplest scan statistics may require elaborate calculations. We propose a new martingale method which allows one to approximate the distribution for a wide variety of scan statistics, including some for which analytical results are computationally infeasible. Keywords: Scan, run, pattern, martingale, stopping time. 2000 Mathematics Subject Classification: Primary: 60C05, 60G42; Secondary: 60G40, 62E17 1. Introduction Scan statistics are used in a wide range of fields including brain imaging (Yoshida et al. (2003)), psychology (Margai and Henry (2003)), veterinary medicine (Enemark et al. (2002)), forestry (Coulston and Riitters (2003)), crime hot-spot analysis Joseph Glaz and Vladimir Pozdnyakov: Department of Statistics, University of Connecticut, 215 Glenbrook Road, U-4120, Storrs, CT 06269-4120. Martin Kulldorff: Department of Ambulatory Care and Prevention, Harvard Medical School and Harvard Pilgrim Health Care, 133 Brookline Avenue, Boston, MA 02215-3920. J. Michael Steele: Wharton School, Department of Statistics, Huntsman Hall 447, University of Pennsylvania, Philadelphia, PA 19104. 1
2 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. (Kaminski et al. (2000)), industrial quality control (Shmueli (2003a,b)), and especially molecular biology (Durand and Sankoff (2003), Goldstein and Waterman (1992), Karlin and Brendel (1992), Naus and Sheng (1997), and Sheng and Naus (1994)). Four recent books summarize the current status of the field: Glaz and Balakrishnan (1999), Glaz et al. (2001), Balakrishnan and Koutras (2002) and Fu and Lou (2003). Different applications use scan statistics of different kinds. In the simple form considered by Naus (1965), there is a temporal Poisson point process which is considered over a fixed time length T and there is a fixed size window of much shorter length. We then move (or scan) the window continuously from the start to end, counting at each location the number of events within the window. The scan statistic is then defined as the maximum number of events as the window moves over all possible locations. In most applications, the main question of interest is whether the cluster of events defined by the maximum is a likely chance occurrence or not, so the most common null-hypothesis is that the point process is a homogeneous Poisson process. That is, we are interested in the probability of observing at least the observed number of events as the maximum, given that the null-hypothesis is true. More generally, we are interested in the distribution of the test statistic. The most commonly used variants of the scan statistic are (i) temporal and other one-dimensional scan statistics versus spatial, spatio-temporal and higher dimensional scan statistics; (ii) continuous scan statistics where events can occur anywhere versus discrete scan statistics with a sequence of trials at which the event either occurs or does not occur, (iii) a homogeneous versus known inhomogeneous background intensity defining the null-hypotheses, (iv) a conditional or unconditional
A MARTINGALE APPROACH TO SCAN STATISTICS 3 scan statistic where the conditioning is on the total number of events observed (v) a fixed versus variable size scanning window, (vi) single scan statistics with only one type of events versus double scan statistics with two or more types of events, and (vii) univariate versus multivariate scan statistics, with the latter simultaneously scanning multiple data streams. While simple to formulate, the probabilistic nature of the scan statistics is very complex due to the dependencies of the overlapping window locations considered. Exact derivations of the distribution function is only available for the simplest scenarios such as temporal scan statistics with fixed window size and a homogeneous null-hypothesis. Good approximations as well as lower and upper bounds are known for additional scan statistics, but for most practically important applications the scan statistic must be evaluated using simulations (Glaz et al. (2001)). Martingales have been used successfully for many practical statistical and probability problems, and their introduction has major impacts on fields such as survival analysis (Aalen (1978), Andersen et al. (1993)). In this article we present a martingale approach to scan statistics with which it is possible to obtain good approximations for the distribution of several scan statistics for which analytical results are not readily available. Using martingales, Li (1980) derived the first moment and we derive the second moment of the waiting time until we observe a specified number of events within one or several windows of specified lengths. Using these two moments we obtain approximations for the distribution of this waiting time. The martingale approach to derive the generating function and moments is an alternative approach to the Markov chain embedding method where the waiting time until reaching a pattern is represented as a hitting time at a state of a relevant
4 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. Markov chain. Elaborations on the Markov chain embedding methods and its applications to the theory of runs and patterns are given, among others, in Fu (1986, 1996 and 2001), Chao and Fu (1991), Fu and Koutras (1994), Uchida (1998), Aki and Hirano (1999), Antzoulakos (2001), Robin and Daudin (2001), Balakrishnan and Koutras (2002), Fu and Chang (2002), Fu and Lou (2003) and Han and Hirano (2003). Related methods on the occurrence of patterns include the method of Markov renewal embedding (Blom and Thornburn (1982) and Biggins and Cannings (1987)) and Markov chain embedding which uses analysis of exponential Markov chains (Stefanov and Pakes (1997) and Stefanov (2000)). Recently, Stefanov (2003) introduced a new approach to evaluate the generating function of the waiting time for a pattern generated by both discrete and continuous processes. The article is organized as follows. In Section 2, we present the martingale approach for deriving the first two moments and generating function of the distribution of the shortest waiting time until the occurrence of one of several predefined patterns in a sequence of iid discrete observations. In Section 3 we use the first two moments to approximate the waiting time distribution. In Section 4, we use these results to evaluate approximations for the distribution of fixed window scan statistics. The accuracy of these approximations is evaluated with the help of available lower and upper bounds. In Sections 5 through 7 new approximations are derived for the variable window scan statistics, the double scan statistics and the multivariate scan statistics. Finally, some concluding remarks and open issues are reviewed in Section 8.
A MARTINGALE APPROACH TO SCAN STATISTICS 5 2. Moments and Generating Functions Here we will derive the first and second moments and the generating function of the waiting time until we observe one element from a set of several predefined patterns. We will then show how these moments yields a computationally feasible approximation for the distribution of the waiting time. 2.1. Expected Time. Let Z be an arbitrary discrete random variable which takes values in the set :, and let fZ;Zkgk¸1 be a sequence of independent, identically distributed random variables. Consider a collection of finite sequences fAjg1-j-K over :, and without loss of generality assume that no sequence contains another as a subsequence. Next, we denote by ?Aj the waiting time until Aj occurs as a run in the series Z1;Z2; :::. We are interested in both expected time of (1) ? = minf?A1 ; :::; ?AKg and probabilities ¼j = P!? = ?Aj ¢. The martingale approach to this problem was introduced in an elegant paper of Li(1980), and it has been further developed by Gerber and Li (1981), Williams (1991), Blom et al. (1994), and Pozdnyakov and Kulldorff (2003). For clarity of presentation, we will briefly review some of these results. Following Li (1980), we introduce a measure of the amount of overlap between two sequences. Let A = (a1; :::; am) and B = (b1; :::; bk) be two sequences over the alphabet :, and for each pair (i; j) we write ±i j =8>>< >>: 1=P(Z = bj) if 1 - i - m; 1 - j - k; and ai = bj 0 otherwise.
6 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. Next, we define A ¤ B by setting (2) A ¤ B = ±1 1±2 2 ¢ ¢ ¢ ±mm + ±2 1±3 2 ¢ ¢ ¢ ±mm!1 + ::: + ±m1; and we set | = (¼1; :::; ¼K)?, Y = (y1; :::; yK)?. Finally, we consider the matrix (3) M = 266666666664 A1 ¤ A1 A1 ¤ A2 ::: A1 ¤ AK A2 ¤ A1 A2 ¤ A2 ::: A2 ¤ AK ::: ::: ::: ::: AK ¤ A1 AK ¤ A2 ::: AK ¤ AK 377777777775 ; which Gerber and Li (1981) proved to be nonsingular. One has two notable results of Li (1980): Theorem 1. (Li, 1980) The expected value of ? is given by E(? ) = 1 y¤1 + ¢ ¢ ¢ + y¤K ; where Y ¤ = (y¤1; :::; y¤K)? is the unique solution to the linear system MY = 1; and 1 = (1; :::; 1)?. Theorem 2. (Li, 1980) The vector of probabilities | = (¼1; :::; ¼K)? satisfy equation M?| = E(? )1: 2.2. Generating function. Martingale arguments for finding the generating function of the waiting time in the case of one pattern were originally developed by Gerber and Li (1981). In their method, the transition from one pattern to many is based on some results on hitting times in a Markov chain, but our approach is based on matching expressions of the stopped martingale for different terminal patterns. This alternate method is intuitive and simple; moreover, it can be employed to get higher order moments.
A MARTINGALE APPROACH TO SCAN STATISTICS 7 To see how this works, we first consider a simple example first, and then we will show how it can be generalized. Example 1. We flip a fair coin and we wait for one of two sequences: A1 = HH and A2 = HTH. We are interested in the generating function of ? = minf?A1 ; ?A2g. Assume that we have two teams of gamblers. Before nth round a new gambler from the first team joins the game and starts betting y1rn dollars on the sequence A1; here 0 < r < 1 and y1 is a number that we will choose later. If Zn 6= H, then he leaves the game with nothing. If Zn = H, he doubles his money, and bets the whole fortune on the event that Zn+1 = H. If he win, he leaves the game with 4y1rn dollars. If he loses, then again he leaves with nothing. The second team bets in the similar fashion on the sequence A2 but the initial bet of the gambler who joins game at nth round is y2rn. Let Xn be the net casino gain at moment n. Since the amount of each bet at nth round is always determined by the history up to the moment n ! 1, and in each case the odds are fair, therefore, the net casino gain is a martingale. It is easy to see that X? =8>>< >>: (y1 + y2)rr?!1 r!1 ! [y1 £ (4r?!1 + 2r? ) + y2 £ 2r? ]; if ? = ?A1 ; (y1 + y2)rr?!1 r!1 ! [y1 £ 2r? + y2 £ (8r?!2 + 2r? )]; if ? = ?A2 : which simplifies to X? =8>>< >>: (y1 + y2)rr?!1 r!1 ! [(4=r + 2)y1 + 2y2]r? ; if ? = ?A1 ; (y1 + y2)rr?!1 r!1 ! [2y1 + (8=r2 + 2)y2]r? ; if ? = ?A2 : Now let us assume that we can choose the initial bets (y¤1; y¤2) in such way that (4=r + 2)y¤1 + 2y¤2 = 1 2y¤1 + (8=r2 + 2)y¤2 = 1:
8 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. Then regardless which sequence occurs first the stopped martingale is given by X? = (y¤1 + y¤2)rr? ! 1 r ! 1 ! r? : Since the expected value of ? is finite, and the increments of the martingale Xn is almost sure bounded, we find by the Optional-Stopping Theorem that 0 = EX? = (y¤1 + y¤2) r r ! 1Er? ! (y¤1 + y¤2) r r ! 1 ! Er? ; and we may solve for Er? to obtain Er? = 1 ! 1 r 1!r(y¤1 + y¤2) + 1: This method also works in the general situation of K stopping sequences, provided that one makes the natural alterations. First we introduce a slightly modified measure of the amount of overlap between two sequences. If A = (a1; :::; am) and B = (b1; :::; bk) are two sequences over : then we define (4) A ¤ B(r) = ±1 1±2 2 ¢ ¢ ¢ ±mm=rm!1 + ±2 1±3 2 ¢ ¢ ¢ ±mm!1=rm!2 + ::: + ±m1=1: Assume that we have K teams that bet on the K sequences in the correspondence with the rules of fair odds as they are described in the above example and the nth player from jth team start his betting on the sequence Aj with an initial bet of yjrn dollars. The net casino gain at time ? is given by X? =8>>>>>>>>>>< >>>>>>>>>>: (y1 + ::: + yK)rr?!1 r!1 !PKi=1 A1 ¤ Ai(r)yir? ; if ? = ?A1 ; (y1 + ::: + yK)rr?!1 r!1 !PKi=1 A2 ¤ Ai(r)yir? ; if ? = ?A2 ; ::: ::: (y1 + ::: + yK)rr?!1 r!1 !PKi=1 AK ¤ Ai(r)yir? ; if ? = ?AK:
A MARTINGALE APPROACH TO SCAN STATISTICS 9 Let (5) M(r) = 266666666664 A1 ¤ A1(r) A1 ¤ A2(r) ::: A1 ¤ AK(r) A2 ¤ A1(r) A2 ¤ A2(r) ::: A2 ¤ AK(r) ::: ::: ::: ::: AK ¤ A1(r) AK ¤ A2(r) ::: AK ¤ AK(r) 377777777775 : Note that M(1) = M and as it was shown in Gerber and Li (1981) the matrices M(r) are non-singular for all 0 < r - 1. The method of Example 1 then yields a general result. Theorem 3. The generating function of ? is given by Er? = 1 ! 1 r 1!r(y¤1 + ::: + y¤K) + 1; where Y ¤ = (y¤1; :::; y¤K)? is the unique solution to the linear system M(r)Y = 1: 2.3. Second Moment. It is perhaps surprising that a more elaborate scheme is needed to apply this general idea of matching the stopped martingale to compute of the second moment of ? . The crucial idea is to introduce two teams for each sequence (i.e. in total we have 2K teams), and to illustrate the idea, we again consider a sequence of Bernoulli trials. Example 2. We flip a fair coin and we wait until we observe one of two sequences: A1 = HH and A2 = HTH. Our goal is to find the second moment of waiting time ? = minf?A1 ; ?A2g. The gambling is organized now in the following way. When a gambler from the first team of those two that bet on Aj joins the game at the nth round he starts his betting with yjn dollars, a gambler from the second team bets zj dollars.
10 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. The net casino gain at the moment ? is given by X? =8>>>>>>>>>>< >>>>>>>>>>: (y1 + y2) ?(?+1) 2 + (z1 + z2)? ![y1(4(? ! 1) + 2? ) + y2? + 6z1 + 2z2]; if ? = ?A1 ; (y1 + y2) ?(?+1) 2 + (z1 + z2)? ![y12? + y2(8(? ! 2) + 2? ) + 2z1 + 10z2]; if ? = ?A2 : Rearranging terms we get X? =8>>>>>>>>>>< >>>>>>>>>>: (y1 + y2) ?(?+1) 2 + (z1 + z2)? ![(6y1 + 2y2)? + 4(!1)y1 + 6z1 + 2z2]; if ? = ?A1 ; (y1 + y2) ?(?+1) 2 + (z1 + z2)? ![(2y1 + 10y22)? + 8(!2)y2 + 2z1 + 10z2]; if ? = ?A2 : Now, let us assume that we can choose the initial bets (y¤1; y¤2) and (z¤1 ; z¤2 ) in such way that we have the relation 6y¤1 + 2y¤2 = 1 2y¤1 + 10y¤2 = 1 and the relation 4(!1)y¤1 + 6z¤1 + 2z¤2 = 1 8(!2)y¤2 + 2z¤1 + 10z¤2 = 1: For such a choice of initial bets the stopped martingale is given by X? = (y¤1 + y¤2) ? (? + 1) 2 + (z¤1 + z¤2 )? ! ? ! 1: After taking the expected value of both sides of the last equation and solving it with respect to E? 2 we get a formula for the second moment. Naturally one needs to employ the Optional Stopping Theorem here, and, a bit later, we will show that this is indeed justified.
A MARTINGALE APPROACH TO SCAN STATISTICS 11 Now, to write the value of the net casino gain at the moment ? we first need to introduce the following notation. If A = (a1; :::; am) and B = (b1; :::; bk) are two sequences over :, then we define (6) A ? B = !±1 1±2 2 ¢ ¢ ¢ ±mm(m ! 1) ! ±2 1±3 2 ¢ ¢ ¢ ±mm!1(m ! 2) ! ::: ! ±m10: The stopped martingale X? is given by X? =8>>>>>>>>>>>>>>>>>>>>>>< >>>>>>>>>>>>>>>>>>>>>>: PKi=1 yi ?(?+1) 2 +PKi=1 zi? !PKi=1 A1 ¤ Aiyi? !PKi=1 A1 ? Aiyi !PKi=1 A1 ¤ Aizi; if ? = ?A1 ; PKi=1 yi ?(?+1) 2 +PKi=1 zi? !PKi=1 A2 ¤ Aiyi? !PKi=1 A2 ? Aiyi !PKi=1 A2 ¤ Aizi; if ? = ?A2 ; ::: ::: PKi=1 yi ?(?+1) 2 +PKi=1 zi? !PKi=1 AK ¤ Aiyi? !PKi=1 AK ? Aiyi !PKi=1 AK ¤ Aizi; if ? = ?AK: Let us define (7) N = 266666666664 A1 ? A1 A1 ? A2 ::: A1 ? AK A2 ? A1 A2 ? A2 ::: A2 ? AK ::: ::: ::: ::: AK ? A1 AK ? A2 ::: AK ? AK 377777777775 : Suppose that we can find such Y ¤ = (y¤1; :::; y¤K)? and Z¤ = (z¤1 ; :::; z¤K)? that MY ¤ = 1 NY ¤ +MZ¤ = 1
12 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. then the stopped martingale X? is given by X? =XK i=1 y¤i ? (? + 1) 2 + XK i=1 z¤i ? ! ? ! 1: Now it is time to apply the Optional Stopping Theorem. However, the increments of the net casino gain Xn are no longer bounded almost sure, so we need a stronger version. The classical Doob's Optional-Stopping Theorem (e.g., Shiryaev (1995, p. 485)) will do the trick; one just needs to note that Xn is at most O(n2), but P(? > n) goes to zero at exponential rate. After some algebra we get a general formula for E? 2. Theorem 4. Let Y ¤ = (y¤1; :::; y¤K)? and Z¤ = (z¤1 ; :::; z¤K)? be the unique solution to the linear system MY ¤ = 1 NY ¤ +MZ¤ = 1 then E? 2 = 1 + (1 !PKi=1 z¤i !PKi=1 y¤i =2)E? PKi=1 y¤i =2 : 3. Approximating the Distribution of the Waiting Time With the first two moments in hand, we can approximate the distribution of the waiting time ? with the help from several possible benchmark distributions. This choice is critical, and the most natural choices may not be the best.In some circumstances one can do better than to use exponential, gamma or Weibull. When selecting the best approximation, it is important to realize that for our purposes the accuracy in the tail of the distribution is important because we are interested in the probability of the waiting time being larger than T, where T is
A MARTINGALE APPROACH TO SCAN STATISTICS 13 relatively far away from 0. Moreover, as time goes on without observing the desired event, the process is more and more independent of the starting conditions, and hence, P(? = Tj? > T ! s) is approximately equal to P(? = T ! 1j? > T ! s ! 1) for large T. This is the property of a homogeneous Poisson process, and hence we would expect that the tail of the waiting time distribution is approximately exponential. This leads us to suggest using the distribution of random variable c + X to approximate the distribution of ? , where c = 1 ! ¾ is a constant, X is exponentially distributed with parameter ¾, 1 = E(? ), and ¾2 = Var(? ). This ensures that the approximate distribution has the same first two moments as the true distribution. We call this the shifted exponential distribution, and it suggests that P(? - n) ¼ 1 ! exp(!(n + 0:5 + ¾ ! 1))=¾); where the 0:5 term is a continuity correction. To show that this is indeed a good approximation of the distribution, we will compare it with two other candidates: 1) exponential P(? - n) ¼ 1 ! exp(!(n ! l)=1); where l is the length of the shortest sequence 2) gamma P(? - n) ¼ 1 !(a) Z (n!l)=b 0 xae!xdx; where l is again the length of the shortest sequence, b = ¾2=1, and a = 1=b. Here the factor l has been introduced to improve the performance of these two approximations, but we will see that even with the best choice of l the shifted exponential distribution does better than each of these. We also investigated the
14 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. Weibull distribution based approximation. But the Weibull approximations are significantly worse than those of the exponential and the gamma, so we omit them. 4. Fixed Window Scan Statistics Example 3. Assume that we observe a sequence of Bernoulli trials where the probability of failure is known and relatively small - 5%. We have an alert if we observe too many failures during a short period of time. Specifically, we stop the process if we observe three or more failures in any 5 sequential trials. The first question is how long we have to wait for an alert which is caused purely by randomness, and this problem can be easily addressed by Theorem 1. Indeed, we have an alert when the following runs occur first time: (1) 3-out-of-3 - FFF, (2) 3-out-of-4 - FFSF; FSFF, (3) 3-out-of-5 - FFSSF; FSFSF; FSSFF. By Theorem 1 and easy numerical calculations one finds the expected time is 1608:4. Moreover, Theorem 4 tells us that the standard deviation of the waiting time is 1604:8, a value that is notably close to the mean. Still, this is not surprising; an alert is a rare event and dependence between two consecutive alerts is weak, so one expects the distribution of the waiting time to be approximately exponential. For the fixed window scan statistic, Glaz and Naus (1991) developed tight lower and upper bounds which are presented in Tables 1 and 2 along with the approximations based on the exponential, shifted exponential, and gamma distributions. As can be seen, the shifted exponential approximation performs consistently well, and it has the reassuring feature of staying between the lower and upper bounds. When 1 is large and ¾ is close to 1, the differences between the various approximations are marginal and all of the estimates are close to the true probability, but one should note if 1 is relatively small and ¾ differs from 1, the approximations based
A MARTINGALE APPROACH TO SCAN STATISTICS 15 on the exponential and gamma distributions do not perform as well as the shifted exponential approximations. In conclusion, we see that the first two moments are sufficient to obtain a very good approximation for the fixed window scan statistic. We will see shortly that the martingale approach can be successfully used for other scan statistics, even those for which no good bounds or approximations were known earlier. 5. Scan Statistics with a Variable Window Size When searching for clusters, the cluster size is often unknown. That means that we do not know the proper window size to use. For example, if we use a window size of 3 days we may be unable to detect a 3 week cluster, or vice versa. To solve this problem, Loader (1991) and Kulldorff (1997) used the likelihood function instead of the event count to rank the potential clusters. This means, for example, that a cluster with 5 events during 10 days may be ranked higher than both a cluster with 6 events during 20 days and a cluster with 2 events during 4 days. Example 4. Suppose that in a sequence of 30 Bernoulli trials with probability of failure p = :25 we observe a window of size 7 with 5 events, and we want to know the probability of observing a cluster of this or higher likelihood during 30 random trails. The first step is then to find other cluster with higher likelihood, which turn out to be a window of 5 with four events and a window of 3 with 3 events. This is, we should monitor for the following three types of alerts: (1) when we observe an F run of length 3, (2) at least 4 F out of 5 consecutive trials, (3) at least 5 F out of 7 consecutive trials.
16 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. It is easy to see then that the alerts of all kinds are produced by only three sequences: FFF; FFSFF; FFSFSFF: Therefore, by Theorems 1 and 4 we find that the expected time for an alert is 72:345, and standard deviation is 69:828. By Theorem 3 and the help of Mathematica, one can show also that for an arbitrary p one has E(r? ) = P(r) Q(r) ; where P(r) = p3r3 + p4qr5 + p5q2r7 and where Q(r) = 1 + (!1 + p)r + (!p + p2)r2 + (!p2 + p3 + p2q)r3 + (!p2q + p3q)r4 + (!p3q + p4q + p3q2)r5 + (!p3q2 + p4q2)r6 + (!p4q2 + p5q2)r7: Since E(? ) = @E(r? ) @r ¯¯¯ r=1; one can get the expected time via differentiation. Now, going back to the original problem, we can see that observing a cluster 5-out-of-7 failures in the sequence of 30 trials is not a rare event since the expected time till having this cluster (or a more extreme one) is about 70. The shifted exponential approximation gives a p-value which is approximately equal to :33. The simulated (10000 simulations) p-value is also ¼ :33.
A MARTINGALE APPROACH TO SCAN STATISTICS 17 Example 5. Assume that we observe iid Bernoulli trials with p = :01 and we scan for (1) at least 2 failures in 10 consecutive trials, (2) or at least 3 in 50 consecutive trials. We are interested in the approximation for the distribution of the waiting time till one of these two situations occur. The total number of stopping patterns that trigger these two alerts is 224. In this case, the exponential and gamma approximations are especially interesting, because it is difficult to get the exact distribution of ? , to the best knowledge the most efficient method is the computationally heavy Markov chain embedding method given by Antzoulakos (2001). The introduced approximations could be useful provided they are accurate, and as we will see they are.The numerical results are given in Table 3, and compared with estimated probabilities based on 100000 replications. We see that the two moment approximation based on the shifted exponential distribution performs quite well, and these approximations are the first approximations that anyone has given for this variable window scan statistic. 6. Double Scan Statistics Naus and Wartenberg (1997) and Naus and Stefanov (2002) considered double scan statistic where one is interested in the probability of observing a cluster where the window contains at least k1 events of type 1 and at least k2 of events of type 2. The martingale approach works for these types of scan statistics as well. Example 6. Assume that we have two types of failures F1 and F2 and suppose that we stop if we have three failures of the first type in a row or at least two F2 out of three consecutive trials. The waiting time for an alert caused by randomness
18 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. is determined by the first occurrence of any of the following four runs: (1) F1F1F1, (2) F2F2, (3) F2F1F2, and (4) F2SF2. If we let P(F1) = p1, P(F2) = p2, and P(S) = q = 1 ! p1 ! p2, then the matrix M(r) is given by M(r) = 266666666664 1 p31r2 + 1 p21r + 1 p1 0 0 0 0 1 p22r + 1 p2 1 p2 1 p2 0 1 p2 1 p22qr2 + 1 p2 1 p2 0 1 p2 1 p2 1 p22qr2 + 1 p2 377777777775 ; and by solving the system M(r)Y = 1 we get generating function for ? E(r? ) = 1 ! 0@1 + r 1 ! r 0@ 1 1 p1 + 1 p21+ 1 r2p31 + 1 p1 + 1q + 1 p1q 1 p2 31q + 1 p1 31 + 1q + 1 rp2q´´1A1A!1 : Here for a natural numerical example, we note that if p1 = :04, p2 = :01, and q = :95, then we get E(? ) = @E(r? ) @r ¯¯¯ r=1 = 3897:7: To find the standard deviation of the waiting time, we now only need to take the second derivative of the generating function, the standard deviation can also be calculated via Theorem 4. In particular, when p1 = :04, p2 = :01, and q = :95, the standard deviation is equal to 3895:6. The closeness of 1 and ¾ suggests that again the exponential approximation to the distribution of ? may be appropriate. Example 7. Assume we have a scanning window of length 10 and we stop the scanning process if we have one of the following two situations: (1) at least two failures of type two, F2, (2) at least three failures of any kind.
A MARTINGALE APPROACH TO SCAN STATISTICS 19 The total number of stopping sequences is 153. We have 1) 9 sequences with exactly two F2 F2F2; F2SF2; :::; F2SSSSSSSF2; 2) 108 sequences with exactly two F1 and one F2 F2F1F1; F2SF1F1; F2F1SF1; :::; F2SSSSSSSF1F1; :::; F2F1SSSSSSSF1; F1F2F1; F1SF2F1; F1F2SF1; :::; F1SSSSSSSF2F1; :::; F1F2SSSSSSSF1; F1F1F2; F1SF1F2; F1F1SF2; :::; F1SSSSSSSF1F2; :::; F1F1SSSSSSSF2; 3) and 36 with exactly three F1 F1F1F1; F1SF1F1; F1F1SF1; :::; F1SSSSSSSF1F1; :::; F1F1SSSSSSSF1: As we can see from Tables 4 and 5 all the approximations do well if 1 is large, and the shifted exponential does better if 1 is relatively small. 7. Multivariate Scan Statistics For a multivariate scan statistic, we have multiple data streams and we have common scanning window. We are interested in the probability of simultaneously observing a specified number of events in each data stream. For example, we may be interested in the probability of seeing at least 3 events in data stream A and 5 events in data stream B during any 10 day period. The probability may be different for the events in the different data streams. Example 8. Let fZigi¸1 will be iid sequence of bivariate random variables, i.e. Zi = [Z(1) i ;Z(2) i ]?. Assume that Z(j) i 2 f1; 2; 3g; j = 1; 2
20 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. and pkm = P(Z(1) i = k;Z(2) i = m); k;m = 1; 2; 3: We stop at time ? if (1) Z(1) ?!1 + Z(1) ? ¸ 5 or (2) Z(2) ?!1 + Z(2) ? = 6. This stopping rule is determined by 33 stopping sequences: 266433 113775 266433 213775 266433 123775 266433 313775 266433 133775::: Now the question is how to compute E(? ). At first glance this "two-dimensional" situation seems significantly different from the considered earlier examples, but it is not. To see how easy it is, we first introduce the following 9-letter alphabet of 2-tuples: 2664113775 2664123775 2664133775 2664213775 2664223775 2664233775 2664313775 2664323775 2664333775 In this alphabet each of the 33 sequences is identified with a two-letter word, so we can again apply our earlier results without any changes. For example, if probabilities pkm are given by :7 :05 :02 :1 :04 :01 :05 :02 :01 then the expected waiting time is 37.007 and the standard deviation is 35.633. Finally, let us provide numerical results in the case of more realistic multivariate iid sequences. Specifically, let us consider a sequence with a different distribution
A MARTINGALE APPROACH TO SCAN STATISTICS 21 over the 9-letter alphabet: :9 :03 :02 :02 :01 :005 :005 :005 :005 Table 6 contains the numerical results for this example. Example 9. Assume fZigi¸1 is an iid sequence of bivariate random variables, i.e. Zi = [Z(1) i ;Z(2) i ]?, where each component is a Bernoulli random variable with the following joint distribution: P(Z(1) i = 0;Z(2) i = 0) = :98; P(Z(1) i = 1;Z(2) i = 0) = :005; P(Z(1) i = 0;Z(2) i = 1) = :005; P(Z(1) i = 1;Z(2) i = 1) = :01: In each row we have a scanning window of length 5, and we stop if in one of the two windows we have at least 2 ones. As before first let us introduce the following 4-letter alphabet: 2664003775 2664103775 2664013775 2664113775 In this new alphabet we have 40 stopping sequences that correspond to the stopping rule described above. Numerical results are presented in Table 7. 8. Discussion The martingale approach yields a formula like that of Theorem 4 for any moment of ? , and, in theory, higher moments should provide better scan statistics approximations. Nevertheless, for the scan statistics of importance in practice, it is evident two moments are all one needs to get very good estimates.
22 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. We used the martingale approach for a number of different scan statistics, but we view it as a general tool of wide applicability. We believe that the martingale methods can also be applied for continuous, inhomogeneous, or spatial scan statistics - all of which are of practical importance. We are less optimistic about the utility of the martingale approach for conditional scan statistics, except to the extent that the unconditional scan statistic is sometimes a good approximation of the conditional scan statistic. When one compares the martingale approach to the Markov chain embedding method recently developed by Antzoulakos (2001), Fu (2001) and Fu and Chang (2002), one finds that neither method dominates the other - each has its own advantages and disadvantages. The martingale approach always results in a smaller set of linear equations to be solved, sometimes significantly reducing computational complexity. Also the martingale method can be used to obtain higher moments. On the other hand, the Markov chain embedding method works for Markov dependent trials, but the martingale approach does not seem to be able to cover this case. References [1] Aalen, O.O. (1978). Nonparametric inference for a family of counting processes. Ann. Statist.. 6, 701-726. [2] Aki, S. and Hirano, K. (1999) Sooner and later waiting time problems for runs in Markov dependent bivariate trials, Ann. Inst. Statist. Math., 51, 17-29. [3] Andersen, P.K., Borgan, 0., Gill, R.D., and Keiding, N. (1993) Statistical methods based on counting processes, Springer Series in Statistics, Springer-Verlag, New York [4] Antzoulakos, D. (2001) Waiting times for patterns in a sequence of multistate trials, J. Appl. Probab. 38, 508-518.
A MARTINGALE APPROACH TO SCAN STATISTICS 23 [5] Balakrishnan, N. and Koutras, M.V. (2002) Runs and Scans with Applications, John Wiley & Sons, Inc., New York. [6] Biggins, J. D. and Cannings, C. (1987). Markov renewal processes, counters and repeated sequences in Markov chains, Adv. Appl. Prob., 19, 521-545. [7] Blom, G. and Thorburn, D. (1982). How many random digits are required until given sequences are obtained?, J. Appl. Prob., 19, 518-531. [8] Blom, G., Holst, L., and Sandell, D. (1994) Problem and snapshots from the world of probability, Springer-Verlag, New York. [9] Breen, S., Waterman, M., and Zhang, N. (1985) Renewal theory for several patterns, J. Appl. Prob., 22, 228-234. [10] Chao, M. T. and Fu, J. C. (1991) The reliability of large series systems under Markov structure, Adv. Appl. Probab. 23, 894-908. [11] Chrysaphinou, O. and Papastavridis, S. (1990) The occurrence of a sequence of patterns in repeated dependent experiments, Theory of Probability and Applications, 35, 145-152. [12] Coulston J., and Riitters K. (2003) Geographic Analysis of Forest Health Indicators Using Spatial Scan Statistics. Environmental Management, 31, 764-773. [13] Durand, D. and Sankoff, D. (2003) Tests for gene clustering. J. Comput. Biology, (in press). [14] Enemark, L., Ahrens, P., Juel, D., Petersen, E., Petersen, R., Andersen, J., Lind, P., Thamsborg, S. (2002) Molecular characterization of Danish Cryptosporidium parvum isolates. Parasitology, 125, 331-341. [15] Feller, W (1968) An introduction to probability theory and its applications, Vol 1, 3rd ed. Wiley, New York [16] Fu, J. C. (1986) Reliability of consecutive-k-out-of-n: F systems with (k ! 1)-step Markov dependence, IEEE Trans. Reliability, R35, 602-606. [17] Fu, J. C. (1996) Distribution theory of runs and patterns associated with a sequence of multi-state trials, Statistics Sinica, 6, 957-974. [18] Fu, J. (2001) Distribution of the scan statistics for a sequence of bistate trials, J. Appl. Prob., 38, 908-916.
24 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. [19] Fu, J. and Chang, Y. (2002) On probability generating functions for waiting time distribution of compound patterns in a sequence of multistate trials, J. Appl. Prob., 39, 70-80. [20] Fu, J. C. and Koutras, M. V. (1994) Distribution theory of runs: A Markov chain approach, J. Amer. Statist. Assoc. 78, 168-175. [21] Fu, J. C. and Lou, W. Y. W. (2003). Distribution Theory of Runs and Patterns, World Scientific Publishing, Singapore. [22] Gerber, H. and Li, S. (1981) The occurrence of sequence patterns in repeated experiments and hitting times in a Markov chain, Stochastic Processes and their Applications, 11, 101-108. [23] Glaz, J. and Balakrishnan, N. (Eds.) (1999) Recent Advances on Scan Statistics, Birkhauser Publishers, Boston. [24] Glaz, J. and Naus, J. (1991) Tight bounds for scan statistics probabilities for discrete data, Ann. Appl. Probab., 1, 306-318. [25] Glaz, J., Naus, J. and Wallenstein, S. (2001) Scan Statistics, Springer, New-York. [26] Goldstein, L. and Waterman, M. S. (1992) Poisson, compound Poisson and process approximations for testing statistical significance in sequence comparisons. Bull. Math. Biology, 54, 785-812. [27] Han, Q. and Hirano, K. (2003) Sooner and later waiting time problems for patterns in Markov dependent trials, J. Appl. Probab., 40, 73-86. [28] Kaminski, R., Jefferis, E., and Chanhatasilpa, C. (2003) A spatial analysis of American police killed in the line of duty. In Turnbull et al. (eds.), Atlas of crime: Mapping the criminal landscape, Oryx Press, Phoenix, AZ. [29] Karlin, S. and Brendel, V. (1992) Chance and statistical significance in protein and DNA sequence analysis. Science 257, 39-49. [30] Kulldorff, M. (1997) A spatial scan statistic. Comm. Statist. Theory and Methods, 26, 1481-1496. [31] Li, S. (1980) A martingale approach to the study of occurrence of sequence patterns in repeated experiments, The Annals of Probability, 8, 1171-1176. [32] Loader, C. (1991) Large deviation approximations to distribution of scan statistics. Adv. Appl. Probab., 23, 751-771.
A MARTINGALE APPROACH TO SCAN STATISTICS 25 [33] Margai, F. and Henry, N. (2003) A community-based assessment of learning disabilities using environmental and contextual risk factors. Social Science and Medicine, 56, 1073-1085. [34] Naus, J. I. and Sheng, K. N. (1996) Screening for unusual matched segments in multiple protein sequences, Comm. Statist., Sim. Comput., 25, 937-952. [35] Naus, J. I. and Sheng, K. N. (1997) Matching among multiple random sequences. Bull. Math. Biol., 59, 483-496. [36] Naus, J.I., Stefanov, V.T. (2002) Double-scan statistics. Method. Comput. Appl. Probab., 4, 163-180. [37] Naus, J. I. and Wartenberg, D. A. (1997) A double-scan statistic for clusters of two types of events. J. Amer. Stat. Assoc. 92, 1105-1113. [38] Pozdnyakov, V. and Kulldorff, M. (2003) On the occurrence of sequence patterns: an alternative proof and extended results, preprint. [39] Robin, S. and Daudin, J.-J. (2001) Exact distribution of the distances between any occurence of a set of words, Ann. Inst. Statist. Math., 53, 895-905. [40] Shiryaev, A.N. (1995) Probability (Springer, New York, 2nd Edition). [41] Sheng, K.-N. and Naus, J. (1994) Pattern matching between two non-aligned random sequences. Bull. Math. Biology, 56, 1143-1162. [42] Shmueli, G. (2003a) Computing consecutive-type reliabilities non-recursively. IEEE Trans. Reliab., 52, in press. [43] Shmueli, G. (2003b) System-wide probabilities for systems with runs and scans rules. Method. Comput. Appl. Probab. 4, 401-419. [44] Stefanov, V. T. (2000). On some waiting time problems, J. Appl. Prob., 37, 756-764. [45] Stefanov, V. T. (2003). The intersite distances between pattern occurrences in strings generated by general discrete-and continuous-time models: an algorithmic approach, J. Appl. Prob., 40, 881-892. [46] Stefanov, V. T. and Pakes, A. G. (1997). Explicit distributional results in pattern formation, Ann. Appl. Prob., 7, 666-678. [47] Uchida, M. (1998) On generating functions of waiting time problems for sequence patterns of discrete random variables, Ann. Inst. Statist. Math., 50, 655-671.
26 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. [48] Williams, D. (1991) Probability with martingales, Cambridge University Press, Cambridge. [49] Yoshida, M., Naya, Y., and Miyashita, Y. (2003) Anatomical organization of forward fiber projections from area TE to perirhinal neurons representing visual long-term memory in monkeys. Proceedings of the National Academy of Sciences of the United States of America, 100, 4257-4262.
A MARTINGALE APPROACH TO SCAN STATISTICS 27 shifted upper lower n exponential exponential gamma bound bound 500 0.01600 0.01589 0.01597 0.01588 0.01589 1000 0.03183 0.03173 0.03179 0.03171 0.03174 1500 0.04741 0.04731 0.04736 0.04729 0.04733 2000 0.06274 0.06265 0.06267 0.06262 0.06267 2500 0.07782 0.07773 0.07775 0.07770 0.07776 3000 0.09266 0.09258 0.09258 0.09254 0.09261 4000 0.12162 0.12155 0.12154 0.12150 0.12169 5000 0.14966 0.14960 0.14957 0.14954 0.14965 Table 1. Fixed window scans: at least 3 out of 10, P(F) = :01, 1 = 30822, ¾ = 30815 shifted upper lower n exponential exponential gamma bound bound 50 0.09110 0.07827 0.08268 0.07713 0.07940 60 0.10977 0.09770 0.10059 0.09543 0.09989 70 0.12807 0.11672 0.11828 0.11337 0.11991 80 0.14599 0.13534 0.13573 0.13095 0.13949 90 0.16354 0.15357 0.15292 0.14819 0.15864 100 0.18073 0.17141 0.16985 0.16508 0.17736 Table 2. Fixed window scans: at least 4 out of 20, P(F) = :05, 1 = 481:59, ¾ = 469:35
28 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. shifted simulated n exponential exponential gamma N=100000 50 0.05857 0.05085 0.05542 0.05029 60 0.07033 0.06285 0.06685 0.06187 70 0.08195 0.07470 0.07817 0.07404 80 0.09342 0.08640 0.08939 0.08623 90 0.10474 0.09796 0.10050 0.09718 100 0.11593 0.10936 0.11150 0.11058 Table 3. Variable window: at least 2 out of 10 or at least 3 out of 50, P(F) = :01, 1 = 795:33, ¾ = 785:85 shifted simulated n exponential exponential gamma N=100000 10 0.02438 0.01480 0.02175 0.01401 15 0.03932 0.03015 0.03568 0.03084 20 0.05403 0.04527 0.04959 0.04508 25 0.06851 0.06015 0.06342 0.06169 30 0.08277 0.07479 0.07714 0.07590 35 0.09681 0.08921 0.09074 0.09134 40 0.11064 0.10340 0.10419 0.10529 45 0.12425 0.11738 0.11749 0.11878 50 0.13766 0.13113 0.13063 0.13342 Table 4. Double scans: three F1 in a row or at least two F2 out of 3, P(F1) = :04, P(F2) = :01, 1 = 324:09, ¾ = 318:34
A MARTINGALE APPROACH TO SCAN STATISTICS 29 shifted simulated n exponential exponential gamma N=100000 100 0.02706 0.02625 0.02681 0.02713 200 0.05393 0.05318 0.05352 0.05489 300 0.08004 0.07936 0.07955 0.08052 400 0.10544 0.10481 0.10488 0.10639 500 0.13014 0.12957 0.12953 0.13299 Table 5. Double scans: at least two F2 out of 10 or at least three of any kind out of 10, P(F1) = :01, P(F2) = :005 1 = 3571:8 ¾ = 3566:2 shifted simulated n exponential exponential gamma N=100000 10 0.01603 0.01814 0.01570 0.01833 20 0.03572 0.03784 0.03513 0.03758 30 0.05500 0.05714 0.05425 0.05718 40 0.07391 0.07606 0.07301 0.07497 50 0.09243 0.09459 0.09143 0.09507 60 0.11058 0.11276 0.10950 0.11422 70 0.12838 0.13056 0.12723 0.13214 80 0.14581 0.14800 0.14461 0.14902 90 0.16290 0.16509 0.16166 0.16301 100 0.17964 0.18184 0.17838 0.17905 Table 6. Bivariate multinomial scans: 1 = 494:92, ¾ = 493:45
30 POZDNYAKOV, V., GLAZ, J., KULLDORFF, M., AND STEELE, J. M. shifted simulated n exponential exponential gamma N=100000 25 0.02883 0.02853 0.02822 0.02828 50 0.05922 0.05904 0.05826 0.05857 75 0.08866 0.08859 0.08747 0.08842 100 0.11718 0.11721 0.11584 0.11776 125 0.14481 0.14494 0.14336 0.14627 150 0.17157 0.17179 0.17005 0.17118 Table 7. Bivariate Bernoulli scans: 1 = 786:31, ¾ = 783:49