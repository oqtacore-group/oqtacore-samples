
1Martingale Methods for Patterns and Scan Statistics Vladimir Pozdnyakov and J. Michael Steele Department of Statistics, University of Connecticut, Storrs, USA Department of Statistics, University of Pennsylvania, Philadelphia, USA Abstract: We show how martingale techniques (both old and new) can be used to obtain otherwise hard-to-get information for the moments and distributions of waiting times for patterns in independent or Markov sequences. In particular, we show how these methods provide moments and distribution approximations for certain scan statistics, including about variable length scan statistics. Each general problem that is considered is also illustrated with a concrete example con¯rming the computational tractability of the method. Keywords and phrases: Scan, pattern, martingale 1.1 Introduction The martingale method for waiting times for patterns in an independent se- quence was pioneered in Li (1980), and in the intervening time many variations on the original idea have been developed. Our ¯rst aim here is to survey these developments using the unifying language of gambling teams. We further show how the martingale method can be extended to cover a great variety of prob- lems in applied probability, including the occurrence of patterns in Markov sequences. One of the key intermediate steps is the development of a clear un- derstanding of the distribution of the ¯rst time of occurrence of a pattern from a ¯nite set of patterns. It is this general problem that leads to methods that are applicable to the theory of scan statistics. 1
2 V. Pozdnyakov and J.M. Steele 1.2 Patterns in an Independent Sequence By fZn; n ¸ 1g we denote a sequence of independent identically distributed random variables with values from a ¯nite set - = f1; 2; :::;Mg which we call the process alphabet. To specify the distribution of Zn, we then set p1 = P(Zn = 1) > 0; p2 = P(Zn = 2) > 0; :::; pM = P(Zn = M) > 0: By a pattern A we mean a ¯nite ordered sequence of letters a1a2 ¢ ¢ ¢ am over the alphabet -. The random variable that is of most interest here is ?A, the ¯rst time that one observes the pattern A as a run in the sequence fZn; n ¸ 1g. Our main goal is to provide methods | and often explicit formulas | for the expected value, the higher moments, and the probability generating function of ?A. 1.2.1 A gambling approach to the expected value We begin with a construction that originates with Li (1980) and which we frame as a gambling scheme. Consider a casino game that generates the sequence fZn; n ¸ 1g, say as the out-put of a biased roulette wheel. Next consider a sequence of gamblers who arrive sequentially so that the n'th gambler arrives right before n'th round when Zn is generated. We also assume that this casino pays fair odds, so that a dollar bet on an event that has probability p would pay 1=p dollars to a winner (and zero to a loser). Now we consider the strategy that is followed by the n'th gambler, the one who arrives just before the n'th round of play. For speci¯city, we ¯rst consider the gambler who enters just before the ¯rst round. This gambler bets one dollar that Z1 = a1. If Z1 is not a1 the gambler stops betting after having lost one dollar. If Z1 yields a1, the gambler wins 1=P(Z1 = a1). He then continues to play, now betting his entire capital on Z2 = a2. If he loses, he stops gambling; otherwise he increases his bet by the factor 1=P(Z2 = a2). The gambler then continues in the same fashion until the entire pattern A is exhausted or until he has lost his original dollar, which ever comes ¯rst. If the ¯rst gambler is very lucky and pattern A is observed after m rounds, the gambler stops and has total winnings of uP(Z1 = a1)P(Z2 = a2) £ ¢ ¢ ¢ £ P(Zm = am)
!1 dollars. Otherwise, the ¯rst gambler simply loses his initial bet of $1.
Martingale Methods 3 In the meanwhile, additional gamblers enter the casino at successive times 2; 3; ::: and each of these gamblers uses the same strategy that was used by the ¯rst gambler. That is, he bets successively on the letters of the pattern, each time \letting his stake ride." We then let Xn denote the total net gain of the casino at the end of the n'th round of play. The game was fair at each stage, so the stochastic process fXn; ¾(Z1; :::;Zn)g is a martingale. Now consider the random variable X?A; this is the casino's net gain at the time when the pattern A is ¯rst observed. This random variable is well-de¯ned since ?A is ¯nite with probability one. In fact, it is easy to show that ?A is bounded by a geometrically distributed random variable, so by Wald's lemma (or the optional stopping theorem, Williams (1991, p. 100)), we have the basic relation E(X?A) = 0: Fortunately, we know more about X?A. Speci¯cally, we know that X?A = ?A !W; where W is the total amount of money that has been won by gamblers by time ?A. The key observation is that W is not a random variable. The value of W is fully determined by the way in which the pattern A overlaps with itself. Moreover, it is reasonably easy to calculate W. For a gambler to have any capital left when pattern A is ¯rst observed, that gambler needs to still be gambling, so in particular the gamblers who entered the game before ?A!m+1 must have all lost their dollar. The gambler who enters the game at time ?A!m+1 is the lucky guy who wins the most, but also some of those gamblers who entered after him may have some amount in their pockets. The total amount of money that these few players have is represented by a certain measure of the overlapping of pattern A with itself. To describe this measure, we ¯rst consider 0 - i; j - m and set ±ij = ( 1=P(Z1 = ai); if ai = aj ; 0; otherwise. With this notation we then ¯nd the explicit formula W = ±11±22 ¢ ¢ ¢ ±mm + ±21±32 ¢ ¢ ¢ ±mm!1 + ¢ ¢ ¢ + ±m1; (1.1) so from our earlier observation that E(X?A) = 0, we ¯nd E(?A) = ±11±22 ¢ ¢ ¢ ±mm + ±21±32 ¢ ¢ ¢ ±mm!1 + ¢ ¢ ¢ + ±m1: (1.2) The relation (1.2) really is quite explicit, and it provides an easily applied answer to our ¯rst question, say as one sees in the following: Example 1.2.1 Let - = f1; 2g and consider the pattern 1121 of length 4. We then have E(?A) = W = (p1 £ p1 £ p2 £ p1)!1 + (p1)!1
4 V. Pozdnyakov and J.M. Steele 1.2.2 Gambling on a generating function By a natural modi¯cation of the preceding method, one can obtain a formula for the generating function of ?A. The trick is to change the initial bet for each gambler. Now instead of $1, the n'th gambler starts his betting by placing a bet of size rn, where 0 < r < 1. Let r?AW(r) be the total winnings of all the gamblers by time ?A. As before, we let Xn denote the casino's gain at the end of the n'th round, and as before fXng is a martingale. For convenience, we denote the total accumulated winnings of the gamblers when the pattern A is ¯rst observed by r?AW(r). Again, the key is that we have a nice relation for the casino's net gain X?A. Speci¯cally, we have X?A = r1 + r2 + ¢ ¢ ¢ + r?A ! r?AW(r) = rr?A ! 1 r ! 1 ! r?AW(r) = r?A u r r ! 1 !W(r)
 ! r r ! 1: As in previous subsection, W(r) is not a random variable, and it has an explicit representation: W(r) = ±11±22 ¢ ¢ ¢ ±mm!1=rm!1 + ±21±32 ¢ ¢ ¢ ±mm!1=rm!2 + ¢ ¢ ¢ + ±m1=1: The optional stopping theorem implies 0 = E(X?A) = E(r?A)u r r ! 1 !W(r)
 ! r r ! 1: When we solve this relation for E(r?A), we obtain E(r?A) = u1 + 1 ! r r W(r)
!1 : Again, this is an explicit useable formula, as one sees in the following example. Example 1.2.2 Let - = f1; 2g and again consider the pattern 1121. One then has W(r) = r!3 p31p2 + 1 p1 ; so by substitution one has E(r?A) = p31p2r4 1 ! r + r3(1 ! p2r)p21p2 = p31p2r4 + p31p2r5 + p31p1r6 + p31p2(1 ! p21p2)r7 + o(r7): As a check, one should note that this formula can be used to con¯rm the calculation of the mean from our ¯rst example: @E(r?A) @r ¯¯¯ r=1 = 1 p31p2 + 1 p1 = E?A:
Martingale Methods 5 1.2.3 Second and higher moments In theory, the ability to compute the probability generating function also gives one the higher moments, but in practice it is often useful to have an alternative method. Here it also seems instructive to show how the method of sequential gamblers can be used to ¯nd E(?2A). This time the trick is that the gambler who joins the game in the n'th round will bet n dollars. If, as always, we let Xn denote the casino's net gain after n rounds, then Xn is again a martingale. Moreover, in this case one can check that at the stopping time ?A we have X?A = 1 + 2 + ¢ ¢ ¢ + ?A !(?A ! m + 1)±11±22 ¢ ¢ ¢ ±mm !(?A ! m + 2)±21±32 ¢ ¢ ¢ ±mm!1 ¢ ¢ ¢ !(?A ! m + m)±m1 = 1 + 2 + ¢ ¢ ¢ + ?A ! ?AW ! N = ?2A+ ?A 2 ! ?AW ! N; whereN = !±11±22 ¢ ¢ ¢ ±mm(m ! 1) ! ±21±32 ¢ ¢ ¢ ±mm!1(m ! 2) ! ¢ ¢ ¢ ! ±m10: It is now time to apply the optional stopping theorem, but in this case the increments of Xn are no longer uniformly bounded, so a more re¯ned version of Doob's optional stopping theorem is needed. Here we can use the stopping time theorem of Shiryaev (1995, p. 485) since we have Xn = O(n2) and since P(?A > n) decays at an exponential rate. The application of this optional stopping theorem leads us to 0 = E(X?A) = E(?2A)=2 + E(?A)=2 !WE(?A) ! N: Solving this equation for E(?2A) gives us E(?2A) = (2W ! 1)E(?A) + 2N = 2W2 !W + 2N; and as a corollary we have the nice formula Var(?A) = W2 !W + 2N: Naturally, variations of this technique can be applied to obtain formulas for any moment. For example, to ¯nd an expression for the third moment, the n'th gambler's bet should now be taken to be n2.
6 V. Pozdnyakov and J.M. Steele Example 1.2.3 For the traditional sample space - = f1; 2g and the pattern 1121 we now ¯nd N = ! 3 p1 £ p1 £ p2 £ p1 ; and Var(?A) = u 1 p1 + 1 p31p2
2 ! 1 p1 ! 7 p31p2 : Here it is interesting to note that when either p1 ! 0 or p2 ! 0 one has the limit relation E(?A) Var(?A)1=2 ! 1: Moreover, there is an intuitive explanation for this limit. When either p1 ! 0 or p2 ! 0 the occurrence of the pattern 1121 becomes a rare event. By the clumping heuristic [c.f. Aldous (1989)], one then expects the distribution of ?A to be well approximated by an exponential distribution, and for an exponential X we have the equality E(X) = pVar(X). 1.3 Compound Patterns and Gambling Teams In many important applications | such as scans | one is concerned about the waiting time until the ¯rst occurrence of one out of many patterns from a ¯nite list of patterns. Here we call a ¯nite collection of K patterns fA1;A2; ¢ ¢ ¢ ;AKg a compound pattern and denote it simply by A. Now, if ?Ai denotes the ¯rst time until the pattern Ai has been observed as a completed run in the i.i.d. series Z1;Z2; ::: then the new random variable of interest is ?A = minf?A1 ; :::; ?AKg: In words, ?A is the ¯rst time when we observe a pattern from A, and one should note that without loss of generality we assume that in A no pattern is a subblock of another. Gerber and Li (1981) studied compound patterns with help of an appropri- ate Markov chain imbedding. We use an alternative method that has several bene¯ts. In particular, the new method gives us clear hints on how we should extend the martingale approach to the case of Markov dependent trials. It also guides us when we consider the case of highly regular patterns, such as those associated with scans or structured motifs.
Martingale Methods 7 1.3.1 Expected time It seems natural in the case of compound pattern A to introduce K gambling teams. The gamblers from each gambling team will bet on a pattern from the list A. But now the problem is that the total amount of winnings of all the gamblers at time ?A is a random variable. It depends on how the game is stopped. However, if one knows which simple pattern from A triggered the stop, then the winnings of a gambling team are not random. This amount is fully determined by overlapping of two patterns: (1) the pattern associated with the gambling team, and (2) the pattern associated with the ending scenario. An explicit expression for this amount will be given a bit later. As we will demonstrate in a moment it is bene¯cial to allow every gambling team to have their own size for an initial bet. More speci¯cally, let yj will be an amount with which the gambler from the j'th gambling team (the team that bets on Aj) starts his betting. Let Wijyj be total winnings of the j'th gambling team in case when game was ended by the i'th scenario (i.e., the pattern Ai is observed at time ?A). If Xn is as before the net casino gain, then it is clear that it forms a martingale, because a weighted sum of martingales is a martingale. The stopped martingale X?A is given by X?A = KXj=1 yj?A ! KXi=1 KXj=1Wijyj1Ei ; where 1Ei is the indicator that the game is ended by the i'th scenario. There is an analogy between the way gambling teams are used here and the notion of hedging in ¯nance. The trick, analogous to arbitrage constructions, is to choose P weights yj in such a way that the total winnings of all the teams Kj=1Wijyj is equal to 1 regardless of an ending scenario. Now, if the vector fyjg1-j-K is a solution of the linear system KXj=1Wijyj = 1; 1 - i - K; (1.3) then the stopped martingale is given by X?A = KXj=1 yj?A ! 1: This puts us on familiar ground. By another application of the optional stopping theorem we obtain a computationally erective representation of E(?A).
8 V. Pozdnyakov and J.M. Steele Theorem 1.3.1 If vector fyjg1-j-K solves the linear system (1.3), then ex- pected value of ?A is given by E(?A) = 1 PKj=1 yj : Here we should make two technical comments. First, in the course of their Markov embedding method, Gerber and Li (1981) showed that the matrix Wij is nonsingular if no pattern from A is a subpattern of another. Consequently, the solution fyjg1-j-K always exists. Second, there is an explicit formula for Wij . For example, consider two patterns A = a1a2 ¢ ¢ ¢ am and B = b1b2 ¢ ¢ ¢ bl. Next, we consider the measure of t-overlap of a su±x of A with a pre¯x of B that is given by the formula ±t(A;B) =8< : 1 Qts=1 P(Z1 = bs) ; if b1 = am!t+1; b2 = am!t+2; :::; bt = am; 0; otherwise: Now, if the j'th gambling team bets on A, and the i'th ending scenario is associated with pattern B then Wij = miXn(m;l) t=1 ±t(A;B): 1.3.2 The generating function and the second moment The method of gambling teams can be used to obtain a formula for the prob- ability generating function E(r?A); 0 < r < 1, for any compound pattern A. The solution is a little more complicated, but it mainly calls for the systematic elaboration of ideas that we have already seen. Here we consider the same number of gambling teams and ending scenarios that we used before, but now the gambler from the j'th team who joins the game in the n'th round will place an initial bet of size of yjrn where the weights fyjg1-j-K will be chosen later. Let Wij(r)yjr?A denote the winnings of the j'th gambling team when the game ends by the i'th ending scenario. If Xn denotes the martingale that gives us the casino's net gain at time n, then the stopped martingale X?A is given by X?A = rr?A ! 1 r ! 1 KXj=1 yj ! KXi=1 KXj=1Wij(r)yjr?A1Ei ; where as before 1Ei is the indicator of the i'th ending scenario. Again, the key fact is that Wij(r) is not a random variable. If the j'th gambling team bets on pattern A, and the i'th ending scenario is linked with pattern B, then Wij(r) = miXn(m;l) t=1 ±t(A;B)r1!t; (1.4)
Martingale Methods 9 where ±t(A;B) is de¯ne as in the preceding section. If weights fyj(r)g1-j-K are chosen such that KXj=1Wij(r)yj(r) = 1; 1 - i - K; (1.5) then the stopped martingale X?A is given by X?A = rr?A ! 1 r ! 1 KXj=1 yj(r) ! r?A: After taking the expectation, a little algebra leads one to a strikingly simple formula for the generating function for ?A. Theorem 1.3.2 If the vector fyj(r)g1-j-K solves the linear system (1.5), then E(r?A) = 1 ! 1 1 +PKj=1 yj(r)r=(1 ! r) : We can use Theorem 1.3.2 to obtain the higher moments of ?A, but it is also possible to use the method of gambling teams more directly. For example, to compute the second moment of ?A we ask the gambler from the j'th team that starts gambling in n'th round to place an initial bet of yj + nzj dollars on the ¯rst letter of Aj and to continue betting his fortune on the subsequent letters of Aj until he either loses or until some gambler observes a pattern from A. This time we write the winnings of the j'th team in the case of the i'th ending scenario by the sumWijyj + ?AWijzj + Nijzj where Wij is as before but where Nij is a new quantity for which we will give an explicit formula shortly. The casino's net gain at time X?A then is given by X?A = KXj=1 yj ?A(?A + 1) 2 + KXj=1 zj?A ! KXi=10@ KXj=1Wijyj?A + KXj=1Nijyj + KXj=1Wijzj1A1Ei : Now, if weights fyjg1-j-K and fzjg1-j-K are such that KXj=1Wijyj = 1; 1 - i - K; (1.6) KXj=1(Nijyj +Wijzj) = 1; 1 - i - K;
10 V. Pozdnyakov and J.M. Steele then the stopped martingale is equal to KXj=1 yj ?A(?A + 1) 2 + KXj=1 zj?A ! ?A ! 1: After the application of the optional stopping theorem we obtain a formula for the second moment. Theorem 1.3.3 If fyjg1-j-K and fzjg1-j-K solve the linear system (1.6), then E(?2A) = 1 + (1 !PKj=1 zj !PKj=1 yj=2)E(?A) PKj=1 yj=2 : As we mentioned above, Nij is just another measure of the overlap of two patterns. Speci¯cally, if the j'th gambling team bets on pattern A and the i'th ending scenario corresponds to pattern B, then we have the explicit recipe: Nij = miXn(m;l) t=1 ±t(A;B)(1 ! t): Here one should also note that from the representation (1.4) for Wij(r) one also has the nice alternative formulas Wij(1) = Wij ; @Wij(r) @r ¯¯¯ r=1 = Nij : As before, an example shows that these representations are all quite explicit. Example 1.3.1 As usual we take - = f1; 2g, but now we consider the com- pound pattern A = f11; 121g. If we further assume that P(Z1 = 1) = P(Z1 = 2) = 1=2; then we ¯nd Wij = " 6 2 2 10 # ; Wij(r) = " 4r!1 + 2 2 2 8r!2 + 2 # ; and Nij = " !4 0 0 !16 # : The theorems of this section then give us the concrete answers: E(?A) = 83; and Var(?A) = 10;
Martingale Methods 11 and E(r?A) = r2(r + 2) 8 ! 4r ! r3 = r2 4 + r3 4 + r4 8 + 3r5 32 + 5r6 64 + 7r7 128 + o(r7): 1.4 Patterns in Markov Dependent Trials Gambling teams provide a handy way to deal with many questions about se- quences of independent symbols, but, when the symbols are generated by a Markov chain, one ¯nds that the method of gambling teams is especially pow- erful | even though some new subtleties are introduced. For example, in the Markov case one typically needs to introduce multiple teams of gamblers who gamble according to direrent rules. To illustrate the basic ideas in the simplest non-trivial case, we ¯rst apply the gambling team method to the calculation of the expected time until one observes a speci¯ed pattern in a sequence generated by a two-state Markov chain. 1.4.1 Two-state Markov chains and a single pattern In next two sections we take fZn; n ¸ 1g to be a Markov chain with state space - = f1; 2g. We suppose the chain has the initial distribution P(Z1 = 1) = p1, P(Z1 = 2) = p2 and the transition matrix " p11 p12 p21 p22 # : Here, as usual, pij is shorthand for P(Zn+1 = j jZn = i). Given a pattern A = a1a2 ¢ ¢ ¢ am, we then let ?A denote the ¯rst time that the pattern is observed in the sequence generated by the Markov chain. Next, we need to make explicit the Markov version of a fair casino where a gambler who bets on the event fZn+1 = ag is assumed to have ¯rst observed Zn. Here, if one ¯rst observes Zn = 1, then the bettor of $1 dollar on the event fZn+1 = ag receives p!1 1a dollars if Zn+1 = a occurs; otherwise the bettor receives 0. Similarly, if one ¯rst observes Zn = 2 and then bets that Zn+1 = a the payors are p!1 2a and 0 respectively. There are now three distinct scenarios under which the pattern A can be observed. Either 2 the pattern A occurs at the beginning of the sequence fZn; n ¸ 1g, or 2 the pattern 1A occurs at the end of the sequence, or
12 V. Pozdnyakov and J.M. Steele 2 the pattern 2A occurs at the end of the sequence. The probability of the ¯rst scenario is easy to ¯nd, but to determine the indi- vidual probabilities of the last two scenarios would be more subtle. Instead we will use another gambling team trick to avoid such calculation. The new trick is to consider two gambling teams and to allow by teams to bet direrently on the pattern A. The added 'exibility will permit us to set things up so that teams total winnings are known if we know how the game ended. For each time n two new gamblers are ready to take action, one from each team. The gamblers now follow the two rules: 1. For each n a gambler from the ¯rst team arrives before round n and watches the result of the n'th trial. He then bets y1 dollars on the ¯rst letter of the sequence A and continues to bet his accumulated winnings on the successive letters in the successive rounds until either he loses or until the patten A is observed, either by himself or by some other gambler from one of the two teams. We call the gamblers on this team straightforward gamblers. 2. Gamblers from the second team bet direrently. If Zn 6= a1 then the n'th gambler from the second team bets y2 dollars on the round n + 1 on the ¯rst letter of the pattern A. This gambler then continues to \let his fortune roll" until either he loses or until A is observed, either by himself or by some other gambler. On the other hand, if Zn = a1 then this gambler (intelligently!) bets y2 dollars on a2 on round n+1 and then he continues to bet on the remaining letters of the pattern a3 ¢ ¢ ¢ am until he loses or until the pattern A is observed by himself or by some other gambler. We call the gamblers of the second team smart gamblers. Now, we let Wijyj , i = 1; 2; 3, j = 1; 2 be amount of money that the j'th team wins if the game ends in the i'th scenario. It is vital to note that the deterministic quantities Wij are easy to compute. The stopped martingale X?A that represents the net casino gain at time ?A is given by X?A = (y1 + y2)(?A ! 1) !X3 i=1 X2 j=1Wijyj1Ei ; where 1Ei is the indicator of i'th ending scenario. To see this note that no money was bet on the ¯rst round, and y1 + y2 was the amount bet by each of the ¯rst time bettors at each of the subsequent rounds. Now, we assume that we can ¯nd fyjg1-j-2 such that X2 j=1Wijyj = 1; 2 - i - 3:
Martingale Methods 13 The existence of y1 and y2 depends on the computed values fWijg, but they will exist except in isolated, degenerate cases. The stopped martingale is then given by the simpler formula X?A = (y1 + y2)(?A ! 1) ! (W11y1 +W12y2)1E1 ! 1Ec1 ; where 1Ec1 is the indicator of the complement of the 1st ending scenario. Taking the expectation and employing the optional stopping theorem we obtain 0 = (y1 + y2)(E(?A) ! 1) ! ¼1(W11y1 +W12y2) ! (1 ! ¼1); where ¼1 is the probability of the ¯rst scenario. As we noted earlier, it is always easy to compute ¼1, so at the end of the day one just solves for E(?A) to ¯nd E(?A) = 1 + ¼1(W11y1 +W12y2) + (1 ! ¼1) y1 + y2 : Example 1.4.1 To see that this is indeed an explicitly computable formula, consider the pattern A = 121. The straightforward gamblers start with a fortune of y1 dollars and successively bet their accumulated fortunes on the successive values of 121. On the other hand, the smart gamblers start with y2 dollars and bet their accumulated fortune one the successive values of 121 if they observed 2 before placing their ¯rst bet, but they bet their money on the successive values of 21 if they observed 1 before placing their ¯rst bet. The three scenarios are (1) the game ends with 121 at the beginning, or (2) the game ends with 2121 at the end of some indeterminate number of rounds, or (3) the game ends with 1121 at the end of some indeterminate number of rounds. The 3 £ 2 (scenarios by teams) matrix fWijg is then given by 26666664 1 p21 1 p12p21 + 1 p21 1 p21p12p21 + 1 p21 1 p21p12p21 + 1 p12p21 + 1 p21 1 p11p12p21 + 1 p21 1 p12p21 + 1 p21 37777775: To determine the initial bet sizes y1 and y2 we then just solve the relations y13 1 p21p12p21 + 1 p21 ´ + y23 1 p21p12p21 + 1 p12p21 + 1 p21 ´ = 1; y13 1 p11p12p21 + 1 p21 ´ + y23 1 p12p21 + 1 p21 ´ = 1; to ¯nd y1 = p11p12p21 p12 + p21 + p12p21 and y2 = p12p21(p21 ! p11) p12 + p21 + p12p21 :
14 V. Pozdnyakov and J.M. Steele The probability ¼1 of the ¯rst scenario is just p1p12p21, so after substitution and simpli¯cation we obtain the pleasingly succinct formula E(?A) = 1 + p2 p21 + 1 p221 + 1 p12p21 : 1.4.2 Two-state Markov chains and compound patterns The next natural challenge is to compute the expected value of ?A the ¯st time that one observes a pattern from the set A = fA1;A2; ¢ ¢ ¢ ;AKg: The gambling teams method again applies, but one more nuance emerges. In particular, it is useful to re¯ne the split notion of ending scenarios into initial-ending scenarios and later-ending scenarios. Speci¯cally, we consider K initial-ending scenarios where in the i'th initial-ending scenario the pattern Ai; 1 - i - K occurs in the beginning of the sequence fZn; n ¸ 1g; and we consider 2K later-ending scenarios where either the pattern 1Ai for some 1 - i - K occurs or else the pattern 2Ai for some 1 - i - K occurs after some indeterminate number of rounds. This gives us complete coverage of how the one of the patterns from A can appear; in fact the coverage is over complete since it is possible that some of the later-ending scenarios need not be achievable as ¯nal blocks of the Markov sequence at time ?A. For example, if A = f212; 22g, then doubling step formally gives us four later-ending scenarios: f¢ ¢ ¢ 1212; ¢ ¢ ¢ 2212; ¢ ¢ ¢ 122; ¢ ¢ ¢ 222g, but 221 and 222 cannot occur as a substring of the string Z1;Z2; :::;Z?A. Similarly, if the initial collection is A = f21; 111g, then the only observable later-scenarios are f¢ ¢ ¢ 121; ¢ ¢ ¢ 221g. Thus, one typically needs to do some cleaning of the initial list of later- ending scenarios, and, if a later-ending scenario cannot be observed in a se- quence that ends at time ?A, then the scenario is eliminated from the original list of 2K later-ending scenarios. The ¯nal list of ending scenarios is then the set of initial-ending scenarios and later-ending scenarios that have not been eliminated. We let N0 denote the number of later-ending scenarios in the ¯nal list.Now we introduce N0 gambling teams, one for each of the later-ending sce- narios. The rule is simple. If in the ¯nal list of scenarios there are two later- ending scenarios associated with the pattern Ai, then we introduce two gambling teams. One bets on Ai in a straightforward way, and one team bets on Ai in the smart way of the previous section. On the other hand, if in the ¯nal list we have only one later-ending scenario associated with the pattern Ai we will use only one gambling team of straightforward gamblers. Finally, if there are no later-ending scenarios in the ¯nal list associated with Ai, no gambling teams linked with Ai are needed. We let Xn denote the casino's net gain at time n. We take yj ; 1 - j - N0 to be the initial bet with which a gambler from the j'th gambling team
Martingale Methods 15 starts his betting, and we let Wijyj , 1 - i - K be total winnings of the j'th gambling team in the case of the i'th initial-ending scenario. Finally, we let yjWij , K + 1 - i - K + N0 be total winnings of the j'th gambling team in the case when the game is ended by the i'th later-ending scenario. Then the stopped martingale X?A is given by X?A = N0X j=1 yj(?A ! 1) ! KXi=1 N0X j=1Wijyj1Ei ! K+N0 X i=K+1 N0X j=1Wijyj1Ei ; where Ei is the event that the i'th scenario occurs. Again, the Wij are not random, and, parallel to our earlier calculations, we assume that one can ¯nd fyjg1-j-N0 such that N0X j=1Wijyj = 1; for all K + 1 - i - K + N0: (1.7) We then have the representation X?A = N0X j=1 yj(?A ! 1) ! KXi=1 N0X j=1Wijyj1Ei ! K+N0 X i=K+1 1Ei ; so the optional stopping theorem tells us that 0 = E(X?A) = N0X j=1 yj(E(?A) ! 1) ! KXi=1 N0X j=1Wijyj¼i ! (1 ! KXi=1 ¼i); where ¼i is the probability that the i'th initial-ending scenario occurs. Solv- ing this equation, we obtain a slightly untidy but still completely computable formula for E(?A). Theorem 1.4.1 If fyjg1-j-N0 solves the linear system (1.7), then E(?A) = 1 + (1 !PKi=1 ¼i) +PK i=1 ¼iPN0 j=1 yjWij PN0 j=1 yj : (1.8) Example 1.4.2 For the collection of patterns A = f11; 212g we ¯nd after the doubling and cleaning steps that the ¯nal list of later-ending scenarios is f211; 1212; 2212g. Together with our initial-ending scenarios, so we have a total of ¯ve ending scenarios which we order as f11; 212; 211; 1212; 2212g: The scenario-by-team win matrix fWijg is then given by
16 V. Pozdnyakov and J.M. Steele 26666666666666664 1 p11 0 0 0 1 p12 1 p21p12 + 1 p12 1 p21p11 + 1 p11 0 0 0 1 p12p21p12 + 1 p12 1 p12p21p12 + 1 p21p12 + 1 p12 0 1 p22p21p12 + 1 p12 1 p21p12 + 1 p12 37777777777777775 ; and, after solving the corresponding linear system, we ¯nd that the appropriate initial team bets are given by y1 = p21p11 1 + p21 ; y2 = p22p21p12 p21 + p12 + p21p12 ; y3 = p21p12(p12 ! p22) p21 + p12 + p21p12 : The probabilities ¼1 and ¼2 that 11 and 212 are initial segments of the process fZn; n ¸ 1g are given by p1p11 and p2p21p12 respectively, so the formula (1.8) leads one to the following result E(?A) = 2 + p1p12 + 1 ! p1p11 p21 ; which we see was not so complicated after all. Finally, one should note that when a martingale method for the expected waiting time is developed, it is usually straightforward to extend the method to obtain formulas for higher moments or generating functions. We have already seen how this can be done in the independent model, and Glaz et. al. (2006) give a more detailed exposition that covers the case of the two-state Markov chains. 1.4.3 Finite state Markov chains Now consider a temporally homogeneous Markov chain fZn; n ¸ 1g with a ¯nite state space - = f1; 2; :::;Mg, initial distribution P(Z1 = m) = pm, 1 - m - M, and transition matrix P = fpijg1-i;j-M where as always pij = P(Zn+1 = jjZn = i): We let A = fA1;A2; :::;AKg denote a compound pattern, and let ?A = minf?A1 ; :::; ?AKg denote the ¯rst time when we observe a pattern from A in the Markov sequence. We also assume that the Markov chain has the following normalization and regularization properties:
Martingale Methods 17 2 We assume that no pattern of A contains another pattern of A as a subpattern. This property holds without loss of generality since if one pattern is a subpattern of another the longer one can be excluded from our list. 2 We assume that P(?A = ?Ai) > 0 for all 1 - i - K. If to the contrary one were to have P(? = ?Ai) = 0 for some i then Ai could simply be excluded from the list. This possibility is excluded by the ¯rst assumption for independent sequences, but for Markov sequences it often needs our attention. For example, if the pattern Ai contains subpattern km and pkm = 0, then Ai can not happen as a run of fZn; n ¸ 1g. 2 We assume that P(?A < 1) = 1. If the patterns of A all contain transient states this condition can easily fail even for a ¯nite Markov chain. Here we should note that for ¯nite Markov chains the basic ¯niteness condition P(?A < 1) = 1 already implies the formally stronger condition E[?A] < 1. The Multi-state Chain Martingale Construction When M = j-j > 2 the critical martingales require a more elaborate de- scription. We begin by decomposing the possible occurrence of a single pattern Ai into an initial list of 1 +M +M2 ending scenarios: 2 Either the sequence Ai occurs as an initial segment of fZn; n ¸ 1g, or 2 for some 1 - k - M, the pattern kAi occurs as an initial segment of the sequence fZn; n ¸ 1g, or 2 for some pair (k;m), 1 - k;m - M, the pattern kmAi occurs after some indeterminant number of rounds. The ¯rst 1 + M ending scenarios are called initial scenarios. The last M2 scenarios are called later scenarios. Since we have K patterns, we have an initial list of (1 +M +M2)K scenarios. For every later scenario associated with the pattern kmAi we introduce a team of gamblers that we call the kmAi-gambling team. Gambler n + 1 from the kmAi-gambling team arrives before round n + 1 to observe the result of n'th trial, Zn. This gambler then starts his betting. If Zn = k he bets a certain amount of money (which is the same for all gamblers from the kmAi-gambling team) on the pattern mAi. If Zn 6= k he bets on Ai. Here, of course, by \betting $1 on the pattern A = a1a2 ¢ ¢ ¢ am, when Zn = a0" we mean the following: 2 After observing Zn the gambler bets a dollar that the next trial yields a1. If Zn+1 6= a1 he loses his dollar and leaves the game. If Zn+1 = a1,
18 V. Pozdnyakov and J.M. Steele he gets 1=pa0a1 . Note that the odds are fair. If he wins he continues his betting. 2 Now he bets his entire capital that the n+2 round yields a2. If it is a2 he increases his capital by factor 1=pa1a2 , otherwise he leaves the game with nothing. He continues to bet his full fortune on the successive letters of the pattern A until either the pattern A is observed, or until some other gambler has succeeded. Now recall that it can happen that some of the scenarios on our initial list simply cannot occur before the waiting time ?A. Moreover, some ending scenarios are impossible simply because some new patterns associated with some ending scenarios cannot be observed at all in the Markov chain. Thus we need to clean the initial list of ending scenarios. Those scenarios that cannot occur at all and those that can occur only after the time ?A must be eliminated. Let K0 denote the number of initial scenarios, and let N0 denote the number of later scenarios that we have in our list after cleaning. For each j'th later scenario in the new list, we introduce the corresponding gambling team, and we assume that the inial amount with which the gamblers of the j'th team start their betting is yj . The values fyjg will be chosen later. Let yjWij , 1 - i - K0 + N0, 1 - j - N0 be the amount of money that the j'th team wins in the i'th ending scenario. Let Xn denote the casino's net gain from all teams at time n. The sequence fXng forms a martingale with respect to the ¯ltration generated by the Markov chain fZn; n ¸ 1g. Indeed, for every gambler in the game the bet size at a current round is fully determined by previous rounds, and odds|as we have seen|are fair. By bookkeeping, one ¯nds for the stopped martingale X?A that X?A = N0X j=1 yj(?A ! 1) ! K0X i=1 N0X j=1Wijyj1Ei ! K0+N0 X i=K0+1 N0X j=1Wijyj1Ei ; where Ei is the event that the i'th scenario occurs. Here, again Wij is not a random variable; it depends only on overlap properties of the pattern associated with the i'th scenario and the pattern associated with the j'th gambling team. If we now assume that we can ¯nd fyjg1-j-N0 such that N0X j=1Wijyj = 1; for all K0 + 1 - i - K0 + N0; (1.9) then X?A has the more tractable representation X?A = N0X j=1 yj(?A ! 1) ! K0X i=1 N0X j=1Wijyj1Ei ! K0+N0 X i=K0+1 1Ei :
Martingale Methods 19 Since fXngn¸1 has bounded increments and E[?A] < 1, the Doob's optional stopping theorem gives us 0 = E(X?A) = N0X j=1 yj(E(?A) ! 1) ! K0X i=1 N0X j=1Wijyj¼i ! (1 ! K0X i=1 ¼i); where ¼i is the probability that the i'th initial scenario occurs. Solving the equation with respect to E(?A) we obtain the main result of this section. Theorem 1.4.2 If fyjg1-j-N0 solves the linear system (1.9), then E(?A) = 1 + (1 !PK0 i=1 ¼i) +PK0 i=1 ¼iPN0 j=1 yjWij PN0 j=1 yj : (1.10) Example 1.4.3 Let - = f1; 2; 3g and A = f323; 313; 33g. Let the initial distribution be given byp1 = 1=3; p2 = 1=3; p3 = 1=3; and let the transition matrix P be given by P = 2643=4 0 1=4 0 3=4 1=4 1=4 1=4 1=2 375: After the eliminating impossible scenarios we get 9 initial scenarios: 323 ¢ ¢ ¢ ; 313 ¢ ¢ ¢ ; 33 ¢ ¢ ¢ ; 1323 ¢ ¢ ¢ ; 2323 ¢ ¢ ¢ ; 1313 ¢ ¢ ¢ ; 2313 ¢ ¢ ¢ ; 133 ¢ ¢ ¢ ; 233 ¢ ¢ ¢ and because transitions 1 ! 2 and 2 ! 1 are impossible we get just six later scenarios: ¢ ¢ ¢ 11323; ¢ ¢ ¢ 22323; ¢ ¢ ¢ 11313; ¢ ¢ ¢ 22313; ¢ ¢ ¢ 1133; ¢ ¢ ¢ 2233: Now we need to calculate the matrix W and we ¯rst consider some sam- ple entries. For instance, the 11323-gambling team in the initial scenario 323 ¢ ¢ ¢ wins 1=p23 = 4. The same team in the later scenario ¢ ¢ ¢ 11323 wins 1=(p11p13p32p23) + 1=p23 = 268=3, and in the later scenario ¢ ¢ ¢ 22323 it wins 1=(p23p32p23) + 1=p23 = 68. Finally, the entries of matrix W that correspond to the later scenarios | the ones that are needed for linear system (1.9) | are
20 V. Pozdnyakov and J.M. Steele given by 266666666666666666664 268=3 64 4 0 4 0 68 256=3 4 0 4 0 0 4 256=3 68 0 4 0 4 64 268=3 0 4 2 2 2 2 38=3 10 2 2 2 2 10 38=3 377777777777777777775 : Finally, from formula (1.10) we have the bottom line: E(?A) = 8 7 15: Higher Moments, the Generating Functions, and Efficiency In parallel with our earlier examples, one can now take initial bets of size yj + zjn to obtain a formula for the second moment, or take initial bets of size yjrn to obtain the corresponding generating function, c.f. Pozdnyakov (2008). Here we should note that while the method of this subsection is also ap- plicable to two-state Markov chains, it is certainly less e±cient than the one given in the Subsection 1.4.2. Here, in the case of two-state Markov chains we would have 4K ending scenarios but the method of Subsection 1.4.2 needs only 2K.Finally, one should remark some of the computational direrences between the martingale technique to the Markov chain imbedding method. To ¯nd the expected time E(?A) via an appropriate Markov chain imbedding one needs to solve a linear system associated with the transition matrix of imbedded Markov, c.f. Fu and Chang (2002, p. 73). The size of the matrix depends on the cardinality K of the compound pattern A and lengths of single patterns in A. Our matrix depends on K and cardinality M of the alphabet. Thus, there are situations when the martingale approach is computationally more erective. For a very simple example, one can take A to consist of just one very long pattern. 1.5 Applications to Scans In its simplest form [c.f. Naus (1965)], the scan statistic is the largest number of \events that occur" in a window of a given ¯xed length when we scan the
Martingale Methods 21 window over a realization of a temporally homogeneous process up to a speci¯ed terminal time. For a concrete example, consider a sequence of independent Bernoulli trials fZn; n ¸ 1g with P(Zi = 1) = p = 1 ! P(Zi = 0): Now given 1 - w - T and 1 - i - T ! w + 1, we consider the sums Yi;w = i+Xw!1 j=i Zj ; and we de¯ne the scan statistic Sw;T to be the maximum of Yi;w; that is, Sw;T = max 1-i-T!w+1 Yi;w: If ?k;w denotes the ¯rst time when one ¯rst observes at least k occurrences of the value 1 in a window of length w, then ?k;w is related to the scan statistic by P(Sw;T ¸ k) = P(?k;w - T): For us, the key observation is that the waiting time ?k;w can as be viewed as the waiting time ?A for an appropriate compound pattern A. For example, for k = 3 and w = 5 the compound pattern A is given by f111; 1101; 1011; 11001; 10101; 10011g: The bottom line message is that knowledge of the distribution of ?A gives us distribution of associated scan statistics. Moreover, this method of association goes well beyond the simple scan of this example. Analogous transformations permit one to treat the variable window scans of Glaz and Zhang (2006) or the double scans considered by Naus and Stefanov (2002) and Naus andWartenberg (1997). 1.5.1 Second moments and distribution approximations Since martingale methods yield erective computations of the moments of the waiting time ?A, it is natural to ask if martingales methods also suggest ap- proximations of the distribution of ?A that use the ¯rst two (or perhaps more) moments of the waiting time. It is reasonable from the clumping heuristic that the stopping time ?A that one associates with a scan statistic should have tail probabilities P(?A - n) that are close to those of the exponential distribution. Still, when one considers the whole distribution, there are natural competitors to the exponential such as the Gamma, the Weibull and the shifted exponentials. The main ¯nding in Pozd- nyakov et al. (2005) was that in many natural situations it is in fact the class
22 V. Pozdnyakov and J.M. Steele of shifted exponential distribution provides the most accurate approximation to the distribution of ?A. To make this approximation explicit, we ¯rst recall that X0 called a shifted exponential provided that X0 = X+c where X has an exponential distribution. We take X0 as our moment matching approximation to ?A provided c is chosen so that E(X + c) = E(?A); Var(X + c) = Var(?A): For the tail probabilities this approximation gives us the relation P(?A - n) ¼ 1 ! exp(!(n + 0:5 + ¾ ! 1))=¾); (1.11) where 1 = E(?A), ¾ = Var(?A), and the 0:5 term provides a continuity correc- tion. As following examples demonstrate, this approximation works remarkably well for a wide variety of scan statistics. Example 1.5.1 (Fixed window scans). Here fZn; n ¸ 1g is a sequence of Bernoulli trials. We consider two scans: at-least-3-out-of-10 (Table 1.1) and at-least-4-out-of-20 (Table 1.2). For the ¯xed window scan statistics, Glaz and Naus (1991) developed tight lower and upper bounds which are provided in Tables 1.1 and 1.2 along with the approximations based on the exponential, shifted exponential, and gamma dis- tributions. The Weibull distribution based approximation is omitted, because the performance of Weibull approximations are signi¯cantly worse than those of the exponential and the gamma. As it can be seen, the shifted exponential approximation does consistently well. In the easy case when 1 is large and ¾ is close to 1, the direrences between the various approximations are marginal, and all of the estimates are close to the true probability. On the other hand, if 1 is relatively small and ¾ dirers from 1, then the approximations based on the exponential and gamma distributions do not perform nearly as well as the shifted exponential approximations. Example 1.5.2 (Variable window scans). Again we let fZn; n ¸ 1g be a se- quence of Bernoulli trials, but this time we scan for the occurrence of either of two situations: either we observe at least 2 failures in 10 consecutive trials, or we observe at least 3 failures in 50 consecutive trials. Here are interested in the approximation for the distribution of the waiting time ? until one of these two situations occurs. In this case we need a compound pattern A with 224 patterns in order for ? and ?A to have the same distribution. The numerical results are given in Table 1.3. Since analytical bounds for this type of scans are not available, the performance of the approximation is judged by comparison with estimated probabilities based on 100; 000 replications. Here, again, we see that the shifted exponential distribution approximation that is calibrated by two moments performs quite well.
Martingale Methods 23 Table 1.1: Fixed window scans: at least 3 failures out of 10 consecutive trials, P(Zn = 1) = :01, 1 = 30822, ¾ = 30815 shifted upper lower n exponential exponential gamma bound bound 500 0.01600 0.01589 0.01597 0.01588 0.01589 1000 0.03183 0.03173 0.03179 0.03171 0.03174 1500 0.04741 0.04731 0.04736 0.04729 0.04733 2000 0.06274 0.06265 0.06267 0.06262 0.06267 2500 0.07782 0.07773 0.07775 0.07770 0.07776 3000 0.09266 0.09258 0.09258 0.09254 0.09261 4000 0.12162 0.12155 0.12154 0.12150 0.12169 5000 0.14966 0.14960 0.14957 0.14954 0.14965 Table 1.2: Fixed window scans: at least 4 failures out of 20 consecutive trials, P(Zn = 1) = :05, 1 = 481:59, ¾ = 469:35 shifted upper lower n exponential exponential gamma bound bound 50 0.09110 0.07827 0.08268 0.07713 0.07940 60 0.10977 0.09770 0.10059 0.09543 0.09989 70 0.12807 0.11672 0.11828 0.11337 0.11991 80 0.14599 0.13534 0.13573 0.13095 0.13949 90 0.16354 0.15357 0.15292 0.14819 0.15864 100 0.18073 0.17141 0.16985 0.16508 0.17736
24 V. Pozdnyakov and J.M. Steele Example 1.5.3 (Double scans). Let fZn; n ¸ 1g be an i.i.d. sequence of ran- dom variables with the three valued distribution speci¯ed by P(Zn = 1) = :04; P(Zn = 2) = :01; and P(Zn = 0): Now we consider two types of \failures"; a type 1 failure corresponds to observ- ing a 1 and a type 2 failure corresponds to observing a 2. Further, we assume that we scan with a window of length 10 for until we observe at least 2 failures of type 2 or observe at least 3 failures (of any combination of kinds). Table 1.4 shows that the shifted exponential approximation works well even when 1 and ¾ are relatively small and signi¯cantly direrent. The initial arguments of Pozdnyakov et al. (2005) in favor of the shifted exponential approximation were predominantly empirical, but subsequently a more theoretical motivation has emerged from work of Fu and Lou (2006, p. 307) which shows that for large n one has P(?A ¸ n) " C¤ exp(!n¯); where the constants C¤ and ¯ are de¯ned in terms of the largest eigenvalue (and corresponding eigenvector) of what Fu and Lou (2006) call the essential transition probability matrix of the imbedded ¯nite Markov chain associated with compound pattern A. One should note that this matrix is not a proper transition matrix; rather it is a restriction of a transition matrix. Now, if omit the continuity factor correction in our shifted exponential ap- proximation (1.11) we have an approximation of exactly the same form: P(?A ¸ n) ¼ exp(!(n + ¾ ! 1)=¾) = exp((1 ! ¾)=¾) exp(!n=¾): These relations suggest that there is a strong connection between the largest eigenvalue of the essential transition matrix of the imbedded Markov chain and the ¯rst and second moments of ?A. In particular, we conjecture that (in the typical case at least) the largest eigenvalue ¸[1] of the essential transition prob- ability matrix of the imbedded ¯nite Markov chain associated with compound pattern A will satisfy the approximation ¸[1] ¼ exp(!1=¾): (1.12) 1.5.2 Scan for clusters of a certain word Let fZn; n ¸ 1g be a sequence of independent identically distributed random variables that takes values over the alphabet - = f1; 2; :::;Mg and let the distribution be given by p1 = P(Zn = 1) > 0; p2 = P(Zn = 2) > 0; ::: ; pM = P(Zn = M) > 0:
Martingale Methods 25 Table 1.3: Variable window: at least 2 failures out of 10 trials or at least 3 failures out of 50 trials, P(Zn = 1) = :01, 1 = 795:33, ¾ = 785:85 shifted simulated n exponential exponential gamma N=100000 50 0.05857 0.05085 0.05542 0.05029 60 0.07033 0.06285 0.06685 0.06187 70 0.08195 0.07470 0.07817 0.07404 80 0.09342 0.08640 0.08939 0.08623 90 0.10474 0.09796 0.10050 0.09718 100 0.11593 0.10936 0.11150 0.11058 Table 1.4: Double scans: at least 2 type II failures out of 10 trials or at least 3 failures of any kind out of 10 trials, P(Zn = 1) = :04, P(Zn = 2) = :01, 1 = 324:09, ¾ = 318:34 shifted simulated n exponential exponential gamma N=100000 10 0.02438 0.01480 0.02175 0.01401 15 0.03932 0.03015 0.03568 0.03084 20 0.05403 0.04527 0.04959 0.04508 25 0.06851 0.06015 0.06342 0.06169 30 0.08277 0.07479 0.07714 0.07590 35 0.09681 0.08921 0.09074 0.09134 40 0.11064 0.10340 0.10419 0.10529 45 0.12425 0.11738 0.11749 0.11878 50 0.13766 0.13113 0.13063 0.13342
26 V. Pozdnyakov and J.M. Steele Given a pattern A = a1a2 ¢ ¢ ¢ am over the alphabet -, we then take a window of length w ¸ m and scan the sequence until the time ? when in the window of width w we have k (possibly overlapping) occurrences of pattern A. One can show that ? is equal to the waiting time until the occurrence of a certain compound pattern A, so formally the moments of ? follow from our pre- vious results. Unfortunately, this approach runs into computational problems since the cardinality of compound pattern A grows exponentially as window width w increases. There seems to be no way to circumvent this problem en- tirely, but given that A can be computed we can greatly cut down on much of the other work. A New Betting Scheme The basic idea is to bet only on the pattern A and to pause the betting between non-overlapping occurrences of A. To make this explicit, we ¯rst take A as given and consider a certain equivalence relation on the patterns from A. Speci¯cally, we say that elements Ai and Aj from A are similar provided that 2 lengths of Ai and Aj are the same, 2 Ai and Aj have the same number of overlapping occurrences of A and, 2 the patterns Ai and Aj have copies of A's at the same positions. Now, to each equivalence class under this relation, we can associate a unique pattern over the extended alphabet 1-= f1; 2; :::;M; ¤g by a simple rule. If the simple pattern Ai 2 A is a representative of an equivalence class, then to construct what we will call the \star-pattern" for the class we replace each symbol of Ai that is not part of a block equal to A by the symbol ¤. This recipe is made clear with an example. Example 1.5.4 Let - = f1; 2; 3g and let A = 121. Suppose we want to scan until we ¯nd the occurrence of at least two copies of A's in a window of 8 symbols. The compound pattern A associated with this scan consist of 11 simple patterns, none of which is subpattern of another): 1. exactly-2-in-5: 12121, 2. exactly-2-in-6: 121121, 3. exactly-2-in-7: 1211121 and 1213121, 4. exactly-2-in-8: 12111121, 12122121, 12113121, 12123121, 12131121, 12132121, and 12133121.
Martingale Methods 27 Now, although we have 11 simple patterns in A we have only 4 equivalence classes which we can enumerate with their star-patterns: 12121; 121121; 121 ¤ 121; 121 ¤ ¤121: Here, it is important to note that ¤ does not mean just \any symbol", because, for example, 12121121 is not in A, and, as a result, class 121 ¤ ¤121 does not include 12121121. Given this reduction to equivalence classes, there are analogous reductions for the rest of our tools, such as the ending scenarios. Now, we introduce a list of ending scenarios associated with the list of equivalence classes (or star- patterns). As before, we associate a gambling team with each element of the ¯nal list of ending scenarios. The real key is the new betting rule. Now, a gambler from a gambling team associated with a star-pattern bets on a symbol if it is a symbol from - but he simply passes when it is a star. For example, a gambler from the gambling team that corresponds to 121 ¤ ¤121 ¯rst bets on 121 in the sequential fashion that should now be quite familiar. If he is successful after those three bets, he then pause for two rounds. After the pause he bets then successively bets his entire capital on 121, the rest of the star-pattern. Assume that we have N0 ending scenarios and N0 gambling teams. A gam- bler from the j'th gambling team that joins the game in n'th round will bet yj dollars. Next let yjWij , 1 - i; j - N0 be the total winnings of the j'th gambling team in the case that game was ended by the i'th scenario. As before, Wij is not random; it is fully determined by the pattern of overlap of the star-patterns associated with the given gambling team and ending scenario. To make this explicit, we let E = e1e2 ¢ ¢ ¢ em and T = t1t2 ¢ ¢ ¢ tl be two patterns over the extended alphabet 1-. We ¯rst de¯ne a measure of \two letters coincidence": ±(ei; tj) =8>< >: 1; if tj = ¤ 1=P(Z1 = tj); if tj 6= ¤; ei = tj ; 0; if tj 6= ¤; tj 6= ei: Next, we de¯ne a general measure of overlap for E and T: W(E; T) = miXn(m;l) i=1 Yi j=1 ±(em!i+j ; ej): Finally, if the i'th ending scenario is associated with the pattern E and the j'th gambling team bets on T, then we have the explicit (and deterministic) formula, Wij = W(E; T):
28 V. Pozdnyakov and J.M. Steele If Xn is the casino's total net gain at the end of the n'th round, then it is again a martingale, since taking a pause preserves a martingale property. At time ?A the stopped martingale is given by X?A = N0X j=1 yj?A ! N0X i=1 N0X j=1Wijyj1Ei ; where 1Ei is the indicator that the game is ended by the i'th scenario, so if the vector fyjg1-j-N0 is a solution of the linear system N0X j=1Wijyj = 1; 1 - i - N0; (1.13) then the stopped martingale has the tidy representation X?A = N0X j=1 yj?A ! 1: Since E(X?A), we come very quickly to our ¯nal formula. Theorem 1.5.1 If the vector fyjg1-j-N0 solves the linear system (1.13), then expected value of ?A is given byE(?A) = 1 PN0 j=1 yj : Example 1.5.5 Let - = f1; 2; 3g, and A = 121, and suppose we scan for at least two As in a window of 8 symbols. As we have seen there are only 4 equivalence classes: 12121; 121121; 121 ¤ 121; 121 ¤ ¤121: The matrix Wij in this case is 2666666666666664 1 p31p22+ 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p31p2 + 1 p21p2 + 1 p1 2 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p41p22+ 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p31p2 + 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p41p22+ 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p21p2 + 1 p1 1 p41p22+ 1 p21p2 + 1 p1 3777777777777775 ; Theorem 1.5.1 gives us the following formula for the expected value: E(?A) = 1 + p1p2(1 + p1p2)(1 + p1(3 ! p2 ! 2p1p2)) p31p22(1 + p1(3 ! p2 ! 2p1p2))
Martingale Methods 29 One obviously can extend this technique to the case of the higher moments and generating function. 1.6 Concluding Remarks The martingale method for studying the waiting time until a compound pattern is now well developed | even the stubborn Markovian case. Still, from the examples given here, one can see that successful application of the method requires some detailed combinatorial information. Speci¯cally, almost always needs to determine explicitly what we have called here the \¯nal list of ending scenarios." For problems, such as those that come from the theory of scan statistics, this ¯nal list can be large. Nevertheless, by the introduction of appropriate equivalence classes, one can still make steady progress. Explicit formulas for moments are possible more often than perhaps one might have guessed. Going forward there are two problems that we believe deserve considera- tion: one general and one speci¯c. The general problem is the identi¯cation of further problems like the one developed in subsection 1.5.2 for clusters of words. Generically, the challenge is to identify the problems in which one can ¯nd a substantial simpli¯cation of what would otherwise be the waiting time problem for a very large class of patterns. Correspondingly, it would be useful to identify as many problems as possible where one has a ¯rm combinatorial understanding of the ¯nal list of ending scenarios. The more speci¯c problem is the conjecture given in equation (1.12). His- torically, there has been considerable value to ¯nding a good representation for the largest eigenvalue for even very special matrices. The class of matrices that are obtained as the essential (improper) transition probability matrix of imbed- ded ¯nite Markov chain associated with compound pattern A is indeed special, yet it is still reasonably large. For this class the conjecture (1.12) provides an explicit | and novel | approach to the analysis of the largest eigenvalue. References 1. Aldous, D. (1989) Probability Approximations Via The Poisson Clumping Heuristic, Springer Publishing, New York. 2. Fu, J.C. and Chang, Y. (2002). On probability generating functions for waiting time distribution of compound patterns in a sequence of multistate trials, Journal of Applied Probability, 39, 70-80.
30 V. Pozdnyakov and J.M. Steele 3. Fu, J.C. and Lou, W.Y.W. (2006). Waiting time distributions of simple and compound patterns in a sequence of r-th order Markov dependent multi-state trials, Annals of the Institute of Statistical Mathematics, 58, 291-310. 4. Gerber, H. and Li, S. (1981). The occurrence of sequence patterns in repeated experiments and hitting times in a Markov chain, Stochastic Processes and their Applications, 11, 101-108. 5. Glaz, J., Kulldorr, M., Pozdnyakov, V., and Steele, J.M. (2006). Gam- bling teams and waiting times for patterns in two-state Markov chains. Journal of Applied Probability, 43, 127-140. 6. Glaz, J. and Naus, J.I. (1991). Tight bounds for scan statistics probabil- ities for discrete data, Annals of Applied Probability, 1, 306-318. 7. Glaz, J., Naus, J.I. and Wallenstein, S. (2001). Scan Statistics, Springer, New-York. 8. Glaz, J. and Zhang, Z., (2006). Maximum scan score-type statistics, Statistics and Probability Letters, 76, 1316-1322. 9. Li, S. (1980). A martingale approach to the study of occurrence of se- quence patterns in repeated experiments, The Annals of Probability, 8, 1171-1176. 10. Naus, J.I. (1965). The distribution of the size of the maximum cluster of points on a line. Journal of The American Statistical Association, 60, 532-538. 11. Naus, J.I. and Stefanov, V.T. (2002). Double-scan statistics. Methodology and Computing in Applied Probability, 4, 163-180. 12. Naus, J.I. and Wartenberg, D. A. (1997). A double-scan statistic for clusters of two types of events. Journal of The American Statistical As- sociation, 92, 1105-1113. 13. Pozdnyakov, V. (2008). On Occurrence of Patterns in Markov Chains: Method of Gambling Teams, to appear in Statistics and Probability Let- ters. 14. Pozdnyakov, V., Glaz, J., Kulldorr, M., and Steele, J.M. (2005). A mar- tingale approach to scan statistics, Annals of the Institute of Statistical Mathematics, 57, 21-37. 15. Pozdnyakov, V., and Kulldorr, M. (2006). Waiting Times for Patterns and a Method of Gambling Teams, The American Mathematical Monthly, 113, 134-143.
Martingale Methods 31 16. Shiryaev, A.N. (1995). Probability, Springer, New York. 17. Williams, D. (1991). Probability with martingales, Cambridge University Press, Cambridge.