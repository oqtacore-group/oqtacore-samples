 Г Л А В А  2 
ВЕРОЯТНОСТНЫЕ МОДЕЛИ ПРИНЯТИЯ РЕШЕНИЙ (ПРОСТЫЕ КЛАССЫ)

В этой главе рассматриваются математические модели принятия решений в условиях неопределенности, когда последняя  имеет вероятностный характер и может быть описана как правило,  в терминах точно известных распределений вероятностей фигурирующих в моделях случайных событий и величин.

:2.1. Основные понятия 
 
Рассмотрим множество  O  объектов, каждый из которых принадлежит к одному и только к одному из  n  классов  Hi  ,   . Обозначая (для каждого  i )  Oi  подмножество всех объектов  i  -го класса из  O , получим разбиение 
  для 
Подчеркнем, что принадлежность каждого объекта к определенному классу является объективной реальностью: результат наблюдения не влияет на истинный класс объекта, а позволяет лишь делать выводы о его классе. При этом наблюдатель в наших моделях лишен возможности точно определять класс объекта, но пытается делать это (с тем или иным уровнем достоверности) по результату измерения некоторой наблюдаемой (часто - векторной) случайной величины - признака  Y , носителями которого каждый объект является и который находится в вероятностной связи с классм объектов. 
Эта связь выражается условными функциями распределения  (в общем случае), условными вероятностями  (при дискретном распределении признака) или условными функциями плотностей распределения  (при непрерывном распределении признака  Y) .
В понятия "объект", "класс", "признак" вкладывается самый широкий смысл: объектом может быть техническая система, его классами - состояние этой системы (исправное или неисправное с классификацией вида неисправности), а признаком -набор наблюдаемых параметров системы. В медико-биологических приложениях эти понятия соответственно могут иметь смысл отдельного живого  организма, его осостояния и результатов физиологических обследований. В экономических приложениях рассматриваемые задачи возникают, например,  при аудиторском анализе деятельности коммерческих фирм ("объекты") с целью определения уровня их экономической состоятельности ("классы") на основе данных отчетной документации ("признаки").
Отнесение объекта к тому или иному классу в условиях описанной неопределенности можно рассматривать как выбор одной из гипотез о классе объекта из набора возможных гипотез . Поэтому класс объекта    и так же обозначаемая гипотеза обычно употребляются, как синонимы.
Признак может представлять собой многомерный случайный вектор с  разнородными зависимыми компонентами.  Помимо этого, возможны случаи, когда измерение признака для каждого объекта осуществляется многократно, т.е. образует выборку его значений. 
Таким образом, в самом общем случае признак    представляет собой выборку векторной случайной величины., т.е.   где  Yi  - вектор. На практике,, однако, чаще других  встречаются  случаи, когда признаком  Y  является одноэлементная ( N = 1)  выборка векторной случайной величины  (  ),  или  N -  элементная простая выборка скалярной случайной величины ( Y=Y (N)).
Во всех случаюх, когда не требуется дополнительных уточнений, признак  будет иметь обозначение Y .
 Пространство признака  будем обозначать .
Ясно, что достоверность решения о классе объекта, принимаемого в результате измерения признака, зависит от того, насколько классы объектов различимы в пространстве : если оно разбивается на непересекающиеся множества  так, что каждое из них  содержит значения признака, соответствующие одному и только одному классу объектов  из  то измерение признака  Y  позволяет классифицировать объект вполне достоверно. Такие "вырожденные" случаи нами, однако, рассматриваться не будут. Вместо этого будет постоянно предполагаться, что взаимооднозначное соответствие между множествами  и классами  при любом разбиении (2.1) отсутствует, но при этом все классы различаются условными распределениями признака, т.е.  для  (ясно, что в противном случае признак  Y  не в состоянии различать классы  Hi  и  Hj ). 
Важным является вопрос о мере информированности наблюдателя о конкретном виде условных распределений признака. Если для класса  Hi  условное распределение признака (в виде, например, ) точно известно, то такой класс (соответственно - гипотеза  Hi ) называется простым. Возможны, однако, случаи, когда для класса  Hi  известно лишь семейство, к которому относится условное распределение признака. Такое положение имеет место, например, когда это распределение имеет  определенный известный тип, но конкретно определяется параметром, значение которого не известно. В таких случаях класс (гипотеза) называется сложным.
К сложным следует отнести и классы, которые представлены лишь выборками значений признака (иногда - при самых общих предположениях о свойствах его условного распределения).
Примером простых классов являются состояния некоторой системы, каждое из которых характеризуется точно фиксированным распределением используемого признака - распределением температуры технического объекта, размера собственного капитала фирмы и т.п. К сложным классам относятся состояния системы, каждому из которых соответствует распределение признака, точно не известное, но принадлежащее некоторому определенному семейству распределений. На практике часто в задаче присутствуют и простые и сложные классы. Так, нормальное (в смысле - номинальное) состояние контролируемой системы (класс  H 1) может характеризоваться определенным точно известным распределением признака (например, гауссовским:  при известных ), а возможные "аномальные" состояния образуют один сложный класс  H 2, для которого , где (2 известно, а  m 2 фиксированное, но неизвестное значение математическое ожидание признака, отвечающее условию  m 2> m 1, т.е. конкурирующие гипотезы о классе объекта определяются условиями

Число сложных классов в задаче может быть, конечно, и больше одного, чему соответствует, например, случай:
,
В самом общем случае набор фигурирующих в задаче гипотез можно представить в виде:

где - заданные множество условных функций распределения, соответствующих классу   ; одноэлементные множества  соответствуют простым классам (гипотезам).
В той главе рассматриваются задачи, в которых фигурируют только простые классы (гипотезы); сложные классы рассматриваются в следующей главе.
Итак, задача принятия решения состоит в выборе класса  по результату измерения значения  y  признака  Y . Правило, по которому осуществляется этот выбор, называется решающим правилом  представляющим собой функцию, определенную на множестве  с значениями в множестве решений  ( . В простых случаях множество  (  состоит из п элементов   d= di   означает решение об отнесении наблюдаемого объекта к классу . Однако возможны решающие правила., у которых число решений  k  не совпадает с числом классов п.
Представляется естественным представить решающее правило разбиению пространства  на  k  непересекающихся областей (по числу решений):
 	для 	       (2.1)
и принимать решения по схеме

Решающие правила, отвечающие такой схеме, носят название нерандомизированных решающих правил, в отличие от рандомизированных решающих правил, которые осуществляются по следующей схеме.
Введем т.н. рандомизирующую функцию: 
     (2.2) 
где  - вероятность выбора решения  при данном значении  y  признака  Y . Теперь, обозначая { d = di } событие, состоящее в принятии решения  di , получим схему принятия решения в виде:

причем случайный выбор решения на последнем шаге осуществляется согласно рандомизирующей функции, задаваемой исследователем и реализуемой (например) при помощи датчика случайных чисел в ЭВМ. Подчеркнем, что выбор рандомизирующей функции здесь никак не связан с истинным классом наблюдаемого объекта.
Ясно, что нерандомизированное решающее правило является частным случаем рандомизированного, у которого функции  являются (соответственно) индикаторами множеств :
                       (2.3)
На первый взгляд применение рандомизированных решающих правил не должно увеличивать достоверность принимаемых решений, поскольку, как было указано, используемый в них случайный механизм выбора гипотезы не зависит от истинного класса наблюдаемого объекта. Однако ниже будет показано, что в некоторых случаях их использование позволяет более успешно распорядиться фигурирующими в задаче ограничениями.
Для выбора наилучшего среди множества  нерандомизированных или множества  рандомизированных решающих правил, (определяемых, соответственно, разбиениями (2.1) или рандомизирующими функциями  (2.2)), необходимо определить критерий оптимальности рашающего правила, т.е. функционал, содержательно выражающий качество решающего правила в смысле решаемой задачи. Различным вариантам задач соответствуют различные критерии, каждый из которых, в свою очередь, определяет решающее правило, являющееся оптимальным в смысле данного критерия. Разнообразие рассматриваемых ниже процедур принятия решений обусловлено различием критериев.
Вероятностный характер исследуемых здесь задач, проявляющийся в ненулевых вероятностях ошибочных решений, приводит к тому, что каждый критерий  так или иначе связан с величинами этих вероятностей или с математическим ожиданием потерь, ими вызываемых.
Прежде чем перейти к формированию конкретных критериев для задач принятия решений, введем понятие матрицы штрафов  С, каждый элемент  которой является платой ("штрафом") лица, принимающего решение (ЛПР), когда принимается решение  при истинности класса объекта , т.е. при осуществлении события , выражаемого конъюнкцией

Матрица С определяет отношение ЛПР к конкретному типу ошибочного решения. Обычно выполняются условия:  Частный вид матрицы, у которой
			    (2.4)
соответствует тому случаю, когда все ошибочные решения равнозначны.
Приведенная схема взыскания штрафа с ЛПР не является, однако, единственно возможной. Иногда штраф может выражать плату за проводимые измерения значений признака. 

:2.2. Критерий Неймана-Пирсона.

Введем условные вероятности

каждая из которых при  равна вероятности ошибочного принятия решения  для объекта класса , а при  - вероятности правильного решения для этого класса.
Для рандомизированного решающего правила ( с рандомизирующей функцией  эту вероятность можно выразить многомерным (при векторном признаке  Y ) интегралом Римана-Стилтьеса:
                  (2.5)
Напомним, что при абсолютно непрерывном распределении признака  Y  с плотностью  этот интеграл превращается в обычный интеграл Римана с заменой  а при дискретном распределении признака  Y  - в сумму или ряд с заменой

Для общности изложения (включающего и случаи смешанных распределений признака) в дальнейшем используются интегралы Римана-Стилтьеса, которые могут легко интерпретироваться читателем для того или иного типа распределения признака.
При нерандомизированном решающем правиле  как указывалось, превращается в индикатор области  (см. (2.3)), и 

Величина
                         (2.6)
равная среднему значению штрафа при классификации объекта  Hi  , носит название условного среднего риска для класса (гипотезы)  Hi . 
Введение величин  позволяет формализовать ряд критериев оптимальности решающего правила, первым из которых мы рассмотрим  критерий Неймана-Пирсона  K НП. 
Суть этого критерия состоит в ограничении значений условных рисков для группы n-1 (из n) классов и минимизации при этом условного риска для класса, не входящего в эту группу (например, без потери общности, для класса Hn).
Формально этот критерий выглядит следующим образом. Пусть  - множество решающих правил, удовлетворяющих условиям

                  (2.7)
где  - ограничения на риски, определяемые условиями задачи. Тогда оптимальное в смысле критерия Неймана-Пирсона решающее правило  находится из условия
	      (2.8)

Наиболее часто критерий Неймана-Пирсона используется при    и для матрицы С вида (2.4).
В этом случае условный риск   представляет собой вероятность ошибочного отнесения объекта класса  к классу ; она носит название вероятности ошибки первого рода и обозначается (. Условный риск  - вероятность неправильной классификации объекта второго класса - именуется вероятностью ошибки второго рода и обозначается (.
В принятых обозначениях критерий Неймана-Пирсона примет вид
	                 (2.9)
где  - оптимальное решающее правило,  - ограничение на вероятность ошибки первого рода.
Иногда в критерии Неймана-Пирсона вместо вероятности ( используют вероятность ( = 1 - (, т.е. вероятность правильной классификации объекта класса  H2 , называемую мощностью решающего правила (иначе -мощностью критерия). В связи с этим, поскольку искомое решающее правило  предписывает максимизировать величину (, его именуют наиболее мощным правилом (критерием).
Заметим, что, применяя рандомизированные решающие правила., обычно можно добиться равенства  (если это сопровождается снижением ). Однако возможны случаи, когда при некотором (  и ( = 0, в силу чего добиваться указанного равенства нет смысла; это относится и к неравенствам в (2.7). Далее, однако, будем обычно полагать, что и в (2.9) ограничение на вероятность ошибки первого рода можно (когда это не вызывает недоразумений) выражать равенством . 
Для всего множества решающих правил  или  взаимно однозначное соответствие между величинами ( и ( (или () отсутствует, что затрудняет выделение из них оптимального решающего правила  простым перебором. Решение этой задачи оказывается, однако, возможным путем выделения упорядоченного подмножества решающих правил, среди которых находится , и их направленного перебора. Суть этого подхода раскрывается в приводимой ниже лемме, доказательство которой опираются на ряд новых понятий. 
До сих пор  мы рассматривали  функции , , как функции, определяющие распределения вероятностей значений признака  Y  при каждой фиксированной гипотезе  Hi . Можно, однако, придать им следующий (несколько иной) смысл, который поясним, сначала, для дискретного распределения признака . 
Пусть  y - полученное значение признака. Тогда, сопоставляя значения вероятностей    при различных i и  при фиксированном  y, можно  оценить, для какой из гипотез это полученное значение признака "правдоподобнее" (т.е., как нетрудо проверить, вероятнее, если полагать, что до измерения признака априорные вероятности для всех гипотез были одинаковы). 
Понимаемое в этом смысле условное распределение называется функцией правдоподобия и обозначается 
Если признак  Y имеет непрерывное распределение с условными функиями плотности распределения   функция правдоподобия, как и в дискретном случае, имеет смысл функии, выражающей правдоподобность полученного (фиксированного) значения признака  y при варьируемой гипотезе; отличие от дискретного состоит, однако, в том, что при сравнении правдоподобности гипотез использутся не условные вероятности  , а функции плотности распределения . Сохраняя обозначение для функции правдоподобия и для этого случая, получим 
Когда признак Y представляет собой простую выборку  Y ( N )  = (Y1 ,    , YN)' , функция правдоподобия имеет вид (в зависимости от типа распределения)

  (2.10) 
 
Введем теперь  для двух гипотез  H1  и  H2  понятие функции отношения правдоподобия (( y ):

              

Интуитивный смысл этой функции достаточно ясен: чем больше её значение при полученном значении признака  y , тем больше оснований полагать, что наблюдаемый объект относится к классу ; наоборот, малые значения (( y ) в большей степени характерны для класса .
Покажем, что функция (( y ) позволяет выделить упорядоченную совокупность рандомизированных решающих правил  содержащую решающее правило, оптимальное в смысле критерия Неймана-Пирсона (напомним, что в множество  входят, как частный случай, и нерандомизированные решающие правила).


Фундаментальная лемма Неймана-Пирсона.
Для каждого фиксированного  оптимальное в смысле критерия Неймана-Пирсона решающее правило  определяется импликациями:

где
	                         (2.12)
а  и  однозначно находятся из условия

Для доказательства леммы сопоставим решающее правило  с каким - либо иным рандомизированным решающим правилом  с рандомизирующей функцией , отвечающей условию  (а в остальном - произвольной). Условие  представимо в виде

Покажем, что при этом условии решающее правило  не может быть мощнее решающего правила , т.е. , что означает оптимальность  в смысле критерия Неймана-Пирсона. Для этого представим разность  в виде
   (2.13)
Запишем функцию отношения правдоподобия в виде
	              2.14)
который, в зависимости от типа распределения признака  Y , следует интерпретировать в смысле (2.11). 
Теперь  (2.13) можно представить в виде
 
 Читатель легко проверит, что, заменяя в последнем равенстве  на  из (2.12), получим

что означает оптимальность 
Важным следствием доказанной леммы является тот факт, что в рассматриваемой задаче при построении оптимального решающего правила можно оперировать с скалярной случайной величиной (=(( Y ) (вместо признака  Y , возможно - многомерного). Это следует из эквивалентности событий

и означает, что в рамках критерия Неймана-Пирсона отношение правдоподобия  является достаточной статистикой, сохраняющей всю информацию о классифицируемом объекте, содержащуюся в исходном признаке  Y .
В случае непрерывных условных распределений величины  (т.е. непрерывности функций распределения  ) , условные вероятностные меры области  равны нулю:

и эта область с одинаковым результатом может быть отнесена к области  или ; в последнем случае оптимальное при  в смысле Неймана-Пирсона решающее правило соответствует условию (взамен (2.15)):
		    (2.16)
где  определяется равенством

В этом случае оптимальное решающее правило относится к нерандомизированным.
Построение оптимального решающего правила в пространстве скалярной статистики (( y ) требует, конечно, вычисления ее условных распределений  что не всегда является простой задачей.
Итак, роль функции отношения правдоподобия  состоит в выделении из всего множества решающих правил  упорядоченного (по значению порога  и величине рандомизирующей вероятности ) подмножества  среди которых находится решающее правило , оптимальное для заданного .
 Ясно, что подмножество  не изменится, если вместо функции  использовать её любую строго монотонную функцию (статистику) Пусть, в частности, признак представляет собой одноэлементную выборку случайного вектора  (  )  с нормальными условными распределениями


Тогда в качестве достаточной статистики Т можно использовать квадратичную форму в выражении 


или, в случае ,

т.е. линейную скалярную функцию компонент признака  Y .
В последнем случае оптимальное решающее правило  определяется эквивалентными областями принятия решений

Легко видеть, что в пространстве  области  разделены при этом гиперплоскостью, что соответствует решающему правилу, называемому линейным классификатором.
В случае  области  разделяются границей, представляющей собой поверхность второго порядка (квадратичный классификатор).

:2.3. Критерий Байеса (минимума среднего риска)

Вернемся к случаю п классов  и произвольной матрицы штрафов  C . Предположим дополнительно, что известны априорные вероятности классов (гипотез) . Это позволяет для каждого решающего правила  с рандомизирующей вектор-функцией  (ср. (2.2)) определить средний (полный)  риск (штраф) (см. (2.6)

Если ( - нерандомизированное решающее правило и индикаторы множеств  в разбиении

(см.(2.3)), то
.		(2.18)
Теперь решающие правила можно сравнивать по величине среднего риска  и в качестве оптимального ((0) выбирать то, при котором эта величина достигает наименьшего значения, т.е.

Такой подход к определению оптимального решающего правила носит название критерия минимума среднего риска (Байеса). Решающее правило, оптимальное в смысле этого критерия будем обозначать 
В отличие от решающего правила  оптимального в смысле критерия Неймана-Пирсона и зависящего от заданного  решающее правило  зависит от распределения априорных вероятностей гипотез  Обозначим  P  множество всех распределений априорных вероятностей классов (гипотез). Варьируя  P ,  P ( P , получим множество  решающих правил, оптимальных в смысле критерия Байеса. Найдем условия, определяющие  при фиксированном  P  . Введем функцию
		              (2.19)
и представим (2.17) и (2.18) соответственно в виде

Справедлива теорема: оптимальное в смысле критерия Байеса решающее правило  при заданном  отвечает разбиению пространства признака  на непересекающиеся области  где 
    (2.20)
и для которых принятие решений осуществляется по схеме

Нестрогое неравенство в (2.20) следует понимать в том смысле, что значения признака, лежащие на границе смежных областей, можно произвольно относить к соседствующим областям: при этом значение среднего риска не изменяется (это достаточно очевидное утверждение полезно проверить читателю самостоятельно).
Доказательство этой теоремы сводится к сравнению значений среднего риска для решающего правила и для произвольного рандомизированного решающего правила  (при неизменном  P ).
Имеет место очевидное неравенство:
 
 т.к. для  и 
Подчеркнем, что полученное оптимальное решающее правило  является нерандомизированным.
Рассмотрим случай  Из (2.19) и (2.20) следует, что при этом  определяется условием
                   (2.21)
Ввиду (2.14), 92.20) и (2.21) эти условия могут быть переписаны с использованием функции , т.е.
		      (2.22)
т.е. для критерия Байеса разрешающей функцией (т.е. функцией, определяющей структуру решающего правила .) снова является функция отношения правдоподобия, пороговое значение  которой непосредственно выражается через априорные вероятности гипотез и матрицу штрафов.
Нетрудно убедиться, что класс решающих правил , оптимальных в смысле критерия Неймана-Пирсона, шире класса решающих правил , оптимальных в смысле критерия Байеса, поскольку второй не содержит рандомизированных решающих правил для случаев  Можно, однако, расширить класс , включив в него и рандомизированные решающие правила, используемые для значений признака  Y , лежащих на границах областей . Ясно, что такие решающие правила эквивалентны нерандомизированным решающим правилам, для которых отнесение этих значений к той или иной области произвольно, т.к. не изменяет значение критерия. Такой расширенный класс байесовских р.п.  совпадает с классом р.п. .

:2.4. Минимаксный критерий

Каждое решающее правило   оптимально относительно фиксированного априорного распределения гипотез  Нередко, однако, это распределение не известно или может непредсказуемо изменяться, причем соответствующая этому изменению  оперативная перенастройка процедуры принятия решений (корректировка решающего правила) оказывается невозможной. В таком случае естественно искать решающее правило, которое гарантировало бы минимальное значение среднего риска  при самом неблагоприятном распределении . Этот подход приводит к минимаксному критерию выбора гипотез и к классу минимаксных решающих правил .
В принятых обозначениях минимаксное решающее правило  определяется равенством 
		    (2.23)
или, ввиду очевидного равенства (ср. (2.16))


Справедлива следующая теорема.
Пусть существует байесовское решающее правило , оптимальное относительно некоторого априорного распределения ,  P ( P  , и для которого 
	         	(2.24)
Тогда  - минимаксное решающее правило, т.е.  Действительно, для  справедливы соотношения

что и соответствует утверждению теоремы.
Заметим, что среди нерандомизированных байесовских решающих правил из  может не оказаться решающего правила, для которого реализуются равенства (2.24). В этом случае следует искать соответствующее рандомизированное решающее правило из расширенного класса  Оказывается, что всегда найдется решающее правило из  для которого выполняются (2.24).
Рассмотрим случай  для которого, очевидно,  определяется условием .
Последнее достигается выбором порогового значения функции отношения правдоподобия  и рандомизирующей вероятности  q , отвечающих условию

Легко заметить, что построение минимаксного решающего правила в общем случае сложнее, чем байесовского решающего правила при известном  P , поскольку в первом случае требуется поиск порогового значения функции отношения правдоподобия  и величины радомизирующей вероятности  q , в то время как во втором случае пороговое значение  определяется заданным  P  и рандомизация решающего правила не требуется .

:2.5. Последовательный анализ Вальда

Выше нами рассматривались процедуры принятия решений при фиксированном объеме выборки п. На практике, однако, учитывая связанные с измерениями затраты (материальные или временные), желательно применять процедуры, в которых число измерений (т.е. объем выборки, по которой принимается решение) заранее не фиксируется и возрастает лишь постольку, поскольку этого требует заданный уровень "качества" принимаемого решения.
Такая процедура может быть построена, если в число допустимых решений включить, наряду с выбором тех или иных гипотез решение о необходимости продолжать испытание, т.е. произвести дополнительно новое измерение случайной величины, увеличив, таким образом объем выборки на единицу.
Пусть  означает выбор гипотезы  h  на  n -м шаге, решение о необходимости произвести -е измерение.
Ниже мы приведем описание т.н. статистических решающих функций общего вида, принадлежащие Вальду.
Рассмотрим отображение  из  в вероятностное пространство , где в качестве  выступает Н - множество гипотез. Другими словами, для каждого  ставит в соответствие вероятностное пространство с множеством элементарных исходов Н. При выполнении некоторых свойств регулярности  и является статистической решающей функцией на -м шаге.
Статистическое решающее правило (т.е. алгоритм принятия решения, соответствующий  выглядит следующим образом: по значению  с помощью выбирается  и затем в результате независимого эксперимента "разыгрывается" (в соответствии с мерой )  которая и является той гипотезой, в пользу которой принимается решение.
В зависимости от того, принадлежит ли  h 0 (гипотеза о проведении следующего измерения) множеству Н, решающее правило будет являться последовательным или правилом с фиксированным числом измерений. В общем случае ((() тоже определена и задает процедуру выбора решения вообще без проведения измерений.
Теперь более подробно рассмотрим важный частный случай последовательного решающего правила, когда множество Н состоит из двух гипотез Н1 и Н2. Кроме того, далее будем считать, что  задает вырожденное распределение вероятностей, т.е. либо  либо  с вероятностью единица. Таким образом, речь идет о т.н. нерандомизированных решающих правилах, для которых значение  однозначно определяет решение.
Пусть  счетная последовательность, 
а - вектор, состоящий из п первых компонент . Определим множества

имеющие смысл областей принятия  j -го решения на п-м шаге. Другими словами,  есть подмножество множества счетных последовательностей, определяемое условием  Очевидны следующие соотношения:
 (

 Кроме  определим еще множества

тогда  есть вероятность того, что решение в пользу  будет принято не позднее п-го шага. Очевидно, что  В дополнение к вышесказанному определим событие

которое означает завершение процедуры за конечное число шагов. Очевидно, что для любого п 

Поскольку
то для того, чтобы Р(В)=1, необходимо и достаточно, чтобы

или, что то же самое,

где  N  - случайная величина, совпадающая с количеством шагов последовательной процедуры для принятия решения.
Покажем, что для конечности  M ( N ) необходимо и достаточно, чтобы

Действительно,

откуда следует, что для конечности математического ожидания  N  необходимо и достаточно сходимости ряда  Пусть существуют условные плотности распределения  случайные величины  Y  и  - функция отношения правдоподобия для выборки , т.е. для п-го шага процедуры. Введем (пока произвольно) два фиксированных числа  и определим на каждом п-м шаге последовательной процедуры следующее решающее правило:
		    (2.25)
при этом, конечно, предполагается, что до п-го шага выбор гипотезы не осуществился.
Пусть  N  - случайная величина, равная количеству измерений до принятия окончательного решения (т.е. решения в пользу какой-либо гипотезы), а  - математическое ожидание числа шагов процедуры при условии, что верна гипотеза .
Как обычно, определим ошибки первого и второго рода:
	    (2.26)
Будем считать, что критерием качества последовательной процедуры (2.25) при истинной плотности  является величина 
		    (2.27)
где - штраф за неверное решение, а  стоимость одного наблюдения. Аналогично, при истинной плотности  критерием качества будет служить величина 
		    (2.28)
Если априорные вероятности классов  известны, то средний риск процедуры ( равен
     (2.29)
где для краткости введено обозначение .
Прежде всего мы сформулируем и докажем теорему об оптимальности последовательной процедуры (2.25) в смысле минимизации среднего риска  из (2.29).
Обозначим через 
		    (2.30)
нижнюю границу риска по классу  процедур, требующих хотя бы одного измерения.
Теорема 2.1. Пусть 
	    (2.31)
и правило  определяется (2.25) при 
	    (2.32)
где  задаются (2.25). Тогда
	    (2.33)
где ( - класс всех решающих правил (в том числе и приписывающих какое-либо решение без проведения измерений).
Доказательство. Мы начнем с исследования того, стоит ли вообще производить наблюдения, в результате чего риск будет меньше  q , или лучше принять решение немедленно.
Докажем, что функция  - вогнутая. Поскольку из (2.29) следует, что 

поэтому

т.е. - вогнутая функция и, поскольку она ограничена снизу, то она непрерывна на (0,1).
Пусть - процедура, которая принимает без измерений, а  - без измерений принимает . Очевидно, что 
        	    (2.34)
Если 

то определим числа  и  из уравнений (см. рис. 2.1)
  	    (2.35)
В рассматриваемом нами случае  существует единственный оптимальный способ поведения на первом шаге: если  то наблюдения не проводятся и принимается гипотеза если  то без проведения наблюдений принимается  и, наконец, если  то производится первое наблюдение. Отметим, что если , 
то  и решение о продолжении измерений принимается с нулевой вероятностью.


			Рис. 2.1.
Пусть теперь произведено п наблюдений с исходами  Это изменит только одну сторону ситуации вероятность того, что гипотеза  верна, будет теперь не (, а 
     (2.36)
т.е. апостериорной вероятности 
Поскольку все измерения (по условию) независимы, одинаково распределены и имеют одинаковую стоимость  то независимо от количества и значения проведенных измерений функция  будет одной и той же.
Как и в предыдущем случае (когда измерения отсутствовали), следует провести следующее, -е измерение, если  (где, как и ранее,  определяется (2.34) и (2.35). С учетом выражения (2.36) для  получаем условие предложения измерений в виде:
	      (2.37)
что совпадает с (2.31). Теорема доказана.
Помимо оптимальности последовательного критерия в смысле (2.29), рассмотрим и другой подход.
Примем, что из двух процедур  используемых для принятия одной из двух простых гипотез  предпочтительнее (, если 
 		    (2.38)
 причем одно из неравенств - строгое.
Теорема 2.2. Процедура (2.25) оптимальна в смысле (2.38).
Доказательство. Пусть - заданные числа. Выбираем числа  так, чтобы выполнялось условие (2.32). Кроме того, зададим числа  так, чтобы удовлетворялось условие (2.31). Тогда процедура (2.25) (обозначим ее (() с выбранными значениями, согласно теореме 2.1б минимизирует критерий (2.29).
Пусть существует процедура (, отличная от , которая предпочтительнее  в смысле (2.38). Но тогда нетрудно убедиться, что  т.е. ( - не оптимальна в смысле (2.29). Полученное противоречие доказывает теорему.
Точное определение границ  соответствующих заданным ( и (, сопряжено с большими трудностями вычислительного характера. Поэтому на практике используются приближенные правила для нахождения , которые приведены ниже.
Обозначим, как и ранее, через  множество тех точек, для которых процедура заканчивается на п-м шаге принятием гипотезы Н2. Тогда


 
	     (2.39)
Аналогично, если  обозначает часть точек , для которых процедура заканчивается на п-м шаге принятием , то 
 		    (2 .40)
При выводе (2.39) и (2.40) мы молчаливо предполагали, что 

где  N  - случайная величина, задающая количество шагов процедуры. Равенство 92.41) означает, что вероятность неограниченного продолжения процедуры равна нулю, т.е. равна нулю вероятность события .
Для доказательства (2.41) обозначим 

Будем считать, что
	    (2.42)
что означает, что  почти всюду.
Обозначим  - пороги последовательной процедуры.
Допустим сначала, что 
Введем событие

где - реализации СВ  Z . Нетрудно убедиться, что событие  влечет за собой и событие

откуда  и, следовательно, 
С учетом  следует, что  Поскольку

то, вычитая эти неравенства, получим

откуда  Аналогично,  поэтому, если через  обозначить событие  то из вышесказанного следует, что  поэтому

				    (2.43)
Предпоследнее равенство имеет место ввиду независимости СВ   . По теореме непрерывности из 

следует, что 

т.е. имеет место конечность процедуры с вероятностью единица.
Пусть теперь  тогда для случайной величины, не равной тождественно нулю  по предположению) существует число  такое, что  откуда с учетом (2.43) получаем  Если выбрать число  так, чтобы  то получим 
	    (2.44)
что и доказывает (2.41).
Докажем, кроме того, что  (при выполнении (2.42)). Действительно, аналогично тому, как мы это делали в общем случае

где  (см.(2.24)).
Неравенства (2.39) и (2.40) наводят на мысль об аппроксимации границ  соответствующих заданным  и , величинами 

В силу (2.39), (2.40) вероятности ошибок  в этой приближенной процедуре (с порогами  удовлетворяют равенствам

Откуда с учетом  получаем
.			    (2.45) 
Обычно  имеют порядок до 0.1, поэтому с большой точностью можно принять, что ошибки обоих типов ограничены сверху заданными . Последнее заключение подкрепляется тем фактом, что 
Это можно видеть, складывая неравенства 

которые следуют из (2.45).
Единственный серьезный риск, связанный с употреблением приближенных границ, состоит, следовательно, в том, что  могут оказаться намного меньше заданных значений, что приведет к существенному увеличению числа необходимых наблюдений. Однако есть причины надеяться, что это увеличение будет умеренным. Действительно, вывод неравенства (2.39) был основан на замене величины  на величину  в области  Для того, чтобы  необходимо, чтобы  в области  что маловероятно, поскольку    (именно поэтому и потребовалось 
п-е измерение ).Таким образом, чтобы одновременно выполнялись сразу оба неравенства:

необходимо, чтобы
		    (2.46)
Если классы различимы не "очень хорошо" (что и представляет практический интерес), то это означает, что неравенство (2.46) не имеет места; во всяком случае, оно выполняется с малой вероятностью. Следовательно,  приблизительно равно  и наряду с (2.39) имеет место приближенное равенство
			    (2.47)
Совершенно аналогично вместе с (2.40) мы можем получить

Таким образом  что подтверждает практическую важность предположенного способа выбора порогов в последовательной процедуре.
В (2.25) мы предположили, что решение в пользу какого-либо класса принимается лишь тогда. Когда отношение правдоподобия превосходит (строгое неравенство) один из порогов. Но все рассуждения применимы, когда решение о продолжении измерений принимается в случае 
Все эти процедуры эквивалентны, когда отношение правдоподобия имеет непрерывное распределение.
Однако в случае дискретного распределения отношения правдоподобия целесообразно сохранить возможность рандомизации на границе, с тем, чтобы получать в точности заданные вероятности ошибок.
Докажем некоторые соотношения для характеристик последовательной процедуры. Прежде всего установим, что имеет место тождество Вальда:
	    (2.48)
Введем случайные величины  где 

Очевидно, что  зависит только от  и, следовательно, не зависит от . Нетрудно убедиться в том. Что 

где  N  - случайная величина, представляющая собой количество шагов последовательной процедуры.
что и доказывает (2.48).
Величина  в последовательной процедуре может быть аппроксимирована величиной  когда  отвергается, и  когда принимается (см. пояснения к (2.46)), поэтому

где - вероятность события, состоящего в том, что решение принято в пользу  Итак,
	    (2.49)
Аналогично
	    (2.50)






