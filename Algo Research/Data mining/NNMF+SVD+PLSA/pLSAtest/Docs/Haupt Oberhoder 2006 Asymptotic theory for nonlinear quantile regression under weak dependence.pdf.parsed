
ASYMPTOTIC THEORY FOR NONLINEAR QUANTILE REGRESSION UNDER WEAK DEPENDENCE¤ Walter Oberhofer and Harry Haupt University of Regensburg Department of Economics and Econometrics, University of Regensburg. Universitaetsstr. 31, 93053 Regensburg, Germany. Tel.: +49-941-943-2739, fax: +49-941-943-4917. walter.oberhofer@wiwi.uni-r.de, harald.haupt@wiwi.uni-r.de Running title: NONLINEAR QUANTILE REGRESSION Address correspondence to: Harry Haupt, Department of Economics and Econometrics, University of Regensburg, UniversitÄatsstr. 31, 93053 Regensburg, Germany e-mail: harald.haupt@wiwi.uni-r.de. Abstract: This paper derives the consistency and asymptotic normality of the nonlinear quantile regression estimator with weakly dependent errors under mild assumptions. The notion of weak dependence introduced in this paper, can be considered in part as a quantile speci¯c local variant of recently introduced concepts. In addition, we provide new and detailed results on the connection between the measure of dependence and the rate of convergence of regression quantiles and introduce a new probability inequality for nonlinear regression estimators based on the L1-norm, extending existing results for least squares estimation. There is an obvious connection of the derived asymptotic results to the corresponding results of least squares estimation. ¤We wish to thank the participants of the workshop \New trouble for standard regression analysis", November 2005, Regensburg/Germany, and the participants of the ICMS workshop \Quantile Regression, LMS Method and Robust Statistics in the 21st Century", June 2006, Edinburgh/Scotland for many valuable comments. The second author gratefully acknowledges the support of the International Centre for Mathematical Sciences (ICMS). We also thank three anonymous referees and an associate editor for helpful comments.
1 Introduction The concept of quantile regression, introduced in the seminal paper of Koenker and Bassett (1978), has become a widely used and accepted technique in many areas of theoretical and applied eco- nometrics. Only recently, the ¯rst monograph on this topic has been published by Roger Koenker (2005), covering a wide scope of well established foundations and (even a `twilight zone' of) actual research frontiers. In addition, many of the numerous contributions in this fast evolving ¯eld have been reviewed and summarized in recent survey articles (see inter alia Buchinsky, 1998, and Yu et al., 2003). In contrast to the more methodological literature, there are also important, non- technical attempts to bring the key concepts and especially the applicability of quantile estimation to a wider audience outside the statistical profession (see for example Koenker and Hallock, 2001). In this paper we consider quantile regressions where the dependent variable y and covariates x1; : : : ; xK satisfy a nonlinear model with additive errors. Nonlinear quantile regression models have been discussed by several authors. Oberhofer (1982) and Wang (1995) considered the consistency and asymptotic normality of the least absolute deviations (LAD) estimator under the assumption of independent and identically distributed (i.i.d.) errors, respectively. The i.i.d. assumption has been challenged in direrent ways in the quantile regression literature. Koenker and Bassett (1982) ¯rst investigated the case of heteroscedasticity based on regression quantiles, other authors discussed this case for the most prominent quantile, the median (see for example Knight, 1999, Zhao, 2001, and the literature cited there). Quantile regression with dependent errors have been studied for LAD estimation by Phillips (1991) and Weiss (1991), for unconditional quantiles in a parametric context by Oberhofer and Haupt (2005), and in a nonparametric context by De Gooijer and Zerom (2003) and Ioannides (2004). In the context of pure time series models, the nonparametric estimation of regression quantiles under dependence has been discussed by Cai (2002), who also provides a survey of the preceding literature in this context. Other relevant works in this context include Portnoy (1991), Koenker and Park (1994), Jureckova and Prochazka (1994), and, more recently, Mukherjee (1999,2000), Chernozhukov and Umantsev (2001), Chen et al. (2004), and Engle and Manganelli (2004). We contribute to this literature by providing mild, quantile speci¯c conditions for the con- sistency and asymptotic normality of nonlinear regression quantiles when the errors are weakly dependent. We also provide an in-depth analysis of the connection between the rate of convergence and the measure of dependence, and derive a new probability inequality for nonlinear regression estimators based on the L1-norm. The following Section 2 introduces the model framework and derives the loss function of condi- tional quantile estimation. The remainder of the paper is organized as follows: In Section 3, after a discussion of the basic consistency assumptions, we prove consistency. In Section 4 we discuss rates of convergence and prove consistency in a wider sense for both linear and nonlinear regression functions. In Section 5 we derive the assumptions for asymptotic normality of regression quantiles under weak dependence, again for both linear and nonlinear regression functions. Finally, after short conclusions in Section 6, in two Appendices we prove several preliminary results required for consistency and asymptotic normality, respectively. 2
2 Regression quantiles Let (-;F; P) be a complete probability space and let fytgt2N be an F-measurable scalar random sequence. Then, consider the regression model yt ! g(xt; ¯0) = ut; 1 - t - T; (2.1) where ¯0 2 D¯ ½ RK is a vector of unknown parameters, the 1£L vectors xt are deterministic and given, the dependent variables yt are observable, g(x; ¯) is in general a nonlinear function de¯ned for x 2 Dx and ¯ 2 D¯ from Dx £ D¯ ! R, where xt 2 Dx for all t, and ut is an error term. Our aim is to analyze the asymptotic behavior of the #-quantile regression estimator ^ ¯T , i.e. ¯ = ^ ¯T minimizing the objective function XT t=1 -#jyt ! g(xt; ¯)j+ + (1 ! #)jyt ! g(xt; ¯)j!¸; (2.2) where 0 < # < 1, and for z 2 R, we de¯ne jzj+ def = ( z if z > 0; 0 if z - 0; and jzj! def = ( 0 if z > 0; !z if z - 0: For technical reasons { i.e. to avoid the need of using E(ut) { instead of the objective function (2.2) we use the equivalent objective function XT t=1 -#jyt ! g(xt; ¯)j+ + (1 ! #)jyt ! g(xt; ¯)j! ! #jutj+ ! (1 ! #)jutj!¸: (2.3) For the derivation and discussion of asymptotic results it is convenient to introduce some further de¯nitions. Using the abbreviation ht = ht(¯) def = g(xt; ¯) ! g(xt; ¯0) the objective function (2.3) can be rendered to XT t=1 -#jut ! htj+ + (1 ! #)jut ! htj! ! #jutj+ ! (1 ! #)jutj!¸: (2.4) A typical element of the sum (2.4), is denoted by at(¯), leading to the objective function AT (¯) = XT t=1 at(¯); where at(¯) def = 8>>>< >>>: !#ht if ut > max(0; ht); ut ! #ht if ht < ut - 0; !ut + (1 ! #)ht if 0 < ut - ht; (1 ! #)ht if ut - min(0; ht): (2.5) 3
Then split up at(¯) in at(¯) = bt(¯) + ct(¯); (2.6) where ct(¯) def = ( !#ht for ut > 0; (1 ! #)ht for ut - 0; (2.7) and, consequently bt(¯) def = 8>>>< >>>: 0 if ut > max(0; ht); ut ! ht if ht < ut - 0; !ut + ht if 0 < ut - ht; 0 if ut - min(0; ht): (2.8) Furthermore, we de¯ne BT (¯) = PTt=1 bt(¯) and CT (¯) = PTt=1 ct(¯). The expression ct(¯) de¯ned in (2.7) has an interesting interpretation. It contains the factor ht, which arises from the deviation between the regression function and its true value, and, as a second component, a Bernoulli random variable f'tg, where 't = !# for yt lying above and 't = 1 ! # for yt lying below the true regression surface. The decomposition of at(¯) in (2.6) allows us to study the asymptotic behavior of the objective function by studying separately that of bt(¯) and ct(¯) in Lemmas 1C-3C, given in Appendix C (for consistency), and in Lemmas 1N-3N, given in Appendix N (for asymptotic normality), respectively. In order to illustrate equations (2.6)-(2.8), consider least squares for the linear regression model yt ! xt¯ = ut. Using the de¯nition of ht and simple algebra, in analogy to (2.4), the least squares loss function can be written as PTt=1(ut ! ht)2 !PTt=1 u2t= !2PTt=1 utht +PTt=1 h2t. Hence, the decomposition is given by BT (¯) = PTt=1 h2tand CT (¯) = !2PTt=1 utht. 3 Consistency 3.1 Basic consistency assumptions In this framework, the existence of a measurable estimator ^ ¯T usually is ensured by Theorem 3.10 of Pfanzagl (1969), which, if ¯0 is an inner point of a compact set D¯, is valid under the assumptions stated below. By Ft(z) we denote the distribution of ut and by Fs;t(z;w) the common distribution of (us; ut) for s 6= t. Assumptions (C1) to (C3) refer to properties of the covariates and the regression, assumptions (C4) to (C6) refer to the error process. (C1) The 1 £ L vectors xt are deterministic and known, t = 1; 2; : : : (C2) D¯ is compact and lim sup T; ¯2D¯ 1T XT t=1 [g(xt; ¯) ! g(xt; ¯0)]2 < 1: 4
(C3) For every 2 > 0 there exists a positive ±g such that lim inf T inf jj¯!¯0jj¸ 2 1T XT t=1 jg(xt; ¯) ! g(xt; ¯0)j > ±g: (C4) There exist a positive f0 and a positive ±0, such that for all j xj - ±0 lim inf t fmin [Ft(j xj) ! Ft(0); Ft(0) ! Ft(!j xj)]g ¸ f0j xj: (C5) Ft(0) = P(ut - 0) = #, 0 < # < 1, for all t. (C6) For 1 - s; t and s 6= t de¯ne rs; t = sup jP(F \ G) ! P(F)P(G)j; where the supremum is taken over all F elements of the ¾-algebra ¾(us) and all G elements of the ¾-algebra ¾(ut): Additionally de¯ne rk = supt j rt; t+kj for k = 1; 2; 3; ::: Then, rk ! 0. In assumption (C1) it is not necessary for the regressors to be deterministic as postulated in assumption (C1), as similar behavior can be expected of random regressors fxtg independent of the disturbances futg. Consider the example of a linear regression function and let fxtg be a sta- tionary sequence with E(x0txt) ¯nite and non-singular. Then, almost all realizations would have the necessary limiting properties. Assumption (C2) is a weaker version of the standard dominance condition given in PÄotscher and Prucha (1997, p.28). In the linear case assumption (C2) is implied by the assumption that T!1Px0txt converges to a ¯nite matrix. The identi¯able uniqueness con- dition in assumption (C3) can be compared with the analogous condition in the linear regression model using least squares, i.e. lim inf T inf jj¯!¯0jj¸ 2 1T XT t=1 !xt¯ ! xt¯0¢2 = lim inf T inf jj¯!¯0jj>2(¯ ! ¯0)0 1T X0X(¯ ! ¯0) > 0; implied by the non-singularity of the limit of the matrix T!1X0X. On the one hand assumption (C2) rules out a too strong growth of the covariates, on the other hand the identi¯cation condition (C3) guarantees enough variation. The resulting trade-or problem between assumptions (C2) and (C3) is rather involved for the general nonlinear case and lies beyond the focus of the present paper. Familiar throughout the literature on quantile regression is the assumption that the density ft(z) of Ft(z) exists in a neighborhood of z = 0 and ft(0) ¸ f0 > 0 for all t, implying assumption (C4). Violations of this assumption are treated in the literature on so called non-standard conditions (e.g., Knight, 1998, and Rogers, 2001). Assumption (C5) is a common normalization in quantile regression, when g contains an intercept, implying E(ct(¯)) = 0. By virtue of assumption (C6) the error process futg is strongly mixing, excluding a too strong dependency. Interestingly, we do not require any moment assumptions. THEOREM 1. Under assumptions (C1)-(C6) the estimator ^ ¯T is consistent. 5
PROOF. From the Chebychev-inequality follows for a stochastic variable Y and a constant M, where both expectation and variance of Y exist, P!M > Y ! E(Y ) > !M¢ > 1 ! V ar(Y ) M2 : Hence, for Y = AT (¯)=T and M = E!AT (¯)=T ¢=2 6= 0 we have P uAT (¯)=T > 12E!AT (¯)=T ¢
 > 1 ! 4V ar!AT (¯)=T ¢ £E!AT (¯)=T ¢¤2 : (3.1) According to Lemma 1C and assumption (C3) for jj¯ ! ¯0jj ¸ 2, where 2 is an arbitrary positive number, follows E!AT (¯)¢=2 > 0 = AT (¯0) ¸ AT ( ^ ¯T )): Thus, equation (3.1) implies P!AT (¯) > AT ( ^ ¯T )¢ > 1 ! 4V ar!AT (¯)=T ¢ £E!AT (¯)=T ¢¤2 : (3.2) Due to assumptions (C2) and (C6) and according to Lemma 2C and Lemma 3C we obtain V ar!AT (¯)=T ¢ = o(1): (3.3) Thus, from (3.2) and (3.3), and making use of assumption (C3) and Lemma 1C, P!AT (¯) > AT ( ^ ¯T )¢ >8>>< >>: 1 ! o(1) (f0±20)2 for ¯ 2 B>; T (2); 1 ! o(1) f2 0 (±g=4)4 for ¯ 2 B-; T (2): (3.4) Due to B>; T (2) [ B-; T (2) = f¯¯¯jj ¯ ! ¯0jj ¸ 2g, (3.4) implies for T ! 1 P(jj ^ ¯T ! ¯0jj > 2) ! 0; (3.5) which proves the assertion. ¥ 4 Rate of convergence When we prove the consistency of ( ^ ¯T ! ¯0), it is su±cient that frkg is a null sequence. We now replace assumption (C6) by (C6') rk = O(k!p), where 0 < p < 1, in order to make an assertion on the convergence rate of the consistent estimate ^ ¯T . As intuition suggests, this rate depends on the size of the mixing coe±cient rk. Hence, in contrast to the argumentation in Theorem 1, we now try to show the consistency of Tr( ^ ¯T!¯0), where 0 < r < p=2. 6
Due to Theorem 1 we can assume without loss of generality that ¯ 2 B-; T (2). It is convenient to introduce the variable r def = Tr(¯ ! ¯0): (4.1) Then, substitute ¯ by ¯0 + r=T r in the objective function, and consider ¯ as a function of r: ¯ = ¯(r). From the minimization of AT (¯(r)) we obtain ^rT . Thus, following from (3.2), PuAT (¯(r)) > AT (¯(^rT ))
 > 1 ! 4V ar!AT (¯(r))=T ¢ £E!AT (¯(r))=T ¢¤2 : (4.2) Having expressed (3.2) in terms of ¯(r) by considering (4.2), we are in a position to derive the ex- tended consistency result outlined above and stated in Theorems 2 and 3 at the end of this section. Due to reasons of clarity we distinguish the cases of linear and nonlinear regression functions. 4.1 Linear regression functions Instead of (2.1) we consider yt ! xt¯0 = ut; 1 - t - T; where L = K. Hence, ht = xt(¯ ! ¯0), and the sums in assumptions (C2) and (C3) are given by lim sup T; ¯2D¯ 1T XT t=1 !xt(¯ ! ¯0)¢2 < 1; and lim inf T inf jj ¯!¯0jj¸ 2 1T XT t=1 j xt(¯ ! ¯0)j > ±g; respectively. According to Theorem 1 and assumption (C2), we can choose ¯ ! ¯0 small enough, such that 1 4T XT t=1 j htj < ±0: (4.3) From assumption (C6') follows T!1PTk=1 rk = O(T!p). In addition, from equation (4.1) follows ht = xt(¯ ! ¯0) = xtr=T r. Together with Lemmas 2C and 3C this leads to V ar!AT (¯(r))=T ¢ = O(T!p)T!2r 1T XT t=1 (xtr)2; (4.4) and, due to E(CT (¯)) = 0 and Lemma 1C, and (4.3), E!AT (¯(r))=T ¢ ¸ T!2rf0 " 1 4T XT t=1 jxtrj#2: (4.5) From (4.4) and (4.5), and by virtue of assumptions (C2) and (C3), follows that the right hand side of (4.2) converges to 1 for every ¯xed r with jj rjj 6= 0, and p > 2r. Consequently, for T ! 1, P(jj ^rT jj > 2) ! 0: (4.6) 7
Since the numerator on the right hand side of (4.2) is of order jj rjj2, and the denominator is of order jj rjj4, convergence still holds for jj rjj ! 1, implying ^ ¯T = ¯T (^rT ) and P(Trjj ^ ¯T ! ¯0jj > 2) ! 0: Since 2 can be chosen arbitrarily small, however, this implies the consistency of ^ ¯T of order T!r, where r < p=2. 4.2 Nonlinear regression functions In the nonlinear case the regression function g(x; ¯) must be smooth in the neighborhood of ¯ = ¯0. We assume that g(x; ¯) has the following Taylor expansion (with remainder) for all x 2 Dx and ¯ in the neighborhood of ¯0: g(x; ¯) = g(x; ¯0) + @g(x; ¯) @¯0 ¯¯¯¯ ¯=¯0(¯ ! ¯0) + (¯ ! ¯0)0 Ã12 @2g(x; ¯) @¯@¯0 ¯¯¯¯ ¯=¯¤!(¯ ! ¯0); (4.7) where ¯¤ = ¯0 + "(¯ ! ¯0) and 0 < " < 1. For ease of notation we introduce the row vector zt def = @g(xt; ¯) @¯0 ¯¯¯¯ ¯=¯0 ; (4.8) and the K £ K matrix Wt(¯¤t ) def = 12 @2g(xt; ¯) @¯@¯0 ¯¯¯¯ ¯=¯¤t ; (4.9) where ¯¤t = ¯0 + "t(¯ ! ¯0) and 0 < "t < 1. Thus we obtain ht = g(xt; ¯) ! g(xt; ¯0) = zt(¯ ! ¯0) + (¯ ! ¯0)0Wt(¯¤t )(¯ ! ¯0): (4.10) Then we assume (C7) If all ¯t are in a neighborhood of ¯0, then, for a ¯xed MW < 1, sup t; jj rjj=1 r0Wt(¯t)r - MW: In addition we need a local version of assumptions (C2) and (C3), (C8) lim supT; jj rjj=1 T!1PTt=1(ztr)2 - ¸0 < 1, (C9) lim infT; jj rjj=1 T!1PTt=1 jztrj ¸ ±0 > 0. Obviously, in the linear case, assumption (C8) corresponds to assumption (C2), and assumption (C9) corresponds to assumption (C3). Note that according to the consistency established in Theo- rem 1, without loss of generality for an arbitrary ´ > 0, we can restrict ¯ such that jj ¯ !¯0jj < ´. 8
Then, due to assumptions (C7) and (C8), 1T XT t=1 h2t- 2 1T XT t=1 !zt(¯ ! ¯0)¢2 + 2 1T XT t=1 !(¯ ! ¯0)0Wt(¯t)(¯ ! ¯0)¢2; and, by inserting ¯ ! ¯0 = r=T r, 1T XT t=1 h2t- T!2r2¸0jjrjj2 + T!4r2MWjjrjj4 = O(T!2r): Then, in analogy to the linear case, V ar!AT (¯(r))=T ¢ = O(T!p!2r): (4.11) Due to assumptions (C7) and (C9), for T large enough, 12 1T XT t=1 j zt(¯ ! ¯0)j - 1T XT t=1 j htj; (4.12) and in Lemma 1C we can use the left hand side of (4.12) as lower bound instead of the right hand side. Thus, according to (4.11) and (4.12), inequality (4.2) can be applied in analogy to the linear case. The considerations in the previous two subsections can be subsumed to the following result: THEOREM 2. For linear regression functions under assumptions (C1)-(C5) and (C6'), and for nonlinear regression functions under assumptions (C1)-(C5), (C6') and (C7)-(C9), for T ! 1, P(Trjj ^ ¯T ! ¯0jj > 2) ! 0; for every 2 > 0 and r < p=2 < 1=2. We now prove a new probability inequality for nonlinear regression estimators based on the L1-norm, which extends results for nonlinear least squares estimation established by of Ivanov (1976) for i.i.d. errors, and Prakasa Rao (1984), and Hu (2004) for mixing errors. Note that the assumptions of Theorem 3 are less strict than the assumptions applied in Hu (2004), where the latter already uses milder assumptions than Prakasa Rao (1984). We have to fortify assumption (C6') in the following way: (C6") rk = O(k!p), where p > 1. Assumption (C6") implies the convergence of P1k=1 rk. THEOREM 3. For linear regression functions under assumptions (C1)-(C5) and (C6"), and for nonlinear regression functions under assumptions (C1)-(C5), (C6") and (C7)-(C9), P(pTjj ^ ¯T ! ¯0jj > ½) - O(½!2): 9
PROOF. For the linear case consider equations (4.4) and (4.5) let technically p = 1 and r = 1=2. Then, the right hand side of (4.2) equals 1 ! T!1PTt=1(xtr)2 f2 0 !T!1PTt=1 j xtrj=4¢4 : (4.13) The fraction in (4.13) is of order O(jj rjj!2). Thus, for all r satisfying jjrjj ¸ ½ > 0, we get P(jj ^rT jj - ½) ¸ 1 ! O(½!2), which proves the assertion. The proof of the nonlinear case is analogous to the proof of Theorem 2 and is left to the reader. ¥ 5 Asymptotic normality For the derivation of asymptotic normality we need consistency and some additional assumptions. Typically, these assumptions are local. First, the regression function g(x; ¯) must be smooth in the neighborhood of ¯ = ¯0 and we assume the Taylor expansion (4.7) and apply the notation introduced in (4.8)-(4.10). Because we try to show the asymptotic normality of pT( ^ ¯T ! ¯0) it is convenient to introduce the variable r def = pT(¯ ! ¯0) (5.1) and replace ¯ by ¯0 + r=pT in the objective function. In the proof of asymptotic normality, we need the compactness of the argument of the objective function, r in this case. If r is an element of a compact set and AT (¯) is maximized, then ¯ must be contained in a set depending on T which is contracted in the limit to the single point ¯0. Due to Theorem 3, we can use this reparametrization and minimize with respect to r. Prakasa Rao (1987) investigates these issues for least squares estimation of (2.1). After replacing ¯ by ¯0 + r=pT we get ht = 1 pT ztr + r0u 1T Wt(¯¤t )
r: (5.2) Note that ¯¤t can be rewritten as ¯¤t (r) = ¯0 + "tr=pT. Then, the left hand side of the model based on (2.1) can be transformed in the following way: yt ! g(xt; ¯) = g(xt; ¯0) + ut ! g(xt; ¯) = ut ! 1 pT ztr ! r0u 1T Wt(¯¤t (r))
r; 1 - t - T: Also note that ht depends on ¯. However, due to the substitution of ¯ by r, ht should be properly denoted as ht(¯(r)). For the same reason we write at(¯(r)); bt(¯(r)), and ct(¯(r)). The following assumptions are familiar for the cases of linear (see Subsection 5.1) and non- linear (see Subsection 5.2) regression functions, respectively. Note that assumption (N1) implies assumption (C4). (N1) For some 2f > 0 the density ft(z) of Ft(z) exists for jzj - 2f > 0, is continuous at z = 0 uniformly in t, and sup1-t-T ft(0) = o(pT). 10
(N2) The density fs; t(z;w) of Fs; t(z;w) exists for j zj - 2f and jwj - 2f , is continuous at (z;w) = (0; 0) uniformly in s and t, r0(kj u) ! 0, where r0(kj u) = supt j ft; t+k(0; 0) ! ft(0)ft+k(0)j, and the supremum is taken over t; k; t + k 2 N. (N3) For 1 - s < t - T de¯ne !s; t = Fs; t(0; 0) ! Fs(0)Ft(0) and !t; t = Ft(0) ! Ft(0)2. Then T!1Z0T-TZT converges for T ! 1 to a K £ K matrix :, where the rows zt are collected in the T £ K matrix ZT , and -T is a T £ T matrix with generic element !s; t. (N4) De¯ne !k = supt j !t; t+kj, k = 0; 1; 2; : : :. Then !k = O(k!¸) for some ¸ > q > 2. (N5) T!1Z0TcTZT converges for T ! 1 to a non-singular K £K matrix V , where cT is a T £ T diagonal matrix with diagonal elements 't = ft(0), 1 - t - T. As some of the assumptions may appear rather abstract at ¯rst sight, the following discussion of the assumptions is illustrated with a comparison of linear least squares and quantile regression, respectively, in Table 1, in order to facilitate intution. If the disturbances are i.i.d., assumptions (N1) and (N2) are implied by the existence of f1(z) in the neighborhood of z = 0, and the continuity of f1(z) at z = 0. Usually, the stronger assumption that ft(0) is uniformly bounded is made, implying assumption (N1). Note that without loss of generality we use the same upper bound 2f in assumptions (N1) and (N2). Assumptions (N2), (N3), and (N4) restrict the dependence structures imposed on the quantile regression model. Assumption (N2) can be considered as an in¯nitesimal weak dependence condition and it can be interpreted as a quantile speci¯c variant of the \dependence index sequence" introduced by Castellana and Leadbetter (1986). In the case of independence, the sum in assumption (N2) is equal to zero for all T. Assumption (N3) ensures the existence of the covariance matrix in the limit. Obviously, independence of the two events fus - 0g and fut - 0g implies !s; t = 0. At the same time (N3) re'ects the dependence structure and heterogeneity of the error process. Note, that a too strong dependence hinders convergence in assumption (N3). If x1t = 1, then, according to assumption (N3), T!1PTs; t=1 !s; t must converge. Further, it is important to note, that the mixing property does not require the whole ¾-algebras ¾(ut¯¯t - s) and ¾(ut¯¯t ¸ s + k), for all s = 1; 2; : : :, respectively. A peculiarity of quantile regression lies in the fact, that the only thing that matters is a local mixing condition for the point (0; 0). In this sense, for s 6= t, we can view !s;t = Fs; t(0; 0)! Fs(0)Ft(0) as a local measure of dependence (or a local mixing coe±cient), and fs; t(0; 0)!fs(0)ft(0) as an analogous in¯nitesimal measure. If the limit of the matrix Z0T-TZT is singular, then the limiting distribution of pT( ^ ¯!¯0) is singular, too. In least squares estimation the matrix Z0T-TZT corresponds to T!1X0TE(uu0)XT =¾2, and Z0TcTZT in assumption (N5), which controls the form of heteroscedasticity, corresponds to T!1¾2X0X, respectively. Obviously, no moments of the error process are required for quantile estimation. As we will see, (#!#2)=ft(0)2 corresponds to variances and (Fs; t(0; 0) ! #2)=fs(0)ft(0) to covariances (see Table 1). Consider the Bernoulli process 't = 1!# for ut - 0, and 't = !# for ut > 0. Then, from assumption (N4) follows that the process f'tg is strongly mixing of size !r, where r > 2. Generally, the properties of the Bernoulli process f'tg, and the behavior of the distribution functions and densities in the near of z = 0, and (z;w) = (0; 0), respectively, are vital for weak dependence concepts in the quantile estimation framework. 11
Least squares Quantile regression regression model yt = xt¯ + ut, 1 - t - T loss function Pt u2tPt [#j utj+ + (1 ! #)j utj!] stochastics 1. and 2. moments exist ft(z) exists, ft(0) > 0, fs;t(z;w) exists normalization E(ut) = 0 Ft(0) = # \variance" E(u2t) = ¾2!t;t #(1 ! #)=f2 t (0) \covariance" E(usut) = ¾2!s;t [Fs;t(0; 0) ! Fs(0)Ft(0)]=fs(0)ft(0) asymptotic ¾2(X0X=T )!1(X0-X=T )(X0X=T )!1 (X0cX=T )!1(X0-X=T )(X0cX=T )!1, \covariance" c is diagonal with Át = 1=ft(0) matrix !s;s = #(1 ! #), !s;t = Fs;t(0; 0) ! #2 Table 1: Comparison of least squares and quantile regression 5.1 Linear regression functions Our aim is to derive the asymptotic distribution of the #-quantile regression estimator ^ ¯T de¯ned in Section 2. In this subsection we discuss only the special case of a linear regression function, and the general nonlinear case will be treated in the next subsection. Again, instead of (2.1) we consider the linear regression model yt ! xt¯0 = ut, 1 - t - T. Then, due to g(xt; ¯) = xt¯, we obtain zt = xt, Wt = 0, ht = xt(¯ ! ¯0). In the linear case, in addition to assumptions (N1)-(N5) we have to assume: (N6) sup1-t-T T!rjj xtjj = o(1), for an r < 1=2, where the assumptions of Theorem 2 are given and T!1PTt=1 jj xtjj4 = O(1): According to Theorem 2, we have plim Trjj ^ ¯T ! ¯0jj = 0. For an arbitrary 2 > 0 we minimize AT (¯) for Trjj ¯ ! ¯0jj - 2. The resulting estimator will be denoted as ¯T; 2. Thus, from Theorem 2 follows the consistency of ^ ¯T , implying P( ^ ¯T 6= ¯T; 2) ! 0 for T ! 1. Hence, if ¯T; 2 converges in distribution, the same applies to ^ ¯T . Then, due to ht = xt(¯ ! ¯0), j htj - T!rjj xtjj ¢ Trjj ¯ ! ¯0jj: (5.3) Due to assumption (N6) and Theorem 2, (5.3) implies for every ¯xed r and for T ! 1 sup 1-t-T ht ! 0: (5.4) As a consequence, we can assume without loss of generality that j htj - 2f , and therefore apply assumptions (N1) and (N2). According to Theorem 2, we can consider ¯ = ¯0 + r=pT. THEOREM 4. For linear regression functions under assumptions (C1)-(C3), (C5), (C6'), and (N1)-(N6), the minimizing value ^rT of AT (¯(r)), where r = pT(¯!¯0), converges in distribution to a normal distribution with mean zero and covariance matrix V !1 lim T!1- 1T X0T-TXT ¸ V !1: 12
PROOF. The proof of asymptotic normality is based on three preliminary Lemmas (given in Appendix N). In Lemma 1N it is shown that E[BT (¯(r))] converges to lim T!112 1T r0X0TcTXTr = 12r0V r: Lemma 2N establishes lim T!1V ar[BT (¯(r))] = 0: In both cases the convergence is uniform for r element of a compact set. Finally, in Lemma 3N it is shown that CT (¯(r)) converges in distribution to Cr for all r, where the 1 £K random vector C is normally distributed with mean zero and covariance matrix lim T!1 1T X0T-TXT : As a consequence, AT (¯(r)) converges in distribution to A(¯(r)) = 12r0V r + Cr; with the minimizing value ^r = !V !1C0 which will be normal with mean zero and covariance matrix V !1 lim T!1[T!1X0T-TXT ] V !1. For the convergence in distribution of the minimizing value ^rT to ^r, it is su±cient that the function AT (¯(r)) is convex and converges uniformly for r 2 C, where C is any compact subset of RK. That the former requirement is ful¯lled has been shown by Pollard (1991) and Geyer (1996), the latter has been shown in Lemmas 1N-3N. Of course this convexity argument can only be applied in the linear case. ¥ Interestingly, for linear regression functions, we do not need a Taylor approximation in the proof. The analogy to the corresponding covariance matrix of least squares for nonspherical disturbances is obvious (see the last line in Table 1). 5.2 Nonlinear regression functions In the nonlinear case (2.1), in addition to assumptions (N1)-(N5) we have to assume: (N6') sup1-t-T T!rjj ztjj = o(1), for an r < 1=2, where the assumptions of Theorem 2 are given and T!1PTt=1 jj ztjj4 = O(1): THEOREM 5. The assertion of Theorem 4 holds for nonlinear regression functions under assumptions (C1)-(C3), (C5), (C6'), (C7)-(C9), (N1)-(N5), and (N6'). PROOF. From Theorem 2 follows plim Trjj ^ ¯T ! ¯0jj = 0. Then we obtain j htj - T!rjj ztjj ¢ Trjj ¯ ! ¯0jj + Tr(¯ ! ¯0)0T!2rWt(¯¤t )Tr(¯ ! ¯0); and, due to assumptions (N6') and (C7), for T ! 1, similarly to (5.4), sup1-t-T ht ! 0. As a consequence, we can assume without loss of generality that j htj - 2f , and therefore apply assumptions (N1) and (N2). 13
According to Theorem 3 we can consider ¯ = ¯0 + r=pT. As a ¯rst step we show that the three preliminary Lemmas 1N-3N in Appendix N remain valid in the case of a nonlinear regression function, if we replace assumption (N6) by assumption (N6'), and add (C7). Lemma 1N remains valid, if (i) PTt=1 h2t= O(1), and (ii) limT!1PTt=1 h2tft(0) = limT!1 T!1PTt=1(ztr)2ft(0) = limT!1 r0T!1Z0TcTZTr = r0V r, hold true. From (5.2) and assumptions (C7) and (C8) follows (i). Making additional use of ass- umption (N5), analogous reasoning establishes (ii). According to assumptions (N6') and (N2) the assertion of Lemma 2N remains true. Finally, Lemma 3N also remains valid if lim T!1XT s; t=1 hs!stht = lim T!1 1T r0Z0T-TZTr = r0:r: (5.5) Due to ht = ztr=pT + r0Wt(¯¤t )r=T we have 1 T3=2 XT s; t (zsr)!st!r0Wt(¯¤t )r¢ - 2 XT!1 k=0 !kvuut1T XT t=1 (ztr)2vuut1 T2 XT t=1 (r0Wt(¯¤t )r)2 (5.6) and 1 T2 XT s; t !r0Ws(¯¤s )r¢!st!r0Wt(¯¤t )r¢ - 2 XT!1 k=0 !k 1 T2 XT t=1 (r0Wt(¯¤t )r)2; (5.7) where the right hand sides of (5.6) and (5.7) tend to zero for T ! 1, respectively, according to assumptions (N4), (C7), and (C8). Then, by virtue of assumption (N3), we have established equation (5.5). Due to the fact that in the nonlinear case the loss function AT (r) is not convex in general, the proof of Theorem 4 can not be extended in such an easy way. In Lemma 1N in Appendix N we consider the matrix T!1Z0TcTZT with the limit V . According to assumption (N5), the matrix V is nonsingular and we de¯ne 1rT def = !V !1!T ; where !T is implicitly used in Lemma 3N, since CT (¯(r)) = XT t=1 ht't = r0 XT t=1 - 1 pT z0t + 1T Wt(¯¤t )r¸ 't def = r0!T : By virtue of Lemma 3N and assumptions (C7) and (C8), !T converges in distribution to C, and C is normally distributed. Therefore, in order to prove Theorem 4 for the nonlinear case, we have to show plim(^rT ! 1rT ) = 0. Due to Lemmas 1N and 2N, the loss function can be written as AT (¯(r)) = BT (¯(r)) + CT (¯(r)) = lim T!1E[BT (¯(r))] + r0!T + RT (r) = 12r0V r + r0!T + RT (r); where plimT!1 RT (r) = 0, for r 2 C, where C is any compact set. 14
Due to the de¯nition of 1rT , we obtain AT (¯(r)) ! AT (¯(1rT )) = 12(r ! 1rT )0V (r ! 1rT ) + RT (r) ! RT (1rT ): (5.8) Finally, due to (5.8) and the positive de¯niteness of V , for r = ^rT (the minimizing value of AT (¯(r))), for every 2 > 0, ´ > 0 there exists a T0 such that for T > T0 P [(^rT ! 1rT )0(^rT ! 1rT ) > 2] - ´: (5.9) This implies plim(^rT ! 1rT ) = 0, and we have shown the assertion. ¥ 6 Conclusions We establish consistency (Theorem 1) and asymptotic normality for linear (Theorem 4) and nonli- near regression (Theorem 5) quantiles under weak dependence, using a set of rather weak assump- tions. The results suggest, that there are even weaker, quantile speci¯c conditions for modeling weak dependence structures. In fact, it can be shown that these conditions are a special case of Doukhan's and Louhichi's (1999) notion of weak dependence, where the latter notion is implied by near epoch dependence as has been shown recently by Nze and Doukhan (2004). These issues remain to be investigated in detail, especially in the light of possible relevant econometric and statistical applications. In addition, we provide a detailed analysis of the connection between the rate of convergence of the quantile regression estimator and the degree of dependence measured by the size of the mixing coe±cient (Theorem 2). As a further result (Theorem 3) of this analysis, we have extended existing results on probability inequalities of Ivanov (1976, i.i.d. errors), Prakasa Rao (1984, mixing errors), and Hu (2004, general forms of dependence) for nonlinear least squares estimators, to the case of nonlinear regression estimators based on the L1-norm and weakly dependent errors. Appendix C: Proofs of Lemmas 1C-3C LEMMA 1C. Under assumptions (C1) and (C4), for every 2 > 0, E- 1T XT t=1 bt(¯)¸ ¸ 8>< >: f0±20 for ¯ 2 B>; T (2); f0 Ã 1 4T XT t=1 j htj!2 for ¯ 2 B-; T (2); where B>; T (2) = (¯¯¯¯¯1 4T XT t=1 j htj > ±0; jj¯ ! ¯0jj ¸ 2); B-; T (2) = (¯¯¯¯¯1 4T XT t=1 j htj - ±0; jj¯ ! ¯0jj ¸ 2): 15
PROOF. Due to assumption (C4) and taking into account the monotonicity of Ft(x), for all t and all ± - ±0, min £Ft(j xj) ! Ft(0); Ft(0) ! Ft(!j xj)¤ ¸ ( f0j xj for j xj - ±; f0± for j xj > ±: From the de¯nition of bt(¯) follows E£bt(¯)¤ = ( R ht R0 (ht ! z)dFt(z) for ht > 0; 0 ht(z ! ht)dFt(z) for ht - 0: (C.1) By limiting the integration domain in (C.1) to [0; ht=2] and [ht=2; 0], respectively, we obtain E- 1T XT t=1 bt(¯)¸ ¸ f0 T XT t; jhtj- 2± uht 2 
2 + f0 T XT t; jhtj>2± ¯¯¯¯ht 2 ¯¯¯¯± ¸ f0± 2T XT t; jhtj>2± jhtj ¸ f0± 2 Ã 1T XT t=1 jhtj ! 2±!: The assertion follows from setting ± = ±0 for ¯ 2 B>; T (2); and ± = 1 4T XT t=1 j htj for ¯ 2 B-; T (2) for T large enough. ¥ LEMMA 2C. Under assumptions (C1) and (C6), V arÃ 1T XT t=1 bt(¯)! - 8T XT!1 k=0 rk 1T XT t=1 h2t; (C.3) where r0 = 1. PROOF. Due to Lemma 3 of Doukhan (1994) and from the de¯nition of bt(¯), we get for s > t jCov (bs(¯); bt(¯))j - 4j hsjj htjrs; t and V ar (bt(¯)) - h2t; where rs; t is de¯ned in assumption (C6). Hence, according to assumption (C6) V arÃ 1T XT t=1 bt(¯)! - 1 T2 XT t=1 h2t+ 1 T2Xs6=t j hsjj htjrjs!tj - " 8T XT!1 k=1 rk + 1T # 1T XT t=1 h2t; (C.4) which proves the assertion. ¥ LEMMA 3C. Under assumptions (C1) and (C6), V arÃ 1T XT t=1 ct(¯)! - 8T XT!1 k=0 rk 1T XT t=1 h2t; where r0 = 1. 16
PROOF. For s 6= t, the covariance between cs(¯) = hs's and ct(¯) = ht't is given by hsht£#2P(us > 0; ut > 0)+(1!#)2P(us - 0; ut - 0)!#(1!#)P(us > 0; ut - 0)!#(1!#)P(us - 0; ut > 0)¤ = hshtf#2[1!Fs; t(1; 0)!Fs; t(0;1)+Fs; t(0; 0)]+(1!#)2Fs; t(0; 0)!#(1!#)[Fs; t(1; 0)! Fs; t(0; 0) + Fs; t(0;1) ! Fs; t(0; 0)]g. Thus, Cov[hs's; ht't] = hsht[Fs; t(0; 0) ! #2:] For 1 - s < t we de¯ne !s; t = Fs; t(0; 0) ! #2. Then, we have j!s; tj - rs; t, and analogously to the proof of Lemma 2C, and, according to the de¯nition of ct(¯), V ar " 1T XT t=1 ct(¯)# - 1 T2 XT t=1 h2t(# ! #2) + 2T XT!1 k=1 rk 1T XT t=1 h2t- 8T XT!1 k=0 rk 1T XT t=1 h2t: (C.5) ¥Appendix N: Proofs of Lemmas 1N-3N LEMMA 1N. For linear regression functions under assumptions (N1), (N5), and (C2), for ¯ = ¯0 + r=pT, lim T!1E " XT t=1 bt(¯(r))# = 12r0 - lim T!1 1T Z0TcTZT ¸ r = 12r0V r: The convergence is uniform for r 2 C ½ RK, where C is any compact set. PROOF. In the following, according to (5.4), we assume j htj - 2f . Then, by the de¯nitions of ht and bt(r), under assumption (N1) we get E£bt(¯(r))¤ = ( R ht 0 (ht ! z)ft(z)dz; if ht > 0; R 0 ht(z ! ht)ft(z)dz; if ht - 0; and for ht > 0, 12h2tinf 0-z-htft(z) - E£bt(¯(r))¤ - 12h2tsup 0-z-htft(z): The argumentation is analogous for the case ht < 0 and is left to the reader. From assumption (C2) follows PTt=1 h2t= T!1PTt=1 !xtr¢2 = O(1). Then, due to (5.5) and assumption (N1), lim T!1E- XT t=1 bt(¯(r))¸ = lim T!1 12 XT t=1 h2tft(0) (N.1) uniformly in r 2 C. Thus, due to assumption (N5), and the de¯nition of ht, from (N.1) follows the proof of the assertion. ¥ LEMMA 2N. For linear regression functions under assumptions (N1), (N2) and (N6), for ¯ = ¯0 + r=pT, lim T!1V arhXT t=1 bt(¯(r))i = 0: The convergence is uniform for r 2 C ½ RK, where C is any compact set. 17
PROOF. In the following, according to (5.4), we assume j htj < 2f . By de¯nition V ar "XT t=1 bt(¯(r))# = XT s;t=1fE[bs(¯(r))bt(¯(r))] ! E[bs(¯(r))]E[bt(¯(r))]g: Then, for s 6= t, hs > 0, ht > 0, and by the de¯nition of bt(¯) Cov!bs(¯(r)); bt(¯(r))¢ = Z 0-z-hs Z 0-w-ht (hs ! z)(ht ! w)[fs; t(z;w) ! fs(z)ft(w)]dzdw - 14h2sh2tsup 0-z-hs 0-w-ht j fs; t(z;w) ! fs(z)ft(w)j: By analogous argumentation for the remaining cases (hs > 0, ht < 0 etc.), we ¯nally get ¯¯¯¯¯¯¯ XT s;t=1 s6=t Cov[bs(¯(r)); bt(¯(r))]¯¯¯¯¯¯¯- 14 XT s;t=1 s6=t h2sh2tsup j zj-j hsj jwj-j htj j fs; t(z;w) ! fs(z)ft(w)j; (N.2) where the expression on the right hand side of (N.2) is bounded from above by 12 XT!1 k=1 sup t; t+k2N sup j zj-j htj jwj-j ht+kj j ft; t+k(z;w) ! ft(z)ft+k(w)j TX!k s=1 h4s: (N.3) Thus, according to (N2) we have ¯¯Cov£bs(¯(r)); bt(¯(r))¤¯¯- 1T XT!1 k=1 r0(kju) 1T TX!k t=1 (xtr)4: (N.4) Due to (N2) and (N6) the left hand side of (N.2) tends to zero for T to in¯nity. Analogously, for s = t we get XT t=1 V ar[bt(¯(r))] - XT t=1 "13j htj3 sup j zj-j htj j ft(z)j# - 1T XT t=1 j xtrj3 1 pT sup 1-s-T jfs(0)j (N.5) and due to (N1) and (N6) the left hand side of (N.5) tends to zero for T to in¯nity. Thus, the assertion follows. ¥ LEMMA 3N. For linear regression functions under assumptions (N3) and (N4) and for ¯ = ¯0+ r=pT, CT (¯(r)) = Pct(¯(r)) converges for T ! 1 in distribution to Cr, where the 1£K ran- dom vector C is normally distributed with mean zero and covariance matrix limT!1 T!1Z0T-TZT . PROOF. Obviously, from the de¯nitions of ct(¯) = ht't, we have E(ht't) = 0 and V ar(ht't) = #(1!#)h2t. In addition according to the considerations in the proof of Lemma 3C follows for s 6= t Cov[hs's; ht't] = hsht£Fs; t(0; 0) ! #2¤ = hsht!s;t: 18
Thus, V ar[CT (¯(r))] = 1T r0Z0T-TZTr: Then, due to assumptions (N3) and (N4), f'tg is r-mixing of size !q, where q > 2, and the proof of the assertion follows from the CLT given in PÄotscher and Prucha (1997, Theorem 10.2) and upon application of the Cram
er-Wold device. ¥ Literatur [1] Buchinsky, M. (1998) Recent advances in quantile regression. Journal of Human Resources, 33, 88-126. [2] Cai, Z. (2002) Regression quantiles for time series. Econometric Theory, 18, 169-192. [3] Castellana, J.V., & M.R. Leadbetter (1986) On smoothed probability density estimation for stationary processes. Stochastic Processes and their Applications, 21, 179-193. [4] Chen, L.-A., L.T. Tran, & L.-C. Lin (2004) Symmetric regression quantile and its application to robust estimation for the nonlinear regression model. Journal of Statistical Planning and Inference, 126, 423-440. [5] Chernozhukov, V., & L. Umantsev (2001) Conditional value-at-risk: Aspects of modeling and estimation. Empirical Economics, 26, 271-292. [6] De Gooijer, J.G., & D. Zerom (2003) On additive conditional quantiles with high-dimensional covariates. Journal of the American Statistical Association, 98, 135-146. [7] Doukhan, P. (1994): Mixing. Springer Verlag, New York. [8] Doukhan, P., & S. Louhichi (1999) A new weak dependence condition and applications to moment inequalities. Stochastic Processes and their Applications, 84, 313-342. [9] Engle, R.F., & S. Manganelli (2004) CAViaR: Conditional autoregressive Value at Risk by regression quantiles. Journal of Business and Economics Statistics, 22, 367-381. [10] Geyer, C.J. (1996) On the asymptotics of convex stochastic optimization. unpublished ma- nuscript. [11] Hu, S.H. (2004) Consistency for the least squares estimator in nonlinear regression model. Statistics and Probability Letters, 67, 183-192. [12] Ioannides, D.A. (2004) Fixed design regression quantiles for time series. Statistics and Proba- bility Letters, 68, 235-245. [13] Ivanov, A.V. (1976) An asymptotic expansion for the distribution of the least squares estimator of the non-linear regression parameter. Theory of Probability and its Applications, 21, 557-570. 19
[14] Jureckova, J., & B. Prochazka (1994) Regression quantiles and trimmed least squares estima- tors in nonlinear regression models. Journal of Nonparametric Statistics, 3, 201-222. [15] Knight, K. (1998) Limiting distributions for L1 regression estimators under general conditions. Annals of Statistics, 26, 755-770. [16] Knight, K. (1999) Asympotics for L1 of regression parameters under heteroscedasticity. Ca- nadian Journal of Statistics, 27, 497-507. [17] Koenker, R. (2005) Quantile regression. Econometric Society Monographs No. 38. Cambridge University Press. [18] Koenker, R., & G. Bassett (1978) Regression quantiles. Econometrica, 46, 33-50. [19] Koenker, R., & G. Bassett (1982) Robust tests for heteroscedasticity based on regression quantiles. Econometrica, 50, 43-61. [20] Koenker, R., & K.F. Hallock (2001) Quantile regression. Journal of Economic Perspectives, 15, 143-156. [21] Koenker, R., & B. Park (1994) An interior point algorithm for nonlinear quantile regression. Journal of Econometrics, 71, 265-283. [22] Mukherjee, K. (1999) Asymptotics of quantiles and rank scores in nonlinear time series. Jour- nal of Time Series Analysis, 20, 173-192. [23] Mukherjee, K. (2000) Linearization of randomly weighted empiricals under long range depen- dence with applications to nonlinear regression quantiles. Econometric Theory, 16, 301-323. [24] Nze, P.A., & P. Doukhan (2004) Weak dependence: models and applications to econometrics. Econometric Theory, 20, 169-192. [25] Oberhofer, W. (1982) The consistency of nonlinear regression minimizing the L1 norm. Annals of Statistics, 10, 316-319. [26] Oberhofer, W., & H. Haupt (2005) The asymptotic distribution of the unconditional quantile estimator under dependence. Statistics and Probability Letters, 73, 243-250. [27] Pfanzagl, J. (1969) On the measurability and consistency of minimum contrast estimates. Metrika, 14, 249-272. [28] Phillips, P.C.B. (1991) A shortcut to LAD estimator asymptotics. Econometric Theory, 7, 450-463. [29] PÄotscher, B.M., & I.R. Prucha (1997) Dynamic nonlinear econometric models: asymptotic theory. Springer-Verlag. 20
[30] Pollard, D. (1991) Asymptotics for least absolute deviation regression estimators. Econometric Theory, 7, 186-199. [31] Portnoy, S. (1991) Asymptotic behavior of regression quantiles in non-stationary, dependent cases. Journal of Multivariate Analysis, 38, 100-113. [32] Prakasa Rao, B.L.S. (1984) The rate of convergence of the least squares estimator in a nonli- near regression model with dependent errors. Journal of Multivariate Analysis, 14, 315-322. [33] Rogers, A.J. (2001) Least absolute deviations regression under nonstandard conditions. Eco- nometric Theory, 17, 820-852. [34] Wang, J. (1995) Asymptotic normality of L1-estimators in nonlinear regression. Journal of Multivariate Analysis, 54, 227-238. [35] Weiss, A.A. (1991) Estimating nonlinear dynamic models using least absolute error estimation. Econometric Theory, 7, 46-68. [36] Yu, K., Z. Lu, & J. Stander (2003) Quantile regression: applications and current research areas. The Statistician, 52, 331-350. [37] Zhao, Q. (2001) Asymptotic e±cient median regression in the presence of heteroscedasticity of unknown form. Econometric Theory, 17, 765-784. 21