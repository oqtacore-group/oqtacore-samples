 2. Классификация на основе байесовской теории решений
2.1. Байесовский подход
Байесовский подход исходит из статистической природы наблюдений. За основу берется предположение о существовании вероятностной меры на пространстве образов, которая либо известна, либо может быть оценена. Цель состоит в разработке такого классификатора, который будет правильно определять наиболее вероятный класс для пробного образа. Тогда задача состоит в определении "наиболее вероятного"  класса.
Задано  классов , а также ,  - вероятность того, что неизвестный образ, представляемый вектором признаков , принадлежит классу . 
 называется апостериорной вероятностью, поскольку задает распределение индекса класса после эксперимента ( a posteriori  - т.е. после того, как значение вектора признаков  x  было получено).
Рассмотрим случай двух классов  и . Естественно выбрать решающее правило таким образом: объект относим к тому классу, для которого апостериорная вероятность выше. Такое правило классификации по максимуму апостериорной вероятности называется Байесовским: если , то  классифицируется в , иначе в . Таким образом, для Байесовского решающего правила необходимо получить апостериорные вероятности, . Это можно сделать с помощью формулы Байеса. 
Формула Байеса, полученная Т. Байесом в 1763 году, позволяет вычислить апостериорные вероятности событий через априорные вероятности и функции правдоподобия.
Пусть  - полная группа несовместных событий. . , при . Тогда апостериорная вероятность имеет вид:
,
где  - априорная вероятность события ,  - условная вероятность события  при условии, что произошло событие . 
Рассмотрим получение апостериорной вероятности , зная  и . 
,   


Если  и  описываются плотностями  и , то
  .
	При проверке классификации сравнение  и  эквивалентно сравнению  и .  В случае, когда , считается, что мера множества  равна нулю.
Таким образом, задача сравнения по апостериорной вероятности  сводится к вычислению величин , , , . Будем считать, что у нас достаточно данных для определения вероятности принадлежности объекта каждому из классов, . Такие вероятности называются априорными вероятностями классов. А также будем считать, что известны функции распределения вектора признаков для каждого класса , . Они называются функциями правдоподобия  по отношению к . Если априорные вероятности и функции правдоподобия неизвестны, то их можно оценить методами математической статистики на множестве прецедентов. Например, , где  - число прецедентов из , .  - общее число прецедентов.  может быть приближено гистограммой  распределения вектора признаков для прецедентов из класса . 
Итак, Байесовский подход к статистическим задачам основывается на предположении о существовании некоторого распределения вероятностей для каждого параметра. Недостатком этого метода является необходимость постулирования как существования априорного распределения для неизвестного параметра, так и знание его формы.
2.2. Ошибка классификации
	Определение. Вероятность  называется ошибкой классификации, ,  - области решения ().
	Теорема. Байесовский классификатор является оптимальным по отношению к минимизации вероятности ошибки классификации.
	Доказательство. Рассмотрим ошибку классификации :
  

 
  
Учитывая формулу Байеса :  ,  получим : 
  
  
  
Таким образом, минимум достигается, когда   .  выбирается из остальных точек.
ч.т.д.
	Данная теорема была доказана для двух классов  и . Обобщим ее на  классов. 
Пусть вектор признаков  относится к классу , если , при , , . Соответственно необходимо доказать, что данное правило минимизирует вероятность ошибки классификации. Для доказательства следует воспользоваться формулой правильной классификации .
Доказательство. Воспользуемся формулой правильной классификации .





Учитывая формулу Байеса :  ,  получим : 




Таким образом, максимум достигается, когда . Аналогично для всех  максимум достигается, когда . 
ч.т.д.
3. Минимизация среднего риска
Вероятность ошибки классификации - не всегда лучший критерий проверки классификатора. В том случае, когда цена ошибок различного типа существенно различается, лучше использовать другой критерий качества классификации - минимум среднего риска. 
Рассмотрим задачу классификации по  классам. ,  - области предпочтения классов . Предположим, что вектор  из класса  лежит в , , т.е. классификация происходит с ошибкой. Свяжем с этой ошибкой штраф  называемый потерями в результате того, что объект из класса  был принят за объект из класса . Обозначим через  матрицу потерь.
	Определение. Выражение  называется риском при классификации объекта класса .
	Определение. Выражение  называется общим средним риском.
	Теперь мы можем поставить задачу о выборе классификатора, минимизирующего этот риск. Преобразуем выражение общего среднего риска:



	Из этого выражения видно, что риск минимален, когда каждый из интегралов в данной сумме минимален, т.е. , если , при , где , .
	Пример. Рассмотрим ситуацию радиолокационной разведки. На экране радара отражаются не только цели, но и помехи. Такой помехой может служить стая птиц, которую можно принять за небольшой самолет. В данном случае это двухклассовая задача. 
Рассмотрим матрицу штрафов:   , ,   .  - это штраф за принятие объекта из класса  за объект класса . Тогда


Пусть  относится у классу , если , т.е.


Т.к.  и , то

Стоящее в левой части неравенства отношение  называется отношением правдоподобия. Неравенство описывает условие предпочтения класса  классу .
	Пример. Рассмотрим двухклассовую задачу, в которой для единственного признака  известна плотность распределения:


Пусть, также, априорные вероятности .
	Задача - вычислить пороги для
минимальной вероятности ошибки
минимального риска при матрице риска   .
Решение задачи  a):
  
 
  
 
	 Решение задачи  b):
 




	Пример. Рассмотрим двухклассовую задачу с Гауссовскими плотностями распределения  и  и матрицей потерь .
	Задача - вычислить порог для проверки отношения правдоподобия.
	Решение. С учетом матрицы потерь отношение правдоподобия

запишется в виде

	Запишем плотности распределения 
  ;   
 
  
  
	Пример. Рассмотрим двух классовую задачу с матрицей потерь , , . Пусть  - вероятность ошибки, соответствующая вектору из класса  и  - вероятность ошибки, соответствующая вектору из класса . Задача - найти средний риск.
	Решение.



 
	 Пример. Доказать, что в задаче классификации по  классам, вероятность ошибки классификации ограничена: . 
Указание: показать, что .
4. Дискриминантные функции и поверхности решения
	Минимизация риска и вероятности ошибки эквивалентны разделению пространства признаков на  областей. Если области  и  смежные, то они разделены поверхностью решения в многомерном пространстве. Для случая минимизации вероятности ошибки поверхность решения задается уравнением:

В данном уравнении приходится оперировать с вероятностями. Иногда вместо вероятностей удобнее работать с функцией от вероятности:
,
где функция  монотонно возрастает.
	Определение. Функция  называется дискриминантной функцией.
	Таким образом, поверхность решения будет задаваться уравнением:
, , , .
	Для задачи классификации по вероятности ошибки или риску не всегда удается вычислить вероятности. В этом случае бывает более предпочтительно вычислить разделяющую поверхность на основе другой функции стоимости. Такие подходы дают решения, субоптимальные по отношению к Байесовской классификации.
5. Байесовский классификатор для нормального распределения
	Распределение Гаусса очень широко используется по причине вычислительного удобства и адекватности во многих случаях. Рассмотрим многомерную плотность нормального распределения :
, 
где  - математическое ожидание случайной величины  x  в классе ,
 - матрица ковариации размерности  для класса , ,
 - определитель матрицы ковариации. 
 Здесь  x ,  (i  - это вектора столбцы, а  xT ,  (iT   - вектора-строки.
5.1. Квадратичная поверхность решения. На основе этих данных необходимо построить байесовский классификатор. Рассмотрим логарифмическую дискриминантную функцию:
 



 
, где 
Эта функция представляет собой квадратичную форму. Следовательно, разделяющая поверхность  является гиперповерхностью второго порядка. Поэтому Байесовский классификатор является квадратичным классификатором.
Пример. Пусть , . Тогда .
 
Разделяющей поверхностью является коническое сечение.
	Пример. Пусть , , , , . Тогда , . Найдем поверхность решения.






Т.к. , то 


 
- эллипс центром в точке .
	Пример. Пусть , , , , . Тогда , . Найдем поверхность решения.
	Из предыдущего примера : 





Т.к. , то 
 - гипербола с центром в точке 
	5.2. Линейная поверхность решения. Условие остается тем же:   ,   .
	В предыдущем пункте мы получили квадратичную форму:
 

 , где   .
Пусть   , тогда
 


 , где  ; ; 
 
 	При    можно сравнивать только    и   . Таким образом, при    мы получили линейную поверхность решения.
	5.2.1. Линейная поверхность решения с диагональной матрицей ковариации. Рассмотрим случай, когда матрица  диагональная с одинаковыми элементами: . Тогда  имеет вид:   ;
  ,
где   ,    В данном случае под нормой понимается евклидова норма. Поверхностью решения является гиперплоскость, проходящая  через точку .
	Если   , то  - это середина вектора   .
	Т.к.   , то   . Следовательно, поверхность решения ортогональна   .
	Пример. Рассмотрим пример разделяющей поверхности решения для двухклассовой задачи с нормальным распределением. Поверхность решения лежит ближе к , если   . Соответственно, поверхность решения лежит ближе к , если   . Также, если  мало по отношению к , то положение поверхности решения не очень чувствительно к изменению    и   . Последнее справедливо, т.к. вектора лежат в малых окрестностях  и , поэтому изменение гиперплоскости их затрагивает не сильно. В центре изображен случай малого, а справа случай большого .
	5.2.2. Линейная поверхность решения с недиагональной матрицей ковариации. В этом случае уравнение :
 
будет иметь несколько иные параметры: 
   и   
В данном случае под нормой понимается так называемая  норма , которая имеет вид:   . Для такой нормы поверхность решения не ортогональна вектору   , Но она ортогональна его образу при преобразовании .
6. Классификаторы по минимуму расстояния
	Будем рассматривать равновероятные классы с одинаковой матрицей ковариации. Тогда     и выражение
  
примет вид
 
 (т.к. логарифм и константа сократятся).
	6.1. Классификатор по минимуму расстояния с диагональной матрицей ковариации. Рассмотрим случай, когда матрица  диагональная с одинаковыми элементами: . Тогда максимизация  влечет минимизацию евклидового расстояния, определяемое выражением   . В данном случае будет считаться, что объект относится к данному классу, если он близок в смысле евклидового расстояния.
	6.2. Классификатор по минимуму расстояния с недиагональной матрицей ковариации. В этом случае максимизация  влечет минимизацию расстояния Махалонобиса, определяемого выражением   .
	Т.к. матрица ковариации является симметрической, ее можно представить в виде:
  ,
где , а  - диагональная матрица с собственными значениями матрицы  на диагонали. Матрица  имеет столбцы, соответствующие собственным векторам матрицы :
 
 Таким образом, получаем линию равноудаленных точек :
 
 Пусть . Тогда координатами  являются , , т.е. проекции  на собственные вектора. Другими словами, мы получили координаты в новой системе, у которой оси определяются собственными векторами , . Тогда последнее уравнение преобразуется в уравнение эллипсоида в новой системе координат:
 
	 При  центр эллипса находится в точке , а главные оси лежат по собственным векторам и имеют длины  и  соответственно.
	Пример. Рассмотрим двумерный двухклассовый случай классификации двух нормально распределенных векторов с ковариационной матрицей  и средними значениями  и .
	Найдем  :


 	Классифицируем вектор . Для этого посчитаем расстояние Махалонобиса:
 







 Таким образом, хотя сама точка  по евклидову расстоянию ближе к точке , чем к точке , но по расстоянию Махалонобиса она ближе к .
	Теперь вычислим главные оси эллипса с центром в точке . Для этого найдем собственные значения :

 ,   .
Тогда собственные вектора (и направление главных осей эллипса) будут иметь вид:
, .
