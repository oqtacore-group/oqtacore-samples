
Оглавление Введ ение . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 I Введение в социально-экономическую статистику 15 1. Основные понятия 17 1.1. Краткаяисторическаясправка . . . . . . . . . . . . . . . . . . . . 17 1.2. Пред мет статистики . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.3. Экономические величины и статистические показатели . . . . . . . 20 1.4. Вероятностная природа экономических величин . . . . . . . . . . . 22 1.5. Проблемыизмерений . . . . . . . . . . . . . . . . . . . . . . . . . 24 1.6. Специфика экономических измерений . . . . . . . . . . . . . . . . 27 1.7. Адекватность экономических измерений . . . . . . . . . . . . . . . 29 1.8. Типывеличин, связимежд уними . . . . . . . . . . . . . . . . . . . 32 1.9. Статистические совокупностиигруппировки . . . . . . . . . . . . 36 1.10.Зад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2. Описательная статистика 48 2.1. Распределение частот количественного признака . . . . . . . . . . 48 2.2. Сред ниевеличины . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 2.3. Мед иана,мод а, квантили . . . . . . . . . . . . . . . . . . . . . . . 66 3
4 Оглавление 2.4. Моментыид ругиехарактеристикираспред еления . . . . . . . . . . 70 2.5. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 83 3. Индексный анализ 89 3.1. Основныепроблемы . . . . . . . . . . . . . . . . . . . . . . . . . . 89 3.2. Способыпостроения инд ексов . . . . . . . . . . . . . . . . . . . . 93 3.3. Факторныепред ставленияприростныхвеличин . . . . . . . . . . . 100 3.4. Случай, когда относительных факторов более одного . . . . . . . . 104 3.5. Инд ексывнепрерывномвремени . . . . . . . . . . . . . . . . . . . 106 3.6. Прикладные следствия из анализа индексов внепрерывномвремени . . . . . . . . . . . . . . . . . . . . . . . . 116 3.7. Факторные представления приростов в непрерывном времени . . . 123 3.8. Зад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 4. Введение в анализ связей 129 4.1. Совместные распределения частот количественных признаков . . . 129 4.2. Регрессионный анализ . . . . . . . . . . . . . . . . . . . . . . . . . 141 4.3. Дисперсионный анализ . . . . . . . . . . . . . . . . . . . . . . . . . 160 4.4. Анализвременныхряд ов . . . . . . . . . . . . . . . . . . . . . . . . 167 4.5. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 172 II Эконометрия-I: Регрессионный анализ 179 5. Случайные ошибки 182 5.1. Первичныеизмерения . . . . . . . . . . . . . . . . . . . . . . . . . 183 5.2. Производ ныеизмерения . . . . . . . . . . . . . . . . . . . . . . . . 192 5.3. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 194 6. Алгебра линейной регрессии 199 6.1. Линейная регрессия . . . . . . . . . . . . . . . . . . . . . . . . . . 199 6.2. Простая регрессия . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 6.3. Ортогональная регрессия . . . . . . . . . . . . . . . . . . . . . . . 205 6.4. Многообразие оценок регрессии . . . . . . . . . . . . . . . . . . . 210
Оглавление 5 6.5. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 216 7. Основная модель линейной регрессии 222 7.1. Различные формы уравнения регрессии . . . . . . . . . . . . . . . 222 7.2. Основные гипотезы,свойства оценок . . . . . . . . . . . . . . . . . 226 7.3. Независимые факторы: спецификация модели . . . . . . . . . . . . 234 7.4. Прогнозирование . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 7.5. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 247 8. Нарушение гипотез основной линейной модели 257 8.1. Обобщенный методнаименьш их квадратов (взвешенная регрессия) . . . . . . . . . . . . . . . . . . . . . . . . 257 8.2. Гетероскед астичность ошибок . . . . . . . . . . . . . . . . . . . . . 258 8.3. Автокорреляцияошибок . . . . . . . . . . . . . . . . . . . . . . . . 265 8.4. Ошибкиизмеренияфакторов . . . . . . . . . . . . . . . . . . . . . 270 8.5. Метод инструментальныхпеременных . . . . . . . . . . . . . . . . 273 8.6. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 278 9. Целочисленные переменные в регрессии 289 9.1. Фиктивныепеременные . . . . . . . . . . . . . . . . . . . . . . . . 289 9.2. Модели с биномиальной зависимой переменной . . . . . . . . . . . 295 9.2.1. Линейная модель вероятности, логит и пробит . . . . . . . . 296 9.2.2. Оценивание моделей с биномиальной зависимой переменной . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 9.2.3. Интерпретация результатов оценивания моделей с биномиальной зависимой переменной . . . . . . . . . . . 302 9.3. Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 304 10.Оценка параметров систем уравнений 314 10.1.Невзаимозависимые системы . . . . . . . . . . . . . . . . . . . . . 314 10.2.Взаимозависимые или одновременные уравнения . . . . . . . . . . 318 10.3.Оценкапараметровотд ельного уравнения . . . . . . . . . . . . . . 324 10.4.Оценка параметров системы идентифицированных уравнений . . . 331 10.5.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 334
6 Оглавление III Эконометрия-I: Анализ временных рядов 345 11.Основные понятия в анализе временных рядов 347 11.1.Введ ение . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347 11.2.Стационарность, автоковариациииавтокорреляции . . . . . . . . 351 11.3.Основные описательные статистики для временных рядов . . . . . 353 11.4.Использование линейной регрессии с детерминированными факторами для моделирования временного ряда . . . . . . . . . . . 356 11.4.1.Тренд ы . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356 11.4.2.Оценкалогистическойфункции . . . . . . . . . . . . . . . . 358 11.4.3. Сезонные колебания . . . . . . . . . . . . . . . . . . . . . . 359 11.4.4.Аномальныенаблюд ения . . . . . . . . . . . . . . . . . . . 360 11.5.Прогнозыпорегрессиисд етерминированнымифакторами . . . . . 361 11.6.Критерии,используемыеванализевременныхряд ов . . . . . . . . 365 11.6.1. Критерии, основанные на автокорреляционной функции . . 366 11.6.2.КритерийСпирмена . . . . . . . . . . . . . . . . . . . . . . 369 11.6.3.Сравнение сред них . . . . . . . . . . . . . . . . . . . . . . . 370 11.6.4.Постоянство д исперсии . . . . . . . . . . . . . . . . . . . . 372 11.7.Лаговыйоператор . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 11.8.Мод елирегрессиисраспред еленнымлагом . . . . . . . . . . . . . 375 11.9.Условныераспред еления . . . . . . . . . . . . . . . . . . . . . . . . 377 11.10.Оптимальное в среднеквадратическом смысле прогнозирование:общаятеория . . . . . . . . . . . . . . . . . . . 378 11.10.1. Условное математическое ожидание какоптимальныйпрогноз . . . . . . . . . . . . . . . . . . 378 11.10.2.Оптимальноелинейноепрогнозирование . . . . . . . . . . 380 11.10.3. Линейное прогнозирование стационарного временного ряда . . . . . . . . . . . . . . . . . . . . . . . 382 11.10.4. Прогнозирование по полной предыстории. РазложениеВольд а. . . . . . . . . . . . . . . . . . . . . . 385 11.11.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . 388 12.Сглаживание временного ряда 391 12.1.Метод скользящихсредних . . . . . . . . . . . . . . . . . . . . . . 391
Оглавление 7 12.2.Экспоненциальное сглаживание . . . . . . . . . . . . . . . . . . . 398 12.3.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 402 13.Спектральный и гармонический анализ 406 13.1.Ортогональность тригонометрических функций ипреобразованиеФурье . . . . . . . . . . . . . . . . . . . . . . . . 406 13.2.ТеоремаПарсеваля . . . . . . . . . . . . . . . . . . . . . . . . . . . 411 13.3.Спектральныйанализ . . . . . . . . . . . . . . . . . . . . . . . . . 412 13.4.Связь выборочного спектра с автоковариационной функцией . . . 414 13.5.Оценка функции спектральной плотности . . . . . . . . . . . . . . 417 13.6.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 422 14.Линейные стохастические модели ARIMA 426 14.1.Мод ельлинейногофильтра . . . . . . . . . . . . . . . . . . . . . . 426 14.2.Влияние линейной фильтрации на автоковариации и спектральную плотность . . . . . . . . . . . . . . . . . . . . . . . 429 14.3.Процессы авторегрессии . . . . . . . . . . . . . . . . . . . . . . . . 431 14.4.Процессыскользящегосред него . . . . . . . . . . . . . . . . . . . 452 14.5.Смешанные процессы авторегрессии-скользящего среднего . . 457 14.6. Модель ARIMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463 14.7.Оценивание, распознавание и диагностика мод елиБокса-Дженкинса . . . . . . . . . . . . . . . . . . . . . . 466 14.8.Прогнозированиепомод елиБокса-Дженкинса . . . . . . . . . . 475 14.9.Мод ели, сод ержащиестохастическийтренд . . . . . . . . . . . . . 485 14.10.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . 490 15.Динамические модели регрессии 500 15.1.Модель распределенного лага: общие характеристики испециальныеформыструктурлага . . . . . . . . . . . . . . . . . 500 15.2. Авторегрессионная модель с распределенным лагом . . . . . . . . 506 15.3. Модели частичного приспособления, адаптивных ожиданий иисправленияошибок . . . . . . . . . . . . . . . . . . . . . . . . . 509 15.4.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 513 16.Модели с авторегрессионной условной гетероскедастичностью 523 16.1.Модель ARCH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524
8 Оглавление 16.2. Модель GARCH . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527 16.3.Прогнозы и доверительные интервалы для модели GARCH . . . . . 531 16.4.Разновидности моделей ARCH . . . . . . . . . . . . . . . . . . . . 535 16.4.1. Функциональная форма динамики условной дисперсии . . . 535 16.4.2.Отказотнормальности . . . . . . . . . . . . . . . . . . . . 536 16.4.3. GARCH-M . . . . . . . . . . . . . . . . . . . . . . . . . . . 537 16.4.4.Стохастическаяволатильность . . . . . . . . . . . . . . . . 537 16.4.5. ARCH-процессысд олгосрочнойпамятью . . . . . . . . . . 538 16.4.6. Многомерные модели волатильности . . . . . . . . . . . . . 539 16.5.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 540 17.Интегрированные процессы, ложная регрессия и коинтеграция 546 17.1.Стационарностьиинтегрированныепроцессы . . . . . . . . . . . . 546 17.2.Разложение Бевериджа-Нельсона для процесса I(1) . . . . . . . 550 17.3.Ложная регрессия . . . . . . . . . . . . . . . . . . . . . . . . . . . 551 17.4.Проверкананаличиеед иничныхкорней . . . . . . . . . . . . . . . 553 17.5.Коинтеграция.Регрессиисинтегрированнымипеременными . . . . 558 17.6.Оценивание коинтеграционной регрессии: под ходЭнгла-Грейнд жера . . . . . . . . . . . . . . . . . . . . . . 560 17.7.Коинтеграцияиобщиетренд ы. . . . . . . . . . . . . . . . . . . . . 561 17.8.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 563 IV Эконометрия-II 567 18.Классические критерии проверки гипотез 569 18.1.Оценка параметров регрессии при линейных ограничениях . . . . . 569 18.2. Тест на существенность ограничения . . . . . . . . . . . . . . . . . 572 18.2.1.ТестГод фрея(наавтокорреляциюошибок) . . . . . . . . . 577 18.2.2. Тест RESET Рамсея (Ramsey RESET test) нафункциональнуюформу уравнения . . . . . . . . . . . . 578 18.2.3.ТестЧоу(Chow-test)напостоянствомод ели . . . . . . . . . 578 18.3.Метод максимального правдоподобия в эконометрии . . . . . . . . 582 18.3.1.Оценкимаксимальногоправд опод обия . . . . . . . . . . . . 582
Оглавление 9 18.3.2. Оценки максимального правдоподобия для модели линейной регрессии . . . . . . . . . . . . . . . . . . . . . . 584 18.3.3. Три классических теста для метода максимального правд опод обия . . . . . . . . . . . . . . . . . . . . . . . . . 587 18.3.4. Сопоставление классических тестов . . . . . . . . . . . . . 592 18.4.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 593 19.Байесовская регрессия 601 19.1.Оценка параметров байесовской регрессии . . . . . . . . . . . . . 603 19.2.Объед инение д вухвыборок . . . . . . . . . . . . . . . . . . . . . . 606 19.3.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 607 20.Дисперсионный анализ 611 20.1. Дисперсионный анализ без повторений . . . . . . . . . . . . . . . . 612 20.2. Дисперсионный анализ с повторениями . . . . . . . . . . . . . . . 618 20.3.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 621 21.Модели с качественными зависимыми переменными 625 21.1.Модель дискретного выбора для двух альтернатив . . . . . . . . . . 625 21.2.Оценивание модели с биномиальной зависимой переменной метод оммаксимальногоправд опод обия . . . . . . . . . . . . . . . 627 21.2.1. Регрессия с упорядоченной зависимой переменной . . . . . 630 21.2.2.Мультиномиальныйлогит . . . . . . . . . . . . . . . . . . . 631 21.2.3. Моделирование зависимости от посторонних альтернатив вмультиномиальныхмод елях . . . . . . . . . . 633 21.3.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 635 22.Эффективные оценки параметров модели ARMA 644 22.1. Оценки параметров модели AR(1) . . . . . . . . . . . . . . . . . . . 644 22.2. Оценка параметров модели MA(1) . . . . . . . . . . . . . . . . . . 647 22.3. Оценки параметров модели ARMA(p, q) . . . . . . . . . . . . . . . 651 22.4.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 652 23.Векторные авторегрессии 654 23.1. Векторная авторегрессия: формулировка и идентификация . . . . . 654 23.2.Стационарность векторной авторегрессии . . . . . . . . . . . . . . 658
10 Оглавление 23.3.Анализреакциинаимпульсы . . . . . . . . . . . . . . . . . . . . . 660 23.4. Прогнозирование с помощью векторной авторегрессии . . . . . . . 662 23.5.Причинность по Грейнджеру . . . . . . . . . . . . . . . . . . . . . . 665 23.6. Коинтеграция в векторной авторегрессии . . . . . . . . . . . . . . 666 23.7.Метод Йохансена . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668 23.8.Коинтеграцияиобщиетренд ы. . . . . . . . . . . . . . . . . . . . . 674 23.9.Упражненияизад ачи . . . . . . . . . . . . . . . . . . . . . . . . . . 676 A. Вспомогательные сведения из высшей математики 691 A.1. Матричная алгебра . . . . . . . . . . . . . . . . . . . . . . . . . . . 691 A.1.1. Опред еления . . . . . . . . . . . . . . . . . . . . . . . . . . 691 A.1.2. Свойстваматриц . . . . . . . . . . . . . . . . . . . . . . . . 694 A.2. Матричное д ифференцирование . . . . . . . . . . . . . . . . . . . . 700 A.2.1. Опред еления . . . . . . . . . . . . . . . . . . . . . . . . . . 700 A.2.2. Свойства . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701 A.3. Сведения из теории вероятностей и математической статистики . . 703 A.3.1. Характеристикислучайныхвеличин . . . . . . . . . . . . . 703 A.3.2. Распред еления,связанныеснормальным . . . . . . . . . . 709 A.3.3. Проверкагипотез . . . . . . . . . . . . . . . . . . . . . . . 712 A.4. Линейныеконечно-разностные уравнения . . . . . . . . . . . . . . 714 A.4.1. Решение однородного конечно-разностного уравнения . . . 714 A.5. Комплексные числа . . . . . . . . . . . . . . . . . . . . . . . . . . 715 B. Статистические таблицы 717
Введение Данный учебник написан на основе курсов, читаемых на экономическом фа-культете Новосибирского государственного университета. С середины 1980-х го-дов читался спецкурс, в котором излагались основы классической эконометрии, относящиеся к регрессионному анализу. В это же время в рамках "Общей теории статистики" достаточно развернуто начал изучаться материал анализа временных рядов.На базе этих дисциплин в начале 1990-х годов был создан единый курс "Эко-нометрия", который, постоянно совершенствуясь, читается как обязательный до настоящего времени. Во второй половине 1990-х годов был разработан и введен в практику преподавания обязательный курс "Эконометрия-II" для магистрантов. В конце 1990-х годов на экономическом факультете был восстановлен-на прин-ципиально новом уровне-курс "Общая теория статистики", дающий начальное представление об эмпирических исследованиях. Эконометрия (другой вариант термина в русском языке-эконометрика)- это инструментальная наука, позволяющая изучать количественные взаимосвязи экономических объектов и процессов с помощью математических и статистиче-ских методов и моделей. Дословно этот термин означает "экономическое изме-рение". Эконометрия связывает экономическую теорию, прикладные экономические исследования и практику. Благодаря эконометрии осуществляется обмен инфор-мацией между этими взаимодополняющими областями, происходит взаимное обо-гащение и взаимное развитие теории и практики. Эконометрия дает методы экономических измерений, а также методы оценки параметров моделей микро-и макроэкономики. При этом экономические теории выражаются в виде математических соотношений, а затем проверяются эмпириче-ски статистическими методами. Кроме того, эконометрия активно используется для прогнозирования экономических процессов и позволяет проводить планирование как в масштабах экономики в целом, так и на уровне отдельных предприятий. В экономике (как и в большинстве других научных дисциплин) не существует и не может существовать абсолютно точных утверждений. Любое эмпирическое утверждение имеет вероятностнуюприроду.Вчастности, экономические измерения содержат различного рода ошибки. Таким образом, в прикладных экономических исследованиях требуется использовать статистические методы. Методы эконометрии, позволяющие проводить эмпирическую проверку тео-ретических утверждений и моделей, выступают мощным инструментом развития самой экономической теории. С их помощью отвергаются одни теоретические кон-цепции и принимаются другие гипотезы. Теоретик, не привлекающийэмпирический материал для проверки своих гипотез и не использующий для этого эконометриче-
12 Введение ские методы, рискует оказаться в мире своих фантазий. Важно, что эконометриче-ские методы одновременно позволяют оценить ошибки измерений экономических величин и параметров моделей. Экономист, не владеющий методами эконометрии, не может эффективно ра-ботать аналитиком. Менеджер, не понимающий значение этих методов, обречен на принятие ошибочных решений. Эта книга адресована студентам, магистрантам и аспирантам экономических факультетов классических университетов. Она соответствует требованиям госу-дарственного образовательного стандарта по дисциплине "Эконометрика". Кроме того, издание будет полезно преподавателям эконометрии, исследователям, рабо-тающим в области прикладной экономики, специалистам по бизнес-планированию и финансовым аналитикам. Учебник предполагает определенный уровень базовой математической подго-товки читателя, владение им основами линейной алгебры, математического анали-за, теории вероятностей и математической статистики в объеме курсов для немате-матических специальностей вузов. Некоторые наиболее важные сведения из этих разделов высшей математики приведены в приложении к учебнику. Необходимость в создании учебника по эконометрии вызвана отсутствием оте-чественного варианта, который бы охватывал все основополагающие позиции со-временной эконометрической науки.Появившиеся в последние годы учебные изда-ния лишь частично покрывают программу курса, читаемого на экономическом фа-культете Новосибирского государственного университета. В частности, эти учеб-ники, посвященные в основном регрессионному анализу, не уделяют достаточного внимания теории временных рядов. При создании настоящего учебника авторы стремились систематизировать и объединить в рамках одного источника различ-ные разделы экономической статистики и эконометрии. Структура учебника примерно соответствует учебному плану экономическо-го факультета НГУ. Соответственно, он состоит из четырех частей: "Введение в социально-экономическую статистику", "Эконометрия-I: регрессионный ана-лиз","Эконометрия-I: анализ временных рядов","Эконометрия-II".Каждая часть покрывает семестровый курс. Соответствующие разделы читаются в качестве обя-зательной дисциплины во втором, четвертом и пятом семестрах бакалавриата и в первом семестре магистратуры. Полный курс эконометрии на ЭФ НГУ (вклю-чая "Введение в социально-экономическую статистику") рассчитан на 152 часа аудиторных занятий (45% лекций, 55% семинарских занятий). В первой части "Введение в социально-экономическую статистику" представ-лен материал, который более глубоко раскрывается в других частях учебника. В данной части рассмотрены особенности экономических величин, изложены про-
Введение 13 блемы экономических измерений, приводится обсуждение основных описательных статистик, рассмотрен индексный анализ, дан обзор основ анализа связей. Вторая часть посвящена классическому регрессионному анализу. Здесь рас-сматривается методнаименьш их квадратов в разных вариантах (включая орто-гональную регрессию), приведена основная модель линейной регрессии, излага-ются методы оценки параметров регрессии в случаях, когда нарушаются требо-вания основной модели (мультиколлинеарность, автокорреляция и гетероскеда-стичность, наличие ошибок в переменных), рассматриваются способы включения в регрессионное уравнение качественных переменных как для факторов (фик-тивные или псевдопеременные), так и для зависимой переменной (модели логит и пробит). Большое внимание уделяется применению основных критериев про-верки статистических гипотез в регрессионном анализе (тестированию): крите-рии Стьюдента, Фишера и Дарбина-Уотсона. Завершается вторая часть изло-жением некоторых проблем и методов оценки параметров одновременных систем уравнений. Особенность этого раздела учебника состоит в использовании матрич-ного подхода, позволяющего достичь общности и лаконичности изложения мате-риала. Третья часть посвящена анализу временных рядов. В ней рассматривается как классический инструментарий-выделение трендов, спектральный и гармониче-ский анализ, модели Бокса-Дженкинса, так и более современные методы-ди-намическая регрессия, ARCH-и GARCH-процессы, единичные корни и коин-теграция, которые недостаточно освещены в отечественной литературе. Классиче-ские методы излагаются исходя из стремления дать математическое обоснование множеству утверждений, которые в существующих учебниках просто констатиру-ются, что существенно затрудняет восприятие материала. Заключительная четвертая часть содержит разделы, в большинстве своем неиз-вестные русскоязычному читателю, однако без их знания практически невозмож-но проведение качественного эконометрического исследования. Это классические критерии проверки гипотез, метод максимального правдоподобия, дисперсионный анализ, основы байесовских методов, модели с качественными зависимыми пере-менными и более сложные разделы анализа временных рядов, в частности, вектор-ная авторегрессия и подход Йохансена к анализу коинтеграционных связей. Учебник содержит большое количество задач и упражнений. Кроме того, в каж-дой главе приведен список литературы, которая может быть использована в каче-стве дополнения к материалу главы. Подготовка ученика осуществлялась при финансовой и методической поддерж-ке программ TEMPUS (TACIS) JEP 08508-94: "Перестройка и совершенство-вание подготовки экономистов в НГУ" (1994-1997гг.) и "Совершенствование
14 Введение преподавания социально-экономических дисциплин в вузах" в рамках "Инноваци-онного проекта развития образования (2002-2004гг.)". В списке литературы после каждой главы звездочкой отмечены основные ис-точники. Авторский коллектив благодарит всех, кто помогал в работе над учебником. Особая благодарность Владимиру Шину, который осуществил верстку оригинал-макета в формате LATEX, а также Марине Шин, проделавшей большую работу по редактированию и согласованию различных частей учебника. Авторы будут признательны читателям за любые комментарии, сообщения о недочетах и опечатках в этом учебнике.
Часть I Введение в социально-экономическую статистику 15
Это пустая страница
Глава 1 Основные понятия 1.1. Краткая историческая справка Практика статистики зародилась давно, по-видимому, вместе со становлением элементов государственности. Не случайно во многих языках статистика и госу-дарство-однокоренные слова. Государству в лице представителей госаппарата всегда надо было хотя бы приблизительно знать численность населения страны, ее экономический потенциал,фактическое состояние дел в разных сферах обществен-ной жизни. Иначе нельзя сколько-нибудь эффективно собирать налоги, проводить крупные строительные работы, вести войны и т.д. Статистическая теория возникла как результат обобщения уже достаточно раз-витой статистической практики. Начало ее становления обычно связывают с рабо-тами английских политических арифметиков XVII века и, прежде всего, с именем Вильяма Петти (1623-1687). В XVIII веке статистическая теория развивалась под флагом государствоведения, зародившегося в Германии. Именно германские уче-ные в конце XVIII века стали использовать термины "статистика", "статистик", "статистический" в смысле, близком к современному. Хотя слово "статистик" на-много старше, его-в ином смысле-можно найти в произведениях Шекспира (начало XVII века). Эти слова, по-видимому, происходят более или менее косвенно от латинского слова "status" в том его смысле, который оно приобрело в средне-вековой латыни,-политическое состояние. Германские авторы, и вследза ними известный английский ученый сэр Джон Синклер, использовали термин "статистика" в смысле простого изложения заслу-
18 Глава 1. Основные понятия живающих внимание данных, характеризующих государство. Причем форма из-ложения являлась преимущественно словесно-текстовой. Для того времени такое понимание было достаточно естественным, т.к. достоверных числовых данных было еще очень мало. Лишь спустя несколько десятилетий с термином "статистика" ста-ли связывать изложение характеристик государства численным методом. Но д аже после образования в Англии Королевского статистического общества в 1834 году такое понимание статистики еще не стало обычным. Одним из ярких представителей статистики XIX века является бельгийский ученый Адольф Кетле (1796-1874)-создатель первого в мире центрального го-сударственного статистического учреждения в Бельгии, организатор и участник первых международных статистических конгрессов. Он установил, что многие мас-совые явления (рождаемость, смертность, преступность и т.д.) подчиняются опре-деленным закономерностям, и применил математические методы к их изучению. ВРоссии первым общегосударственным органом статистики явилосьСтатисти-ческое отделение Министерства полиции (1811), а затем-Министерства внут-ренних дел (1819). Его начальником был один из первых российских статистиков Герман К.Ф. (1767-1838)-автор первого русского оригинального труда по тео-рии статистики-книги "Всеобщая теория статистики" (1809). Корни современной теории статистики, прежде всего математической стати-стики, могут быть прослежены в работах Лапласа и Гаусса по теории ошибок на-блюдения, но начало расцвета самой науки относится только к последней четверти XIX века. Значительную роль на этом этапе сыграли работы Гальтона и Карла Пирсона. 1.2. Предмет статистики В статистике собираются и систематизируются факты, которые затем анали-зируются и обобщаются в "содержательных" общественных науках. Поэтому не всегда бывает просто провести границу между собственно статистикой и той об-щественной наукой, которую она "снабжает" информацией. И многие статистики склонны расширять рамки своей дисциплины за счет "содержательной" тематики. Это-их право, но в строгом смысле статистика является наукой о методах ко-личественного (численного) отражения фактов общественной жизни. Именно так понимается статистика в данной книге. Требуется пояснить, почему в данном определении статистики она связывается именно с науками об обществе. Любая наука, основываясь на наблюдениях за реальными фактами, стремится их систематизировать, обобщить, выявить закономерности, найти законы, постро-ить теоретические модели, объясняющие наблюдаемую действительность. Други-
1.2. Предмет статистики 19 ми словами, наука стремится выявить и затем количественно определить структуру причинно-следственных связей. Но события реальной жизни происходят под вли-янием многих причин, и простое пассивное наблюдение далеко не всегда дает возможность найти эти причины. Более того, такое наблюдение может привести к выводам,прямопротивоположнымдействительности."Неверь глазам своим"- фраза, резюмирующая многовековой опыт подобных наблюдений. Однако, в так называемых точных науках научились проводить наблюдения так, чтобы однозначно и, как правило, в количественной форме определять причинно-следственные связи. Такая организация наблюдения называется экспериментом. Ученые-физики, химики, биологи могут провести "натурный" эксперимент, на входе которого фиксируются одна-две величины и определяется в результате, на что и как они влияют "при прочих равных условиях". В точных науках ана-лизируются и обобщаются, как правило, наблюдения-результаты экспериментов, т.е. "рафинированные" экспериментальные данные.Прогресс в этих науках самым непосредственным образом связан с целенаправленным развитием возможностей экспериментирования, с развитием "синхрофазотронов". Возможности проведения управляемых экспериментов в общественной жизни крайне ограничены. Поэтому общественные науки вынуждены опираться на неэкс-периментальные данные, т.е. на результаты пассивных наблюдений, в потоке кото-рых трудно уловить, а тем более количественно определить причинно-следственные связи. И статистика как раз и занимается методами сбора и подготовки таких дан-ных к анализу, методами их первичного анализа, методами проверки теоретических гипотез на основе таких данных. Конечно, и во многих необщественных сферах знания остается большое поле для статистики. Метеоролог строит свои прогнозы, основываясь в конечном сче-те на статистических данных; возможности управляемого эксперимента все еще ограничены в биологии и т.д. Но главным объектом статистики все-таки является общественная жизнь. Статистикой называют не только науку о методах организации пассивного наблюдения, методах систематизации и первичного анализа таких наблюдений, но и сами массивы этих наблюдений. Статистика рождаемости и смертности, ста-тистика выпуска продукции и т.д.-это совокупности чисел, характеризующих количество рождений и смертей, объемы выпуска продукции и т.д. В этом смысле термин "статистика" эквивалентен термину "информация". Английским эквивалентом слова "статистика" в указанных смыслах является "statistics", т.е. слово во множественном числе. Это слово используется в ан-глийском языке и в единственном числе-"statistic", как определенное число, являющееся результатом некоторого статистического расчета. В этом смысле сло-во "статистика" используется и в русском языке: статистика Фишера, статистика
20 Глава 1. Основные понятия Стьюдента, статистика Дарбина-Уотсона-это определенные числа, полученные в результате достаточно сложных расчетов, по величине которых судят о разум-ности тех или иных статистических гипотез. Например, гипотезы о наличии связи между изучаемыми величинами. Термин "статистики" (во множественном числе), используемый также в русском языке, относится к совокупности таких чисел. 1.3. Экономические величины и статистические показатели Экономическая величина-есть некоторое количество определенного эконо-мического "качества". Обычно экономические величины обозначают буквами ла-тинского, реже-греческого алфавита. Когда говорят, что x -объем произ-водства или объем затрат, или объем капитала, то подразумевают, что эта буква обозначает некоторое количество произведенной продукции, осуществленных за-трат, наличного капитала. Обозначенные таким образом экономические величины используются обычно как переменные и параметры математических моделей эко-номики, в которых устанавливаются зависимости между экономическими величи-нами. Примером такой модели может являться межотраслевой баланс: X = AX + Y, где X и Y -вектор-столбцы объемов производства валовой и конечной продук-ции по отраслям; A -квадратная матрица коэффициентов материальных затрат. Или в покомпонентной записи: xi =j aijxj + yi для всех i, где aij -коэффициент затрат продукции i-го вида на производство единицы про-дукции j-го вида. Эта модель определяет зависимость между валовой, промежуточной и конеч-ной продукцией, а именно: валовая продукция является суммой промежуточной и конечной продукции. Кроме того, в этой модели определяется прямо пропорцио-нальная зависимость текущих материальных затрат от валового выпуска. Одна из возможных форм зависимости между выпуском продукции и использу-емыми ресурсами устанавливается производственной функцией Кобба-Дугласа: X = aCαLβ, где X -выпуск продукции; C -затраты основного капитала; L -затраты труда; a, α, β -параметры функции.
1.3. Экономические величины и статистические показатели 21 В этих записях экономические величины выступают, прежде всего, как некие теоретические понятия, то есть именно как "количества определенного эконо-мического качества". Вопрос об измеримости этих величин непосредственно не ставится, но предполагается, что этот вопрос в принципе разрешим. Статистическим (экономическим) показателем является операциональное определение экономической величины. Такое определение представляет собой ис-черпывающий перечень операций, которые необходимо провести, чтобы измерить данную величину. Этот перечень включает обычно и операции по сбору первичной информации-первичных наблюдений. Операциональные определения экономи-ческих величин-показателей, особенно обобщающегохарактера, таких как валовой внутренний или валовой национальный продукт, являются сложными методиками расчетов, далеко не все этапы которых безоговорочно однозначны. Эти операци-ональные определения являются предметом изучения и построения в социально-экономической статистике. Одной экономической величине могут соответствовать несколько статистиче-ских показателей, которые раскрывают разные стороны соответствующего теоре-тического понятия. Так, например, понятию "цена" на практике соответствуют: основные цены, цены производителей, оптовые и розничные цены, цены покупа-телей и т.д. Даже такая, казалось бы, простая величина, как население, имеет несколько "конкретизаций": население на момент времени или в среднем за пери-од, население постоянное или наличное. Статистическим показателем называют также конкретное число-результат измерения экономической величины, характеризующей определенный объект в определенный момент времени. Например, чистая прибыль такого-то предприя-тия в таком-то году составила столько-то миллионов рублей. В этом случае эконо-мическая величина "чистая прибыль" характеризует данное предприятие в данном году. С этой точки зрения понятно, почему в статистике экономические величи-ны в привязке к объекту и времени иногда называют признаками этого объек-та. В свою очередь статистический показатель-число называют статистическим наблюдением. Все множество величин-признаков или показателей-наблюдений можно обозначить следующим образом: X = {xtij}, где t -индекс времени, i -индекс объекта, j -индекс признака, то есть но-мер экономической величины в перечне всех экономических величин, которые могут характеризовать изучаемые объекты. Итак,экономическаявеличина-признак-теоретическое понятие, статисти-ческий показатель-определение обеспечивает практическую измеримость теоре-тической величины, статистический показатель-наблюдение-результат изме-рения величины-признака конкретного объекта в конкретный момент времени.
22 Глава 1. Основные понятия 1.4. Вероятностная природа экономических величин Статистическое исследование строится в предположении, что все экономиче-ские величины без исключения являются случайными с вполне определенными, часто неизвестными, законами распределения вероятности. Наблюдаемые значе-ния суть реализации соответствующих случайных величин, выборки из каких-то генеральных совокупностей. Такое отношение к экономическим величинам долгое время отрицалось в отечественной (советской) науке на том основании, что в соци-алистической экономике, которая сознательно и планомерно организуется, не мо-жет быть места случайной компоненте. В настоящее время такую позицию никто практически не занимает, но определенные сомнения в вероятностной природе экономических величин высказываются. Некоторые экономисты не склонны признавать вероятностный характер не-массовых, единичных и уникальных событий. На том основании, что такой немас-совый, нерегулярно повторяющийся характер имеет большинство экономических явлений, "отец" кибернетики Норберт Винер вообще отрицал возможность при-менения количественных методов в экономических и социальных науках. Многие ученые-статистики отрицают необходимость вероятностного подхода к изучению даже массовых явлений, если для них можно провести сплошное наблюдение и по-лучить в свое распоряжение-как они считают-полную генеральную совокуп-ность. Они работают в рамках особого раздела статистики, который называется анализом данных. Нельзя не видеть, что высказываемые сомнения в вероятностной природе эко-номических явлений имеют основания. Понятие вероятности, вероятностные под-ходы к анализу зарождались и развивались в естественных науках, амир физических величин очень сильно отличается от "материи" экономической. В физике, химии генеральные совокупности очень велики, многие из них, по-видимому, можно счи-тать бесконечными. Очень велики и исследуемые выборки, и, что чрезвычайно важно, их, как правило, можно неограниченно увеличивать в управляемом экс-перименте, воспроизводя нужные условия в специальных физических установках, в химической аппаратуре. В такой ситуации совершенно естественным кажется определение вероятности как предела относительной частоты появления нужного признака. Но и в физическом мире многие явления представляются единичными и уни-кальными, со всеми вытекающими отсюда трудностями для классического, "объ-ективистского", "частотного" понимания вероятности. Например, как может от-ветить на вопрос о том, какова вероятность жизни на Марсе, "объективист-частотник"? Если он относится кМарсу как к уникальному явлению, единственной в своем роде планете во вселенной, то в лучшем случае его ответ будет 0 или 1.
1.4. Вероятностная природа экономических величин 23 Если жизнь есть-1, если ее нет-0. Но, скорее всего, он просто отметит некор-ректность этого вопроса, поскольку для него вероятность-это характеристика совокупности, а не единичного явления. В экономике подобных нарушений классических условий появления вероят-ности-масса. Можно сказать, что вся экономика состоит из таких нарушений. Мир людей, если к нему относиться "сильно материалистично", без некоторой раскованности в мышлении, уникален и ограничен. Генеральные совокупности ко-нечны и малы, так что многие массивы данных можно интерпретировать как исчер-пывающие генеральные совокупности. Ряды наблюдений весьма коротки. И, что сильно отличает экономику от физики, невозможно проведение натурных экспери-ментов с воспроизводимыми условиями. В таком положении полезным и продуктивным, по крайней мере внешне, пред-ставляется подход субъективной вероятности. Субъективная вероятность-это мера доверия исследователя к утверждению, степень уверенности в его справед-ливости, наконец, мера готовности действовать в ситуации, связанной с риском. "Субъективист" может давать вероятности любым, даже уникальным событиям, включая их тем самым в строгий анализ. Основываясь на своих знаниях, опыте, интуиции, он может определить вероятность жизни на Марсе, вероятность вхож-дения России в число развитых стран, вероятность экологической катастрофы на планете или мировой войны к середине столетия. Конечно, его оценки индивиду-альны и субъективны, но если их несколько и даже много, то после своего согла-сования они, несомненно, приобретут элемент объективности. Полезно понимать, что и в таком случае подход к вероятности совершенно отличен от классического "объективистски-частотного". Направления субъективной и объективной вероятности развивались парал-лельно. Если формальное определение объективной вероятности дано впервые Пуассоном во второй четверти прошлого века (1837 год), то Бернулли еще в на-чале XVIII века (1713 год) предположил, что вероятность- это степень доверия, с которой человек относится к случайному событию, и что эта степень доверия за-висит от его знаний и у разных людей может быть различной. Во второй половине прошлого века Байес доказал известную теорему об условной вероятности и ин-терпретировал используемые в ней параметры вероятности как степени уверенно-сти. Эти идеи легли в основу современной теории принятия решений в условиях неопределенности и, вообще, подхода субъективной вероятности, который часто называется байесовским. Бурное развитие этого направления началось в XX веке в связи с усилением ин-тереса к наукам об обществе, к экономической науке. Следует назвать по крайней мере двух ученых, внесших фундаментальный вклад в становление теории субъек-тивной вероятности и связанных с ней теорий полезности- это Джон М. Кейнс и Фрэнк П. Рамсей. В СССР в 40-х годах прошлого столетия проходила дискуссия
24 Глава 1. Основные понятия о началах теории вероятностей. Представители субъективной школы потерпели поражение. Существует подход, объединяющий в определенном смысле идеи субъективной и объективной вероятности. Он основан на понимании многовариантности разви-тия общества вообще и экономики в частности. Имеется множество возможных состояний экономики и путей ее развития, наблюдаемые факты в полном своем объеме являются лишь выборкой из гипотетической генеральной совокупности, образованной этим множеством. В рамках такого подхода снимается ряд проти-воречий частотного понимания вероятностей в экономике. Так, например, вероят-ность вхождения России в число развитых стран к 2050 году есть относительная частота возможных вариантов развития страны, при которых "вхождение" состо-ялось к 2050 году, в общем числе возможных вариантов. Вопрос остается толь-ко в том, как можно найти эти варианты или хотя бы посчитать их количество, т.е. как можно практически работать с гипотетическими генеральными совокуп-ностями. Современная экономическая наука располагает соответствующим инструмен-тарием: это математическое моделирование. Всякая математическая модель пред-ставляет бесконечное пространство возможных состояний экономики, расчет по модели дает точку или траекторию в этом пространстве. Модель выступает инструментом проведения экономических экспериментов почти в таком же смыс-ле, как и в естественных науках. Конечно, главным при этом является вопрос об адекватности модели. Но все это-темы других курсов. По-видимому, "субъективист", хотя бы в некоторых ситуациях приписывая вероятности тем или иным событиям, пользуется неявно частотным подходом при-менительно к некоторым гипотетическим генеральным совокупностям. При этом конструировать эти гипотетические совокупности и работать с ними помогают ему его знания, опыт и интуиция. 1.5. Проблемы измерений Методы измерения развивались на протяжении всей истории человеческой ци-вилизации вместе с развитием математики и естественных наук. В прошлом веке математизация социальных и экономических наук дала новый импульс этим про-цессам. Проводилось серьезное переосмысление феномена измерений, осуществ-лялись продуктивные попытки разработать общие теории измерения.Шел интен-сивный поток литературы, посвященной этой проблематике.Следует назвать таких ученых, как Н.Р. Кэмпбел, один из родоначальников современных теорий измере-ния; С.С.Стивенс, одна из его книг, 2-х томная "Экспериментальная психология", в 1969 г. опубликована на русском языке; И.Пфанцагль, книга которого в соавтор-
1.5. Проблемы измерений 25 стве с двумя другими учеными "Теория измерений" вышла в нашей стране в 1971 г.; П. Суппес и Дж.Л. Зинес, их работа "Основы теории измерения" опубликована у нас в 1967 г. Существенный вкладв теорию экономических измерений внесен работой Дж. фон Неймана и О. Моргенштерна "Теория игр и экономическое по-ведение", вышедшей у нас в 1970 г. Характерно, что все эти исследователи, кроме Кэмпбела, разрабатывали проблематику нефизических измерений. Если взять "Большую Советскую Энциклопедию" или "Математическую Эн-циклопедию" более позднего издания, то можно узнать, что измерение-это про-цесс сопоставления измеряемого явления с единицей измерения. Такое определе-ние достаточно поверхностно, оно не раскрывает существа возникающих проблем. В настоящее время практически всеобщим признанием пользуется репрезен-тативная теория, в соответствии с которой измерение есть процесс присваива-ния числовых выражений объекту измерения для его репрезентации (представле-ния), т.е. для того, чтобы осмысленно выводить заключения о свойствах объекта. Это определение дано Кэмпбелом. Он делает акцент на целях измерения. Измере-ние осуществляется не ради самого измерения, а с тем, чтобы можно было извлечь пользу из его результатов. По Стивенсу, измерение-это приписывание чисел вещам в соответствии с определенными правилами. Он акцентирует внимание на измерительных опе-рациях. Теорию измерения, развиваемую им, можно было бы назвать операцио-нальной. Следует привести также определение формальной теории, которое вытекает из теории математических моделей А. Тарского. Измерить-значит установить однозначное (гомоморфное) отображение эмпирической реляционной структуры в числовуюреляционнуюструктуру. Реляционная структура-это множество объ-ектов вместе со всеми отношениями и операциями на нем. В соответствии с этим определением, если объекты находятся в реальной действительности (в эмпириче-ской реляционной структуре) в некоторых отношениях друг с другом (одинаковы, больше, меньше, лучше, хуже, являются суммой или разностью), то в этих же отношениях должны находиться числа, приписанные им в результате измерения (числовая реляционная структура). Это определение находится в русле репрезен-тативной теории. Множества чисел, в которых проводится измерение, образуют измерительные шкалы. В концептуальном отношении Стивенсом выделено 4 основных типа шкал. 1) Номинальная шкала, шкала наименований, шкала классификаций. Объ-ектам приписываются любые числа, которые играют роль простых имен и исполь-зуются с целью различения объектов и их классов. Примеры: номера футболистов, числовые коды различных классификаторов. Основное правило такого измерения: не приписывать одно число объектам разных классов и разные числа объектам
26 Глава 1. Основные понятия одного класса. В этой шкале вводится только два отношения: "равно" и "не рав-но". В ней измеряются объекты, которые пока научились или которые достаточно только различать. Понятно, что в данном случае речь идет об измерении в очень слабом смысле. Результаты измерения X в этой шкале всегда можно изменить, подвергнув их взаимнооднозначному преобразованию f . Говорят, что математиче-ская структура этой шкалы определяется преобразованием f , таким что f= 0. 2) Ординальная или порядковая, ранговая шкала. В этой шкале измеряются объекты, которые одинаковы или предпочтительнее друг друга в каком-то смысле. Принимаются во внимание только три отношения, в которых могут находиться чис-ла этой шкалы: "равно", "больше", "меньше".Математическая структура шкалы определяется монотонно возрастающим преобразованием f : f> 0. Пример та-кой шкалы дает теория порядковой полезности. 3) Интервальная шкала. Шкала используется для измерения объектов, отно-сительно которых можно говорить не только больше или меньше, но и на сколько больше или меньше. Т.е. в ней введено расстояние между объектами и, соответ-ственно, определены единицы измерения, но нет пока нуля, и бессмысленнен во-прос о том, во сколько раз больше или меньше.Математическая структура шкалы: f = aX + b, гд еa > 0 (a -коэффициент изменения единицы измерения, b - "сдвиг" нуля). В этой шкале измеряются некоторые физические величины, напри-мер, температура. Если ночью по Цельсию было 5 градусов тепла, а днем- 10, то можно сказать, что днем теплее на 5 градусов, но утверждение, что днем в 2 ра-за теплее, чем ночью, бессмысленно. В шкале Фаренгейта или Кельвина данное отношение совсем другое. 4) Шкала отношений. В ней, по сравнению с предыдущей шкалой, введен ноль (естественное начало шкалы) и определено отношение "во сколько раз больше или меньше". Математическая структура шкалы: f = aX (a -коэффициент изменения единицы измерения), a > 0. Это обычная шкала, в которой проводится большинство метрических измерений. Первые два типа шкал неметрические, они используются в нефизическом из-мерении (в социологии, психологии, иногда в экономике), которое в этом случае называется обычно шкалированием.Метрическими являются шкалы двух послед-них типов. Экономические величины измерены, как правило, в шкале отношений. Существуют различные виды измерений. С точки зрения дальнейшего изло-жения важно выделить два вида: прямые или первичные, которые в физических измерениях иногда называютфундаментальными, и косвенные или производные. Измерения 1-го вида сводятся к проведению эмпирических операций в непосред-ственном контакте с измеряемым объектом. Это-опрос, анкетирование, наблю-дение, счет, считывание чисел со шкал измерительных приборов. Измерения 2-го
1.6. Специфика экономических измерений 27 вида связаны с проведением вычислительных операций надпе рвично измеренными величинами. Таким образом, в измерении используются и эмпирические, и вычислительные операции. Некоторые теоретики измерения склонны минимизировать роль вы-числения и отделить собственно измерение, как преимущественно эмпирическую операцию, от вычислений. Однако грань между этими двумя понятиями достаточно расплывчата, особенно при экономических измерениях. 1.6. Специфика экономических измерений Специфические особенности экономических измеренийможно свести в 5 групп: 1) Измеряться могут только операционально определенные величины. В эко-номике разработка операциональных определений величин-это сложный и не-однозначный исследовательский процесс теоретического характера. Теоретики по-стоянно дискутируют на темы измерения общих итогов экономического развития, экономической эффективности, производительности общественного труда, эконо-мической динамики, инфляционных процессов, структурных сдвигов и т.д. Не вы-работано строгой и единой системы операциональных величин, однозначно пред-ставляющих эмпирическую экономическую систему. Одно из следствий такого по-ложения, как уже говорилось, заключается в том, что каждому теоретическому понятию, как правило, соответствует несколько операциональных величин, отра-жающих различные точки зрения и используемых с разными целями. Очень сильно различались системы статистического учета, сложившиеся в СССР и в мировой практике. В России к концу прошлого столетия в целом завер-шен переход на западные стандарты, но нельзя не видеть положительных моментов, имевшихся и в отечественной системе статистики. В мировой практике статистики к настоящему времени сложилась более или менее устойчивая, хотя и имеющая национальные особенности, система статистического отображения экономической действительности:Национальные счета на макроуровне, Бухгалтерский учет вфир-мах.Иэти вопросы не являются предметом активных дискуссий теоретиков.Но нет сомнений, что подвоз действием накапливаемых изменений в общественной жизни "взрывы" таких дискуссий ожидают нас впереди. Таким образом, экономические измерения, в отличие от многих физических, в очень большой степени обусловлены теоретическими моментами. 2) Специфику экономических измерений создают и те особенности экономики, которые обсуждались выше в связи с пониманием особенностей статистики как науки и вероятностной природой экономических явлений. Короткие ряды наблюде-ний и неэкспериментальный характер данных очень затрудняют процесс измерения и нередко ставят под сомнение научную значимость его результатов.
28 Глава 1. Основные понятия В процессе управляемого эксперимента можно изменить значение некото-рой величины и определить, на что и каким образом она влияет, т.к. остальные величины-факторы остаются неизменными.Неэкспериментальные данные исклю-чают возможность анализа "при прочих равных". В потоке наблюдений за "всеми сразу" величинами, как уже отмечалось, трудно уловить структуру взаимосвязей и измерить их интенсивность. Чисто эмпирически это, пожалуй, невозможно сде-лать. Это обстоятельство еще в бо´ льшей степени увеличивает нагрузку на теорию, "силу абстракции" исследователя. И оно не добавляет надежности результатам измерения. 3) В экономике не существует таких объектов и не изобретено таких "линеек", совмещение которых позволило бы путем считывания чисел со шкалы определить объем валового внутреннего продукта или темп инфляции. Экономические изме-рения почти всегда косвенные, производные. Экономические величины опреде-ляются путем расчета, исчисления, формула которого задается операциональным определением величины. Более того, первичные измерения, имеющие в физике фундаментальное значение, в экономике, как правило, экономического характе-ра не имеют. Это-счет, физические измерения веса, объема, д лины, первичная регистрация цен, тарифов и т.д. Экономический характер они приобретают лишь после своей свертки в экономические величины. 4) В естественных науках единицы измерения: килограмм, метр, джоуль, ватт и т.д.-четко и однозначно определены. Специфические единицы экономических измерений: цены, тарифы, ставки, единицы полезности-постоянно меняются. Важно даже не то, что они меняются во времени, а то, что их изменения зависят от объема и пропорций тех величин, которые они призваны измерять. Если в струк-туре производства или в потребительском наборе доля какого-то продукта умень-шается, то его цена или полезность, как правило, растет. И наоборот. Учет такого рода зависимостей и изменчивости единиц измерения-очень сложная проблема, совершенно неизвестная в физических измерениях. 5) В процессе измерения инструмент взаимодействует определенным образом с объектом измерения, вследствие чего положение этого объекта может изме-ниться, и результатом измерения окажется не та величина, которая имела место до самого акта измерения. Пример: если попытаться в темной комнате на ощупь определить положение бильярдного шара на столе, то он обязательно сдвинется с места. Эта проблема так или иначе возникает в любых измерениях, но только в экономических и, вообще, социальных измерениях она принимает угрожающие масштабы. Экономические величины складываются под воздействием определенной дея-тельности человека и каким-то характеризуют образом эту деятельность. Поэтому люди, как те, кто измеряет, так и те, чья деятельность измеряется, обязательно заинтересованы в результатах измерения. Взаимодействия в процессе измерения,
1.7. Адекватность экономических измерений 29 возникающие по этим причинам, могут приводить к огромным отличиям получае-мых значений измеряемых величин от их действительных значений. В физических измерениях влияние этого субъективного фактора практически отсутствует. 1.7. Адекватность экономических измерений Подад екватностью измерений обычно понимают степень соответствия изме-ренных значений действительным или истинным. Разность этих значений образует ошибку измерения. Теория ошибок, основанная на теории вероятностей и матема-тической статистике, изучается в следующей части книги. Здесь рассматривается значение учета ошибок экономических измерений, причины этих ошибок и приво-дятся некоторые примеры. Любые измерения, а экономические в особенности, содержат ошибки. Точ-ные величины суть не более чем теоретические абстракции. Это происходит хотя бы в силу случайного характера величин. Исследователи располагают выбороч-ными значениями величин и могут лишь приблизительно судить об их истинных значениях в генеральной совокупности. Измерения без указания ошибки доста-точно бессмысленны. Фразу: "Национальный доход равен 10 600 млрд. руб."- если она не содержит сведений о точности или не подразумевает таких знаний у читателя (например, судя по количеству приведенных значащих цифр, ошибка составляет ±50 млрд. руб.),-всегда можно продолжить: "или любой другой ве-личине". К сожалению, понимание этого элементарного факта в экономике пока еще не достигнуто. Например, можно встретить такие статистические публикации, в которых численность населения бывшего СССР дается с точностью до одно-го человека. Кстати, "точные" науки знают меру своей неточности, и результаты физических измерений обычно даются с указанием возможной ошибки. Ошибки обычно подразделяют на случайные и систематические.Для экономи-ки можно ввести еще один класс ошибок: тенденциозные. Случайные ошибки- предмет строгой теории (см. гл. 5), здесь внимание сосредоточено на систематиче-ских и тенденциозных ошибках. В чем причины этих ошибок экономических измерений? В предыдущем разделе приводились 5 особенностей экономических измере-ний. Каждая из них вносит в ошибку свою лепту, и немалую, сверх "обычной" ошибки физических измерений. 1) Ошибки теории.Операциональные определения экономических величин- продукт теории. И если теория неверна, то, как бы точно в физическом смысле не проводились измерения исходных ингредиентов, какими бы совершенными вы-числительными инструментами не пользовались, ошибка-возможно очень боль-шая-обязательно будет присутствовать в результатах измерения.
30 Глава 1. Основные понятия О величине этих ошибок в практике нашей статистики можно судить лишь косвенно. Если сравнивать показатели совокупного производства, которые ис-пользовались в СССР и используемые в мировой практике, то можно отметить две основные компоненты ошибки. Вмировой практике используются показатели типа конечной продукции, в советской статистике-типа валовой продукции, которые сильно искажаются повторным счетом и другими "накрутками", содержащимися в промежуточном продукте. И второе: в советской статистике расчет этих показа-телей проводился только по так называемой материальной сфере. Бо´ льшая часть продукта, создаваемого в нематериальной сфере, не попадала в итоги. 2) Ошибки инструмента, в данном случае-принятых статистических проце-дур расчета. Наибольшим дефектом в советской статистике страдали процедуры оценки динамики цен. Они скрывали реальные темпы инфляции. Пример. На практике применяется два метода расчета национального дохода или валового внутреннего продукта (ВВП): потребительский-для определения использованного национального дохода как суммы фактических объемов накопле-ния и непроизводственного потребления, и производственный-для расчета про-изведенного национального дохода как суммы чистой продукции (добавленной сто-имости) по отраслям производства. Эти показатели жестко связаны между собой: их разница равна величине потерь и сальдо экспорта-импорта. Такая зависимость выдерживалась в государственной статистике только в текущих ценах.Всопостави-мых ценах произведенный национальный доход устойчиво обгонял использованный ежегодно на несколько миллиардов рублей.Если начать отсчет с начала 70-х годов, то к концу 80-х разрыв между произведенным и используемым национальным дохо-дом достигал 15 последнего. Эти 100−150 млрд. руб. разрыва-одна из оценок ошибки расчета национального дохода в сопоставимых ценах. В настоящее время в государственной статистике возникла в некотором смыс-ле обратная проблема. ВВП, рассчитанный по производству, оказывается заметно меньшей величиной, чем рассчитанный по использованию. Причем разрыв также достигал в отдельных случаях 15 ВВП. Это происходит потому, что часть продук-ции производится в так называемой "теневой" экономике и не находит отражения в официальной статистике. Использование же продукции учитывается в более пол-ных объемах. Страдали и страдают несовершенством и другие статистические процедуры. Еще один пример-из области международных сопоставлений динамики итоговых показателей развития. Если известны темпы роста национального дохода, напри-мер, СССР и США, то можно легко установить, как менялось соотношение этих показателей и насколько успешно СССР "догонял" США. Независимо от этого в советской статистике проводились прямые сопоставления национальных доходов, показывающие, какуючасть национального доходаСШАсоставляет национальный доход СССР. Долгое время оставался незамеченным факт серьезного несоответ-
1.7. Адекватность экономических измерений 31 ствия результатов этих двух расчетов: по данным динамики национального дохода СССРдогонялСШАгораздобыстрее, чемпо даннымпрямых сопоставлений.Мож-но не сомневаться в том, что искажены были и те и другие данные, но динамика национального дохода была искажена в большей степени. 3) Тенденциозные ошибки. Являются следствием субъективного фактора в процессе измерения. Искажение и сокрытие информации-элемент рациональ-ной стратегии экономического поведения. Это общепризнанный факт, но в СССР, в силу значительной идеологической нагрузки на статистику, искажение инфор-мации, особенно итоговой, достигало удручающе больших размеров. По оценкам Г.И. Ханина, реально национальный доход за период с начала 1-й пятилетки (кон-ца 20-х годов прошлого столетия) до начала 80-х годов прошлого века вырос не в 90 раз, как по официальной статистике, а всего в 7-8 (что тоже, кстати, очень неплохо). В современной официальной статистике в России такие ошибки также имеют место. Но если во времена СССР совокупные объемы производства преувеличи-вались, то теперь они занижаются. Это-результат "бартеризации" экономики, выведения хозяйственной деятельности из-поднал огообложения. Косвенным под-тверждением этих фактов является то, что при резком сокращении общих (офи-циальных) объемов производства в последнем десятилетии прошлого века объемы потребления электроэнергии, топлива, тепла, объемы грузоперевозок уменьши-лись гораздо в меньшей степени. 4) Ошибки единиц измерения. Имеется серьезное отличие понимания точ-ности в физическом и экономическом измерении. Даже если измерения точны в физическом смысле, т.е. правильно взвешены и измерены первичные величины, использована бездефектная теория для свертки этих величин, ошибки в экономи-ческом смысле могут присутствовать и, как правило, присутствуют. Дело в том, что практически всегда искажены по сравнению со своими истинными значениями наблюдаемые экономические единицы измерения: цены, тарифы и т.д. Особенно велик масштаб этих деформаций был в централизованной экономике. Влияние их на результаты измерения и далее на процессы принятия решений в СССР было огромным. Это стало особенно очевидным в конце горбачевской "перестройки", когда разные республики и территории бывшего СССР начали выдвигать взаим-ные претензии, рассуждая на тему о том, кому, кто и сколько должен. Если взять Западную Сибирь, то по официальным данным на конец 80-х годов XX века ее сальдо вывоза-ввоза было хоть и положительно, но очень невелико. Расчеты же в равновесных ценах давали цифру плюс 15-20 млрд. руб., а в ценах мирового рынка-плюс 25-30. Доля ошибок такого рода была велика и в реформируемой России, когда це-новые пропорции были неустойчивы и быстро менялись, значительно рос общий
32 Глава 1. Основные понятия уровень цен.Сложной и не решаемой однозначно оказывается проблема "очистки" итоговых за годпок азателей от факторов инфляции. 1.8. Типы величин, связи между ними Экономические величины могут быть двух типов: экстенсивные, или объем-ные, и интенсивные, или относительные. Первые обладают единицами измере-ния, и их можно складывать, т.е. агрегирование проводится обычным сложени-ем; вторые не имеют единиц измерения, а могут обладать только определенной размерностью, и они не аддитивны, их агрегирование проводится путем расчета средневзвешенных величин. Экстенсивные величины, в свою очередь, могут иметь тип запаса или потока. Величины типа запаса регистрируются на конкретный момент времени и имеют элементарные единицы измерения: рубль, штука, тонна, метр и т.д. Примеры: ос-новные фонды, материальные запасы, население, трудовые ресурсы. Величины типа потока определяются только за конкретный период времени и имеют размер-ность "объем в единицу времени": рубль в год, штука в час и т.д. К этим величинам относятся выпуск продукции, потребление, затраты, инвестиции, доходы и т.д. Величины запаса и потока жестко связаны между собой: Sb[v] + Pi[vt]t = Se[v] + Po[vt]t, где Sb и Se -запасы на начало и конец периода (v -единица измерения), Pi и Po -потоки по увеличению и уменьшению запаса (t -период). Это соотношение лежит в основе большинства балансовых статистических таблиц. Например, в балансе движения основных фондов по полной стоимости Sb и Se -основные фонды на начало и конец года, Pi и Po -вводи выбытие основных фондов; в балансе (межотраслевом) производства и потребления продук-ции Sb и Se -материальные запасы на начало и конец года, Pi -производство и импорт продукции, Po -текущее потребление (производственное и непроиз-водственное), инвестиции и экспорт. Интенсивные величины являются отношениями экстенсивных или интенсив-ных величин. Они могут иметь разное содержание, разную размерность или быть безразмерными. Примеры интенсивных величин как отношений объемных величин: - в классе P/S: производительность труда, фондоотдача, коэффициенты рож-даемости и смертности населения; - в классе S/P: трудо-и фондоемкость производства;
1.8. Типы величин, связи между ними 33 - в классе S/S: фондовооруженность труда; - в классе P/P: материало-и капиталоемкость производства, коэффициенты перевода капитальных вложений во ввод основных фондов. Размерность этих величин определяется формулой их расчета. Интенсивные величины, получаемые отношением величин одного качества (экстенсивных или интенсивных), размерности не имеют. К ним относятся темпы роста и прироста, коэффициенты пространственного сравнения, показатели отраслевой и террито-риальной структуры. Такие безразмерные относительные величины могут даваться в процентах или промиллях (если a -относительная величина, то a - 100 o/o - ее выражение в процентах, a - 1000 o/oo -в промиллях). Если две величины y и x связаны друг с другом, то одним из показателей этой связи является их отношение: yx -средний коэффициент связи (например, трудо-, материало-, фондоемкость производства). Иногда пользуются приростным коэффициентом (например, капиталоемкость производства как приростной коэффициент фондоемкости): ΔyΔx, гд еΔy и Δx -приросты величин y и x за определенный период времени. Если величины y и x связаны гладкой непрерывной функцией, то непрерыв-ным (моментным) приростным коэффициентом является производная dydx. В этом же ряду находится так называемый коэффициент эластичности, пока-зывающий отношение относительных приростов: Δy y : Δx x = xΔy yΔx = Δy Δx - xy . Непрерывным (моментным) коэффициентом эластичности является показа-тель степени при степенной зависимости y от x : y = axα, т.к. dy dx = aαxα−1 = αyx, откуда α = dy dx - xy . При наличии такой зависимости y от x моментный коэффициент эластичности рассчитывается как ln(ya) ln x . Это-примеры относительных величин, имеющих размерность. Далее приво-дятся примеры безразмерных относительных величин. Пусть y =iyi. Например, y -совокупный объем производства на опреде-ленной территории, yi -объем производства (в ценностном выражении) в i-й отрасли; или y -общий объем производства какого-то продукта в совокупности
34 Глава 1. Основные понятия регионов, yi -объем производства продукта в i-м регионе. Тогда yiy -коэф-фициент структуры, отраслевой в первом случае, территориальной (региональной) во втором случае. Если yi и yj -значения некоторого признака (объемного или относитель-ного) двух объектов (i-го и j-го), например, двух отраслей или двух регионов, то yiyj -коэффициент сравнения, межотраслевого в первом случае, простран-ственного (межрегионального) во втором случае. Пусть yt -значение величины (объемной или относительной) в момент вре-мени t. Для измерения динамики этой величины используются следующие пока-затели: Δyt = yt+1 − yt (или Δyt+1 = yt+1 − yt)-абсолютный прирост, yt+1yt -темп роста, Δytyt = yt+1yt − 1-темп прироста. В случае, если динамика y задана гладкой непрерывной функцией y(t), то непрерывным темпом прироста в момент времени (моментным темпом при-роста) является d ln y(t) dt , поскольку d ln y dy = 1y , а непрерывным (моментным) аб-солютным приростом выступает dy(t) dt .Последнее следует пояснить (почему dy(t) dt выступает моментным абсолютным приростом в единицу времени). Пусть единич-ный периодв ремени [t, t+1] разбит на n равных подпериодов, и в каждом из них одинаков абсолютный прирост. Тогда абсолютный прирост в целом за единичный периодрав ен y t + 1n− y (t) 1n , и предел его при n→∞, по определению производной, как раз и равен dy(t) dt . Непрерывным (моментным) темпом роста является ed ln y(t)/dt (e -основа-ние натурального логарифма). Действительно, пусть опять же единичный период времени [t, t + 1] разбит на n равных подпериодов, и темпы роста во всех них одинаковые. Тогда темп роста за этот период (единицу времени) окажется равным ⎛⎝y t + 1ny (t) ⎞⎠n,
1.8. Типы величин, связи между ними 35 Таблица 1.1 За период (единичный) Моментный Темп дискретн. непрерывный Роста yt+1 yt expt+1 t d ln y (t) dtdt= y (t + 1) y (t) expd ln y (t) dt Прироста yt+1 yt − 1 t+1 t d ln y (t) dtdt= ln y (t + 1) y (t) d ln y (t) dt = 1y dy (t) dt и переходом к пределу при n→∞ будет получено искомое выражение для мо-ментного темпа роста. Проще найти предел не этой величины, а ее логарифма. То есть lim n→∞ ln y t + 1n− ln y (t) 1n . По определению производной, это есть d ln y (t) dt , т.е. моментный темп при-роста. Следовательно, как и было указано, моментным темпом роста является e в степени d ln y (t) dt . Непрерывный темп роста за периодот t до t + 1 определяется следующим образом: et+1 t d ln y(t) dtdt. В этом легко убедиться, если взять интеграл, стоящий в показателе: t+1 t d ln y (t) dtdt= lny tt+1 t = ln y (t + 1) y (t) , и подставить результат (его можно назвать непрерывным темпом прироста за еди-ничный периодвр емени) в исходное выражение непрерывного темпа роста за пе-риод: eln y (t + 1) y (t) = y (t + 1) y (t) .
36 Глава 1. Основные понятия Построенные относительные показатели динамики сведены в таблице 1.1. Относительные величины, с точки зрения их измерения, являются производ-ными, т.е. их размер определяется путем расчета. Такой характер относительные величины имеют и в других предметных науках. Но в экономике существуют интен-сивные величины особого типа, имеющие первичный или фундаментальный харак-тер. Это экономические единицы (измерения): цены продукции, тарифы за услуги, ставки заработной платы, ставки процента, дивиденды, а также особые управля-ющие параметры-нормативы, например, ставки налогов и дотаций. Эти величины имеют разнуюразмерность или безразмерны, но регистрируются они как величины запаса-на определенные моменты времени. 1.9. Статистические совокупности и группировки Статистической совокупностью, или просто совокупностью, называют множе-ство объектов, однородное в определенном смысле. Обычно предполагается, что признаки объектов, входящих в совокупность, измерены (информация имеется) или по крайней мере измеримы (информация может быть получена). Полное мно-жество величин-признаков или показателей-наблюдений было обозначено выше как {xtij}. Совокупность объектов-это его подмножество по i. Об однородности совокупности можно говорить в качественном и количествен-ном смысле. Пусть Ji -множество признаков, которые характеризуют i-й объект. Совокупность однородна качественно, если эти множества для всех входящих в нее объектов идентичны или практически идентичны. Такие совокупности обра-зуют, например, сообщества людей, каждого из которых характеризуют имя, дата и место рождения, пол, возраст, вес, цвет глаз, уровень образования, профессия, место проживания, доход и т.д. В то же время понятно, что, чем большие сооб-щества людей рассматриваются, тем менее однородны они в этом смысле. Так, например, совокупность, включающая европейцев и австралийских аборигенов, не вполне однородна, поскольку набор признаков для последних включает такие характеристики, которые бессмысленны для первых (например, умение бросать бумеранг), и наоборот. Совокупность промышленных предприятий качественно достаточно однородна. Но более однородны совокупности предприятий конкретных отраслей, поскольку каждая отрасль имеет свою специфику в наборе всех возможных признаков. Чемменьшеобщеепересечениемножеств Ji, темменее однородна в качествен-ном смысле совокупность i-х объектов. Объекты, общее пересечение множеств признаков которых мало, редко образуют совокупности. Так, достаточно бессмыс-
1.9. Статистические совокупности и группировки 37 ленна совокупность людей и промышленных предприятий, хотя все они имеют имя, дату и место "рождения", возраст. Допустимая степень неоднородности совокупности зависит, в конечном счете, от целей исследования. Если, например, изучаются различия средней продолжи-тельности жизни различных представителей животного мира, то в исследуемую совокупность включают и людей, и лошадей, и слонов, и мышей. Количественная однородность зависит от степени вариации значений призна-ков по совокупности. Чем выше эта вариация, тем менее однородна совокупность в этом смысле. В разных фрагментах количественно неоднородных совокупностей могут различаться параметры зависимостей между величинами-признаками. Такие совокупности иногда также называют качественно неоднородными. Для них невоз-можно построить единой количественной модели причинно-следственных связей. Так, например, люди с низким уровнем дохода увеличивают спрос на некоторые то-вары при снижении своего дохода (малоценные товары) или/и при росте цен на эти товары (товары "Гиффена"). Люди с высоким уровнем дохода реагируют на такие изменения обычным образом-снижают спрос. Однородные совокупности обычно имеют простое и естественное название: "люди" или "население", "промышленные предприятия". Выделяются эти сово-купности с целью изучения, соответственно, человеческого сообщества, промыш-ленности и т.д. Массив информации по совокупности часто называют матрицей наблюде-ний. Ее строкам соответствуют объекты и/или время, т.е. наблюдения, столб-цам-величины-признаки или переменные. Обозначают эту матрицу через X, ее элементы-через xij, гд е i -индекс наблюдения, j -индекс переменной-признака. В конкретном исследовании все множество признаков делится на 2 части: фак-торные признаки, или независимые факторы,-экзогенные величины и резуль-тирующие (результативные) признаки, илиизучаемые переменные,-энд оген-ные величины. Целью исследования обычно является определение зависимости ре-зультирующих признаков от факторных.При использовании развитых методов ана-лиза предполагается, что одни результирующие признаки могут зависеть не только от факторных, но и от других результирующих признаков. В случае, если факторных признаков несколько, используют методы регресси-онного анализа, если наблюдениями являются моменты времени, то применяются методы анализа временных рядов, если наблюдения даны и по временным момен-там, и по территориально распределенным объектам, то целесообразно применить методы анализа панельных данных. Если наблюдений слишком много и/или совокупность недостаточно однород-на, а также для изучения внутренней структуры совокупности или при применении
38 Глава 1. Основные понятия особых методов анализа связи, предварительно проводится группировка совокуп-ности. Группировка-деление совокупности на группы по некоторым признакам. Наиболее естественно проводится группировка по качественным признакам. Такие признаки измеряются обычно в шкале наименований или в порядковой шка-ле. Например, признак "пол": 1 -мужской, 2 -женский (или −1 и 1, 0 и 1, 1 и 0 и т.д.); "академическая группа": 1 -студент 1-й группы, 2 -студ ент 2-й группы и т.д. (это-примеры использования шкалы наименований); "обра-зование": 1 -отсутствует, 2 -начальное, 3 -среднее, 4 -высшее (номи-нальная шкала с элементами порядковой); оценка, полученная на экзамене: 1 - неудовлетворительно, 2 -удовлетворительно, 3 -хорошо, 4 -отлично (по-рядковая шкала с элементами интервальной). Качественный признак принимает определенное количество уровней (напри-мер: "пол"- 2 уровня, "образование"- 4 уровня), каждому из которых при-сваивается некоторое целое число. Перестановка строк матрицы наблюдений по возрастанию или убыванию (если шкала данного признака порядковая, то обыч-но-по возрастанию) чисел, стоящих в столбце данного фактора, приводит к груп-пировке совокупности по этому фактору. В результате строки матрицы, соответ-ствующие наблюдениям-объектам с одинаковым уровнем данного качественного фактора, оказываются "рядом" и образуют группу. Группировка по количественному (непрерывному или дискретному) признаку производится аналогичным образом, но после переизмерения этого признака в по-рядковой (или интервальной) шкале. Для этого проводятся следующие операции. Пусть xij, i = 1, . . . , N -значения j-го количественного признака в матри-це N наблюдений, по которому проводится группировка-деление совокупности на kj групп. Весь интервал значений этого признака [z0j, zkjj ], гд е z0j min i xij , а zkjj max i xij , делится на kj полуинтервалов [z0j, z1j ], (zij−1, j, zij j ], ij = 1, . . . , kj . Первый из них закрыт с обеих сторон, остальные закрыты справа и открыты слева. Количество и размеры полуинтервалов определяются целями ис-следования.Но существуют некоторые рекомендации. Количество полуинтервалов не должно быть слишком малым, иначе группировка окажется малоинформатив-ной. Их не должно быть и слишком много, так, чтобы большинство из них были не "пустыми", т.е. чтобы в них "попадали" хотя бы некоторые значения количе-ственного признака. Часто размеры полуинтервалов принимаются одинаковыми, но это не обязательно. Теперь j-й столбец матрицы наблюдений замещается столбцом рангов наблю-дений по j-му признаку (рангов j-го признака), которые находятся по следующему правилу: i-му наблюдению присваивается ранг ij, если xij принадлежит ij-му по-луинтервалу, т.е. если zij−1, j < xij zij j (если ij = 1, условие принадлежности имеет другую форму: z0j xij z1j ). Таким образом, если значение наблюдения
1.9. Статистические совокупности и группировки 39 попадает точно на границу двух полуинтервалов (что достаточно вероятно, напри-мер, при дискретном характере количественного признака), то в качестве его ранга принимается номер нижнего полуинтервала. В результате данный признак оказы-вается измеренным (пере-измеренным) в порядковой (ранговой) шкале с элемен-тами интервальной шкалы, или-при одинаковых размерах полуинтервалов- в интервальной шкале. В случае, если исходные значения данного признака потре-буются в дальнейшем анализе, столбец рангов не замещает столбец наблюдений за данным признаком, а добавляется в матрицу наблюдений. Сама группировка осуществляется также перестановкой строк матрицы на-блюдений по возрастанию ранга данного признака. В результате ij-ю группу об-разуют наблюдения-объекты, имеющие ij-й ранг, а группы в матрице наблюдений располагаются по возрастанию ранга от 1 до kj . Группы, полученные в результате группировки по одному признаку, могут быть разбиты на подгруппы по какому-нибудь другому признаку. Процесс деления со-вокупности на все более дробные подгруппы по 3-му, 4-му и т.д. признаку может быть продолжен нужное количество раз-в соответствии с целями конкретного исследования. Перестановка строк матрицы наблюдений при группировке по каж-дому последующему признаку осуществляется в пределах ранее выделенных групп. Некоторые пакеты прикладных программ (электронные таблицы, базы данных) имеют специальную операцию, называемую сортировкой.Эта операция перестав-ляет строки матрицы наблюдений по возрастанию (или убыванию) значений ранга (уровня) сначала 1-го, потом 2-го, 3-го и т.д. указанного для этой операции при-знака. В этом смысле термины группировка и сортировка эквивалентны. Признаки, по которым группируются объекты совокупности, называются груп-пирующими. Если таких признаков больше одного, группировка называется мно-жественной, в противном случае-простой. Пусть группирующими являются первые n признаков j = 1, . . . , n, и j-й при-знак может принимать kj уровней (может иметь ранги от 1 до kj ). По этим при-знакам совокупность в конечном итоге будет разбита на K групп, где K = nj=1 kj . Это-так называемые конечные или заключающиегруппы.Последовательность группирующих признаков определяется целями проводимого исследования, "важ-ностью" признаков. Чем ближе признак к концу общего списка группирующих признаков, тем более младшим он считается. Однако с формальной точки зрения последовательность этих признаков не важна, от нее не зависит характер группи-ровки, с ее изменением меняется лишь последовательность конечных групп в мат-рице наблюдений. Общее число полученных групп существенно больше количества конечных групп. Каждый j-й признак по отдельности разбивает совокупность на kj групп, вместе с признаком j-на kjkjгрупп, вместе с признаком j-на kjkjkj
40 Глава 1. Основные понятия групп и т.д. Поэтому, не сложно сообразить, общее число групп, включая саму совокупность, равно j(1 + kj). Действительно: j (1 + kj) = 1+k1 + k2 + - - - + k1k2 + k1k3 + - - - + k1k2k3 + - - - + k1k2 . . . kn, -слагаемые правой части показывают количества групп, выделяемых всеми воз-можными сочетаниями группирующих признаков. Конечные группы можно назвать также группами высшего, в данном случае n-го порядка, имея в виду, что они получены группировкой по всем n признакам. Любое подмножество группирующих признаков, включающее nэлементов, где 0 < n< n делит совокупность на "промежуточные" группы, которые можно на-звать группами порядка n. Каждая такая группа является результатом объедине-ния определенных групп более высокого, в частности, высшего порядка. Конкрет-ное подмножество группирующих признаков, состоящее из nэлементов, образует конкретный класс групп порядка n. Всего таких классов Cnn (это-число соче-таний из n по n, равное, как известно, n! n!(n − n)! ). Группой нулевого порядка является исходная совокупность. Общее число всех групп от нулевого до высшего порядка, как отмечено выше, равно j(1 + kj). Дальнейшее изложение материала о группировках будет иллюстрироваться примером, в котором при n = 2 первым группирующим признаком является "студенческая группа" с k1 = 4 (т.е. имеется 4 студенческие группы), вторым группирующим признаком-"пол" с k2 = 2, а при n = 3 добавляется третий группирующий признак-"оценка", полученная на экзамене, с k3 = 4. В этом примере (при n = 3) имеется 32 конечные группы (третьего порядка), образую-щие класс с именем (все элементы которого имеют имя) "студенты". Существуют 3 класса групп 2-го порядка (C23 = 3). Класс А1, образуемый подмножеством группирующих признаков (12), включает 8 групп с именем "юноши или девушки такой-то студенческой группы", А2-образуемый подмножеством (13), включа-ет 16 групп с именем "студенты такой-то группы, получившие такую-то оценку", и А3-образуемый подмножеством (23), включает 8 групп с именем "юноши или девушки, получившие такую-то оценку на экзамене". Классов групп первого порядка имеется также 3 (C13 = 3). Класс Б1, образуемый подмножеством (1), включающий 4 группы с именем "такая-то студенческая группа", Б2-подмно-жеством (2), включающий 2 группы с именем "юноши или девушки", и Б3-под-множеством (3), включающий 4 группыс именем"студенты, получившиетакую-то оценку на экзамене".
1.9. Статистические совокупности и группировки 41 Каждой конечной группе соответствует конкретное значение так называемо-го мультииндекса I порядка n (состоящего из n элементов), который имеет следующую структуру: i1i2 . . . in (I = i1i2 . . . in ). Для всех наблюдений конечной группы, имеющей такое значение мультииндекса, первый группирующий признак находится на уровне (имеет ранг) i1, второй группирующий признак-на уровне i2 и т.д., последний, n-й-на уровне in. Линейная последовательность (последо-вательность в списке) значений мультииндекса совпадает с последовательностью конечных групп в матрице наблюдений. На первом месте стоит значение I1, все элементы которого равны единице (конечная группа, для всех наблюдений которой все группирующие признаки находятся на первом уровне). Далее работает прави-ло: быстрее меняются элементы мультииндекса, соответствующие более младшим группирующим признакам. Так, в иллюстрационном примере при n = 2 последо-вательность значений мультииндекса такова: 11, 12, 21, 22, 31, 32, 41, 42. Последним значением мультииндекса является IK = k1k2 . . . kn. Поскольку по-следовательность значений мультииндекса однозначно определена, I I=I1 означает суммирование по всем значениям мультииндекса от I1 до I. В некоторых случаях мультииндексы групп называют кодами групп. После за-вершения группировки столбцы группирующих признаков часто исключаются из матрицы наблюдений, т.к. содержащаяся в них информация сохраняется в муль-тииндексах-кодах. Если из "полного" мультииндекса порядка n вычеркнуть некоторые элементы-признаки, то получается мультииндекс более низкого порядка n, который именует определенную группу порядка n. Операция вычеркивания проводится заменой в исходном мультииндексе вычеркиваемых элементов символом " ∗ " (иногда ис-пользуется символ точки или какой-нибудь другой). Это необходимо для того, чтобы сохранить информацию о том, какие именно признаки вычеркнуты из мультииндек-са. В иллюстративном примере группы класса А1 имеют мультииндекс со звездоч-кой на третьем месте, а класса Б2-на первом и третьем местах. Для того чтобы подчеркнуть принадлежность мультииндекса I к конечным группам, мультииндек-сы групп более низкого порядка можно обозначать I(∗). Теперь вводится еще один специальный мультииндекс J, который в "полном формате" (при порядке n ) представляет собой последовательность целых чисел от 1 до n и обозначается G. В этом мультииндексе J все элементы, которые заме-нены звездочкой в мультииндексе I(∗), также заменены на звездочку. Пусть J∗ - последовательность из n звездочек (все элементы заменены на " ∗ "). Для индек-сации групп можно использовать пару индексов I, J (в этом случае к I излишне приписывать (∗) ).Вэтом случае из этих мультииндексовможно в действительности вычеркнуть все звездочки, т.к. информация о вычеркнутых признаках сохраняет-ся в J. Так, например, группа "студенты второй группы, получившие "отлично"
42 Глава 1. Основные понятия на экзамене" именуется мультииндексом I(∗), равным 2∗4, или парой мультиин-дексов I, J - 24, 13. Второй способ удобен, когда речь идет о группах низких порядков. В данном изложении будет использоваться первый способ индексации. Группа I(∗) (с мультииндексом I(∗) ) является объединением конечных групп с такими значениями мультииндекса I, что: а) все те их элементы, которые соот-ветствуют элементам, не вычеркнутыми из I(∗), совпадают с ними; б) все элемен-ты, соответствующие вычеркнутым из I(∗) элементам, пробегают все свои зна-чения. Такую операцию объединения естественно обозначить I(∗). Так, например, группа 1∗4 является объединением групп 114 и 124, а группа 42∗ -объедине-нием групп 421, 422, 423 и 424. Если I(∗) = J∗, объединяются все конечные группы и образуется исходная совокупность, а сам I(∗), равный J∗, формально выступает мультииндексом всей совокупности. Через J обозначается класс групп, образованных подмножеством признаков, не замененных в J звездочками. Так, продолжая пример, А2 является классом 1∗3, а Б2-классом ∗2∗. Количество групп в J-классе KJ является произве-дением kj c такими j, которые не заменены звездочками в J ; такую операцию произведения естественно обозначить J. При J = G оно равно количеству ко-нечных групп K, а при J = J∗ принимается равным 1 (исходная совокупность- одна). Пусть NI -число наблюдений-объектов в конечной группе I. Тогда число на-блюдений в группе более низкого порядка I(∗), которое можно обозначить NI(∗), равно I(∗)NI , где операция I(∗) выполняется аналогично операции I(∗). Эти числа называются групповыми численностями, все они больше либо равны нулю, в слу-чае равенства нулюсоответствующая группа пуста.Если I(∗) = K∗, то NI(∗) = N. Каждому наблюдению-объекту можно также поставить в соответствие муль-тииндекс порядка n + 1, имеющий структуру IiI, гд е I мультииндекс конечной группы, к которой принадлежит данное наблюдение, а iI -номер данного на-блюдения в этой группе. Так, в иллюстрационном примере 3125-мультииндекс пятой девушки в списке девушек третьей группы, получивших на экзамене "удо-влетворительно".Исходный линейный индекс i наблюдения смультииндексом IiI равен I− I=I1 NI+ iI, гд е I− -значение мультииндекса конечной группы, предше-ствующее I в последовательности всех значений мультииндекса. Так, в примере значение мультииндекса 423 предшествует значению 424, а значение 314- значению 321. Мультииндекс, в котором (n + 1)-й элемент замещен звездочкой, обозначает все множество наблюдений группы. Так, 1∗3∗ мультииндекс списка всех студентов первой группы, получивших на экзамене "хорошо".
1.9. Статистические совокупности и группировки 43 Результаты группировки применяются для решения задач 3-х типов. 1) Используя информацию о групповых численностях, анализируют распреде-ление частот или эмпирических вероятностей признаков, теоретическим обоб-щением которых являются функции распределения вероятностей и плотности вероятностей случайных величин. Потому такие распределения частот иногда называют эмпирическими функциями распределения вероятностей и плотностей вероятностей признаков. Если группировка является множественной, то говорят о совместном распределении признаков (группирующих), которое может исполь-зоваться в анализе зависимостей между этими признаками. В таком случае груп-пирующие признаки делятся на факторные и результирующие. Так, в иллюстра-ционном примере можно изучать зависимость оценки, полученной на экзамене, от факторов "студенческая группа" и "пол". Приемы построения эмпирических распределений вероятностей и простейшие методы анализа связей с помощью сов-местных распределений изучаются в этой части книги. При решении задач этого типа группирующие признаки являются, как правило, количественными. 2) Все группирующие признаки выступают факторными, и исследуется их вли-яние на некоторые другие-результирующие признаки xj, j >n.В этом случае группирующие (факторные) признаки являются обычно качественными, и исполь-зуются методы дисперсионного анализа, элементарные сведения о котором дают-ся в главе 4 этой части (более основательно эти методы рассматриваются в III-й части книги). В иллюстрационном примере при n = 2 признак "оценка" не входит в число группирующих, и если взять его в качестве результирующего, то можно также исследовать влияние факторов "студенческая группа" и "пол" на оценку. В пункте 1) говорилось о других методах изучения этого влияния. 3) Анализируются зависимости между признаками внутри выделенных групп и/или между группами, т.е. внутригрупповые и/или межгрупповые связи. Во вто-ром случае в анализе используются средние значения признаков в группах. В обоих случаях факторные и результирующие признаки не входят во множество группи-рующих признаков. Методы регрессионного анализа, используемые для анализа связей, и методы проверки гипотез о существенности различий параметров свя-зей между различными группами изучаются во II-й и III-й частях книги. В главе 4 настоящей части даются общие сведения о некоторых из этих методов. Особенность рассмотренных методов группировки заключается в том, что деле-ние на группы всякий раз проводится по значениям строго одного признака. В одну группу попадают наблюдения-объекты с близкими (или-для качественных при-знаков-совпадающими) значениями признака. Каждый последующий признак лишь "дробит" ранее выделенные группы. Между тем, существуют методы выде-ления групп сразу по несколькимпризнакам.Притаких группировках используются
44 Глава 1. Основные понятия различные меры близости векторов. Наблюдения i и iпопадают в одну группу, если по выбранной мере близки вектора xij и xij , j = 1, . . . , n. Методы таких группировок используются в кластерном анализе (кластер-класс). Существуют и обратные задачи, когда новое наблюдение-объект надо отнести к какому-то из-вестному классу. Такие задачи решаются методами распознавания образов, они возникают, например, при машинном сканировании текстов или машинном вос-приятии человеческой речи. Признаки также образуют совокупности разной степени однородности, пони-маемой в этом случае только в качественном смысле. Как и в анализе совокупности объектов можно обозначить через Ij множество объектов, обладающих j-м при-знаком. Степень однородности совокупностей признаков тем выше, чем больше общее пресечение этих множеств для признаков, входящих в совокупность. Одно-родные совокупности признаков часто называют системами, акцентируя внимание на наличии связей между признаками совокупности. Совокупности признаков обычно также группируются. Особенностью их груп-пировок является то, что они имеют строго иерархический характер, т.е. после-довательность групп признаков разного порядка строго определена. Когда же речь идет о группировках наблюдений-объектов, то их иерархия (последовательность групп от низших порядков к высшим) условна, она всегда может измениться при изменении порядка группирующих признаков. Группы признаков обычно называют классами и подклассами или классами разного уровня (иерархии). На нулевом уровне иерархии признаков размещается имя всей совокупности признаков, например, "показатели развития промышленных предприятий". Далее следуют классы первого уровня с их именами, например, "материальные ресур-сы", "затраты", "результаты", "финансовые пассивы", "финансовые активы" и т.д. Эти классы детализируются на втором уровне: например, "материальные ресурсы" делятся на "основной капитал", "запасы готовой продукции", "произ-водственные запасы", "незавершенное производство". На третьем уровне иерар-хии "запасы готовой продукции", например, делятся по видам продукции. И так далее. Разные направления иерархии могут иметь разное количество уровней де-тализации (иерархии). Например, "материальные ресурсы" могут иметь 4 уровня, а "финансовые активы"-3. В исходной матрице наблюдений только признаки низшего уровня иерархии (классов высшего порядка) имеют числовые значения (после группировки признаков и обработки матрицы наблюдений могут быть вве-дены столбцы со значениями итоговых показателей по некоторым или всем классам и подклассам признаков). Сама группировка формально может быть проведена так же, как и группировка объектов (но с некоторыми отличиями). Разным классам одного уровня, образу-ющим один класс предыдущего уровня, присваиваются различные целые числа-ранги, т.е. классы "измеряются" в номинальной шкале. Как видно, "измерение"
1.10. Задачи 45 классов одного уровня зависит от результатов "измерения" классов предыдущего уровня, чего не было при группировке совокупностей объектов. Далее, в матрицу наблюдений вводятся строки "классы первого уровня", "классы второго уровня" и т.д. с рангами, присвоенными соответствующим классам, в столбцах признаков. И, наконец, осуществляется перестановка столбцов матрицы наблюдений по воз-растанию рангов сначала классов первого уровня, потом второго уровня и т.д. Ранги классов образуют мультииндексы или коды признаков. После завершения группировки введенные строки классов можно убрать. Обычно эти операции не проводятся, т.к. признаки группируются уже при со-ставлении матрицы наблюдений. Как исходныемассивы и матрицынаблюдений, так и результаты их группировок или других обработок могут изображаться в виде таблиц и графиков. Таблица- это визуализированный двухмерный массив с общим названием-титулом, назва-ниями строк и названиями столбцов. Первый столбец (столбцы), в котором разме-щены названия строк, называется подлежащим таблицы, первая строка (строки) с названиями столбцов-сказуемым таблицы. Подлежащее и сказуемое часто включают мультииндексы-коды соответствующих объектов или признаков. В титул обычно выносится общее имя совокупности элементов (объектов или признаков) сказуемого и/или подлежащего. Существует несколько вариантов таблиц для массивов типа {xtij}, имеющих 3 размерности: время t, объекты i и признаки j. Если в подлежащем-время, а в сказуемом-объекты, то в титул должно быть вынесено имя признака; если в подлежащем-объекты, в сказуемом-признаки, то в титуле должно быть указано время и т.д. Всего таких вариантов- 6. Если в табулируемой матрице не произведено группировок, то таблица явля-ется простой с простыми именами строк и столбцов. Если строки и/или столбцы сгруппированы, то их имена в таблице являются составными: кроме индивидуаль-ных имен строк и столбцов они включают и имена их групп и классов. В случае, когда столбцов таблицы не слишком много, информация может быть представлена (визуализирована) графиком. Ось абсцисс соответствует обычно подлежащему таблицы, а ось ординат-сказуемому. Сами значения показателей-признаков изображаются в виде различных графических образов, например, в виде "столбиков". Если в подлежащем размещенымоменты времени, график выражает траектории изменения показателей. 1.10. Задачи 1. Определить пункты, которые являются выпадающими из общего ряда. 1.1 а) отношений, б) порядковая, в) количественная, г) классификаций; 1.2 а) Пуассон, б) Рамсей, в) Бернулли, г) Байес;
46 Глава 1. Основные понятия 1.3 а) темпы роста, б) относительные, в) производные, г) первичные; 1.4 а) Кейнс, б) Байес, в) Синклер, г) Бернулли; 1.5 а) фондоемкость, б) материалоемкость, в) трудоемкость, г) срок окупа-емости инвестиций; 1.6 а) Стивенс, б) Кэмпбел, в) реляционная структура, г) Тарский; 1.7 а) капитал, б) население, в) инвестиции, г) внешний долг; 1.8 а) Пуассон, б) Рамсей, в) Бернулли, г) Байес; 1.9 а) Суппес, б) Стивенс, в) Пуассон, г) Пфанцагль; 1.10 а) величина-признак, б) величина-показатель, в) показатель-определе-ние, г) показатель-наблюдение; 1.11 а) Герман, б) Кетле, в) Моргенштерн, г) Синклер; 1.12 а) Тарский, б) операциональная, в) репрезентативная, г) Кэмпбел; 1.13 а) Зинес, б) Суппес, в) Моргенштерн, г) Петти; 1.14 а) статистика, б) statistics, в) информация, г) statistic; 1.15 а) наименований, б) интервальная, в) ординальная, г) шкалирование; 1.16 а) Суппес, б) интервальная, в) Стивенс, г) порядковая; 1.17 а) Бернулли, б) субъективная, в) Байес, г) объективная; 1.18 а) Пфанцагль, б) Зинес, в) Нейман, г) Кэмпбел; 1.19 а) управляемый эксперимент, б) пассивное наблюдение, в) статистика, г) операциональное определение; 1.20 а) Кетле, б) Кейнс, в) Петти, г) Герман; 1.21 а) производственные мощности, б) выпуск продукции, в) затраты, г) амортизационные отчисления; 1.22 а) Пуассон, б) Рамсей, в) Бернулли, г) Байес; 1.23 а) кластер, б) класс, в) группа, г) совокупность; 1.24 а) абсолютная, б) относительная, в) экстенсивная, г) интенсивная; 1.25 а) дискретный, б) непрерывный, в) моментный, г) интервальный; 1.26 а) подлежащее, б) предлог, в) сказуемое, г) таблица. 2. Какой тип-запаса или потока-имеют следующие величины: а) инвести-ции; б) население; в) основные фонды; г) активы? 3. К какому классу относятся и какую размерность имеют следующие интен-сивные величины: а) фондоемкость; б) материалоемкость; в) трудоемкость; г) фондоотдача?
1.10. Задачи 47 Таблица 1.2 Год Объем производства, млрд. руб. Абсолютный прирост, млрд. Темп роста (годовой) Темп прироста (годовой), % Абсолютное значение 1% прироста, млрд. 1 2 3 4 5 6 1992 127 1993 1.102 1994 7.1 1995 164.6 1996 1997 9.9 1.75 4. Пусть yt -значение величины в момент времени t. Запишите формулу моментного темпа прироста и непрерывного темпа роста. 5. Имеются данные об объеме производства в отрасли (табл. 1.2). Вычислить и вставить в таблицу недостающие показатели. 6. Была проведена группировка студентов НГУ по трем признакам: 1-й признак: место постоянного жительства (город; село); 2-й признак: средний балл в аттестате (выше 4.5; от 3.5 до 4.5; ниже 3.5); 3-й признак: средний балл за вступительные экзамены (выше 4.5; от 3.5 до 4.5; ниже 3.5). Определите: а) общее число групп и число групп высшего порядка; б) количество классов групп 1-го, 2-го и 3-го порядка; в) количество групп в классах 2, 13, 23; г) число конечных групп в каждой группе класса 2, 13, 23. д) Число элементов конечной группы 221 равно 5, в остальных конечных группах по 2 элемента. Каково значение линейного индекса второго элемента конечной группы 232? е) Сколько всего элементов в совокупности?
Глава 2 Описательная статистика Исходныймассив наблюденийможет достигать значительных размеров, и непо-средственно по его информации трудно делать какие-либо содержательные заклю-чения о свойствах изучаемых совокупностей. Задача описательной статистики- "сжать" исходный массив, представить его небольшим набором числовых характе-ристик, которые концентрированно выражают свойства изучаемых совокупностей. Граница между описательной статистикой, с одной стороны, и математической ста-тистикой, эконометрией, анализом данных, с другой стороны, достаточно расплыв-чата.Обычно в описательной статистике даются элементарные сведения, достаточ-ные для проведения начальных этапов экономико-статистического исследования, которые более углубленно и более строго рассматриваются в других научных дис-циплинах статистического ряда (в последующих разделах книги). 2.1. Распределение частот количественного признака Пусть имеются наблюдения xi, i = 1, . . . , N за некоторой непрерывной ко-личественной величиной-признаком, т.е. матрица наблюдений имеет размерность N × 1. Такую матрицу наблюдений обычно называют рядом наблюдений. В ста-тистике совокупность этих значений иногда называется также вариационным ря-дом. Пусть проведена группировка совокупности по этому признаку с выделением k групп. Всоответствии с обозначениями предыдущей главы мультииндексом груп-пы является I, равный i1, гд е i1 -индекс группы. В этом и ряде последующих
2.1. Распределение частот количественного признака 49 пунктов (при n = 1) в качестве индекса группы будет использоваться не i1, что-бы не путать его с линейным индексом i наблюдения, а l. Соответственно, zl , l = 0, 1, . . . , k -границы полуинтервалов, Nl -групповые численности, ко-торые в этом случае называют частотами признака. Следует иметь в виду, что x - случайная величина, но все z -детерминированы. Размеры полуинтервалов, Δl = zl − zl−1, обычно берут одинаковыми. При выборе размера полуинтервалов можно исполь-зовать одно из следующих правил: Δ = 3.5sN−1/3 (правило Скотта) или Δ = 2IQRN−1/3 (правило Фридмена-Диакониса), где s -среднеквадратическое отклонение, IQR = x0.75−x0.25 -межквартиль-ное расстояние (определение величин s, x0.25 и x0.75 дается ниже). В литературе также часто встречается правило Стёрджесса для количества групп: k = 1+log2 N ≈ 1 + 1.44 ln N, однако было показано, что оно некорректно, поэтому использовать его не реко-мендуется. В качестве значения признака на l-м полуинтервале можно принять среднее значение признака на этом полуинтервале: ¯xl = 1 Nlxl∗ (использовано введенное в предыдущей главе обозначение xl∗ всех наблюдений, попавших в l-ю группу). Однако, как правило, в качестве этого значения прини-мается середина полуинтервала: ¯xl = 12 (zl + zl−1) = zl−1 + Δl 2 , αl = Nl N , -относительные частоты признака или оценки вероятностей (эмпиричес-кие вероятности) попадания значений признака в l-й полуинтервал, то есть α1 = P (z0 x z1), αl = P (zl−1 < x zl), l = 2, . . . , k. fl = αl Δl (2.1) -плотности относительной частоты или оценки плотности вероятности.
50 Глава 2. Описательная статистика полигон гистограмма кумулята 1 f, F Рис. 2.1. Графическое изображение плотностей частоты Очевидно, что αl = 1, или flΔl = 1. (2.2) Далее: Fl = l l=1 αl, или Fl = l l=1 flΔl, (2.3) -накопленные относительные частоты или оценки вероятностей того, что значение признака не превысит zl, т.е. Fl = P (x zl). Крайние значения этих величин равны 0 и 1: F0 = 0, Fk = 1. Числа αl , fl, Fl (l = 1, . . . , k) характеризу-ют разные аспекты распределения частот количе-ственного признака. Понятно, что, если размеры полуинтервалов одинаковы, αl и fl различают-ся с точностью до общей нормировки и являются одинаковыми характеристиками распределения. Графическое изображение плотностей частоты называется гистограммой, а на-копленных частот-кумулятой. Поскольку плотности частот неизменны на каж-дом полуинтервале, гистограмма ступенчатая функция (точнее, график ступенча-той функции). Накопленные частоты линейно растут на каждом полуинтервале, поэтому кумулята-кусочно-линейная функция. Видэтих графиков приведен на рисунке 2.1. 1 fF Рис. 2.2 Еще один графический образ плотностей частоты называется полигоном. Этот график образован отрезками, соединяющими середи-ны ступенек гистограммы. При этом первый отрезок соединяет середину первой ступень-ки с точкой z0 оси абсцисс, последний от-резок-середину последней ступеньки с точ-кой zk. Теоретически можно представить ситуа-цию, когда N и k → ∞, при этом следует допустить, что z0 → −∞, а zk → +∞. В ре-зультате функции f(z) и F(z), графиками которых были гистограмма и кумулята, станут гладкими (рис. 2.2). В математической статистике их называют, соответ-ственно, функцией плотности распределения вероятности и функцией распре-деления вероятностей случайной величины (см. Приложение A.3.1).
2.1. Распределение частот количественного признака 51 Формулы (2.1-2.3) преобразуются, соответственно, в dF (z) dz = f (z) , +∞ −∞ f (z) dz = 1, F(z) = z −∞ f zdz. 0 Рис. 2.3 Обычно функции f и F записываются с аргументом, обозначенным символом слу-чайной величины: f(x) и F(x). При этом предполагается, что в такой записи x есть детерминированный "образ" соответствую-щей случайной величины (в математической статистике для этого часто используют со-ответствующие прописные символы: f(X) и F(X)). Такие функции являются теорети-ческими и выражают различные законы рас-пределения, к которым лишь приближаются эмпирические распределения. Наиболее распространенным в природе является так называемый закон нор-мального распределения, плотность которого в простейшем случае (при нулевом математическом ожидании и единичной дисперсии) описывается следующей функ-цией: f (x) = √12π e−x2 2 асимметрия идеальная правая левая идеальная Рис. 2.4 Ее график, часто называемый кривой Гаусса, изображен на рисунке 2.3. Наиболее вероятное значение величи-ны, имеющей такое распределение,-нуль. Распределение ее симметрично, и вероят-ность быстро падает по мере увеличения ее абсолютной величины. Обычно такое рас-пределение имеют случайные ошибки изме-рения (при разной дисперсии). Различают несколько типов распределе-ний признака (случайной величины). На рисунке 2.4 показаны асимметрич-ные или скошенные распределения: с пра-вой и левой асимметрией, идеальная правая и идеальная левая асимметрия.При правой (левой) асимметрии распределение скошено в сторону больших (меньших)
52 Глава 2. Описательная статистика значений. При идеальной правой (левой) асимметрии вероятность падает (увели-чивается) с ростом значения величины на всем интервале ее значений, наиболее вероятно ее минимальное (максимальное) значение. В данном случае идеальными названы распределения с предельной асимметрией. предельное островершинное остро-вершинное плоско-вершинное равномерное 1 Рис. 2.5 На рисунке 2.5 приведен вид высоко-или островершинных и низко-или плоско-вершинных распределений. В первом слу-чае основная часть значений признака со-средоточена в узкой центральной области распределения, во втором-центральная область распределения "размыта". Плос-ковершинное распределение в пределе пре-вращается в равномерное, плотность кото-рого одинакова на всем интервале значений. Предельным островершинным распределе-нием является вертикальный отрезок еди-ничной длины-распределение детермини-рованной величины. Распределения с одним пиком плотности вероятности называют унимодальны-ми. На рисунке 2.6 приведен пример бимодального распределения и предельного бимодального распределения, называемого U-образным. В общем случае распре-деление с несколькими пиками плотности называют полимодальным. бимодальное U-образное Рис. 2.6 В математической статистике множе-ство всех теоретически возможных значе-ний случайной величины x, характеризу-емое функциями f и F, называют гене-ральной совокупностью, а ряд наблюде-ний x1, . . . , xN -выборочной совокуп-ностью, или выборкой. Вообще говоря, гистограмму и кумуляту можно построить непосредственно по дан-ным ряда наблюдений без предварительной группировки. Если предположить для простоты, что все значения в ряде наблюдений различны, то k принимается равным N. В качестве границ полуинтервалов zi, i = 1, . . . , N − 1 принимаются полу-суммыдвух соседних значений в ряде наблюдений, упорядоченном по возрастанию (строго говоря, само упорядочение является операцией группировки в простейшем случае): zi = 12(xi + xi+1).
2.2. Средние величины 53 В качестве z0 и zN естественно принять, соответственно, 2x1 − z1 и 2xN − zN−1 , так что первое и последнее значение в ряде наблюдений оказы-ваются в точности на середине своих полуинтервалов. Относительные частоты для всех полуинтервалов одинаковы и равны 1N . Однако плотность частоты раз-личается: она тем выше, чем короче полуинтервал, т.е. чем плотнее наблюдения расположены на числовой оси. 2.2. Средние величины Средние величины, или просто средние, являются особым подклассом интен-сивных величин, т.к. рассчитываются как отношения других величин. Они выступа-ют наиболее общими характеристиками совокупности объектов. Каждая средняя рассчитывается по конкретному признаку, характеризующему объекты совокупно-сти, и является качественно такой же величиной, имеет те же единицы измерения или туже размерность (или она безразмерна), что и усредняемый признак. Характер средних по объемным и относительным величинам несколько различается. Ниже рассматриваются сначала средние объемные и на их примере-виды средних, затем-средние относительные величины. Пусть xi -некоторый объемный признак i-го объекта, 1, . . . , N, то есть количество объектов в совокупности равно N, как и прежд е, x =ixi, тогда расчет среднего по совокупности значения данного объемного признака, который обычно обозначается тем же символом, но без индекса объекта и с чертой над символом, осуществляется по следующей формуле: ¯x = 1N x = 1N i xi. Это-среднее арифметическое (среднеарифметическое) простое или сред-няя арифметическая (среднеарифметическая) простая. Оно является отноше-нием двух объемных величин: суммарного по совокупности признака и количества объектов в совокупности. Пусть теперь вся совокупность делится на k групп, Nl -количество объектов в l-й группе, N =lNl, значение признака внутри каждой группы не варьируется и равняется xl. Тогд а ¯x = 1 N l Nlxl =l αlxl, где αl = Nl N , αl = 1-вес l-й группы. Это-среднее арифметическое (среднеарифметическое)взвешенное(сред-неарифметическая взвешенная).
54 Глава 2. Описательная статистика К аналогичной формуле для средней по исходной совокупности можно придти и иначе. Пусть, как и сначала, признак варьирует по всем объектам совокупности, а ¯xl -среднеарифметическое простое по l-й группе. Очевидно, что x =Nl¯xl, и ¯x =αl¯xl. По такой же формуле производится расчет средней по данным эмпирического распределения частот признака (см. предыдущий пункт). В качестве ¯xl в таком случае принимают не среднее по l-й группе, а, как отмечалось выше, середину l-го полуинтервала. Предполагая, что все объекты совокупности имеют разные веса (вес i-го объ-екта равен αi), среднее по совокупности записывается как взвешенное: x =αixi. Это-более общая формула среднеарифметического: при равных весах, то есть в случае, если αi = 1N для всех i, она преобразуется в формулу средне-арифметического простого. Для нахождения средней величины типа запаса за некоторый период време-ни используется среднее арифметическое взвешенное, называемая средним хро-нологическим (или средней хронологической). Смысл этой величины поясняется рисунком 2.7. x x(t) A D t BE CF x_ Рис. 2.7 Среднюю хронологическую ¯x надо найти так, что-бы площадь ABCD подлинией динамики x(t) (BC), т.е. сумма значений показателя за период, равнялась площади прямоугольника AEFD подлинией средней EF (см. рис. 2.7). Другими словами, для расчета средней хронологи-ческой используется формула: ¯x = площадь ABCD длина AD . На практике в дискретном случае этот расчет можно провести следующим об-разом. Пусть x0, x1, . . . , xN -значения некоторой объемной величины типа запаса в моменты времени t0, t1, . . . , tN, и τi = ti − ti−1, i = 1, . . . , N, τ = τi (длина AD). Если предположить, что на каждом временном отрезке τi динамика показателя линейна, то его суммарное значение на этом отрезке рассчитывается как τi xi + xi−1 2 ,
2.2. Средние величины 55 и для общей средней хронологической справедливо соотношение: ¯x = 1 2τ Ni=1 τi (xi + xi−1). В выражении этой величины как среднеарифметической взвешенной веса име-ют следующие значения: α0 = τ1 2τ, αi = τi + τi+1 2τ , i = 1, . . . , N − 1, αN = τN 2τ . Их сумма равна единице. Если все временные отрезки τi одинаковы, то веса первого и последнего x в средней хронологической будут равняться 12N , а веса всех промежуточных "x-ов"- 1N . На практике чаще всего рассчитывают средние величины типа запаса за период времени (обычно за год) по данным на начало и конец этого периода (года). Т.е. ре-шается задача нахождения средней хронологической ¯x за некоторый период, для которого известно значение показателя на начало- x0 и конец периода- x1. Эта величина, чаще всего, находится как средневзвешенное арифметическое: ¯x = (1 − α) x0 + αx1, или ¯x = x0 + αΔ, или Δ = x1 − x0. Если динамика показателя равномерна (линейна), то α = 12; если более интенсивные сдвиги в величине показателя происходят в 1-й половине периода, то α > 12; в противном случае- α < 12. В советской статистике при расче-те, например, среднегодовых основных фондов α принимался в интервале от 0.3 до 0.4, поскольку в плановой экономике вводы и выбытия фондов обычно сдвига-ются к концу года-к моменту отчета по плану. Этот параметр иногда называют среднегодовым коэффициентом. При предположении, что на данном отрезке времени неизменным остается от-носительный прирост (моментный темп прироста), и динамика имеет экспоненци-альный характер, справедливы следующие выражения (как и прежде, τ -длина данного временного отрезка, Δ -прирост показателя за период): xt = x0 x1 x0tτ , при 0 t τ, ¯x = x0 τ τ 0 x1 x0tτ dt = x1 − x0 ln x1 − ln x0 = Δ ln 1 + Δx0 .
56 Глава 2. Описательная статистика Взнаменателе этого выражения для средней хронологической находится непре-рывный темп прироста за период(см. п. 1.8), т.е. средняя хронологическая опре-деляется делением абсолютного прироста на относительный прирост за пери-од. Это-особый вид средней, которую иногда и называют собственно хро-нологической. Чтобы лучше понять ее смысл, полезно найти ее предельное значение при Δ → 0. Для этого логарифм в знаменателе раскладывается в степенной ряд: ln1 + Δx0= Δx0 − Δx02 12 + Δx03 14 − Δx04 14 + - - -, затем сокращается Δ в числителе и знаменателе, и он (Δ) приравнивается нулю. Искомый предел равен x0. Таким образом, на бесконечно малых отрезках времени значение этой величины равно самому показателю, а на конечных отрезках-его среднему значению при предположении, что темп роста на этом отрезке остается неизменным. Возвращаясь к общему случаю N + 1 временной точки, среднюю хронологи-ческую при предположении неизменности темпа роста внутри каждого временного периода можно рассчитать следующим образом: ¯x = 1τ Ni=1 xi − xi−1 ln xi − ln xi−1 τi. Несложно убедиться в том, что в случае, если средние в единицу времени темпы роста xi xi−11 τi на всех временных отрезках одинаковы и равны среднему в едини-цу темпу роста за весь период xN x0 1τ , среднее хронологическое рассчитывается только по двум крайним значениям: ¯x = xN − x0 ln xN − ln x0 . Расчет средних хронологических величин типа запаса является необходимой операцией для приведения этих величин к форме, сопоставимой с величинами ти-па потока, имеющими другое качество. Так, например, производительность труда рассчитывается как отношение выпуска продукции за определенный период вре-мени к средней хронологической занятых в производстве за этот же период. Если величины типа запаса и потока имеют одно качество (потоки выражают измене-ние запасов за периодвр емени), то используются и показатели отношения потока к запасу на начало или конец периода (или наоборот). Так, например, отношение
2.2. Средние величины 57 выбывших в течение года основных фондов к основным фондам на начало года называется коэффициентом выбытия фондов, а отношение годового ввода фондов к фондам на конец года-коэффициентом обновления фондов. Среднеарифметическое является частным случаем так называемого средне-степенного или среднего степенного, которое рассчитывается по следующей фор-муле: ¯x = αixki1k . Следует обратить внимание, что эта величина существует не при всех k, если некоторые из xi отрицательны. Чтобы избежать непринципиальных уточнений, в дальнейшем предполагается, что все значения признака положительны. При k = 1 среднее степенное превращается в обычное среднеарифметическое, при k = 2 это-среднеквадратическое, используемое для оценки степени вари-ации признака по совокупности, при k = −1 -сред нее гармоническое, примеры использования которого приводятся при рассмотрении средних относительных ве-личин, при k = 0 -сред нее геометрическое. Последнее утверждение доказывается путем нахождения предела среднего степен-ного при k → 0. Для того чтобы сделать такой предельный переход, обе части формулы среднего степенного возводятся в степень k, затем ¯xk и все xkiпредстав-ляются разложением в степенные ряды: 1 + k ln ¯x 1! + (k ln ¯x)2 2! + - - - =αi(1 + k ln xi 1! + (k ln xi)2 2! + - - - ), далее в обеих частях полученного выражения сокращаются единицы (1 = αi) , и эти обе части делятся на k. Теперь при k = 0 получается следующее равенство: ln ¯x =αi ln xi, откуда ¯x = xαi i , что и требовалось доказать. Средние геометрические используются при построении некоторых специаль-ных индексов. Но это тема следующей главы. Простые примеры использования средней геометрической дает производственная функция. Пусть в производственной функции Кобба-Дугласа так называемая отдача на мас-штаб постоянна, т.е. сумма показателей степеней в выражении функции равна еди-нице, и при увеличении использования ресурсов в одинаковое количество раз выпуск продукции растет в такое же количество раз: X = aCαL1−α,
58 Глава 2. Описательная статистика или в более развернутой форме: X = (CaC)α(LaL)1−α, где aC -коэффициент фондоотдачи при нормальном соотношении между основ-ным капиталом и трудом, aL -коэффициент производительности труда при тех же нормальных условиях. Нормальное соотношение труда и капитала определяется сложившимся организа-ционно-технологическим уровнем производства. Это-фиксированная величина: sn = CL . Откуда aC = a (sn)α−1, aL = a (sn)α. Таким образом, в общем случае (при любых соотношениях ресурсов) выпуск про-дукции является средневзвешенной геометрической потенциального выпуска, ко-торый мог бы быть обеспечен основным капиталом при нормальном соотношении его с трудом (величины CaC), и потенциального выпуска, который обеспечивает-ся трудом при нормальном его соотношении с капиталом (LaL). Коэффициент a в исходной записи производственной функции равен aαCa1−α L , и он может называть-ся коэффициентом общей производительности ресурсов, поскольку является также среднегеометрической нормальной фондоотдачи и нормальной производительности труда. Более общая форма связи между выпуском и ресурсами дается производственной функцией с постоянной эластичностью замены ресурсов. В развернутом виде она записывается следующим образом: X = α (CaC)−ρ + (1 − α) (LaL)−ρ−1ρ . Это-пример использования среднего степенного при нецелочисленных значениях параметра степени, поскольку ρ (равный −k в общей формуле среднего степен-ного) может принимать любые значения на отрезке [−1,+∞] (при ρ → 0, в силу приведенного выше доказательства, производственная функция с постоянной эла-стичностью замены преобразуется к форме Кобба-Дугласа). От величины это-го параметра зависят возможности взаимного замещения ресурсов, допускаемые в данной модели производства. Чем выше его величина, тем более затруднено это замещение. Такое свойство производственной функции с постоянной эластичностью замены эк-вивалентно известному свойству среднего степенного: оно увеличивается с ростом k. Среднее степенное увеличивается с ростом k, в частности, по возрастанию средние степенные располагаются в следующем порядке: гармоническое, геомет-рическое, арифметическое, квадратическое. Это свойство иногда называют мажо-рантностью средних.
2.2. Средние величины 59 Пусть ¯x(k) -среднее степенное, пусть далее k2 > k1, и требуется доказать, что ¯x(k1) > ¯x(k2). Эти средние можно записать в следующем виде: ¯x (k1) = αi xk2 i k1 k2 1 k1 , ¯x (k2) = αixk2 i k1 k2 1 k1 , и ввести промежуточные обозначения (чтобы не загромождать изложение): yi = xk2 i , q = k1 k2 , f (y) = yq, v = d2f dy2 = q (q − 1) yq−2, a1 =αif(yi), a2 = f αiyi. В этих обозначениях утверждение, которое следует доказать, записывается следую-щим образом: a 1 k1 2 > a 1 k1 1 . Далее рассматривается три возможных случая: 1) k2 > k1 > 0, 2) k2 > 0 > k1, 3) 0 > k2 > k1. В первом случае q < 1, v < 0 , т.е. функция f вогнута (выпукла вверх) и a2 > a1 по определению такой функции. После возведения обеих частей этого неравенства в положительную степень 1k1 знак его сохраняется, что и завершает доказатель-ство в этом случае. Во втором и третьем случаях v > 0, и функция f выпукла (выпукла вниз). Поэто-му a2 < a1, и после возведения обеих частей этого неравенства в отрицательную степень 1k1 оно меняет знак, приобретая тот, который нужно для завершения доказательства. Свойство мажорантности средних выражается и в том, что предельные зна-чения среднего степенного при k = ±∞ равны, соответственно, максимальному и минимальному значению признака в выборке.
60 Глава 2. Описательная статистика Для доказательства этого факта в выражении среднего выносится за скобки x1 : ¯x = x1 α1 + N i=2 αi xi x1k1k . Если xi упорядочены по возрастанию и x1 = minxi, то xix1 1, и при k →−∞ выражение в скобках стремится к ki=1 αi, гд е k-число объектов, для которых усредняемый признак минимален (если минимум единственный, то k= 1), т.е. ко-нечно. Это выражение возводится в степень 1k , которая стремится к нулю при k → −∞. Следовательно, среднее степенное при k → −∞ равно минимальному значению усредняемых признаков. Предположив теперь, что xi упорядочены по убыванию, аналогичным образоммож-но доказать, что среднее степенное при k → +∞ равно максимальному значению признака по совокупности. Существует наиболее общая запись средневзвешенного: ¯x = f−1 αif(xi). (2.4) Если f -степенная функция xk, то речь идет о средней степенной, если f -логарифмическая функция ln x, то это-средняя логарифмическая, которая является частным случаем средней степенной при k = 0, если f -показательная функция ax, то это-средняя показательная и т.д. Особенностью средних относительных величин является то, что они, как пра-вило, рассчитываются как средние взвешенные. Пусть i-й объект, i = 1, . . . , N характеризуется зависимыми друг от друга объемными величинами yi и xi. Показателем этой зависимости является отно-сительная величина ai = yixi . Это может быть производительность, фондово-оруженность труда, рентабельность и т.д. Понятно, что средняя по совокупности объектов относительная величина a (знак черты надсимвол ом, обозначающим среднее относительное, часто опускается) рассчитывается по следующей формуле: a = yi xi , которая легко преобразуется в формулу средней взвешенной: a =αxiai, где αxi= xi xi , или a = 1 αyi ai , где αyi = yi yi .
2.2. Средние величины 61 Таким образом, если веса рассчитываются по структуре объемных величин, стоящих в знаменателе, то средняя относительная является средней взвешенной арифметической, если эти веса рассчитываются по объемнымвеличинам, стоящим в числителе, то она является средней взвешенной гармонической. Формально можно рассчитать простую среднюю (например, арифметическую) a= 1 N ai, но содержательного смысла она иметь небудет.Это становится понятным, как толь-ко осуществляется попытка привести слагаемые yixi к общему знаменателю. Тем не менее, такая средняя также может использоваться в анализе. Например, ее иногда полезно сравнить с фактической средней a для выявления некоторых ха-рактеристик асимметрии распределения признака по совокупности. Если a > a, то в совокупности преобладают объекты с повышенной величиной ai, и, по-видимому, имеет место правая асимметрия, в противном случае в совокупности больший удельный вес занимают объекты с пониженной ai (левая асимметрия). Однако в статистике имеются более четкие критерии асимметрии распределения. Особое место среди средних относительных занимают средние темпы роста. Темпы роста величин типа потока выражают отношение потока за единицу (пе-риод) времени к потоку за некоторую предыдущую единицу (предыдущий период) времени. Темпы роста величин типа запаса показывают отношение запаса в момент времени к запасу в некоторый предыдущий момент времени. Такойже смысл имеют и средние темпы роста. Средние за периодтемпы роста рассчитываются обычно как средние геометрические. Пусть x0, x1, . . . , xN значения некоторой объемной величины в моменты времени t0, t1, . . . , tN, если эта величина типа запаса, или в последнюю еди-ницу времени, соответственно, 0-го, 1-го и т.д., N-го периода времени, если речь идет о величине типа потока (t0 -последняя единица времени 0-го периода, [ti−1 + 1, ti] - i-й период). Как и прежде, τi = ti−ti−1, i = 1, . . . , N, τ = τi. Предполагается, что i -целые положительные числа. Тогда λi = xi xi−1 -темп роста за i-й периодв ремени, λ = xN x0 = Ni=1 λi-общий темп роста. Если все периоды одинаковы и равны единице (τi = 1), то средний в единицу времени темп роста определяется по формуле: ¯λ= xN x0 1 N = λi1 N ,
62 Глава 2. Описательная статистика т.е. он равен простому среднему геометрическому темпу по всем периодам. В общем случае (при разных τi) данная формула приобретает видсред невзве-шенной геометрической: ¯λ= xN x0 1τ =¯λαi i , где ¯λi = λ1/τi i -средний за единицу времени темп роста в i-м периоде, αi = τ iτ . Для величин типа запаса имеется еще одна форма средних темпов роста: от-ношение средней хронологической за период времени к средней хронологической за некоторый предыдущий период. Такую форму среднихможно рассмотреть на сле-дующем примере. Пусть x0, x1, x2 -значение величины типа запаса в три момента времени: на начало первого периода, конец первого периода, который одновременно яв-ляется началом второго периода, конец второго периода. Оба периода времени одинаковы. Средние хронологические за первый и второй периоды времени равны, соответственно, ¯x1 = (1 − α) x0 + αx1, ¯x2 = (1 − α) x1 + αx2. Темп роста средней величины типа запаса ¯λ= ¯x2¯x1 можно выразить через средние взвешенныетемповроста закаждыйиз двух периодоввремени λ1 = x1x0 , λ2 = x2x1 следующим образом: ¯λ= α11λ1 + α12λ2, где α11= (1 − α) x0 ¯x1 , α12= αx1 ¯x1 , α21+ α12= 1, или ¯λ= 1 α21/λ1 + α22/λ2 , где α21= (1 − α) x1 ¯x2 , α22= αx2 ¯x2 , α21+ α22= 1. Таким образом, темп роста средней хронологической является средней взве-шенной арифметической темпов роста за отдельные периоды, если веса рассчиты-ваются по информации первого периода, или средней взвешенной гармонической, если веса рассчитываются по информации второго периода. Если коэффициент α, представляющий внутрипериодную динамику, различа-ется по периодам, т.е. динамика величины в разных периодах качественно различна, то темп роста средней хронологической перестает быть средней арифметической или гармонической темпов роста по периодам, т.к. сумма весов при этих темпах роста не будет в общем случае равняться единице. В разных ситуациях средние темпы роста могут рассчитываться различным об-разом, чтоможно проиллюстрировать на простых примерах, взятых изфинансовых расчетов.
2.2. Средние величины 63 В финансовых расчетах аналогом темпа прироста капитала (величины типа запаса) выступает доходность на вложенный (инвестированный) капитал. Пусть инвестированный капитал x0 в течение периода τ приносит доход Δ. Тогд а капитал к концу периода становится равным x1 = x0+Δ, и доходность капитала за этот периодопр еделяется как δ = Δx0 = x1 x0 − 1, т.е. совпадает по форме с темпом прироста. Средняя за период доходность в зависимости от поведения инвестора (субъекта, вложившего капитал) рассчитывается различным образом. Ниже рассматривается три возможные ситуации. 1) Если позиция инвестора пассивна, и он не реинвестирует полученные доходы в течение данного периода времени, то средняя доходность в единицу времени опре-деляется простейшим способом: ¯δ = 1τ Δx0 . Фактически это-средняя арифметическая простая, т.к. Δx0 является общей до-ходностью за период времени τ . Такой способ расчета средней доходности наиболее распространен. Эта формула используется и при τ < 1. Так, обычно доходности за разные периоды времени приводятся к среднегодовым, т.е. единицей времени является год. Пусть речь идет, например, о трехмесячном депозите. Тогда τ = 0.25, и среднегодовая доходность получается умножением на 4 доходности Δx0 за 3 месяца. 2) Пусть доходность в единицу времени ¯δ в течение рассматриваемого периода вре-мени не меняется, но доходы полностью реинвестируются в начале каждой единицы времени. Тогда за каждую единицу времени капитал возрастает в 1 + ¯δ раз, и для нахождения ¯δ используется формула: 1 + Δx0 = 1 + ¯δτ , т.е. ¯δ = 1 + Δx01τ − 1. Эта формула справедлива при целых положительных τ . Действительно (предпола-гается, что начало периода инвестирования имеет на оси времени целуюкоординату), если τ < 1, ситуация аналогична предыдущей, в которой используетсяформула про-стой средней арифметической. Если τ не целое, то такая же проблема возникает для последней, неполной единицы времени в данном периоде. Естественно предположить, что ¯δ < 1, тогд а 1 + ¯δτ > 1 + τ ¯δ (что следует из раз-ложения показательной функции в степенной ряд) и 1τ Δx0 > ¯δ. Это соотношение лучше интерпретировать "в обратном порядке": если по усло-виям инвестиционного контракта ¯δ фиксирована и допускается реинвестирование
64 Глава 2. Описательная статистика доходов в течение периода, чем пользуется инвестор, то фактическая доходность на инвестированный капитал будет выше объявленной в контракте. 3) Пусть в течение данного периода времени доходы реинвестируются n раз через равные промежутки времени. Тогда для ¯δ справедлива следующая формула: 1 + Δx0 = 1 + τ ¯δ n n (она совпадает с предыдущей в случае n = τ ). Теоретически можно представить ситуацию непрерывного реинвестирования, ко-гда n→∞. В таком случае ¯δ = 1τ ln1 + Δx0, поскольку lim n→∞1 + τ ¯δ n n = eτ ¯δ. В соответствии с введенной ранее терминологией, это-непрерывный темп при-роста в единицу времени. Данную формулу можно использовать при любом (есте-ственно, положительном) τ . Понятно, что средние доходности в единицу времени, полученные в рассмотренных трех случаях, находятся в следующем соотношении друг с другом: 1τ Δx0 пассивное поведение > 1+ Δx01τ − 1 дискретное реинвестирование > 1τ ln1 + Δx0непрерывное реинвестирование . Это соотношение при интерпретации в "обратном порядке" означает, что чем ча-ще реинвестируется доход, тем выше фактическая доходность на первоначальный капитал. В финансовых расчетах для приведения доходностей к разным единицам времени используется 1-я формула. Теперь рассматривается общий случай с N +1 моментом времени и расчетом сред-ней доходности за N подпериодов. 1) Если позиция инвестора пассивна в течение всего периода времени, то средняя доходность в i-м подпериоде и в целом за период равны: ¯δi = 1 τi Δi x0 , ¯δ = 1τ Δx0 , где Δi = xi − xi−1, Δ = Ni=1Δi = xN − x0 (τ и τi определены выше). Средняя доходность в целом за период удовлетворяет формуле средней взвешенной арифме-тической: ¯δ =αi¯δi, где αi = τi τ .
2.2. Средние величины 65 2) Пусть теперь доходы реинвестируются в начале каждого подпериода времени. Тогда в течение i-го подпериода капитал вырастает в 1 + τi¯δi раз, где ¯δi = 1 τi Δi xi−1 . Если предположить, что все подпериоды имеют одинаковую длину ¯τ, то в сред нем за подпериод доход вырастает в 1 + ¯τ ¯δi1/N раз, и это количество раз равно 1 + ¯τ ¯δ. Поэтому ¯δ = 1¯τ 1 + ¯τ ¯δi1/N − 1. Это формула простой средней приведенного выше общего вида f−1 1N f (xi), где f = ln(1+x). Аналогичную формулу можно использовать и в случае подпериодов разной длины τi : ¯δ = 1¯τ 1+ τi¯δi1N − 1, где ¯τ = 1 N τi. Фактически эти формулы являются вариантами формул простой средней геометри-ческой. 3) Пусть теперь все τi являются целыми положительными числами, и реинвести-рование доходов происходит в начале каждой единицы времени. Тогда ¯δi = 1 + Δi xi−11τi − 1, ¯δ = 1 + Δx01τ − 1. Средняя в единицу времени доходность в целом за период равна средней взвешенной геометрической средних доходностей по подпериодам: ¯δ =1 + ¯δiαi − 1, где αi = τi τ . 4) Наконец, в теоретическом случае непрерывного инвестирования ¯δi = 1 τi ln1 + Δi xi−1, ¯δ = 1τ ln1 + Δx0, и средняя доходность за весь период, как и в первом случае, равна средней взвешен-ной арифметической средних доходностей по подпериодам: ¯δ =αi¯δi, где αi = τi τ . В заключение этого раздела следует отметить, что особую роль в статистике играют средние арифметические. Именно они выступают важнейшей характери-стикой распределения случайных величин. Так, в обозначениях предыдущегопункта величину ¯x = αixi можно записать как ¯x = xifiΔi или, при использовании теоретической функции плотности распределения, как ¯x = xf(x) dx.
66 Глава 2. Описательная статистика Теоретическое арифметическое среднее, определенное последней формулой, называется в математической статистике математическим ожиданием. Матема-тическое ожидание величины x обозначают обычно как E(x), сохраняя обозначе-ние ¯x для эмпирических средних (см. Приложение A.3.1). 2.3. Медиана, мода, квантили Мода имедиана, наряду со средней, являются характеристиками центра распре-деления признака. Медиана, обозначаемая в данном тексте через x0.5,-величина (детерминированная), которая "делит" совокупность пополам. Теоретически она такова, что x0.5 −∞ f (x) dx = +∞ x0.5 f (x) dx = 0.5, где f(x) -функция распределения (см. Приложение A.3.1). 0.5 F1 Fl-1 zl-1 zl (zl-1+Δl) x0.5 Рис. 2.8 По выборочным данным x1, . . . , xN, упоряд очен-ным по возрастанию, за нее принимается x(N+1)/2 в случае, если N нечетно, и (xN/2 + xN/2+1)/2, если N четно. Значение медианы может быть уточнено, если по данным выборки построено эмпирическое распре-деление частот zl, l = 0, . . . , k, Δl, αl, fl, Fl, l = = 1, . . . , k. Пусть l-й полуинтервал является меди-анным, т.е. Fl−1 < 0.5 Fl . Тогда, линейно интер-полируя значения функции распределения F на этом полуинтервале, медиану определяют по следующей формуле: x0.5 = zl−1 +Δl 0.5 − Fl−1 αl . Ее смысл поясняется на графике (рис. 2.8). Этот график является фрагментом кумуляты. Мода, обозначаемая в данном тексте через ox, показывает наиболее вероят-ное значение признака. Это-значение величины в "пике" функции плотности распределения вероятности (см. Приложение A.3.1): f o x= max x f (x).
2.3. Медиана, мода, квантили 67 Величины с унимодальным распределением имеют одну моду, полимодальные распределения характеризуются несколькими модами. Непосредственно по вы-борке, если все ее значения различны, величину моды определить невозможно. Если какое-то значение встречается в выборке несколько раз, то именно его- по определению-принимают за моду. В общем случае моду ряда наблюдений f1 fl-1 fl+1 zl-1 x ox - zl+1 zl x1 zl-2 x - 2 x - 2 m22 1 2 Рис. 2.9 находят по данным эмпирического распреде-ления частот. Пусть l-й полуинтервал является модаль-ным, т.е. fl > fl−1 и fl > fl+1 (во избежание непринципиальных уточнений случай "" не рассматривается). Функция плотности веро-ятности аппроксимируется параболой, прохо-дящей через середины ступенек гистограммы, и ее максимум определяет положение искомой моды. График (рис. 2.9) поясняет сказанное. В случае если размеры полуинтервалов Δl−1, Δl и Δl+1 одинаковы и равны Δ, такая про-цедура приводит к определению моды по фор-муле: o x= zl−1 +Δ fl − fl−1 (fl − fl−1) + (fl − fl+1) . В справедливости этой формулы несложно убедиться. Действительно, коэффициен-ты a, b и c аппроксимирующей параболы ax2 +bx+c удовлетворяют следующей системе уравнений:⎧⎨ ⎩ a¯x2l−1 + b¯xl−1 + c = fl−1, a(¯xl−1 +Δ)2 + b(¯xl−1 +Δ) + c = fl, a(¯xl−1 + 2Δ)2 + b(¯xl−1 + 2Δ)+c = fl+1. Если из второго уравнения вычесть первое, а затем третье, то получится более простая система из двух уравнений: Δ(a(2¯xl−1 +Δ)+b) = fl − fl−1, Δ(a(−2¯xl−1 − 3Δ) − b) = fl − fl+1. Первое из этих уравнений дает выражение для b через a : b = fl − fl−1 Δ − a (2¯xl−1 +Δ), а их сумма-выражение для определения параметра a : −2aΔ2 = (fl − fl−1) + (fl − fl+1).
68 Глава 2. Описательная статистика Очевидно, что a отрицательно, и поэтому парабола имеет максимум в точке −b2a (в этой точке производная 2ax+b равна нулю), т.е. ox= −b2a , и после подстановки сюда полученных выражений для b и a, учитывая, что ¯xl−1 + Δ2 = zl−1, получается искомая формула. Все три характеристики центра распределения: мода, медиана, среднее-на-ходятся в определенных соотношениях между собой. В случае идеальной (теоретически) симметрии f (x0.5 +Δ) = f (x0.5 − Δ) (2.5) при любом Δ 0, все эти три характеристики совпадают. Доказательство этого утверждения проводится для теоретической функции плотно-сти распределения f(x), в предположении, что она является гладкой, т.е. непрерыв-ной и непрерывно дифференцируемой. Дифференцирование выражения (2.5) по Δ в точке 0 дает условие f(x0.5) = = −f(x0.5), из чего, в силу непрерывной дифференцируемости f, след ует ра-венство нулю производной в точке x0.5. И поскольку распределение унимодально, то мода совпадает с медианой. Теперь доказывается совпадение математического ожидания с медианой. Для слу-чайной величины x−x0.5 с той же функцией распределения плотности f(x), в силу того, что +∞ −∞ f (x) = 1, имеет место следующее тождество: E(x) − x0.5 = +∞ −∞ (x − x0.5) f (x) dx. Его правая часть разбивается на два слагаемых и преобразуется следующимобразом: E(x) − x0.5 = x0.5 −∞ (x − x0.5) f (x) dx + +∞ x0.5 (x − x0.5) f (x) dx = (в первом слагаемом производится замена переменных x − x0.5 = −Δ и переста-новка пределов интегрирования 0 +∞ → −+∞0, во 2-м слагаемом-замена пере-менных x − x0.5 = Δ) = − +∞ 0 Δf (x0.5 − Δ) dΔ+ +∞ 0 Δf (x0.5 +Δ) dΔ =
2.3. Медиана, мода, квантили 69 (вводя соответствующие обозначения) = −A− + A+. (2.6) Поскольку выполнено условие симметричности распределения (2.5), A− = A+ и математическое ожидание (среднее) совпадает с медианой. Это завершает рас-смотрение случая симметричных распределений. Для асимметричных распределений указанные три характеристики различа-ются, но так, что медиана всегда находится между средней и модой. При правой асимметрии ox< x0.5 < ¯x, при левой асимметрии, наоборот, ¯x < x0.5 <ox. В этом легко убедиться.Пусть речь идет, например, о правой асимметрии. Распреде-ление скошено в сторону больших значений случайной величины-признака, поэтому A− < A+ (это соотношение можно рассматривать в качестве определения правой асимметрии), и, в силу выполнения тождества (2.6), среднее должно превышать медиану: x0.5 < E(x), (x0.5 < ¯x). Условие A− < A+ может выполняться только в случае, если при достаточно боль-ших Δ имеет место неравенство f (x0.5 +Δ) > f (x0.5 − Δ) (веса больших зна-чений признака больше, чем веса равноудаленных от медианы малых значений). Но тогда для малых Δ, т.е. в окрестности медианы, должно иметь место обратное неравенство (поскольку +∞0f (x0.5 − Δ) dΔ = +∞0f (x0.5 +Δ) dΔ = 0.5): f (x0.5 − Δ) > f (x0.5 +Δ), а это означает, что мода смещена влево от медианы: ox< x0.5. Проведенное рассуждение о положении моды относительно медианы не являет-ся строгим, оно предполагает как бы "плавный" переход от симметрии к правой асимметрии. При строгом доказательстве существенную роль играет предположе-ние об унимодальности распределения. Случай левой асимметрии рассматривается аналогично. Для больших выборок, как правило, подтверждается еще одно утверждение об относительном расположении трех рассматриваемых характеристик: при уме-ренной асимметрии мода удалена отмедианы на расстояние приблизительно в 2 ра-за большее, чем среднее. То есть ox−x0.5≈ 2 | ¯x − x0.5 |.
70 Глава 2. Описательная статистика Для того чтобы легче запомнить приведенные здесь соотношения, можно ис-пользовать следующее мнемоническое правило. Порядок следования среднего, ме-дианы и моды (при левой асимметрии) такой же, как слов mean, median, mode в английском словаре (при правой асимметрии порядок обратный). Причем, как и соответствующие им статистические характеристики, слово mean расположено в словаре ближе к median, чем mode. Квантилем называют число (детерминированное), делящее совокупность в определенной пропорции. Так, квантиль xF (используемое в данном тексте обо-значение квантиля) делит совокупность в пропорции (верхняя часть к нижней) 1 − F к F (см. Приложение A.3.1): P(x xF) = F или F (xF) = xF −∞ f (x) dx = F. В эмпирическом распределении все границы полуинтервалов являются кван-тилями: zl = xFl . По данным этого распределения можно найти любой квантиль xF с помощью приема, использованного выше при нахождении медианы. Если l-й полуинтервал является квантильным, т.е. Fl−1 < F Fl, то xF = zl−1 +Δl F − Fl−1 αl . Иногда квантилями называют только такие числа, которые делят совокупность на равные части. Такими квантилями являются, например, медиана x0.5, д елящая совокупность пополам, квартили x0.25, x0.5, x0.75, которые делят совокупность на четыре равные части, децили x0.1, . . . , x0.9, процентили x0.01, . . . , x0.99. Для совокупностей с симметричным распределением и нулевым средним (со-ответственно, с нулевой модой и медианой) используют понятие двустороннего квантиля ˆxF : P (−ˆxF x ˆxF) = F (ˆxF ) − F (−ˆxF) = ˆxF −ˆxF f (x) dx = F. 2.4. Моменты и другие характеристики распределения Моментом q-го порядка относительно c признака x называют величину (q и c -величины детерминированные) m(q, c) = 1N Ni=1 (xi − c)q,
2.4. Моменты и другие характеристики распределения 71 в случае, если она рассчитывается непосредственно по выборке; m(q, c) = k l=1 αl (¯xl − c)q = k l=1 fl (¯xl − c)q Δl, если используются данные эмпирического распределения частот; μ (q, c) = +∞ −∞ f (x) (x − c)q dx = E((x − c)q) -для теоретического распределения вероятности (cм. Приложение A.3.1). В эконометрии для обозначения теоретических или "истинных" значений ве-личины (в генеральной совокупности) часто используются буквы греческого алфа-вита, а для обозначения их эмпирических значений (полученных по выборке) или их оценок-соответствующие буквы латинского алфавита. Поэтому здесь в пер-вых двух случаях момент обозначается через m, а в третьем случае-через μ. В качестве общей формулы эмпирического момента (объединяющей первые два случая) будет использоваться следующая: m(q, c) = Ni=1 αi (xi − c)q . В принципе, моменты могут рассчитываться относительно любых c, од нако в статистике наиболее употребительнымоменты, рассчитанные при c, равном нулю или среднему. В первом случае моменты называют начальными, во втором- центральными. В расчете центральных моментов используются величины xi − ¯x, которые часто называют центрированными наблюдениями и обозначают через ˆxi. Средняя является начальным моментом 1-го порядка: ¯x = m(1, 0), E(x) = μ (1, 0). Благодаря этому обстоятельству центральные моменты при целых q всегда можно выразить через начальные моменты. Для этого надо раскрыть скобки (воз-вести в степень q) в выражении центрального момента. Центральный момент 2-го порядка или 2-й центральный момент называется дисперсией и обозначается через s2 (эмпирическая дисперсия) или σ2 (теорети-ческая дисперсия): s2 = m(2, ¯x) , σ2 = μ (2, E(x)) .
72 Глава 2. Описательная статистика При вычислении дисперсии иногда удобнее пользоваться начальным моментом 2-го порядка. Связь с ним устанавливается следующим образом: s2 =αi (xi − ¯x)2 =αix2i− 2¯x αixi ←−−−→ ¯x +¯x2 = =αix2i− ¯x2 = m(2, 0) − m2 (1, 0) . Корень квадратный из дисперсии- s или σ -является среднеквадрати-ческим отклонением, иногда (главным образом, в англоязычной литературе) его называют стандартным отклонением. Величины ˆxis называют центрированными и нормированными наблюдени-ями. Они измеряют значения признака в единицах среднеквадратического откло-нения от среднего. Такая шкала измерения иногда называется стандартизованной или стандартизированной. Дисперсия (и среднеквадратическое отклонение) является мерой абсолютного рассеяния или разброса значений признака в совокупности. В принципе такой ме-рой мог бы служить 2-й момент относительно какого-то другого c, не равного ¯x, но лежащего в центральной области распределения признака. Однако используют именно дисперсию, поскольку ее величина однозначно определена и, в некотором смысле, не зависит от c. Дисперсия минимальна среди всех 2-х моментов относи-тельно c. Действительно, производная 2-го момента по c d (x − c)2f(x)dxdc = −2xf(x)dx − c f(x)dx= −2 (E(x) − c) равна 0 в точке c = E(x). Это точка минимума, поскольку 2-я производная по c в ней равна 2, т.е. положительна. В статистике используются и другие показатели разброса. Примерами показа-телей абсолютного разброса являются: max xi − min xi 2 -общий размах вариации, x1−F − xF 2 -квантильный размах вариации, где F < 0.5 (достаточно часто используется квартильный размах вариации, то есть этот показатель при F = 0.25), αi |ˆxi| -среднее линейное отклонение.
2.4. Моменты и другие характеристики распределения 73 Среднее линейное отклонение имеетсмысл рассчитывать не относительно сред-него ¯x, а относительно медианы x0.5, поскольку именно в таком случае оно при-нимает минимально возможное значение. Действительно, производная по c среднего линейного отклонения относительно c d |x − c| f (x) dxdc = dc −∞ (c − x) f (x) dx + +∞c(x − c) f (x) dxdc = = c −∞ f (x) dx − +∞ c f (x) dx равна 0 при c = x0.5 (2-я производная в этой точке равна 2f(x0.5) и положительна по определению функции f ). Для характеристики относительного разброса применяются различные фор-мы коэффициента вариации. Например, он может рассчитываться как отношение среднего квадратичного отношения к среднему, общего или квантильного размаха вариации к медиане. Иногда его рассчитывают как отношения max xi к min xi или x1−F к xF (при F <0.5). Достаточно распространен еще один тип коэффициентов вариации, которые рассчитываются как отношения средней по верхней части совокупности к средней по нижней части совокупности. Для того чтобы дать определение таким коэффициентам вариации, необходимо вве-сти понятие среднего по части совокупности. Математическое ожидание можно представить в следующей форме: E(x) = F 1F xF −∞ xf (x) dx + (1 − F) 1 1 − F +∞ xF xf (x) dx = = F EF (x) + (1 − F) E+F (x) . Квантиль xF делит совокупность на две части, по каждой из которых определяется свое математическое ожидание: EF (x)-по нижней части, E+F (x)-по верхней части совокупности. Приведенное тождество определяет связь между двумя этимиматематическими ожи-даниями: E+F (x) = 1 1 − F (E(x) − F EF (x)).
74 Глава 2. Описательная статистика По выборке аналогичные частичные средние рассчитываются следующим образом. Пусть xi, i = 1, . . . , N ряд наблюдений, упорядоченный по возрастанию. Тогда Fi = i N , i = 1, . . . , N -накопленные относительные частоты, ¯xi = 1i i i=1 xi-i-я средняя по нижней части, i = 1, . . . , N (¯x0 = 0) , ¯x+i = 1 N − i N i=i+1 xi= 1 1 − Fi (¯x − Fi ¯xi) -i-я средняя по верхней части, i = 0, 1, . . . , N ¯x+N = 0. Такой расчет не имеет необходимой иногда степени общности, поскольку позволяет найти частичные средние лишь для некоторых квантилей, которыми в данном случае являются сами наблюдения (xi = xFi ). Для квантилей xF при любых F частич-ные средние находятся по данным эмпирического распределения (предполагается, что l-й полуинтервал является квантильным): ¯xF = 1F l−1 l=1 αl¯xl+ (F − Fl−1) 12 (zl−1 + xF )-средняя по нижней части совокупности (здесь 12 (zl−1 + xF ) -центр последне-го, неполного полуинтервала, F −Fl−1 -его вес). После подстановки выражения для квантиля xF , полученного в предыдущем пункте, эта формула приобретает сле-дующий вид:¯xF = 1F l−1 l=1 αl¯xl+ (F − Fl−1)zl−1 + F − Fl−1 2αi Δl. При расчете средней по верхней части совокупности проще воспользоваться полу-ченной выше формулой: ¯x+F = 1 1 − F (¯x − F ¯xF ). Для расчета квантильного коэффициента вариации совокупность делится на 3 ча-сти: верхняя часть, объемом не более половины, нижняя часть такого же объема и средняя часть, не используемая в расчете. Данный коэффициент, называемый F × 100-процентным (например, 15-процентным), рассчитывается как отношение средних по верхней и нижней части совокупности: ¯x+1−F ¯xF = ¯x − (1 − F) ¯x1−F F ¯xF , E+1−F (x) EF (x) = E(x) − (1 − F)E1−F (x) F EF (x) , где F 0.5.
2.4. Моменты и другие характеристики распределения 75 При использовании непосредственно данных выборки этаформула имеет другой вид: ¯x+N−i ¯xi = ¯x − (1 − Fi) ¯xN−i Fi ¯xi , где i N2 . Такие коэффициенты вариации называют иногда, как и соответствующие квантили, медианными, если F = 0.5, квартильными, если F = 0.25, децильными, если F = 0.1, процентильными, если F = 0.01. Наиболее употребительны децильные коэффициенты вариации. При расчете коэффициентов вариации в любой из приведенных форм предпола-гается, что характеризуемый признак может принимать только неотрицательные значения. Кривая Лоренца Накопленные относительные частоты (%%) Накопленные доли суммар-ного признака (%%) Рис. 2.10 Существует еще один-графический- способ представления степени разброса зна-чений признака в совокупности. Он исполь-зуется для совокупностей объемных призна-ков, принимающих положительные значения. Это-кривая Лоренца или кривая концен-трации. По абсциссе расположены доли на-копленной частоты, по ординате-доли на-копленного суммарного признака. Она име-ет вид, изображенный на графике (рис. 2.10). Чем более выпукла кривая, тем сильнее диф-ференцирован признак. По оси абсцисс кривойЛоренца расположены значения величины F ×100%, по оси ординат-в случае использования теоретического распределения-значения ве-личины: xF 0xf (x) dx +∞0xf (x) dx × 100% (предполагается, что x 0), или, используя введенные выше обозначения д ля ча-стичных средних, F EF (x) E(x) × 100%. При использовании данных эмпирического распределения по оси ординат располо-жены значения величины F ¯xF ¯x × 100%.
76 Глава 2. Описательная статистика При построении кривой непосредственно поданнымряда наблюдений сначалана гра-фике проставляются точки Fi × 100%, Fi ¯xi ¯x × 100%, i = 1, . . . , N, а затем они соединяются отрезками прямой линии. В случае, если значение признака в совокупности не варьируется, средние по всем ее частям одинаковы, и кривая Лоренца является отрезком прямой линии (пунктирная линия на рис. 2.10). Чем выше вариация значений признака, тем более выпукла кривая. Степень ее выпуклости или площадь выделенной на рисунке области может являться мерой относительного разброса. Кривую Лоренца принято использовать для иллюстрации распределения дохода или имущества в совокупностях людей, представляющих собой население отдельных стран или регионов. Отсюда ее второе название-кривая концентрации. Она вы-ражает степень концентрации богатства в руках меньшинства. В статистике центральные моменты q-го порядка обычно обозначаются через mq (μq -для теоретических величин): mq = m(q, ¯x) (μq = μ(q,E(x)). Нормированный центральный момент 3-го порядка d3 = m3 s3 δ3 = μ3 σ3часто используется как мера асимметрии (скошенности) распределения. Если рас-пределение симметрично, то этот показатель равен нулю. В случае его положи-тельности считается, что распределение имеет правую асимметрию, при отрица-тельности-левую асимметрию (см. Приложение A.3.1). Следует иметь в виду, что такое определение левой и правой асимметрии может не соответствовать определению, данному в предыдущем пункте. Возможны такие ситуации, когда распределение имеет правую асимметрию, и среднее превышает медиану, но данный показатель отрицателен.Инаоборот, среднее меньше медианы (левая асимметрия), но этот показатель положителен. В этом можно убедиться, рассуждая следующим образом. Пусть ϕ(x) -функция плотности вероятности симметричного относительно нуля распределения с дисперсией σ2, т.е. +∞ −∞ xϕ (x) dx = 0, +∞ −∞ x2ϕ (x) dx = σ2, +∞ −∞ x3ϕ (x) dx = 0, 0 −∞ ϕ (x) dx = +∞ 0 ϕ (x) dx = 0.5, ϕ(x) = ϕ(−x).
2.4. Моменты и другие характеристики распределения 77 x -a 0 a ΔϕРис. 2.11 Рассматривается случайная величина x, имеющая функцию плотности вероятности f(x) = ϕ(x) + γΔϕ(x). Функция Δϕ вносит асимметрию в распределение x. Ее график имеет вид- сплошная линия на рисунке 2.11, а свойства таковы: Δϕ(x) = −Δϕ(−x), +∞ −∞ Δϕ (x) dx = 0, 0 −∞ Δϕ (x) dx = +∞ 0 Δϕ (x) dx = 0. Параметр γ не должен быть слишком большим по абсолютной величине, чтобы со-хранялась унимодальность распределения (и, конечно же, неотрицательность функ-ции плотности). Можно обозначить− a 0 Δϕ (x) dx = +∞ 0 Δϕ (a + x) dx = S > 0 и определить величины a1 и a2 : a 0 xΔϕ (x) dx = −a1S, +∞ 0 xΔϕ (a + x) dx = a2S. Понятно, что a1 -математическое ожидание случайной величины, заданной на от-резке [0, a] и имеющей плотность распределения −1SΔϕ (x), поэтому 0 < a1 < a. Аналогично, a2 -математическое ожидание случайной величины, заданной на от-резке [0,∞] с плотностью вероятности 1SΔϕ (a + x), поэтому 0 < a2.
78 Глава 2. Описательная статистика Теперь легко видеть, что (вводя дополнительное обозначение a3) +∞ 0 xΔϕ(x) dx = a 0 xΔϕ(x) dx ←−−−−−−−→ −a1S ++∞ a xΔϕ(x) dx x=a+y = −a1S + a+∞ 0 Δϕ(a + y) dy ←−−−−−−−−−−→ S + + +∞ 0 yΔϕ (a + y) dy ←−−−−−−−−−−−−→ a2S = S (−a1 + a + a2) = a3 > 0. Аналогичным образом можно доказать, что +∞ 0 x3Δϕ (x) dx = a4 > 0. Прибавление γΔϕ к ϕ не меняет медиану, т.к. +∞ 0 f (x) dx = +∞ 0 ϕ (x) dx ←−−−−−−→ 0.5 + γ+∞ 0 Δϕ (x) dx ←−−−−−−−→ 0 = 0.5, но сдвигает среднее (из нуля): E(x) = +∞ −∞ xf (x) dx = +∞ −∞ xϕ (x) dx ←−−−−−−−→ 0 + γ+∞ −∞xΔϕ (x) dx = = γ ⎛⎜⎜⎜⎜⎜⎜⎝ 0 −∞ xΔϕ (x) dx ←−−−−−−−−−→ a3 + +∞ 0 xΔϕ (x) dx ←−−−−−−−−−→ a3 ⎞⎟⎟⎟⎟⎟⎟⎠ = 2γa3. Таким образом, в соответствии с данным выше определением, если γ > 0, распре-деление имеет правую асимметрию (увеличивается плотность вероятности больших значений признака), и среднее, будучи положительным, выше медианы. Если γ < 0, распределение характеризуется левой асимметрией, и среднее ниже медианы.
2.4. Моменты и другие характеристики распределения 79 Теперь находится 3-й центральный момент: μ3 = +∞ −∞ (x − E(x))3 f (x) dx = = +∞ −∞ x3f (x) dx − 3E(x) +∞ −∞ x2f (x) dx + 3E2(x) +∞ −∞ xf(x)dx − E3(x) ←−−−−−−−−−−−−−−−−−−−→ 2E3(x) = = +∞ −∞x3ϕ(x)dx ←−−−−−−−→ 0 +γ +∞ −∞x3Δϕ(x)dx ←−−−−−−−−−→ 2a4 −3E(x)+∞ −∞x2ϕ(x)dx ←−−−−−−−→ σ2 +γ +∞ −∞x2Δϕ(x)dx ←−−−−−−−−−→ 0 + + 2E3(x) E(x)=2γa3 = 2γ(a4 − 3a3σ2 + 8γ2a33=2γ (D + R), где D = a4 − 3a3σ2, R = 8γ2a33. Второе слагаемое в скобках- R -всегда положительно, и, если D (первое сла-гаемое) неотрицательно, то введенный показатель асимметрии "работает" правиль-но: если он положителен, то асимметрия-правая, если отрицателен, то-левая. Однако D может быть отрицательным. Это легко показать. Пусть при заданном Δϕ эта величина положительна (в этом случае a4 3a3σ2 > 1). Сжатием графика этой функции к началу координат (пунктирная линия на рис. 2.11) всегда можно добиться смены знака данной величины. Преобразованная (сжатая) функция асимметрии Δ˜ϕ связана с исходной функцией следующим образом: Δ˜ϕ (x) = Δϕ (kx), гдеk > 1. Свойства этой новойфункции теже, что и исходной, и поэтому все проведенные выше рассуждения для новой случайной величины с функцией плотности ϕ + γΔ˜ϕ дадут те же результаты. Новая величина D, обозначаемая теперь ˜D, связана с исходными величинами следующим образом: ˜D= ˜a4 − 3˜a3σ2 = 1 k2 1 k2 a4 − 3a3σ2например, ˜a3 = +∞ 0 xΔϕ (kx) dx kx=y, x=1k y, dx=1k dy = 1 k2 +∞ 0 yΔϕ (y) dy = 1 k2 a3и при k >   a4 3a3σ2 > 1 она отрицательна.
80 Глава 2. Описательная статистика Таблица 2.1 X −3 −2 −1 0 1 2 3 ϕ 0.0625 0.125 0.1875 0.25 0.1875 0.125 0.0625 Δϕ 0 −1 1 0 −1 1 0 Δ˜ϕ −0.2 −1 1 0 −1 1 0.2 В такой ситуации (если γ достаточно мал, и вслед за ˜Dотрицательно и ˜D+ ˜R) 3-й центральный момент оказывается отрицательным при правой асимметрии и по-ложительным при левой асимметрии. Можнопривести числовой пример совокупности справой асимметрией, 3-й цен-тральный момент которой отрицателен.Исходные данные приведены в таблице 2.1. При γ = 0.03 среднее равно 0.06 (превышает медиану, равную 0), а 3-й цен-тральный момент равен −0.187. Но стоит немного растянуть функцию асимметрии от начала координат (последняя строка таблицы), как ситуация приходит в норму. При том же γ среднее становится равным 0.108, а 3-й центральный момент равен +0.097. Проведенный анализ обладает достаточной степенью общности, т.к. любую функцию плотности вероятности f можно представить как сумму функций ϕ и Δϕ с указан-ными выше свойствами (при этом γ = 1). Эти функции определяются следующим образом (предполагается, что медиана для функции f равна 0): ϕ (x) = 12 (f (x) + f (−x)) , Δϕ (x) = 12 (f (x) − f (−x)). Таким образом, если асимметрия "сосредоточена" вблизи от центра распределения (функция асимметрии Δϕ достаточно "поджата" к медиане), то 3-й центральный момент не может играть роль показателя асимметрии. Надежным показателем асимметрии является величина (¯x − o x) s или, учитывая приведеннуюв предыдущемпункте эмпирическуюзакономерность в расположении моды, медианы и среднего, 3 (¯x − x0.5) s . Достаточно употребителен также квартильный коэффициент асимметрии, рас-считываемый как отношение разности квартильных отклонений от медианы к их сумме: (x0.75 − x0.5) − (x0.5 − x0.25) (x0.75 − x0.5) + (x0.5 − x0.25) = x0.25 + x0.75 − 2x0.5 x0.75 − x0.25 .
2.4. Моменты и другие характеристики распределения 81 Эти три коэффициента положительны при правой асимметрии и отрицатель-ны при левой. Для симметричных распределений значения этих коэффициентов близки к нулю. Здесь требуется пояснить, что означает "близки к нулю". Рассчитанные по выборке, значения этих коэффициентов-пусть они обо-значаются через Kc (c -calculated)-не могут в точности равняться нулю, да-же если истинное распределение в генеральной совокупности симметрично. Как и исходные для их расчета выборочные данные, эти коэффициенты являются слу-чайными величинами K с определенными законами распределения. Эти законы (в частности, функции плотности вероятности) известны в теории статистики, если справедлива нулевая гипотеза, в данном случае-если истинное распределение симметрично. А раз известна функция плотности, то можно определить область, в которую с наибольшей вероятностью должно попасть расчетное значение коэф-фициента Kc в случае справедливости нулевой гипотезы. Эта область, называе-мая доверительной, выделяется квантилем KF с достаточно большим F.Обычно принимают F = 0.95. В данном случае K могут быть как положительными, так и отрицательными, их теоретическое распределение (при нулевой гипотезе) сим-метрично относительно нуля, и использоваться должен двусторонний квантиль. Если расчетное значение Kc попадает в доверительную область, т.е. оно по абсолютной величине не превосходит KF , то нет оснований считать, что истинное распределение не симметрично, и нулевая гипотеза не отвергается. На основа-нии этого не следует делать вывод о симметричности истинного распределения. Установлено только то, что наблюдаемые факты не противоречат симметрично-сти. Другими словами, если распределение симметрично, то расчетное значение попадает в доверительную область. Но обратное может быть не верным. Если расчетное значение не попадает в доверительную область или, как гово-рят, попадает в критическую область, то маловероятно, что величина K имеет принятое (при нулевой гипотезе) распределение, и нулевая гипотеза отвергается с вероятностью ошибки (1-го рода) 1−F (обычно 0.05). Причем если Kc > KF , то принимается гипотеза о правой асимметрии, если Kc < −KF , то принимается гипотеза о левой асимметрии. Границы доверительной (критической) области зависят от числа наблюдений. Чем больше наблюдений, тем меньше KF , при прочих равных условиях, т.е. тем у´же доверительная область-область "нуля". Это означает, что чем больше использовано информации, тем точнее, при прочих равных условиях, сделанные утверждения. Таким образом, фраза "Kc близко к нулю" означает, что |Kc| KF . Приведенные здесь рассуждения используются в теории статистики при про-верке статистических гипотез, или тестировании (по англоязычной терминоло-гии), а также при построении доверительных интервалов (областей).
82 Глава 2. Описательная статистика Подробнее о проверке гипотез см. Приложение A.3.3. Нормированный центральный момент 4-го порядка d4 = m4 s4 δ4 = μ4 σ4называется куртозисом (от греческого слова κυρτoι -горбатый). По его вели-чине судят о высоковершинности унимодального распределения. Если распреде-ление близко к нормальному, то этот показатель равен приблизительно 3 ("при-близительно" понимается в том же смысле, что и "близко к нулю" в предыдущем случае). Если r4 > 3, то распределение высоковершинное, в противном случае- низковершинное.На этом основании вводится показатель, называемый эксцессом (см. Приложение A.3.1): d4 −3 (δ4 − 3). Его используют для оценки высоковершинности распределения, сравнивая с 0. Граничным для куртозиса является число 3, поскольку для нормального распре-деления он равен точно 3. Действительно, плотность f(x) нормально распределенной с математическим ожи-данием ¯x и дисперсией σ2 случайной величины x равна 1 σ√2π e− (x − ¯x)2 2σ2 . В "Справочнике по математике" И.Н. Бронштейна и К.А. Семендяева (М., 1962) на стр. 407 можно найти следующую формулу: ∞ 0 xne−ax2dx = Γ n+1 2 2an+1 2 , приa > 0 иn > 1, где Γ -гамма-функция, обладающая следующими свойствами: Γ (x+ 1) = xΓ (x) , Γ (n) = (n − 1)!, при n целом и положительном, Γ (x)Γx + 12= √π 22x−1Γ (2x) . Отсюда легко установить, что при целом и четном q μq = 1- 3 - 5 - . . . - (q − 1) σq = (q − 1)!! - σq и, в частности, μ4 = 3σ4. О свойствах нормального распределения см. Приложение A.3.2. В практике статистики моменты более высоких порядков используются крайне редко.
2.5. Упражнения и задачи 83 2.5. Упражнения и задачи Упражнение На основании данных о росте студентов курса построить ряд распределения, дать табличное и графическое его изображение (представив на графике гистограм-му, полигон, кумуляту). Какие из графиков соответствуют эмпирической функции плотности распределения вероятности, а какие-эмпирической функции распре-деления вероятности? Изобразить на графике гистограммы положение моды, ме-дианы и средней арифметической. Подтвердить их соотношения расчетами харак-теристик центра распределения. Найти дисперсию, коэффициент вариации, а так-же показатели асимметрии и эксцесса. Оценить степень однородности элементов совокупности. Задачи 1. Определить пункты, которые являются выпадающими из общего ряда. 1.1 а) частота, б) плотность, в) гистограмма, г) график; 1.2 а) арифметическое, б) геометрическое, в) алгебраическое, г) квадрати-ческое; 1.3 а) мода, б) медиана, в) квантиль, г) квартиль; 1.4 а) бимодальное, б) нормальное, в) асимметричное, г) U-образное; 1.5 а) математическое ожидание, б) биномиальное, в) нормальное, г) сред-нее; 1.6 а) момент, б) период, в) дисперсия, г) среднее; 1.7 а) центральный, б) начальный, в) исходный, г) момент. 2. Количественный признак принимает значения 2, 3, 4, 9. Какова плотность относительной частоты 2-го и 3-го элемента? 3. Распределение семей по доходам (в условных единицах вмесяц) представлено в группированном виде количеством Nl семей, попавших в l полуинтервал (zl−1; zl] (табл. 2.2). Заполните в таблице недостающие характеристики. Изобразите графики гистограммы, полигона и кумуляты. 4. Какова средняя хронологическая величин 1, 2, 5, 9, характеризующих по-следовательность равных промежутков времени?
84 Глава 2. Описательная статистика Таблица 2.2 (zl−1; zl] 500;700 700;900 900;1100 1100;1300 1300;1500 Nl 4 8 5 2 1 αl Fl fl 5. На что нужно поделить y1 − y0, чтобы получить среднюю хронологическую на временном отрезке [0, 1] ? 6. Чему равны простые средние: геометрическая, арифметическая, гармониче-ская чисел 1, 2, 4? 7. Три объекта характеризуются следующимиотносительнымипризнаками: 16, 13, 14. Веса этих объектов по числителю равны 0.1, 0.2, 0.7, вес первого объекта по знаменателю- 0.15. Чему равен вес второго объекта? 8. Какая из двух величин(a + b + c) 3 , или 3 1a + 1b + 1cбольше и почему? 9. Капитал за первый годне изменился, за второй-вырос на 12%. Средне-годовой коэффициент, одинаковый по годам, равен 38. Каков темп роста среднегодового капитала? 10. За первое полугодие капитал вырос на 12.5%, за второе-в 2 раза. Ка-кова среднегодовая доходность (в процентах), если позиция инвестора была пассивной, или если он реинвестировал доход в середине года? 11. Совокупность предприятий была разделена на группы в зависимости от вели-чины стоимости реализованной продукции. Количество предприятий в каж-дой группе и среднее значение стоимости реализованной продукции в каждой группе даны в таблице:
2.5. Упражнения и задачи 85 Номер группы 1 2 3 4 5 Количество предприятий в группе (ед.) 4 4 5 7 5 Среднее значение стоимости реализован-ной продукции (ден. ед.) 15 20 25 30 35 Определить среднюю стоимость реализованной продукции по совокупности предприятий в целом. 12. По металлургическому заводу имеются следующие данные об экспорте про-дукции: Вид продукции Доля вида продукции в общей стоимости реали-зованной продукции,% Удельный вес продукции на экспорт,% Чугун 25 35 Прокат листовой 75 25 Определить средний удельный вес продукции на экспорт. 13. Совокупность населенных пунктов области была разделена на группы в зави-симости от численности безработных. Количество населенных пунктов в каж-дой группе и средняя численность безработных в каждой группе даны в таб-лице: Номер группы 1 2 3 4 5 Количество населенных пунктов в группе 4 8 2 3 3 Средняя численность безработных 10 12 15 20 30 Определить среднюю численность безработных по совокупности населенных пунктов в целом. 14. В таблице даны величины стоимости основных фондов на конец года за ряд лет: Год 0 1 2 3 4 Стоимость основных фондов на конец года 100 120 125 135 140 Предположим, что стоимость фондов на конец года t совпадает со стоимостью на начало года t + 1. Среднегодовой коэффициент равен 0.3. Определить: а) среднегодовую стоимость основных фондов в 1, 2, 3 и 4 году;
86 Глава 2. Описательная статистика б) среднегодовой темп прироста среднегодовой стоимости основных фон-дов за период с 1 по 4 годы. 15. В первые два года исленность занятых в экономике возрастала в среднем на 4% в год, за следующие три-на 5% и в последние три года среднегодо-вые темпы роста составили 103%. Определите среднегодовые темпы роста и базовый темп прироста численности занятых за весь период. 16. В первые три года численность безработных возрастала в среднем на 2% в год, за следующие три-на 4% и в последние два года среднегодовые темпы роста составили 103% . Определите среднегодовые темпы роста и базовый темп прироста численности безработных за весь период. 17. В таблице даны величины дохода (в%), приносимые капиталом за год: Год 1 2 3 4 Доходность 10 12 8 6 Определить среднегодовую доходность капитала в течение всего периода, если: а) позиция инвестора пассивна; б) позиция инвестора активна. 18. В первом квартале капитал возрастает на 20%, во втором-на 15%, в тре-тьем-на 10%, в четвертом-на 20%. Определите среднегодовую доход-ность капитала, если: а) позиция инвестора пассивна; б) позиция инвестора активна, т.е. он ежеквартально реинвестирует доход. 19. Во сколько раз вырастает ваш капитал за год, вложенный в начале года под 20% годовых, если вы а) не реинвестировали проценты; б) реинвестировали их один раз в середине года; в) реинвестировали три раза в начале каждого очередного квартала; г) реинвестировали в каждый последующий момент времени. В первом квартале капитал возрастает на 12%, во втором-на 15%, в тре-тьем-на 20%, в четвертом-на 15%. Определите среднегодовую доход-ность капитала, если:
2.5. Упражнения и задачи 87 20. Объем продукции в 1995 г. составил 107% от объема продукции 1990 г., в течение последующих двух лет он снижался на 1% в год, потом за 4 года вырос на 9% и в течение следующих трех лет возрастал в среднем на 2% в год. На сколько процентов возрос объем продукции за вес период? На сколько процентов он возрастал в среднем в год в течение этого периода. 21. Дана функция распределения F(x) = 1/(1 + e−x). Найти медиану и моду данного распределения. 22. В эмпирическом распределении z0 = 0, все дельты = 1, F3 = 0.21, F4 = 0.4, F5 = 0.7, F6 = 0.77. Чему равны медиана и мода? 23. Известна гистограмма бимодального ряда наблюдений. На каком отрезке лежит медиана? 24. Медиана больше моды, где лежит среднее? Какая из трех характеристик центра распределения количественного призна-ка является квантилем и каким? Медиана и средняя равны, соответственно, 5 и 6.Каково вероятное значение моды? Почему? 25. Наоснове информации о возрасте всех присутствующихна занятиях (включая преподавателя) определить характер асимметрии функции распределения? 26. Дать определение 5%-го квантиля и написать интерполяционную формулу расчет 5%-го квантиля для эмпирического распределения.Привести графи-ческое обоснование формулы. 27. В эмпирическом распределении z0 = 0, все дельты = 1, F4 = 0.4, F5 = 0.7, F6 = 0.8, среднее равно 4.3. Какова асимметрия: правая (+) или левая (−) ? Чему равен 75%-ый квантиль? 28. Найти значение 30%-го квантиля, если известно эмпирическое распределе-ние: Границы интерва-лов 10-15 15-20 20-25 25-30 Частоты 1 3 4 2 29. Для ряда 1, 2, 3, 6 найти медианный и квартильный коэффициент вариации. 30. Чему равна ордината кривой Лоренца при абсциссе 13 для ряда 1, 2, 3? 31. Чему равен медианный коэффициент вариации для ряда 1, 2, 3?
88 Глава 2. Описательная статистика 32. Как посчитать децильный коэффициент вариации? 33. Задан ряд наблюдений за переменной x: 3, 0, 4, 2, 1. Подсчитать основ-ные статистики данного ряда, среднее арифметическое, медиану, дисперсию (смещенную и несмещенную), показатель асимметрии и куртозиса, размах выборки. 34. Для представленных ниже комбинаций значений показателей асимметрии δ3 и эксцесса δ4 дать графическое изображение совокупности и указать на графике положение моды, медианы и средней арифметической: а) δ3 > 0, δ4 > 3; б) δ3 < 0, δ4 > 3; в) δ3 < 0, δ4 < 3; г) δ3 > 0, δ4 = 3; д) δ3 = 0, δ4 > 3; е) δ3 < 0, δ4 = 3; ж) δ3 = 0, δ4 < 3. Рекомендуемая литература 1. Венецкий И.Г., Венецкая В.И. Основные математико-статистические по-нятия и формулы в экономическом анализе.-М.: Статистика, 1979. (Разд. 1-4, 6). 2. Догуерти К. Введение в эконометрику.-М.: Инфра-М, 1997. (Гл. 1). 3. Кейн Э. Экономическая статистика и эконометрия.-М.: Статистика, 1977. Вып. 1. (Гл. 4, 5, 7). 4. (*) Коррадо Д. Средние величины.-М.: Статистика, 1970. (Гл. 1). 5. Judge G.G., Hill R.C., GriffithsW.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch. 5).
Глава 3 Индексный анализ До сих пор термин "индекс" использовался исключительно как указатель места элемента в совокупности ("мультииндекс"-в сгруппированной совокупности). В данном разделе этот термин применяется в основном для обозначения показате-лей особого рода, хотя в некоторых случаях он используется в прежнем качестве; его смысл будет понятен из контекста. 3.1. Основные проблемы В экономической статистике индексом называют относительную величину, по-казывающую, во сколько раз изменяется некоторая другая величина при переходе от одного момента (периода) времени к другому (индекс динамики), от одного ре-гиона к другому (территориальный индекс) или в общем случае-при изменении условий, в которых данная величина измеряется. Так, например, в советской ста-тистике широкое распространение имел индекс выполнения планового задания, который рассчитывается как отношение фактического значения величины к ее плановому значению. Значение величины, с которым производится сравнение, часто называют ба-зисным (измеренным в базисных условиях). Значение величины, которое сравни-вается с базисным, называют текущим (измеренным в текущих условиях).Эта тер-минология сложилась в анализе динамики, но применяется и в более общей си-туации. Если y0 и y1 -соответственно базисное и текущее значение величины, то индексом ее изменения является λ01 y = y1 y0 .
90 Глава 3. Индексный анализ В общем случае речь идет о величинах yt, измеренных в условиях t = 0, . . . , T, и об индексах λrs y = ys yr , где r и s принимают значения от 0 до T , и, как правило, r < s. При таком определении система индексов обладает свойством транзитивности или, как говорят в экономической статистике, цепным свойством (нижний индекс-указатель опущен): λrs = λrt1λt1t2 - . . . -λtns, гд е r , s и все ti, i = 1, . . . , n также находятся в интервале от 0 до T , и, как следствие, свойством обратимости: λrs = 1 λsr , поскольку λtt = 1. Это-самое общее определение индексов, не выделяющее их особенности среди других относительных величин. Специфика индексов и сложность проблем, возникающих в процессе индексного анализа, определяется следующими тремя обстоятельствами. 1) Задача индексного анализа состоит в количественной оценке не только само-го изменения изучаемой величины, но и причин, вызвавших это изменение. Необ-ходимо разложить общий индекс на частные факторные индексы. Пусть (верхний индекс-указатель опущен) y = xa, (3.1) где y и x -объемные величины, a -относительная величина. Примерами таких "троек" являются: (a) объем производства продукта в стоимостном выражении, тот же объем производства в натуральном выражении, цена единицы продукта в на-туральном выражении; (b) объем производства, количество занятых, производительность труда; (c) объем производства, основной капитал, отдача на единицу капитала; (d) объем затрат на производство, объем производства, коэффициент удельных затрат. В общем случае формула имеет вид y = x n j=1 aj , (3.2) где все aj являются относительными величинами. Примером использования этой формулы при n = 2 может явиться сочетание приведенных выше примеров (a) и (b). В этом случае y -объем производства
3.1. Основные проблемы 91 продукта в стоимостном выражении, x -количество занятых, a1 -производи-тельность труда, a2 -цена единицы продукта. Этот пример можно усложнить на случай n = 3: a1 -коэффициент использования труда, a2 -"технологиче-ская" производительность труда, a3 -цена. Дальнейшие рассуждения будут, в основном, проводиться для исходной ситуа-ции ( n = 1, нижний индекс-указатель у a1 опускается). По аналогии с величиной λrs y , которую можно назвать общим индексом, рас-считываются частные или факторные индексы для x и a : λrs x = xs xr, λrs a = as ar . Первый из них можно назвать индексом количества, второй-индексом каче-ства. Оба частных индекса, как и общий индекс, транзитивны и обратимы. Кро-ме того, вслед за (3.1) выполняется следующее соотношение (верхние индексы-указатели опущены): λy = λxλa, и поэтому говорят, что эти три индекса облада-ют свойством мультипликативности. Таким образом, факторные индексы количе-ственно выражают влияние факторов на общее изменение изучаемой величины. 2) Пока неявно предполагалось, что величины y, x , a и, соответственно, все рассчитанные индексы характеризуют отдельный объект, отдельный элемент со-вокупности. Такие индексы называют индивидуальными, и их, а также связанные с ними величины, следует записывать с индексом-указателем i объекта (верх-ние индексы-указатели t, r, s опущены): yi, xi, ai, λyi, λxi, λai. До сих пор этот индекс-указатель опускался. Никаких проблем в работе с индивидуальными индексами не возникает, в частности, они по определению обладают свойством транзитивности и мультипликативности. Предметом индексного анализа являются агрегированные величины. Предпо-лагается, что yi аддитивны, т.е. выражены в одинаковых единицах измерения, и их можно складывать. Тогда (верхние индексы-указатели опущены) y = Ni=1 yi = Ni=1 xiai. В дальнейшем выражения типа Ni=1 xiai будут записываться как (x, a), т.е. как скалярные произведения векторов x и a. Благодаря аддитивности yi индексы λrs y рассчитываются однозначно и явля-ются транзитивными. Если xi также аддитивны, их сумму x = Ni=1 xi можно вынести за скобки и записать y = xa, гд е a -средняя относительная величина, равная (αx, a),
92 Глава 3. Индексный анализ Таблица 3.1 αrx αsx ar as λrs a λsr a = 1/λrs a 1 0.3 0.7 1.25 1.0 0.8 1.25 2 0.7 0.3 0.4 0.5 1.25 0.8 Итого 1.0 1.0 0.66 0.85 1.30 0.7 αxi = xix. Такая ситуация имеет место в приведенных выше примерах (b), (c), (d), если объемы производства и затрат измерены в денежном выражении. В этом случае все формулы, приведенные выше для индивидуальных индексов, остаются справедливыми. Индексы агрегированных величин обладают свойствами транзитивности и мультипликативности. Индексы агрегированных величин или собственно индексы должны обладать ещеоднимсвойством-свойством среднего.Это означает, что их значения не долж-ны выходить за пределы минимального и максимального значений соответствую-щихиндивидуальных индексов.Ссодержательной точки зрения это свойство весьма желательно. Иногда индексы так и определяются-как средние индивидуальных индексов. Например, индексы динамики-как средние темпы роста. Легко убедиться в справедливости следующих соотношений ( xi по-прежнему аддитивны): λrs y =i αryiλrs yi , где αryi = yri yr , λrs x =i αrxiλrs xi, где αrxi = xri xr , λrs a =i αraiλrs ai , где αrai = αsxiari αrxiari. Как видно из приведенных соотношений, индексы объемных величин являются средними индивидуальных индексов, т.к. суммы по i весов αryi и αrxi равны едини-це. Индекс же относительной величины этим свойством не обладает. В частности, он может оказаться больше максимального из индивидуальных индексов, если при переходе от условий r к условиям s резко возрастает вес αxi объекта с высоким показателем λrs a .Инаоборот, индекс средней относительной величины может ока-заться меньше минимального индивидуального индекса, если резко увеличивается вес объекта с низким относительным показателем. Эту особенность индекса относительной величины можно проиллюстрировать следующим числовым примером при N = 2 (см. табл. 3.1).
3.2. Способы построения индексов 93 При переходе от r к s резко увеличивается (с 0.3 до 0.7) доля 1-го объекта с высоким уровнем относительного показателя. В результате значение итогового индекса- 1.43 -оказывается больше значений обоих индивидуальных индек-сов- 0.8 и 1.25. При переходе от s к r ситуация противоположна (в данном случае индексы обратимы), и итоговый индекс меньше индивидуальных. Характерно, что этот парадокс возникает в достаточно простой ситуации, когда объемы xi аддитивны. 3) Собственно проблемы индексного анализа возникают в случае, когда xi неаддитивны. Такая ситуация имеет место в приведенном выше примере (а).Имен-но данный пример представляет классическую проблему индексного анализа. В его терминах часто излагается и сама теория индексов. Общий индекс, называемый в этом случае индексом стоимости, который рассчитывается по формуле λrs y = (xs, as) (xr, ar) , необходимо разложить на два частных факторных индекса (представить в виде произведения этих частных индексов): λrs x -индекс объема (физического объема) и λrs a -индекс цен. В случае аддитивности xi аналогичные проблемы возникают для индекса не объемной величины y, который раскладывается на факторные индексы естествен-ным образом (как было показано выше), а относительной величины a = yx.Общий индекс этой величины, называемый индексом переменного состава и удовлетво-ряющий соотношению λrs a = (αsx, as) (αrx, ar) , надо представить как произведение факторных индексов: λrs α -индекс структуры (структурных сдвигов) и λrs (a) -индекс индивидуальных относительных величин, называемый индексом постоянного состава. 3.2. Способы построения индексов Возникающая проблема разложения общего индекса на факторные индексы может решаться различным образом. Возможны следующие подходы: (1) λrs y = (xs, ar) (xr, ar) (xs, as) (xs, ar) = λrs x λrs a .
94 Глава 3. Индексный анализ Индекс объема считается как отношение текущей стоимости в базисных ценах к фактической базисной стоимости, а индекс цен-как отношение фактической текущей стоимости к текущей стоимости в базисных ценах. (2) λrs y = (xs, as) (xr, as) (xr, as) (xr, ar) = λrs x λrs a . В этом случае индекс объема рассчитывается делением фактической текущей стоимости на базисную стоимость в текущих ценах, а индекс цен-делением ба-зисной стоимости в текущих ценах на фактическую базисную стоимость. Оба эти варианта имеют очевидный содержательный смысл, но результаты их применения количественно различны, иногда-существенно. (3) Промежуточный вариант, реализуемый, например, если взять среднее гео-метрическое с равными весами индексных выражений (1) и (2) : λrs y = !(xs, ar) (xr, ar) (xs, as) (xr, as)!(xs, as) (xs, ar) (xr, as) (xr, ar) = λrs x λrs a . (4) Индекс объема можно рассчитать как некоторое среднее взвешенное ин-дивидуальных индексов объема: λrs x = i αi (λrs xi)k1k , i αi = 1, где k, как правило, принимает значение либо 1 (среднее арифметическое), ли-бо 0 (среднее геометрическое), либо −1 (среднее гармоническое). А индекс цен по формуле λrs a = λrs y λrs x , так чтобы выполнялось мультипликативное индексное вы-ражение. (5) Обратный подход: λrs a = i αi (λrs ai )k1k , i αi = 1, λrs x = λrs y λrs a . Индекс объема в подходе (4) и инд екс цен в под ход е (5) можно находить и другими способами.
3.2. Способы построения индексов 95 (6−7) Например, их можно взять как некоторые средние индексов, определен-ных в подходах (1) и (2) (т.е. использовать другой вариант подхода (3)): λrs x = (xs, ar + as) (xr, ar + as) , λrs a = λrs y λrs x , λrs a = (xr + xs, as) (xr + xs, ar) , λrs x = λrs y λrs a . (8−9) Или рассчитать по некоторым нормативным ценам an и весам xn : λrs x = (xs, an) (xr, an) , λrs a = λrs y λrs x , λrs a = (xn, as) (xn, ar) , λrs x = λrs y λrs a . Подходы (4−5) при определенном выборе типа среднего и весов агрегирования оказываются эквивалентны подходам (1−2). Так, если в подходе (4) индекс объема взять как среднее арифметическое инди-видуальных индексов объема с базисными весами αry, то будет получено индексное выражение подхода (1), поскольку (xs, ar) (xr, ar) = yri λrs xi yr и, как прежде, αryi = yri yr . Аналогично, если в том же подходе (4) индекс объема рассчитать как сред-нее гармоническое индивидуальных индексов с текущими весами αsy, то получится индексное выражение подхода (2). Подход (5) окажется эквивалентным подходу (1), если в нем индекс цен опре-делить как среднее гармоническое индивидуальных индексов с текущими весами; он будет эквивалентен подходу (2), если индекс цен взять как среднее арифмети-ческое с базисными весами. Здесь приведено лишь несколько основных подходов к построению мультипли-кативных индексных выражений. В настоящее время известны десятки (а с неко-торыми модификациями-сотни) способов расчета индексов. Обилие подходов свидетельствует о том, что данная проблема однозначно и строго не решается. На этом основании некоторые скептики называли индексы способом измерения неизмеримых в принципе величин и ставили подсомнение саму целесообразность их применения. Такая точка зрения ошибочна. Во-первых, индексы дают единственную возможность получать количествен-ные макрооценки протекающих экономических процессов (динамика реального
96 Глава 3. Индексный анализ производства, инфляция, уровень жизни и т.д.), во-вторых, они и только они поз-воляют иметь практические приложения многих абстрактных разделов макроэко-номики как научной дисциплины. Так, например, даже самое элементарное макро-экономическое уравнение денежного обмена PQ = MV, где P -уровень цен, Q -товарная масса, M -денежная масса, V -ско-рость обращения денег, не имеет непосредственно никакого практического смысла, ибо ни в каких единицах, имеющих содержательный смысл, не могут быть измере-ны P и Q. Можно измерить лишь изменения этих величин и-только с помощью техники индексного анализа. Например, измеримыми могут быть переменные урав-нения денежного оборота в следующей форме: Y 0λ01 P λ01 Q = M1V 1, где Y 0 -валовой оборот (общий объем производства или потребления) базис-ного периода в фактических ценах. Проблема выбора конкретного способа построения индексов из всего множе-ства возможных способов решается на практике различным образом. В советской статистике был принят подход (1). Аргументация сводилась к сле-дующему. Количественный (объемный) признак является первичным по отноше-нию к качественному (относительному) и поэтому при переходе от базисных усло-вий к текущим сначала должен меняться он (количественный признак): x0, a0→ x1, a0→ x1, a1. Первый шаг этого "перехода" дает индекс объема, второй-индекс цен. Внятных разъяснений тому, почему количественный признак первичен и почему именно пер-вичный признак должен меняться первым, как правило, не давалось. Тем не менее, применение этого подхода делает весьма наглядным понятие объемов (производ-ства, потребления, . . . ) в сопоставимых или неизменных ценах. Действительно, пусть оценивается динамика в последовательные периоды вре-мени t = 0, . . . , N, и индексы для любого периода t > 0 строятся по отно-шению к одному и тому же базисному периоду t = 0. Тогда при использовании подхода (1) указанный выше "переход" для любого периода t > 0 принимает форму x0, a0→ xt, a0→ xt, at, и выстраивается следующая цепочка пока-зателей физического объема: (x0, a0), (x1, a0), . . . , (xt, a0), . . . , (xN, a0). Оче-видна интерпретация этих показателей-это объемы в сопоставимых (базисных) или неизменных ценах. Однако "наглядность" не всегда обеспечивает "правиль-ность". Об этом пойдет речь в пункте 3.6.
3.2. Способы построения индексов 97 Всовременной индексологии проблема выбора решается в зависимости от того, какому набору требований (аксиом, тестов) должны удовлетворять применяемые индексы. Требования-это свойства, которыми должны обладать индексы. Вы-ше были определены три таких свойства: мультипликативности, транзитивности и среднего. Приведенные выше подходы к построению индексов с этой точки зре-ния не одинаковы. Все они удовлетворяют требованию мультипликативности- по построению. А транзитивными могут быть, например, только в подходах (4−5) , при k = 0. Свойством среднего могут не обладать индексы цен подходов (4, 6, 8) и индексы объемов в подходах (5, 7, 9). Иногда добавляют еще одно требование-симметричности. Это требование означает, что оба факторных индекса должны рассчитываться по одной и той же формуле, в которой лишь меняются местами переменные и нижние индексы с x на a или наоборот. Из всех приведенных выше подходов только (3) приводит к индексам, отвечающим этому требованию. Многие экономисты считают это тре-бование надуманным. Так, например, даже при естественном разложении общего индекса, которое имеет место в случае аддитивности объемного признака, фактор-ные индексы асимметричны. При выборе способа расчета индексов полезно проводить математический ана-лиз используемых формул. В некоторых случаях эти математические свойства та-ковы, что результат расчета неизбежно будет содержать систематическую ошибку. Пусть, например, речь идет о расчете индекса цен как среднего индивидуальных индексов (подход (5)), и веса взвешивания остаются неизменными во времени. В данном случае (как и в ряде других случаев) имеет смысл проверить, как ведет себя индекс на осциллирующих рядах индивидуальных цен. Цены осциллируют-значит меняются циклически с периодом две единицы времени: λt, t+1 ai = 1 λt+1, t+2 ai , t= 0, 1, 2, . . . . Поэтому общий индекс цен за период времени, включающий четное количество временных единиц, всегда равен единице: λt, t+2T a = 1. Этот результат понятен, поскольку индивидуальные цены лишь колеблются, не из-меняя своего общего уровня. Этот же общий индекс можно рассчитать по цепному правилу: λt, t+1 a λt+1, t+2 a . . . λt+2T−1, t+2T a . Индекс в такой форме в дальнейшем будет называться цепным и обозначаться λt, t+1, ..., t+2T a или λt∗t+2T a , гд е" ∗ " заменяет последовательность временных под-периодов-единиц времени внутри общего периода.
98 Глава 3. Индексный анализ Рассчитанный таким образом индекс равен единице только при использовании сред-него геометрического (при k = 0) в расчете индексов за каждую единицу времени. Это проверяется непосредственной подстановкой формулы среднего геометриче-ского при неизменных во времени весах индивидуальных индексов. Из свойства мажорантности средних следует, что при использовании средних арифметических общий цепной индекс будет обязательно больше единицы, а при использовании средних гармонических-меньше единицы. Другими словами, результат будет ли-бо преувеличен, либо преуменьшен. Причем ошибка будет тем выше, чем длиннее рассматриваемый период(че м больше T ). Из этого следует два вывода: - при расчете общего индекса как среднего индивидуальных индексов веса не должны оставаться постоянными во времени, - общий индекс как среднее арифметическое индивидуальных индексов может преувеличить реальный рост изучаемой величины, а как среднее гармониче-ское-преуменьшить его. Формальный анализ индексного выражения позволяет выяснить, с какими по-грешностями связано его использование при изучении реальных процессов. Например, полезно исследовать, к каким погрешностям приводит нетранзитивность индексов. Как ужеотмечалось, вобщемслучае индексывсехприведенных вышеподходов не об-ладают свойством транзитивности. В частности, индекс цен подхода (1) не транзи-тивен, т.к. λ012 a = x1, a1(x1, a0) x2, a2(x2, a1) = x2, a2(x2, a0) = λ02 a . Вопрос о том, какая из этих величин больше, сводится, как не сложно убедить-ся, путем элементарных преобразований к вопросу о соотношении следующих двух возможных значений индекса λ01 a : x1, a1(x1, a0) и x2, a1(x2, a0) , которые можно обозначить, соответственно, через λ01 a (1) и λ01 a (2). Их, в свою очередь, можно представить как средневзвешенные индивидуальных индексов цен λ01 ai (индексы-указатели опущены): λ (1) = (α (1) , λ), λ(2) = (α (2) , λ), где αi (1) = x1ia0i (x1, a0), αi (2) = x2ia0i (x2, a0) . Для рыночной экономики характерно сокращение объемов покупок товара при росте цен на него.Если предположить, что динамика цен и объемов устойчива в рассматри-ваемом периоде, и направленность их трендов (вверх или вниз) не меняется на нем
3.2. Способы построения индексов 99 (такое предположение необходимо сделать, т.к. динамика цен на подпериоде 01 связывается в данных индексах с динамикой объемов на подпериоде 12), то в таких условиях λ01 a (1) > λ01 a (2). Из этого следует, что для рыночной экономики значение цепного индекса λ012 a в под -ходе (1) больше значения соответствующего обычного индекса λ02 a . Аналогичный анализ индексов цен подхода (2) показывает, что для них характерно противоположное соотношение: цепной индекс принимает меньшее значение, чем обычный индекс за период времени. Несколько слов о терминах. Факторные индексы, используемые в подходах (1−2) , называются агрегатны-ми. Такие индексы были предложены немецкими экономистами Э. Ласпейресом и Г. Пааше во второй половине XIX века. Индекс Ласпейреса строится так, чтобы в числителе и знаменателе неизменными оставались объемы или цены на базис-ном уровне, поэтому знаменателем этого индекса является фактическая базисная стоимость, а числитель образован и базисными, и текущими значениями. Этот индекс является среднеарифметическим индивидуальных индексов с базисными весами. Таковыми являются индекс объема в подходе (1) и индекс цен подхо-да (2). В числителе и знаменателе индекса Пааше одинаковыми объемы или цены фиксируются на текущем уровне. Его числителем является фактическая текущая стоимость, знаменатель имеет смешанный состав. Такой индекс выступает средне-гармоническим индивидуальных индексов с текущими весами. Это-индекс цен подхода (1) и индекс объема подхода (2). В мультипликативном представлении общего индекса стоимости один из фак-торных индексов-индекс Ласпейреса, другой-Пааше. В 20-х годах XX века Фишером было предложено рассчитывать индексы как средние геометрические соответствующих индексов Ласпейреса и Пааше с равны-ми весами. Потому индексы подхода (3) называются индексами Фишера. Фишер показал, что в его системе тестов (требований, аксиом) они являются наилучшими из всех возможных (им рассмотренных). Индексы цен, рассчитанные каким-то способом, например, как в подходах (5), (7), (9), или заданные нормативно (при прогнозировании) с целью дальнейшего определения индексов объемов из требования мультипликативности иногда назы-вают дефляторами стоимости (например, дефляторами ВВП-валового внутрен-него продукта). А такой способ расчета индексов цен и объемов-дефлятирова-нием.
100 Глава 3. Индексный анализ На практике при построении индексов цен часто используют нормативный под-ход (9). Причем структуру весов обычно принимают облегченной-не по всем товарам (их, как правило, бывает много), а по товарам-представителям, кажд ый из которых представляет целуютоварную группу. Такой характер имеют, например, индексы цен по потребительской корзине, в которую включаются от нескольких десятков до нескольких сотен основных потребительских продуктов. Итак, рассмотрены основные проблемы и подходы, существующие при прове-дении индексного анализа, с помощью которого изучается вопрос о том, во сколько раз меняется значение величины при переходе от одних условий к другим-в це-лом и за счет отдельных факторов. 3.3. Факторные представления приростных величин Во многом схожие проблемы возникают и в анализе вопроса о том, на сколько и за счет каких факторов меняется значение изучаемой величины. В таком анализе общее изменение величины во времени или в пространстве требуется разложить по факторам, вызвавшим это изменение: ys − yr = Δrs y = Δrs x +Δrs a . В случае, когда y -результат (какая-то результирующая величина, напри-мер, объемпроизводства), x -затраты (например, основной капитал или занятые в производстве), a -эффективность использования затрат (отдача на капитал или производительность труда), то говорят о проблеме разложения общего прироста результирующей величины на экстенсивные и интенсивные факторы. При изучении изменений относительной величины at = αt, atво времени или в пространстве-в случае аддитивности объемных признаков xi -возни-кает аналогичная проблема разделения прироста этой величины Δrs a на факторы изменения структуры Δrs α и изменения индивидуальных относительных величин Δrs (a). Так, например, общее различие материалоемкости совокупного производства между двумя регионами можно попытаться разбить на факторы различия отрасле-вых структур производства и отраслевых материалоемкостей производства. Эти проблемы можно решить так же, как и в подходах (1−3) индексного ана-лиза. (1) Впод ход е (1) индексного анализа общийиндекс λrs y умножается и делится на величину (xs, ar), и после перегруппировки сомножителей получается искомое индексное выражение. Теперь, аналогично, к общему приросту Δrs y прибавляется и из него вычитается та же величина (xs, ar). После перегруппировки слагаемых
3.3. Факторные представления приростных величин 101 образуется требуемое пофакторное представление: Δrs y = [(xs, ar) − (xr, ar)] + [(xs, as) − (xs, ar)] = = (xs − xr, ar) + (xs, as − ar) = Δrs x +Δrs a . (2) Теперь работает величина (xr, as) : Δrs y = [(xs, as) − (xr, as)] + [(xr, as) − (xr, ar)] = = (xs − xr, as) + (xr, as − ar) = Δrs x +Δrs a . (3) Берется среднее арифметическое пофакторных представлений (1) и (2) с равными весами: Δrs y = xs − xr, ar + as 2 + xr + xs 2 , as − ar= Δrs x +Δrs a . Существует более общий подход, в рамках которого пофакторное представле-ние общего прироста результирующей величины строится на основе определенного мультипликативного индексного выражения λrs y = λrs x λrs a . Для относительного прироста результирующей величины можно записать сле-дующее тождество: λrs y −1 = (λrs x − 1) + (λrs a − 1) + (λrs x − 1)(λrs a − 1). Выражение для общего абсолютного прироста результирующей величины по-лучается умножением обеих частей этого соотношения на yr, равный (xr, ar). Первое слагаемое правой части этого тождества показывает влияние изме-нения объемной величины (экстенсивные факторы), второе слагаемое-влияние изменения относительной величины (интенсивныефакторы), а третье слагаемое- совместное влияние этих факторов. Эта ситуация иллюстрируется рисунком 3.1. B C E G F H A 1 1 D λa rs λx rs Рис. 3.1 Общему изменению результирующей величины соответствует площадь фигу-ры ABDFGH, влиянию объемного фак-тора-площадь ABCH, влиянию отно-сительного фактора-площадь GHEF, совместному влиянию факторов-пло-щадь HCDE. Вопрос получения искомо-го пофакторного представления общего прироста сводится к тому, как распреде-лить между факторами "вклад" их сов-местного влияния. Здесь возможны три подхода.
102 Глава 3. Индексный анализ (1) Все совместное влияние факторов можно отнести на относительный фак-тор: λrs y −1 = (λrs x − 1) + λrs x (λrs a − 1) = 1 yr (Δrs x +Δrs a ). В этом случае влиянию относительного фактора соответствует на рисунке пло-щадь GCDF, а влиянию объемного фактора-площадь ABCH. ( 2) Совместное влияние факторов относится на количественный фактор: λrs y −1 = (λrs x − 1) λrs a + (λrs a − 1) = 1 yr (Δrs x +Δrs a ). Теперь влиянию объемного фактора соответствует на рисунке площадь ABDE, а влиянию относительного фактора-площадь GHEF. Несложно убедиться в том, что в подходе (1) фактически к общему относи-тельному приросту λrs x λrs a − 1 прибавляется и отнимается индекс объемной вели-чины λrs x , а затем нужным образом группируются слагаемые; в подходе (2) - прибавляется и отнимается индекс относительной величины λrs a , а затем также нужным образом группируются слагаемые. Другими словами, имеется определен-ная аналогия с подходами (1) и (2). Можно сказать, что в подходе (1) сначала меняет свое значение объемный признак, а затем-относительный: 1 × 1 → λrs x × 1 → λrs x λrs a , и первый шаг в этом переходе определяет вклад объемного фактора, второй- относительного фактора. В подходе (2), наоборот, сначала меняется значение относительного признака, а затем-объемного: 1 × 1 → 1 × λrs a → λrs x λrs a , и теперь первый шаг перехода дает вклад относительного фактора, второй-объ-емного. (3) Берется среднее арифметическое с равными весами пофакторных пред-ставлений (1) и (2) : λrs y −1 = (λrs x − 1) 1 + λrs a 2 + 1 + λrs x 2 (λrs a − 1) = 1 yr (Δrs x +Δrs a ). Вэтом случае влияние объемногофактора выражаетплощадьтрапеции ABDH, а влияние относительного фактора-площадь трапеции GHDF. Итак, на основе каждого мультипликативного индексного выражения можно получить по крайней мере три пофакторных представления прироста изучаемой ве-личины. Причем, если неопределенность (множественность подходов) построения
3.3. Факторные представления приростных величин 103 индексного выражения связана с агрегированным характером изучаемой величины и неаддитивностью объемных факторных величин, то неопределенность пофактор-ных представлений приростов имеет место и для неагрегированных величин. Это объясняется тем, что она (неопределенность) является следствием наличия ком-поненты совместного влияния факторов, которую необходимо каким-то образом "разделить" между факторами. Пусть, например, используется индексное выражение подхода (1). На его ос-нове получается три следующих пофакторных представления общего прироста. (1 − 1) Δrs x = (xs − xr, ar) , Δrs a = (xs, as − ar) . Эти выражения полученыв результате подстановки индексовподхода (1) вфор-мулу подхода (1) и умножения на yr . Интересно, что результат совпадает с под-ходом (1). (1 − 2) Δrs x = (xs − xr, ar) (xs, as) (xs, ar) , Δrs a = (xs, as − ar) (xr, ar) (xs, ar) . (1 − 3) Δrs x = (xs − xr, ar) xs, ar+as 2 (xs, ar) , Δrs a = (xs, as − ar) xr+xs 2 , ar(xs, ar) . Теперь используется индексное выражение подхода (2). (2 − 1) Δrs x = (xs − xr, as) (xr, ar) (xr, as) , Δrs a = (xr, as − ar) (xs, as) (xr, as) . (2 − 2) Δrs x = (xs − xr, as), Δrs a = (xr, as − ar).
104 Глава 3. Индексный анализ Этот результат аналогичен подходу (2). (2 − 3) Δrs x = (xs − xr, as) xr, ar+as 2 (xr, as) , Δrs a = (xr, as − ar) xr+xs 2 , as(xr, as) . При всеммногообразии полученных пофакторных представлений прироста изу-чаемой величины все они являются "вариациями на одну тему": вклады объемного и относительного факторов определяются в результате различных скаляризаций, соответственно, векторов xs − xr и as − ar .Кроме того, несложно установить, что для индивидуальных (неагрегированных) величин подходы (1) ≡ (1−1) и (2−1) эквивалентны, также как подходы (1−2) и (2) ≡ (2−2) и под ход ы (3), (1−3) и (2−3), т.е. различия между ними, по существу, связаны с разными способами разделения совместного влияния факторов. 3.4. Случай, когда относительныхфакторов более одного Теперь можно дать обобщение подходов (1 − 3), (1− 3) и (1− 3) на случай, когда относительных факторов в мультипликативном выражении (3.2) два или более. Пусть n = 2, т.е. yt =i xtiat1iat2i. Для краткости будем далее использовать обозначение xt, at1, at2= i xtiat1iat2i. Речь идет о построении индексного выражения λrs y = λrs x λrs 1 λrs 2 в идеологии подходов (1 − 3), гд е λrs 1 и λrs 2 -индексы первого и второго отно-сительного признака. Построение мультипликативного индексного выражения зависит от того, в ка-кой последовательности факторные величины меняют свои значения от базисных к текущим. Пусть эта последовательность задана такой же, как и в исходном муль-типликативном выражении, т.е. сначала меняет свое значение объемный признак, затем первый относительный признак и, в последнюю очередь, второй относитель-ный признак: (xr, ar1, ar2) → (xs, ar1, ar2) → (xs, as1, ar2) → (xs, as1, as2).
3.4. Случай, когда относительных факторов более одного 105 Тогда λrs x = (xs, ar1, ar2) (xr, ar1, ar2), λrs 1 = (xs, as1, ar2) (xs, ar1, ar2), λrs 2 = (xs, as1, as2) (xs, as1, ar2) . Такой способ построения индексного выражения полностью аналогичен подхо-ду (1). Пусть теперь последовательность включения факторных величин измени-лась. Например, объемный признак по-прежнему меняет свое значение первым, затем-второй и, наконец, первый относительный признак: (xr, ar1, ar2) → (xs, ar1, ar2) → (xs, ar1, as2) → (xs, as1, as2). Тогда λrs x = (xs, ar1, ar2) (xr, ar1, ar2), λrs 1 = (xs, as1, as2) (xs, ar1, as2), λrs 2 = (xs, ar1, as2) (xs, ar1, ar2) . Общее количество возможных последовательностей включения факторных ве-личин равно числу перестановок из 3 элементов: 3! = 6, т.е. имеется 6 возможных мультипликативных индексных выражений. Аналогом индексного выражения (3) является среднее геометрическое с равными весами указанных 6-ти вариантов. Аналогичнымобразом строятся пофакторные представления типа (1−3) ити-па (1− 3). Во 2-м случае, если принята исходная последовательность "включе-ния" факторных признаков: 1 × 1 × 1 → λrs x × 1 × 1 → λrs x λrs 1 × 1 → λrs x λrs 1 λrs 2 , то Δrs x = yr (λrs x − 1) , Δrs 1 = yrλrs x (λrs 1 − 1) , Δrs 2 = yrλrs x λrs 1 (λrs 2 − 1); если относительные признаки в принятой последовательности меняются местами: 1 × 1 × 1 → λrs x × 1 × 1 → λrs x × 1 × λrs 2 → λrs x λrs 1 λrs 2 , то Δrs x = yr (λrs x − 1) , Δrs 1 = yrλrs x λrs 2 (λrs 1 − 1) , Δrs 2 = yrλrs x (λrs 2 − 1), и общее количество вариантов таких представлений- 6. Аналогом представле-ния (3) будет являться среднее арифметическое этих 6-ти вариантов с равными весами. В общем случае при n относительных величин в мультипликативном представ-лении результирующей величины имеется (n + 1)! вариантов индексных выра-жений, аналогичных (1−2), и пофакторных представлений, аналогичных (1−2)
106 Глава 3. Индексный анализ и (1−2) (в основном случае, рассмотренном в пунктах 3.1-3.2, n = 1, иимелось по 2 таких варианта). Усреднение этих вариантов с равными весами дает результа-ты, аналогичные, соответственно, подходам (3), (3) и (3). В пунктах 3.1-3.4 рассмотрены проблемы, которые возникают в практике по-строения индексных выражений и пофакторных представлений динамики резуль-тирующей величины. Проведенный анализ можно назвать прикладным. 3.5. Индексы в непрерывном времени Для лучшего понимания проблем, возникающих при индексном анализе, и воз-можностей решения этих проблем полезно рассмотреть их на примере индексов в непрерывном времени. Анализ индексов в непрерывном времени можно назвать теоретическим. В этом случае динамика объемных и относительных величин зада-ется непрерывными дифференцируемыми функциями y(t), x(t), a(t), и возможны три типа индексов: в момент времени t (моментные), сопоставляющие два мо-мента времени t1 и t0 ("момент к моменту") и два периода времени [t1, t1 + τ ] и [t0, t0 + τ ], τ |t1 − t0| ("периодк периоду"). Ниже рассматриваются эти три типа индексов. 1) Моментные индексы. Индивидуальными индексами такого типа являются моментные темпы роста, рассмотренные в пункте 1.8 (нижние индексы-указатели объекта опущены): λ[ ] (t) = expd ln [ ] (t) dt , ln λ[ ] (t) = d ln [ ] (t) dt = Δλ[ ] (t), где λ[ ](t) -моментный темп роста, Δλ[ ](t) -моментный темп прироста, а на месте [ ] стоит либо y -для объемной результирующей величины (стоимости), либо x -для объемной факторной величины (объема), либо a -для относи-тельной величины (цены). Легко убедиться в том, что эти индивидуальные индексы вслед за (3.1) облада-ют свойством мультипликативности или, как говорят, удовлетворяют требованию (тесту) мультипликативности (здесь и далее при описании моментных индексов указатель на момент времени (t) опущен): ln λy = d ln(xa) dt = d ln x dt + d ln a dt = lnλx + lnλa, т.е. λy = λxλa. Вопрос о транзитивности моментных индексов обсуждается ниже в связи с пе-реходом к индексам "момент к моменту". Понять, как перемножаются индексы
3.5. Индексы в непрерывном времени 107 в бесконечной последовательности бесконечно малых моментов времени, можно только в интегральном анализе. Агрегированный моментный индекс или собственно моментный индекс объ-емной результирующей величины строится следующим образом (возвращаются нижние индексы-указатели объекта): ln λy = 1y dyi dt = 1ydyi dt =αi 1 yi dyi dt =αi ln λyi, где αi = yi y , т.е. λy = λαi yi . Таким образом, индекс результирующей величины есть средняя геометриче-ская индивидуальных индексов с весами-долями объектов в этой объемной резуль-тирующей величине. Как видно из приведенного доказательства, это-следствие аддитивности результирующей величины. Не сложно провести разложение общего индекса на факторные: ln λy = 1ydxiai dt = 1yai dxi dt + xi dai dt = = 1yyi 1 xi dxi dt + yi 1 ai dai dt =αi ln λxi +αi ln λai, т.е. λy = λαi xiλαi ai , и, если факторные индексы определить как λx =λαi xi, λa =λαi ai , то получается искомое мультипликативное выражение λy = λxλa. Чрезвычайно интересно, что и факторный индекс объемной величины, которая может быть неаддитивной, и факторный индекс относительной величины, которая принципиально неаддитивна, рассчитываются также, как общийиндекс аддитивной результирующей величины-как средние геометрические индивидуальных индек-сов. Причем во всех этих трех индексах используются одинаковые веса-доли объектов в результирующей величине. Итак, моментные индексы мультипликативны, транзитивны, что будет показа-но ниже, обладают свойством среднего и симметричны по своей форме. Следова-тельно, обсуждаемые выше проблемы являются следствием не принципиальных особенностей индексов, а разных способов привязки их ко времени.
108 Глава 3. Индексный анализ 2) Индексы "момент к моменту" (индексы за период времени). Индивидуальные индексы такого типа рассмотрены в пункте 1.8 как непрерыв-ные темпы роста за период(ниж ние индексы-указатели объектов опущены): λ[ ] (t0, t1) = et1 t0 ln λ[ ](t)dt = [ ] (t1) [ ] (t0) , где λ[ ](t0, t1) -индекс за период [t0, t1], а на месте [ ], как и прежде, стоит либо y -для объемной результирующей величины (стоимости), либо x -для объемной факторной величины (объема), либо a -для относительной величины (цены). Это выражение, прежде всего, означает транзитивность моментных индексов. Чтобы убедиться в этом, можно провести следующие рассуждения (указатель [ ] в этих рассуждениях опущен). Пусть моментный индекс в периоде [t0, t1] неизменен и равен λ(t1), тогда, вычислив t1 t0 ln λ(t)dt, можно увидеть, что λ (t0, t1) = λ (t1)t1−t0 , т.е. для того, чтобы привести моментные индексы к форме, сопоставимой с индек-сами за период, надо их возводить в степень, равную длине периода. Теперь, разбив общий период [t0, t1] на n равных подпериодов длиной τ = t1 − t0 n и обозначив t(j) = t0 + jτ , можно записать исходное выражение связи индекса за период с моментными индексами в следующем виде: ln λ (t0, t1) = n j=1 t(j) t(j−1) ln λ (t) dt. Пусть в каждом j-м подпериоде [t(j−1), t(j)] моментный индекс неизменен и равен λ(t(j)). Тогда из этого выражения следует, что λ (t0, t1) = ⎛⎝ n j=1 λ t(j)⎞⎠τ . В результате перехода к пределу при n→∞ получается соотношение, кото-рое можно интерпретировать как свойство транзитивности моментных индексов. Возведение цепного моментного индекса в степень τ необходимо, как было только что показано, для приведения его к форме, сопоставимой с индексом за период.
3.5. Индексы в непрерывном времени 109 Индивидуальные индексы "момент к моменту" транзитивны по своему опреде-лению: ln λ[ ] (t0, t2) = t2 t0 ln λ[ ] (t) dt = t1 t0 ln λ[ ] (t) dt + t2 t1 ln λ[ ] (t) dt = = lnλ[ ] (t0, t1) + lnλ[ ] (t1, t2) , т.е. λ[ ](t0, t2) = λ[ ](t0, t1) - λ[ ](t1, t2). Их мультипликативность следует непосредственно из мультипликативности мо-ментных индексов. Действительно: ln λy (t0, t1) = t1 t0 (ln λx(t) + lnλa(t)) ←−−−−−−−−−−−−→ ln λx(t)λa(t) ←−−−−−→ λy(t) dt = lnλx(t0, t1) + lnλa(t0, t1), т.е. λy (t0, t1) = λx (t0, t1) - λa (t0, t1) . Теперь рассматриваются агрегированные индексы "момент к моменту" (воз-вращаются нижние индексы-указатели объекта). Индексы такого типа были пред-ложены в конце 20-х годов XX века французским статистиком Ф. Дивизиа, и по-этому их называют индексами Дивизиа. Как было показано выше, моментный индекс результирующей величины яв-ляется средним геометрическим индивидуальных индексов. Для индекса Дивизиа результирующей величины такое свойство в общем случае не выполняется. Дей-ствительно: ln λy (t0, t1) =i t1 t0 αi (t) lnλyi (t) dt, и, если бы веса αi(t) не менялись во времени, их можно было бы вынести за знак интеграла и получить выражение индекса как среднего геометрического индивиду-альных индексов. Однако веса меняются во времени, и такую операцию провести нельзя.Можно было бы ввести средние за период веса по следующему правилу: αi (t0, t1) = αi (t) lnλyi (t) dt ln λyi (t) dt , и получить выражение ln λy (t0, t1) =αi (t0, t1) lnλyi (t0, t1), (3.3)
110 Глава 3. Индексный анализ которое являлось бы средним геометрическим, если бы сумма средних за период весов равнялась единице. Но равенство единице их суммы в общем случае не гарантировано. Имеется один частный случай, когда общий индекс является средним геомет-рическим индивидуальных. Если индивидуальные моментные индексы не меняются во времени и, как было показано выше, равны (λyi (t0, t1))1/(t1−t0) , то их можно вынести за знак интеграла и получить выражение, аналогичное по форме (3.3): ln λy (t0, t1) =αi (t0, t1) lnλyi (t0, t1), где теперь αi (t0, t1) = 1 t1 − t0 t1 t0 αi (t) dt -средние хронологические весов.Их сум-ма равна единице, т.к. αi (t) = 1: αi (t0, t1) = 1 t1 − t0 t1 t0 αi (t) dt = 1 t1 − t0 t1 t0 dt = 1. Тем не менее, индекс Дивизиа результирующей величины обладает свойством среднего в общем случае. В силу аддитивности yi, этот индекс является обычной средней относительной и, как отмечалось в пункте 2.2, может быть представлен как среднее арифметическое индивидуальных индексов с базисными весами (по знаменателю) или среднее гармоническое индивидуальных индексов с текущими весами (по числителю). Вслед за мультипликативностью моментных индексов, индексы Дивизиа так-же мультипликативны. В этом не сложно убедиться, если определить факторные индексы Дивизиа естественным образом: ln λx (t0, t1) = t1 t0 ln λx (t) dt =t1 t0 αi (t) lnλxi (t) dt, ln λa (t0, t1) = t1 t0 ln λa (t) dt =t1 t0 αi (t) lnλai (t) dt. Действительно: ln λy (t0, t1) = t1 t0 ln λx (t) λa (t) " λy#($t) % dt = t1 t0 ln λx (t) dt + t1 t0 ln λa (t) dt = = lnλx (t0, t1) + lnλa (t0, t1) ,
3.5. Индексы в непрерывном времени 111 т.е. λy (t0, t1) = λx (t0, t1) - λa (t0, t1). Факторные индексы не могут быть представлены как средние геометрические индивидуальных индексов-кроме частного случая, когда индивидуальные мо-ментные индексы неизменны во времени. Это было показано на примере индекса результирующей величины. В случае аддитивности xi факторный индекс объема все-таки обладает свойством среднего (как и индекс результирующей величины). В общем случае факторные индексы требованию среднего не удовлетворяют. Непосредственно из определения индексов Дивизиа следует их транзитивность. Но факторные индексы этим свойством обладают в специфической, не встречав-шейся ранее форме. До сих пор при наличии транзитивности общий за период индекс можно было рассчитать двумя способами: непосредственно по соотноше-нию величин на конец и начало периода или по цепному правилу-произведением аналогичных индексов по подпериодам: λ (t0, tN) = λ (t0, t1) - λ (t1, t2) - . . . - λ (tN−1, tN), t0 < t1 < . . . < tN. Именно выполнение этого равенства трактовалось как наличие свойства тран-зитивности. Теперь (для факторных индексов Дивизиа) это равенство-определе-ние общего индекса (t0, tn), т.к. другого способа его расчета-непосредственно по соотношению факторных величин на конец и начало общего периода-не су-ществует. В частности, общий за периодфак торный индекс зависит не только от значений факторных величин на начало и конец периода, но и от всей внутрипери-одной динамики этих величин. Эту особенность факторных индексов Дивизиа можно проиллюстрировать в случае, когда моментные темпы роста всех индивидуальных величин неизменны во времени. В этом случае, как было показано выше (указатели периода времени (t0, t1) опу-щены), λy =λαi yi, λx =λαi xi, λa =λαi ai , (3.4) где αi -средние хронологические веса по результирующей величине y. Пусть N = 2, тогда выражение для этих средних хронологических весов можно найти в аналитической форме. Для периода (0, 1) ( t0 = 0, t1 = 1, из таблицы неопределенных интегралов: dx b + ceax = xb − 1 ab ln (b + ceax)): α1 = 1 0 y1 (0) λy1t y1 (0) λy1t + y2 (0) λy2t dt = ln λy2 λy ln λy2 λy1 , (3.5)
112 Глава 3. Индексный анализ Таблица 3.2. Объемы производства и цены в три последовательных момента времени Моменты времени 0 1 2 i y x a y x a y x a 1 20 10 2 30/45 12/18 2.5 60 20 3 2 10 10 1 30 15 2 90 30 3 Итого 30 60/75 150 α2 = 1 0 y2 (0) λy2t y1 (0) (λy1)t + y2 (0) λy2t dt = ln λy1 λy ln λy1 λy2 , где λy, λy1, λy2 -общий (агрегированный) и индивидуальные индексы "момент к моменту" для результирующей величины y. Указанная особенность факторных индексов Дивизиа иллюстрируется на примере, исходные данные для которого при-ведены в двух таблицах 3.21 и 3.3. Динамика физических объемов дана в 2 вариантах (через знак "/"). Физический объем 1-го продукта в момент времени " 1 " в варианте (а) составляет 12 единиц, в варианте (б)-18. Это-единственное отличие вариантов. Результаты расчетов сведены в двух таблицах 3.4 и 3.5. Расчет средних хронологических весов за периоды (0, 1) и (1, 2) в 1-й результирую-щей таблице проводился по формулам (3.5), индексы 2-й таблицы за периоды (0, 1) 1Физические объемы производства продуктов имеют разные единицы измерения (например, тон-ны и штуки) и не могут складываться, т.е. x неаддитивен. Таблица 3.3. Индексы наблюдаемых величин Периоды времени (0,1) (1,2) (0,2) i λy λx λa λy λx λa λy λx λa 1 1.5/2.25 1.2/1.8 1.25 2/1.333 1.667/1.111 1.2 3 2 1.5 2 3 1.5 2 3 2 1.5 9 3 3 Итого 2/2.5 2.5/2 5
3.5. Индексы в непрерывном времени 113 Таблица 3.4. Веса индивидуальных индексов Варианты (а) (б) Моменты и периоды времени Моменты и периоды времени i 0 (0, 1) 1 (1, 2) 2 0 (0, 1) 1 (1, 2) 2 1 0.667 0.585 0.5 0.450 0.4 0.667 0.634 0.6 0.5 0.4 2 0.333 0.415 0.5 0.550 0.6 0.333 0.366 0.4 0.5 0.6 Итого 1 1 1 1 1 1 1 1 1 1 и (1, 2) рассчитывались по формулам (3.4), а за период (0, 2) -в соответствии с определением по цепному правилу. Данный пример показывает, что даже относительно небольшое изменение"внутрен-ней" динамики-увеличение физического объема 1-го товара в "средний" момент времени " 1 " с 12 до 18 единиц-привело к увеличению индекса физического объ-ема за весь период (0, 2) с 2.426 до 2.510 и к соответствующему снижению индекса цен с 2.061 до 1.992. "Концевые" (на начало и конец периода) значения факторных величин при этом оставались неизменными. В обоих вариантах факторные индексы транзитивны, поскольку индексы за период (0, 2) равны произведению индексов за периоды (0, 1) и (1, 2). Можно сказать, что факторные индексы Дивизиа обладают свойством тран-зитивности в усиленной дефинитивной форме, т.к. это свойство определяет сам способ расчета индексов за периоды, включающие подпериоды. Такая особенность факторных индексов в конечном счете является следствием того, что физический объем x(t), как таковой, и относительная величина a(t) в общем случае не на-Таблица 3.5. Индексы Дивизиа Варианты (а) (б) Периоды λy λx λa λy λx λa (0, 1) 2.0 1.316 1.519 2.5 1.684 1.485 (1, 2) 2.5 1.843 1.357 2.0 1.491 1.342 (0, 2) 5.0 2.426 2.061 5.0 2.510 1.992
114 Глава 3. Индексный анализ блюдаемы, и для их измерения, собственно говоря, и создана теория индексов, в частности индексов Дивизиа. Полезно напомнить, что индекс Дивизиа результи-рующей величины и все индивидуальные индексы Дивизиа удовлетворяют требо-ванию транзитивности в обычной форме. Итак, индексы "момент к моменту" продолжают удовлетворять требованиям мультипликативности, транзитивности (в дефинитивной форме), симметричности, но теряют свойство среднего. Факторные индексы Дивизиа обычно записывают в следующей форме: λx (t0, t1) = exp⎛⎝ t1 t0 ai (t) dxi (t) xi (t) ai (t) ⎞⎠, λa (t0, t1) = exp⎛⎝ t1 t0 xi (t) dai (t) xi (t) ai (t) ⎞⎠. В том, что это форма эквивалентна используемой выше, легко убедиться. Для этого достаточно вспомнить, что, например для индекса объемной величины: αi (t) = xi (t) ai (t) xi (t) ai (t) , ln λxi (t) = d ln xi (t) dt = 1 xi (t) dxi (t) dt . Индексы Дивизиа могут служить аналогом прикладных индексов, рассмотрен-ных в пунктах 1-3 данного раздела, в случае, если речь идет о величинах x и y типа запаса, поскольку такие величины измеряются на моменты времени. 3) Индексы "период к периоду". Чаще всего предметом индексного анализа является динамика величин типа потока, поэтому именно непрерывные индексы "период к периоду" являются наи-более полным аналогом прикладных индексов, рассмотренных в пунктах 1-3 этого раздела. Сначала необходимо определить следующие индивидуальные величины (здесь и далее нижний индекс-указатель объекта i опущен): y (t, τ) = t+τ t y tdt-результирующая величина в периоде [t, t + τ ], x (t, τ) = t+τ t x tdt-объемная величина в периоде [t, t + τ ], a (t, τ) = y (t, τ ) x (t, τ ) =t+τ t αxtatdt-относительная величина в периоде [t, t + τ ], где αx (t) = x (t) x (t, τ ) -временные веса относительной величины.
3.5. Индексы в непрерывном времени 115 Таким образом, при переходе к суммарным за период величинам проявилось принципиальное различие объемных и относительных величин. Первые аддитивны во времени и складываются по последовательным моментам времени, вторые- неаддитивны и рассчитываются за период как средние хронологические с весами, определенными динамикой объемной факторной величины. Именно с этим обсто-ятельством связана возможная несимметричность факторных индексов, которая имеет место для большинства прикладных индексов, рассмотренных в пункте 3.2. Индивидуальные индексы "период к периоду" строятся естественным спосо-бом: λ[ ] (t0, t1, τ) = [ ] (t1, τ) [ ] (t0, τ) , где λ[ ](t0, t1, τ) -индекс, сопоставляющий периоды [t1, t1 + τ ] (текущий) и [t0, t0 + τ ] (базисный), а на месте [ ], как и прежде, стоит либо y -для объем-ной результирующей величины (стоимости), либо x -для объемной факторной величины (объема), либо a -для относительной величины (цены). Если динамика (траектория изменения) показателя [ ](t) в базисном и текущем периодах одинакова, то для любого t ∈ [t0, t0 + τ ] индекс "момент к моменту" λ[ ](t, t+t1−t0) неизменен и равен λ[ ](t0, t1). Тогда для любого t ∈ [t1, t1 + τ ] име-ет место равенство [ ] (t) = [ ](t − t1 + t0) λ[ ] (t0, t1), и индекс "период к периоду" объемной величины ( [ ] -есть либо y, либо x) можно представить следующим образом: λ[ ] (t0, t1, τ) = ←−−−−−−−−−=−[−](−t1−,τ−)−−−−−−−−→ t1+τ t1 [ ](t − t1 + t0) ←−=−−co−n−st−→ λ[ ] (t0, t1) dt t0+τ t0 [ ](t) dt ←−−−−−−→ =[ ](t0,τ ) = = λ[ ] (t0, t1) t1+τ t1 [ ] (t − t1 + t0) dt t0+τ t0 [ ] (t) dt ←−−−−−−−−−−−−−−→ =1 = λ[ ] (t0, t1) , т.е. он совпадает с индексом "момент к моменту". Для того чтобы такое же равенство имело место для индексов относительной величины, необходима идентичность динамики в базисном и текущем периодах времени не только самой относительной величины, но и объемной факторной ве-личины.Иначе веса αx(t) в базисном и текущем периодах времени будут различны
116 Глава 3. Индексный анализ и интегралы в числителе и знаменателе выражения индекса "период к периоду" относительной величины (после выноса λa(t0, t1) за знак интеграла в числителе) не будут равны друг другу. Тогда, если в базисном и текущем периодах времени одинакова динамика всех индивидуальных величин, то индексы "период к периоду" совпадают с индексами Дивизиа. Чаще всего считается, что различия в динамике индивидуальных величин в базисном и текущем периодах времени не существенны, и в качестве непрерыв-ных аналогов прикладных индексов поэтому принимают индексы Дивизиа. Именно на таком допущении построено изложение материала в следующем пункте. В случае, если указанные различия в динамике величин принимаются значимы-ми, приходится вводить поправочные коэффициенты к индексам Дивизиа, чтобы приблизить их к индексам "период к периоду". Теоретический анализ таких индекс-ных систем в непрерывном времени затруднен и не дает полезных для практики результатов. 3.6. Прикладные следствия из анализа индексов в непрерывном времени Теоретически "правильными" в этом пункте принимаются индексы Дивизиа. Это предположение можно оспаривать только с той позиции, что внутренняя дина-мика сопоставляемых периодов времени существенно различается. Здесь предпо-лагается, что эти различия не значимы. Из проведенного выше анализа индексов Дивизиа следует по крайней мере три обстоятельства, важные для построения прикладных индексов. 1) Факторные индексы за период, включающий несколько "единичных" под-периодов, правильно считать по цепному правилу, а не непосредственно из сопо-ставления величин на конец и на начало всего периода. Для иллюстрации разумно-сти такого подхода проведены расчеты в условиях примера, приведенного в конце предыдущего пункта. Результаты этих расчетов сведены в таблицу 3.6. Из приведенных данных видно, что - во-первых, агрегатные индексы, рассчитанные в целом за период(по "кон-цам"), не реагируют, по понятным причинам, на изменение внутренней ди-намики и одинаковы для вариантов (а) и (б); индекс Ласпейреса-особенно в варианте (а)-заметно преуменьшает реальный (по Дивизиа) рост физи-ческого объема, индексПааше, наоборот, преувеличивает этот рост. Вдругой (числовой) ситуации индексЛаспейреса мог быпреувеличивать, а индексПа-аше преуменьшать реальную динамику. Важно то, что оба эти индекса дают оценки динамики существенно отличные от реальной.
Прикладные следствия из анализа индексов 117 Таблица 3.6 Варианты (а) (б) Индексы: λy λx λa λy λx λa Дивизиа 5.0 2.426 2.061 5.0 2.510 1.992 В целом за период-(02) (1) Ласпейрес-Пааше 5.0 2.333 2.143 5.0 2.333 2.143 (2) Пааше-Ласпейрес 5.0 2.500 2.000 5.0 2.500 2.000 (3) Фишер 5.0 2.415 2.070 5.0 2.415 2.070 По цепному правилу-(012) (1) Ласпейрес-Пааше 5.0 2.383 2.098 5.0 2.493 2.005 (2) Пааше-Ласпейрес 5.0 2.469 2.025 5.0 2.525 1.980 (3) Фишер 5.0 2.426 2.061 5.0 2.509 1.993 - во-вторых, рассчитанные по цепному правилу индексы имеют более реали-стичные значения. Так, например, реальный рост физического объема в ва-рианте (а), равный 2.426, заметно преуменьшенный индексом Ласпейреса в целом за период- 2.333, получает более точную оценку тем же индексом Ласпейреса, рассчитанным по цепному правилу,- 2.383. Цепные индексы дают более правильные оценки динамики. Но, вообще говоря, это свойство цепных индексов не гарантировано. Так, в варианте (б)физический рост 2.510 преуменьшен индексом Пааше в целом за период- 2.500 (хотя и в мень-шей степени, чем индексом Ласпейреса- 2.333), и преувеличен этим же индексом по цепному правилу- 2.525. Принимая предпочтительность цепного правила, следует с сомнением отне-стись к принятым правилам расчета объемов в неизменных (сопоставимых) це-нах: (x0, a0), (x1, a0), . . . , (xt, a0), . . . , (xN, a0) (см. п. 3.2). Правильнее считать физический объем в году t в ценах, сопоставимых с базисным периодом, как y0λ01 x - . . . - λt−1, t x или yt/λ01 a - . . . - λt−1, t a . В этом случае теряется наглядность, но приобретается соответствие теории. Следует отметить, что в действующей сей-час Системе национальных счетов, рекомендованных ООН в 1993 г. для использо-вания национальными правительствами, при расчете индексов применяется цепное правило, но при расчете физических объемов в неизменных ценах-обычный под-
118 Глава 3. Индексный анализ ход, основанный на индексахПааше в целом за период. Это противоречие остается, по-видимому, для сохранения принципа наглядности. 2) Индексы Дивизиа рассчитываются как средние индивидуальных индексов с некоторыми весами, занимающими промежуточное положение между базисным и текущим моментами (периодами) времени. Из рассмотренных прикладных ин-дексов такому подходу в большей степени удовлетворяют индексы Фишера. Действительно, в рассматриваемом примере индекс физического объема Фи-шера в целом за период- 0.415 -более точно отражает реальнуюдинамику, чем индексПааше или Ласпейреса-в варианте (а). В варианте (б) более точным ока-зывается индексфизического объема Пааше. Зато индексыФишера, рассчитанные по цепному правилу, дают практически точное приближение к реальной динамике. 3) Если предположить (как это делалось в предыдущем пункте), что индиви-дуальные моментные индексы всех величин не меняются во времени в отдельных периодах, то расчет индексов Дивизиа как средних геометрических индивидуаль-ных индексов становиться вполне операциональным.Сложность заключается лишь в определении средних хронологических весов по результирующей величине. Вслу-чае двух продуктов соответствующие интегралы, как это показано в предыдущем пункте, берутся аналитически. В общем случае их всегда можно найти численным приближением. Однако такой подход вряд ли применим в практике, поскольку он достаточно сложен с точки зрения вычислений и не обладает наглядностью хоть в какой-нибудь степени. Возможен компромисс, при котором веса для средних гео-метрических индивидуальных индексов находятся как средние базисных и текущих долей объектов в результирующей величине по формуле, более простой и нагляд-ной, чем интеграл теоретической средней хронологической. Для индекса результирующей величины, которая аддитивна по объектам, спра-ведливы следующие соотношения: λrs y =αriλrs yi = 1 αsi&λrs yi, где αri, αsi-доли объектов в результирующей величине, соответственно, в ба-зисном и текущем периодах времени. Теперь рассчитываются два индекса результирующей величины λrs y (r), λrs y (s) как средние геометрические индивидуальных индексов по весам, соответственно, базисного и текущего периодов: λrs y (r) =λrs yiαri , λrs y (s) =λrs yiαsi . По свойству мажорантности средних степенных: λrs y (r) < λrs y < λrs y (s),
Прикладные следствия из анализа индексов 119 и уравнение относительно γrs : λrs y = λrs y (r)γrs λrs y (s)1−γrs , будет иметь решение 0 < γrs < 1. Тогда αrs i = γrsαri+ (1 − γrs) αsiмогут сыграть роль средних хронологических весов в формулах индексов Дивизиа (соотношения, аналогичные (3.4)): λrs y =λrs yiαrs i , λrs x =(λrs xi)αrs i , λrs a =(λrs ai )αrs i . Теперь эти соотношения являются формулами расчета прикладных индексов, обладающих всеми свойствами теоретических индексовДивизиа: они мультиплика-тивны, транзитивны (в дефинитивной форме), симметричны и являются средними индивидуальных индексов. В прикладном анализе иногда используют похожие индексы, называемые по имени автора индексами Торнквиста. В их расчете в качестве γrs всегда прини-мают 0.5, и потому индекс результирующей величины Торнквиста не равен в общем случае его реальному значению. Предложенные здесь индексы можно назвать мо-дифицированными индексами Торнквиста. Для того чтобы оценить качество прикладных индексов, проводился численный эксперимент, в котором значения факторных признаков (объем и цена) задава-лись случайными числами (случайными величинами, равномерно распределенны-ми на отрезке [0, 1]), и определялись отклонения прикладных индексов от значения теоретического индекса Дивизиа (по абсолютной величине логарифма отношения прикладного индекса к теоретическому). Рассматривались три системы: 2 про-дукта- 2 периода (как в приводимом выше примере), 2 продукта- 3 периода, 3 продукта- 2 периода. В случае двух продуктов значения модифицированного индекса Торнквиста и индекса Дивизиа совпадают, т.к. уравнение λrs y = λrs y1αrs 1 λrs y21−αrs 1 имеет относительно αrs 1 единственное решение. Поэтому в этих случаях индекс Дивизиа сравнивался с индексами Ласпейреса, Пааше и Фишера, рассчитанными в целом за периоди по цепному правилу. В случае 3-х продуктов индекс Дивизиа, рассчитанный с использованием численной оценки интеграла среднехронологиче-ских весов (для этого единичный периодв ремени делился на 100 подпериодов), сравнивался также и с модифицированным индексом Торнквиста. В каждом из этих трех случаев проводилось около 1 000 000 численных расчетов, поэтому получен-ные оценки вероятностей достаточно точны. Оценки вероятности для случая "2 продукта- 2 периода" приведены в таб-лице 3.7. В этой же таблице стрелочками вверх и вниз отмечено, как меняются
120 Глава 3. Индексный анализ Таблица 3.7. Вероятности того, что индекс в подлежащем дает большую ошибку, чем индекс в сказуемом таблицы (для индексов объемной факторной величины) В целом за период По цепному правилу Ласпейрес Пааше Фишер Ласпейрес Пааше В целом за период Пааше 0.500 0 - - - Фишер 0.415 ↓↓ 0.415 ↓↓ 0 - - По цепному правилу Ласпейрес 0.482 ↑↓ 0.479 ↑↓ 0.524 ↑↑ 0 - Пааше 0.479 ↑↓ 0.482 ↑↓ 0.524 ↑↑ 0.500 0 Фишер 0.052 ↑↑ 0.052 ↑↑ 0.060 ↑↑ 0.053 ↑↑ 0.053 ↑↑ соответствующие показатели при переходе к ситуации "2 продукта- 3 периода" и д алее "3 продукта- 2 периода". По данным этой таблицы преимущество цепного правила проявляется не столь очевидно. Цепные индексы Ласпейреса иПааше лишь в 48%случаев (чуть меньше половины) дают более высокую ошибку, чем те же индексы, рассчитанные в целом за период. Это преимущество растет (падает соответствующий показатель вероят-ности) с увеличением числа объектов (продуктов) в агрегате и исчезает с увеличе-нием числа периодов (при 3-х периодах соответствующие вероятности становятся больше 0.5). Зато преимущество индекса Фишера становится явным. Рассчитан-ные в целом за период, эти индексы хуже соответствующих индексов Ласпейреса и Пааше только в 41.5% случаев, причем их качество повышается с ростом как числа объектов, так и количества периодов. Особенно "хороши" цепные индексы Фишера: они лишь в 5-6%случаев дают ошибку большую, чем любые другие ин-дексы. К сожалению, с ростом числа объектов и количества периодов их качество снижается. В ситуации "3 продукта- 2 периода" рассчитывались модифицированные индексы Торнквиста. Они оказались самыми лучшими. Вероятность того, что они дают более высокую ошибку, чем индексы Ласпейреса и Пааше, а также Фише-ра, рассчитанного в целом за период, на 2-3% ниже, чем для цепного индекса Фишера. Итак, можно сказать, что модифицированные индексы Торнквиста, рассчиты-ваемые как средние геометрические индивидуальных индексов с особыми весами,
Прикладные следствия из анализа индексов 121 в наилучшей степени соответствуют теории. Темне менее, в существующей практи-ке статистики индексы как средние геометрические величиныфактически не приме-няются. В действующей (рекомендованной ООН в 1993 г.) Системе национальных счетов применение индексов Торнквиста (обычных, не модифицированных) реко-мендуется лишь в весьма специфических ситуациях. Индексы как средние геометрические индивидуальных применялись в прак-тике статистики, в том числе в России и СССР, в первой трети ХХ века. Затем практически всеобщее распространение получили агрегатные индексы. Это про-изошло по крайней мере по двум причинам. Первая: агрегатные индексы наглядны и поэтому понятны. Вторая: средние геометрические величины, если веса взве-шивания принять за константы, весьма чувствительны к крайним значениям ин-дивидуальных индексов. Так, например, очень большое значение какого-то одного индивидуального индекса приведет к существенному преувеличению общего ин-декса (в крайней ситуации, когда базисное значение индивидуальной величины равно нулю, т.е., например, какой-то продукт в базисном периоде еще не про-изводился, общий индекс окажется бесконечным). Наоборот, очень малое зна-чение единственного индивидуального индекса существенно преуменьшит общий индекс (обратит его в ноль, если текущее значение соответствующей индивиду-альной величины равно нулю-данный продукт уже не производится в текущем периоде). Указанные доводы против среднегеометрических индексов вряд ли серьезны. По поводу первого из них следует еще раз напомнить, что наглядность и понятность нельзя считать критерием истины. Второй доводне выдерживает критики, посколь-ку резким изменениям могут подвергаться малые индивидуальные величины, ко-торые входят в среднюю с малыми весами и поэтому не могут заметно повлиять на ее уровень. В крайних ситуациях, когда индивидуальный индекс по какому-то объекту принимает нулевое или бесконечное значение, такой объект вообще не должен участвовать в расчете общего индекса (его вес в среднем геометрическом равен нулю). Действительно, λy (0, 1) = Ni=1yi (1) yi (0)αi(0,1), где по определению yi (1) yi (0) = λyi (0, 1), а αi (0, 1) = 1 0 yi (0) yi(1) yi(0)t Ni=1 yi (0) yi(1) yi(0)t dt = 1 0 yi (0)1−t yi (1)t Ni=1 yi (0)1−t yi (1)t dt.
122 Глава 3. Индексный анализ Далее рассматривается только компонента i-го объекта yi (1) yi (0)αi(0,1) (обознача-емая ниже ˜λyi), для которого либо yi(0), либо yi(1) равны нулю (продукт либо еще не производился в базисном моменте, либо уже не производится в текущем моменте времени). Пусть периодв ремени [0, 1] делится на n равных подпериодов, и tj -середи-на j-го подпериода. Тогда рассматриваемую компоненту ˜λyi можно приближенно представить выражением (в силу аддитивности интеграла) n j=1˜λyij , где˜λyij = yi (1) yi (0)yi (0)1−tj yi (1)tj Ni=1 yi (0)1−tj yi (1)tj 1n , которое в пределе при n → ∞ совпадет с исходным значением этой компоненты. При конкретном n < ∞ и любом tj , которое в таком случае обязательно больше нуля и меньше единицы, ˜λyij → 1, при yi(0) → 0 или yi(1) → 0.Это можно доказать аналитически, но проще показать численно. В первом случае ( yi(0) → 0) указанная величина ˜λyij стремится к едини-це сверху, во втором-снизу, т.е. в крайней ситуации, когда либо yi(0), либо yi(1) равны нулю, nj=1˜λyij равно единице. И в результате перехода в этом выражении к пределу при n→∞ оказывается, что компонента i-го объекта ˜λyi также равна единице. Это означает, что данный i-й объект не участвует в расчете среднегеомет-рического индекса. Индексы Дивизиа при гипотезе неизменности во времени всех индивидуальных моментных индексов, а вслед за ними-модифицированные индексы Торнкви-ста-должны рассчитываться по сопоставимому набору объектов (продуктов). Втакой набор входят только такие объекты, которые существовали как в базисном, так и в текущем периодах времени (только те продукты, которые производились и в базисном, и в текущем периодах). Это правило выступает дополнительным аргументом в пользу цепных индексов, поскольку за длительные периоды време-ни наборы объектов (продуктов) могут меняться заметно, тогда как их изменения за короткие единичные периоды не так существенны. Взаключение следует заметить, чтомультипликативные индексные выражения, построенные на основе индексов Дивизиа и модифицированных индексов Торнкви-ста, естественным образом обобщаются на случай более одного относительного фактора в мультипликативном представлении результирующей величины.
3.7. Факторные представления приростов в непрерывном времени 123 3.7. Факторные представления приростов в непрерывном времени Моментные приросты делятся на факторы естественным и однозначным обра-зом: Δλy (t) = d ln y (t) dt = d ln x (t) dt + d ln a (t) dt = Δλx (t) +Δλa (t). Принимая во внимание, что непрерывным за периодтемпом прироста Δλy (t0, t1) является ln λy (t0, t1), аналогично делятся на факторы и приросты за период(т .к. индексы "момент к моменту" мультипликативны): Δλy (t0, t1) = lnλy (t0, t1) = lnλx (t0, t1) + lnλa (t0, t1) = = Δλx (t0, t1) +Δλa (t0, t1) . В прикладном анализе такое правило деления приростов на факторы также вполне операционально, и его имеет смысл использовать. Каждому мультипликативному индексному выражению λrs y = λrs x λrs a следует сопоставить не три варианта факторных разложений (1−3), как в пункте 3.3, а одно: ln λrs y = lnλrs x + lnλrs a . Однако, поскольку ln λrs y = Δrs y yr , правильнее из этого факторного разложения определять лишь доли экстенсивных и интенсивных факторов: γrs x = ln λrs x ln λrs y , γrs a = ln λrs a ln λrs y , которые, в свою очередь, использовать в расчете вкладов факторов: Δrs x = γrs x Δrs y , Δrs a = γrs a Δrs y . Такой подход успешно работает при любом количестве относительных факторов в мультипликативном представлении результирующей величины. 3.8. Задачи 1. Определить пункты, которые являются выпадающими из общего ряда. 1.1. а) Ласпейрес, б) Пирсон, в) Фишер, г) Пааше; 1.2. а) Ласпейрес, б) Пааше, в) Фишер, г) Торнквист;
124 Глава 3. Индексный анализ 1.3. а) индекс, б) дефлятор, в) корзина, г) коробка; 1.4. а) Ласпейрес, б) транзитивность, в) мультипликативность, г) Пааше; 1.5. а) коммутативность, б) транзитивность, в) мультипликативность, г) симметричность; 1.6. а) Торнквист, б) цепное правило, в) транзитивность, г) Фишер; 1.7. а) прирост, б) экстенсивные, в) интегральные, г) интенсивные; 1.8. а) дефлятор, б) темп роста, в) индекс, г) темп прироста; 1.9. а) постоянного состава, б) относительная величина, в) структуры, г) стоимости; 1.10. а) цепное, б) обратимости, в) симметрии, г) среднего; 1.11. а) среднего, б) транзитивности, в) обратимости, г) цепное; 1.12. а) дефлятор, б) темп роста, в) симметрии, г) среднего; 1.13. а) частный, б) факторный, в) цен, г) стоимости; 1.14. а) непрерывность, б) Дивизиа, в) геометрическое, г) дискретность; 1.15. а) сопоставимый набор, б) цепное правило, в) Торнквист, г) Фишер. 2. Индексы стоимости и объема для совокупности из 2 товаров равны соответ-ственно 1.6 и 1.0. Стоимость в текущий период распределена между това-рами поровну. Индивидуальный индекс цен для одного из товаров равен 1.3, чему он равен для другого товара? 3. Объемы производства 2 товаров в базисном и текущем периодах равны 10, 20 и 30, 40 единиц, соответствующие цены- 2, 1 и 4, 3. Чему равны индексы объема Ласпейреса и Пааше? Чему равны те же индексы цен? 4. Объемы производства 2 товаров в базисном и текущем периодах равны 10, 20 и 30, 40 единиц, соответствующие цены- 2, 1 и 4, 3. Чему равен вес 1-го товара в индексе Торнквиста? Чему равны факторные индексы Дивизиа? 5. Объемы производства 2 товаров в базисном и текущем периодах равны 10, 20 и 20, 10 тыс. руб., материалоемкости их производства- 0.6, 0.5 и 0.7, 0.6. Чему равны индексы структурных сдвигов Ласпейреса и Пааше? Чему равны те же индексы постоянного состава? 6. Объемы производства 2 товаров в базисном и текущем периодах равны 10, 20 и 20, 10 тыс. руб., материалоемкости их производства- 0.6, 0.5 и 0.7, 0.6. Чему равны факторные индексы ("количества" и "качества") в "трой-ке": материальные затраты равны объемам производства, умноженным на материалоемкость?
3.8. Задачи 125 7. В 1999 годуВВПв текущих ценах составил 200 млрд. руб. В2000 годуВВПв текущих ценах вырос на 25%,а в сопоставимых снизился на 3%. Определите дефлятор ВВП. 8. Сумма удорожания продукции за счет повышения цен составила 200 млн. руб., прирост физического объема продукции составил 300 млн. руб. На сколько процентов повысились цены и возрос физический объем продукции, если стоимость продукции в базисном периоде составила 3 млрд. руб.? 9. Физический объем продукции возрос на 300 млн. руб., или на 20%. Це-ны снизились на 10%. Найти прирост стоимости продукции с учетом роста физического объема продукции и снижении цен. 10. Стоимость продукции в текущем периоде в текущих ценах составила 1600 млн. руб. Индекс цен равен 0.8 , индекс физического объема- 1.2. Опре-делить прирост стоимости продукции, в том числе обусловленный ростом физического объема продукции и снижением цен. 11. Расходы на потребительские товары составили 20 тыс. руб., что в текущих ценах больше соответствующих расходов прошлого года в 1.2 раза, а в сопо-ставимых ценах на 5% меньше. Определите индекс цен на потребительские товары и изменение их физического объема (абсолютно и относительно). 12. По данным, приведенным в таблице, рассчитайте: Показатель Продукт Базовый период Текущий период Объем производства, сталь 2400 3800 тыс. т чугун 3700 4800 Цена, сталь 1.5 3.0 тыс. руб./т чугун 1.0 0.8 а) индивидуальные и общие индексы изменения стоимости; б) индексы Ласпейреса, Пааше, Фишера цен и физического объема. 13. По данным, приведенным в таблице: Показатель Отрасль Базовый период Текущий период Валовый растениеводство 720 1800 выпуск животноводство 600 900 Численность растениеводство 200 250 занятых животноводство 300 330 
126 Глава 3. Индексный анализ а) рассчитайте производительность труда по отраслям и сельскому хозяй-ству в целом; б) рассчитайте одним из методов влияние изменения отраслевых показа-телей численности занятых и производительности труда на динамику валового выпуска сельского хозяйства. 14. По данным, приведенным в таблице, рассчитайте: Годы ВВП (текущие цены, трлн. руб.) Индексы+дефляторы ВВП (в разах к предыдущему году) 1990 0.644 1.2 1991 1.398 2.3 1992 19.006 15.9 1993 171.510 9.9 1994 610.592 4.1 1995 1630.956 2.8 а) ВВП России в 1991-1995 гг. в сопоставимых ценах 1990 г.; б) базовые индексы-дефляторы. 15. Используя один из подходов, вычислите индексы товарооборота,физического объема и цен в целом по мясопродуктам на основании данных из таблицы. Розничный товарооборот, Мясопродукты млрд. руб. март апрель Индекс цен, % Мясо 1128 1517 Колбасные изделия 2043 3120 Мясные консервы 815 1111 16. Вычислите общие индексы стоимости, физического объема и цен по закупкам мяса на основании данных из таблицы: Мясо Год Говядина Свинина Баранина Количество проданного мяса, тыс. т базисный отчетный 238 245 183 205 40 48 Закупочная цена за 1 т, тыс. руб. базисный 35 30 28 Закупочные цены в отчетном году по сравнению с базисным возросли на говядину-на 160%, свинину-на 80%, на баранину-на 50%.
3.8. Задачи 127 17. По данным, приведенным в таблице, рассчитайте: Показатель Регион Базовый период Текущий период Валовой Западная Сибирь 3600 4000 выпуск Восточная Сибирь 2700 2500 Производственные Западная Сибирь 2400 2500 затраты Восточная Сибирь 2000 2200 а) материалоемкость производства по регионам и Сибири в целом; б) индексы переменного и постоянного состава и структурных сдвигов ма-териалоемкости. 18. По данным, приведенным в таблице, рассчитайте: Показатель Подразделение Базовый период Текущий период 1Kй цех 80 160 Валовой выпуск 2Kй цех 120 90 1Kй цех 50 75 Основной капитал 2Kй цех 240 240 а) фондоотдачу по цехам предприятия и заводу в целом; б) индексы переменного и постоянного состава и структурных сдвигов фон-доотдачи. 19. Используя один из подходов, вычислите общие индексы стоимости, физиче-ского объема и цен по закупкам зерновых на основании следующих данных: Зерновые Год пшеница рожь гречиха Количество проданного зерна, тыс. т базисный отчетный 548 680 385 360 60 75 Закупочная цена за 1 т, тыс. руб. отчетный 7.2 7.0 12 Закупочные цены в отчетном году по сравнению с базисным возросли на пшеницу-на 60%, рожь-на 40%, гречиху- 50%.
128 Глава 3. Индексный анализ Рекомендуемая литература 1. Аллен Р. Экономические индексы.-М.: "Статистика", 1980. (Гл. 1, 5). 2. (*) Зоркальцев В.И. Индексы цен и инфляционные процессы.-Новоси-бирск: "Наука", 1996. (Гл. 1, 4-6, 15). 3. КёвешП. Теория индексов и практика экономического анализа.-М.: "Фи-нансы и статистика", 1990.
Глава 4 Введение в анализ связей Одна из задач статистики состоит в том, чтобы по данным наблюдений за при-знаками определить, связаны они между собой (зависят ли друг от друга) или нет. И если зависимость есть, то каков ее вид(линейный, квадратичный, логистический и т.д.) и каковы ее параметры. Построенные зависимости образуют эмпирические (эконометрические) модели, используемые в анализе и прогнозировании соответ-ствующих явлений. Часто задача ставится иначе: используя данные наблюдений, подтвердить или опровергнуть наличие зависимостей, следующих из теоретических моделей явления. Математические методы решения этих задач во многом идентич-ны, различна лишь содержательная интерпретация их применения. В этой главе даются самые элементарные сведения об этих методах. Более развернуто они представлены в следующих частях книги. 4.1. Совместные распределения частот количественных признаков Пусть имеется группировка совокупности по n признакам (см. п. 1.9), где n > 1, и NI -количество объектов в I-й конечной группе (группо-вая численность), т.е. частота одновременного проявления 1-го признака в i1-м полуинтервале, 2-го признака в i2-м полуинтервале и т.д., n-го признака в in-м полуинтервале (уместно напомнить, что I = i1i2 . . . in, см. п. 1.9). Как и прежде, αI = NI N -относительные частоты или оценки вероятности того,
130 Глава 4. Введение в анализ связей что zi1−1, 1 < x1 zi11, . . . , zin−1, n < xn zinn (если ij = 1, то левые строгие неравенства записываются как ). Пусть Δij (j) -длина ij-го полуинтервала в группировке по j-му фактору, а ΔI = nj=1Δij(j). Тогд а fI = αI ΔI -плотности относительной частоты совмест-ного распределения или оценки плотности вероятности. Очевидно1, что IKI=I1 αI = 1 , или I fIΔI = 1. (4.1) Далее: FI = II αI(FI = i1i1=1 . . . inin=1 αI-новая по сравнению с п. 1.9 опе-рация суммирования) или FI = II fIΔI(4.2) -накопленные относительные частоты совместного распределения, или оценки вероятностей того, что xj zij j, j = 1, . . . , n. F0 -оценка вероятности того, что xj < z0j , j = 1, . . . , n, т. е. F0 = 0. FIK = 1. Введенные таким образом совместные распределения частот признаков яв-ляются прямым обобщением распределения частоты одного признака, данного в пункте 2.1. Аналогичным образом можно ввести совместные распределения любого под-множества признаков, которое обозначено в пункте 1.9 через J, т.е. по группам более низкого порядка, чем конечные, образующим класс J. Для индексации этих групп в этом разделе будет использован 2-й способ (см. п. 1.9)-составной муль-тииндекс I(J), в котором и из I, и из J исключены все ∗. Так, инд екс 51(13) именует группу, в которой 1-й признак находится на 5-м уровне, 3-й-на 1-м, а остальные признаки "пробегают" все свои уровни. При 1-м способе (исполь-зуемом в п. 1.9) и при n = 3 эта группа именуется двумя мультииндексами 5∗1 и 1∗3. Введенное выше обозначение длин полуинтервалов Δij (j) построено по этому 2-му способу. Распределение частот признаков множества J, т.е. по группам класса J опре-деляется следующим образом. 1Операция такого суммирования объясняется в пункте 1.9, тогда же через IK был обозначен мультииндекс, в котором все факторы находятся на последнем уровне; в данном случае эту операцию можно записать так: k1i1=1 . . . knin=1 αI = 1.
4.1. Совместные распределения частот количественных признаков 131 NI(J) -частота, количество объектов, попавших в группу I(J). Если вер-нуться к обозначениям пункта 1.9 для мультииндекса этой группы- I(∗) (в пол-ном мультииндексе I все те позиции, которые соответствуют не вошедшим в J признакам, заменены на ∗, например: 51(13) → 5∗1, и воспользоваться введен-ной в том же пункте операцией I(∗), то NI(J) =I(∗)NI . Но в данном случае обозначение этой операции следует уточнить. Пусть ¯ J - множество тех признаков, которые не вошли в J, а операция '+' в соответствую-щем контексте такова, что J + ¯ J = G через G в п. 1.9 было обозначено полное множество факторов {12 . . . n}и I(J) + I( ¯ J) = I (например, 13 + 2 = 123 и 51(13) + 3(2) = 531), тогдаNI(J) =¯ J NI(J)+I( ¯ J), где суммирование ведется по всем уровням признаков указанного под знаком сум-мирования множества (далее операция мн-во призн. будет пониматься именно в этом смысле). αI(J) = NI(J) N -относительные частоты, которые, очевидно, удовлетворяют условию: JαI(J) = 1, fI(J) = αI(J) ΔI(J) -плотности относительной частоты, где ΔI(J) =JΔij (j) (операция такого перемножения объясняется в п. 1.9), FI(J) = I(J)I(J) αI(J) накопленные относительные частоты, где I(J) -те-кущие ("пробегающие") значения уровней признаков J. Такие распределения по отношению к исходному распределению в полном мно-жестве признаков называются маргинальными (предельными), поскольку накоп-ленные относительные частоты (эмпирический аналог функции распределения ве-роятностей) таких распределений получаются из накопленных относительных ча-стот исходного распределения заменой в них на предельные уровни kj факторов, не вошедших в множество J: FI(J) = FI(J)+IK( ¯ J). (4.3) Действительно, поскольку вследза NI(J) αI(J) =¯ J αI(J)+I( ¯ J), (4.4)
132 Глава 4. Введение в анализ связей тоFI(J) = I(J)I(J) αI(J) = I(J)I(J)¯ J αI(J)+I(J¯) = = I(J) I(J) I( ¯ J) IK( ¯ J) αI(J)+I( ¯ J)= FI(J)+IK( ¯ J). Кроме того, fI(J) =¯ J fI(J)+I( ¯ J)ΔI( ¯ J), (4.5) т.к. ΔI = ΔI(J)ΔI( ¯ J). Действительно: ¯ J fI(J)+I( ¯ J)ΔI( ¯ J) =¯ J αI(J)+I( ¯ J) ΔI(J)ΔI( ¯ J)ΔI( ¯ J) = 1 ΔI(J)¯ J αI(J)+I(J¯) = fI(J). Крайним случаем предельных распределений являются распределения частот отдельных признаков (см. п. 2.1), которые получаются, если множества J вклю-чают лишь один элемент (признак) из j = 1, . . . , n. Для таких распределений I (J) → ij (j). В частном, но достаточно важном случае при n = 2 частоты распределения обычно представляют в таблице сопряженности, или корреляционной таблице: 1 - - - i2 - - - k2 Y 1 N11 - - - N1i2 - - - N1k2 N1(1) ....... . . .... . . ...... i1 Ni11 - - - Ni1i2 - - - Ni1k2 Ni1(1) ....... . . .... . . ...... k1 Nk11 - - - Nk1i2 - - - Nk1k2 Nk1(1) Y N1(2) - - - Ni2(2) - - - Nk2(2) N
4.1. Совместные распределения частот количественных признаков 133 В этом случае существует только два маргинальных распределения частот - отдельно для 1-го признака (итоговый столбец таблицы сопряженности) и для 2-го признака (итоговая строка). Для частот и других параметров этих распределений удобнее и нагляднее 1-й способ обозначения: вместо Ni1(1) и N12(2) использует-ся, соответственно, Ni1∗ и N∗i2 . Этот способ обозначений удобен, если n мало, но описать общий случай, как это сделано выше, с его помощью весьма затрудни-тельно. Формулы (4.3) в случае двух признаков принимают вид(после запятой эти же формулы даются в обозначениях 1-го способа): Fi1(1) = Fi1k2, Fi1∗ = Fi1k2 ; Fi2(2) = Fk1i2, F∗i2 = Fk1i2 . Аналогично, для формул (4.5): fi1(1) = k2 i2=1 fi1i2Δi2(2), fi1∗ = k2 i2=1 fi1i2Δ∗i2 ; fi2(2) = k1 i1=1 fi1i2Δi1(1), f∗i2 = k1 i1=1 fi1i2Δi1∗. Если в таблице сопряженности разместить не частоты, а плотности относи-тельных частот, и на каждой клетке таблицы построить параллелепипед высотой, равной соответствующему значению плотности, то получится трехмерный аналог гистограммы, который иногда называют стереограммой. Ее верхнюю поверхность называют поверхностью двухмерного распределения. Если предположить, что N, k1, k2 → ∞, допуская при этом, что z01, z02 →−∞, а zk11, zk22 →∞, то f и F станут гладкими функциями f(x1, x2) и F(x1, x2), соответственно, распределения плотности вероятности и распре-деления вероятности. Это-теоретические функции распределения. Формулы (4.1-4.3, 4.5) записываются для них следующим образом: ∞ −∞ ∞ −∞ f (x1, x2) dx1dx2 = 1, F (x1, x2) = x1 −∞ x2 −∞ f x1, x2dx1dx2, F (x1) = F (x1,∞), F(x2) = F (∞, x2), f (x1) = ∞ −∞ f (x1, x2) dx2, f (x2) = ∞ −∞ f (x1, x2) dx1.
134 Глава 4. Введение в анализ связей Легко представить возможные обобщения таблицы сопряженности на случай n > 2. Ее аналогом является n-мерный прямоугольный параллелепипед, в ито-говых гранях которого (в таблице сопряженности таких граней две-итоговые столбец и строка) даны все возможные маргинальные распределения частот. Ито-говые грани-крайние, предельные, маргинальные части параллелепипеда. Это дает еще одно объяснение используемому термину-"маргинальные распределе-ния". Исходное распределение и любое маргинальное распределение частот строятся по всей совокупности. Однако важное значение имеют и распределения, построен-ные по отдельным частям выборки. Так, наряду с рассмотренным распределением частот признаков J по группам класса J, можно говорить о распределении частот признаков ¯ J (всех оставшихся признаков) по конечным группам в каждой отдель-ной группе класса J. Это-условные распределения частот. Они показывают распределения частот признаков ¯ J при условии, что все остальные признаки J зафиксированы на определенных уровнях I(J). В таблице сопряженности тако-выми являются распределения 1-го признака в каждом отдельном столбце, если J = 2, и распределения 2-го признака в каждой отдельной строке, если J = 1. αI( ¯ J) | I(J) = NI(J)+I( ¯ J) NI(J) -относительные частоты условного распределения признаков ¯ J по I(J). Если числитель и знаменатель правой части этой формулы поделить на N, то получится αI( ¯ J) | I(J) = αI(J)+I(J¯) αI(J) или αI( ¯ J) | I(J)αI(J) = αI(J)+I( ¯ J). (4.6) fI( ¯ J) | I(J) = αI( ¯ J) | I(J) ΔI( ¯ J) -плотности относительных частот условного распре-деления. Если левую часть равенства (4.6) разделить на ΔI( ¯ J)ΔI(J), а правую- на ΔI (оба этих делителя, как отмечено выше, равны), то получится аналогичное (4.6) равенство для плотностей: fI( ¯ J) | I(J)fI(J) = fI(J)+I( ¯ J). (4.7) В случае двух признаков и при использовании 1-го способа индексации: fi1∗ | ∗i2 = Ni1i2 N∗i2 1 Δi1∗, f∗i2 | i1∗ = Ni1i2 Ni1∗ 1 Δ∗i2 , Δi1∗ и Δ∗i2- результат использования первого способа индексации для Δi1(1) и Δi2(2); fi1∗ | ∗i2f∗i2 = fi1i2, f∗i2 | i1∗fi1∗ = fi1i2.
4.1. Совместные распределения частот количественных признаков 135 В результате объединения двух последних равенств и перехода к непрерывному случаю получаются известные формулы математической статистики об условных распределениях:f (x1 | x2) f (x2) = f (x1, x2) = f (x2 | x1) f (x1), из которых, в частности, следует тождество теоремы Байеса: f (x1 | x2) f (x2) = f (x2 | x1) f (x1). Далее, по определению, FI(J¯) | I(J) = I( ¯ J)I( ¯ J) αI( ¯ J) | I(J) -накопленные относительные частоты условного распределения. Правую часть этого равенства можно преобразовать: FI(J¯) | I(J) = I( ¯ J)I( ¯ J) NI(J)+I( ¯ J) NI(J) = N NI(J) I( ¯ J)I( ¯ J) NI(J)+I( ¯ J) N =FI(J)+I( ¯ J) FI(J) , т.е. для накопленных относительных частот получается соотношение такое же, как и для плотностей относительных частот f : FI( ¯ J) | I(J)FI(J) = FI(J)+I(J¯). (4.8) В непрерывном случае для двух признаков: F(x1 | x2)F(x2) = F(x1, x2) = F(x2 | x1)F(x1), F(x1 | x2)F(x2) = F(x2 | x1)F(x1). Количество параметров относительной частоты (также как и плотности отно-сительной частоты и накопленной относительной частоты) αI( ¯ J) | I(J) условного распределения признаков ¯ J по I(J) равно K ¯ J = ¯ J kj-числу всех возмож-ных сочетаний уровней признаков ¯ J. Таких условных распределений признаков ¯ J имеется KJ -для каждого возможного сочетания уровней факторов J. Так, при n = 2 в таблице сопряженности структура каждого столбца (результат деления элементов столбца на итоговый-сумму элементов) показывает относительные частоты условного распределения 1-го признака по уровням 2-го признака (если J = 2). Количество параметров относительной частоты каждого такого условного
136 Глава 4. Введение в анализ связей распределения- k1, а число столбцов-условных распределений- k2. Анало-гично-для строк таблицы сопряженности (если J = 1). Маргинальное распределение признаков ¯ J может быть получено из этой со-вокупности условных распределений (для плотностей относительных частот): fI( ¯ J) =J fI( ¯ J) | I(J)αI(J) (4.9) или fI( ¯ J) =J fI( ¯ J) | I(J)fI(J)ΔI(J). Действительно, в соответствии с (4.5) fI( ¯ J) =J fI(J)+I( ¯ J)ΔI(J), а, учитывая (4.7), J fI(J)+I( ¯ J)ΔI(J) =J fI( ¯ J) | I(J)αI(J). Соотношение, аналогичное (4.9), выполняется и для самих относительных ча-стот: αI( ¯ J) =J αI( ¯ J) | I(J)αI(J) (4.10) (оно получается умножением обеих частей соотношения (4.9) на ΔI( ¯ J)), а вследза ним и для накопленных относительных частот: FI(J¯) =J FI( ¯ J) | I(J)αI(J). (4.11) Такая связь условных и маргинального распределений наглядно иллюстриру-ется таблицей сопряженности (для относительных частот). Очевидно, что средне-взвешенный, по весам итоговой строки, вектор структур столбцов этой матрицы алгебраически есть вектор структуры итогового столбца. Аналогично-для строк этой матрицы (для условных и маргинального распределений 2-го признака). В непрерывном случае при n = 2 соотношение (4.9) имеет вид: f (x1) = ∞ −∞ f (x1 | x2) f (x2) dx2, f(x2) = ∞ −∞ f (x2 | x1) f (x1) dx1.
4.1. Совместные распределения частот количественных признаков 137 Если итоговые грани n-мерного прямоугольного параллелепипеда параметров распределения (обобщения таблицы сопряженности), как отмечалось выше, дают все возможные маргинальные распределения, то ортогональные "срезы" этого параллелепипеда (как строки и столбцы таблицы сопряженности) представляют все возможные условные распределения. Условные распределения, сопоставляющие в определенном смысле вариации признаков двух разных групп ¯ J и J, используются в анализе связей между этими двумя группами признаков. При этом чрезвычайно важно понимать следующее. Речь в данном случае не идет об анализе причинно-следственных связей, хотя фор-мально изучается поведение признаков ¯ J при условии, что признаки J принимают разные значения, т.е. признаки J выступают как бы "причиной", а признаки ¯ J - "следствием". Направление влияния в таком анализе не может быть определено. Это-предмет более тонких и сложных методов анализа. Более того, содержа-тельно признаки этих групп могут быть не связаны, но, если они одновременно зависят от каких-то других общих факторов, то в таком анализе связь между ними может проявиться. Такие связи в статистике называют ложными корреляция-ми (или ложными регрессиями). Поэтому всегда желательно, чтобы формальному анализу зависимостей предшествовал содержательный, в котором были бы сфор-мулированы теоретические гипотезы и построены теоретические модели. А ре-зультаты формального анализа использовались бы для проверки этих гипотез. То есть из двух задач статистического анализа связей, сформулированных в преамбуле к этому разделу, предпочтительней постановка второй задачи. Если признаки двух множеств ¯ J и J не зависят друг от друга, то очевид-но, что условные распределения признаков ¯ J не должны меняться при изменении уровней признаков J. Верно и обратное: если условные распределения признаков ¯ J одинаковы для всех уровней I(J), то признаки двух множеств ¯ J и J не зависят друг от друга. Таким образом, необходимым и достаточным условием независи-мости признаков двух множеств ¯ J и J является неизменность совместных рас-пределений признаков ¯ J при вариации уровней признаков J. Это условие можно сформулировать и в симметричной форме: неизменность совместных распределе-ний признаков J при вариации уровней признаков ¯ J. Для таблицысопряженности это условие означает, что структурывсех ее столб-цов одинаковы. Одинаковы и структуры всех ее строк. Итак, в случае независимости данных множеств признаков относительные ча-стоты αI( ¯ J) | I(J) не зависят от I(J) и их можно обозначить через ˜αI( ¯ J). Тогд а из соотношения (4.10) следует, что относительные частоты этого распределения совпадают с относительными частотами соответствующего маргинального распре-деления: ˜αI( ¯ J) = αI( ¯ J), т.к. JαI(J) = 1, и соотношения (4.6) приобретают вид: αI( ¯ J)αI(J) = αI(J)+I( ¯ J). (4.12)
138 Глава 4. Введение в анализ связей В случае двух признаков при использовании первого способа индексации: αi1∗α∗i2 = αi1i2 . Не сложно убедиться в том, что аналогичные соотношения в случае независи-мости признаков выполняются и для f и F: fI( ¯ J)fI(J) = fI(J)+I( ¯ J), (4.13) fi1∗f∗i2 = fi1i2 , а в непрерывном случае: f(x1)f(x2) = f(x1, x2), FI(J¯)FI(J) = FI(J)+I(J¯). (4.14) Fi1∗F∗i2 = Fi1i2 , F(x1)F(x2) = F(x1, x2). Любое из соотношений (4.12), (4.13), (4.14) является необходимым и достаточ-ным условием независимости признаков ¯ J и J. Необходимость следует из самого вывода этих соотношений. Достаточность легко показать, например, для (4.12). Так, если выполняется (4.12), то в соответствии с (4.4): αI( ¯ J) | I(J) = αI( ¯ J)+I(J) αI(J) = αI( ¯ J)αI(J) αI(J) = αI( ¯ J), т.е. условные распределения признаков ¯ J не зависят от уровней, которые занима-ют признаки J, а это означает, что признаки ¯ J и J не зависят друг от друга. Можно доказать, что из независимости признаков ¯ J и J следует взаимная независимость признаков любого подмножества ¯ J с признаками любого подмно-жества J. Пусть J = J1 + J2, тогда соотношение (4.12) можно переписать в форме: αI( ¯ J)αI(J1)+I(J2) = αI(J1)+I(J2)+I( ¯ J), и, просуммировав обе части этого выражения по J2 (т.е., в соответствии с введен-ной операцией J2 ,-по всем уровням признаков J2), получить следующее: αI( ¯ J)αI(J1) (4.4) = J2 αI( ¯ J)αI(J1)+I(J2) (4.12) = J2 αI(J1)+I(J2)+I( ¯ J) (4.4) = αI(J1)+I( ¯ J), т.е. αI( ¯ J)αI(J1) = αI(J1)+I(J¯), (4.15) что означает независимость признаков ¯ J и J1 в рамках маргинального распреде-ления признаков ¯ J + J1. Пусть теперь ¯ J = ¯ J1 + ¯ J2. После проведения аналогичных операций с (4.15) (в частности операции суммирования по ¯ J2) получается соотношение
4.1. Совместные распределения частот количественных признаков 139 αI( ¯ J1)αI(J1) = αI(J1)+I( ¯ J1), что означает независимость признаков ¯ J1 и J1 в рам-ках маргинального распределения ¯ J1 + J1. Что и требовалось доказать, т.к. ¯ J1 и J1 -любые подмножества ¯ J и J. Пока речь шла о независимости двух множеств признаков. Точно так же можно говорить и о независимости трех множеств. Пусть G = ¯ J + J1 + J2, гд е J = J1 + J2. Необходимым и достаточным усло-вием взаимной независимости этих трех множеств признаков является следующее равенство: αI( ¯ J)αI(J1)αI(J2) = αI(J1)+I(J2)+I( ¯ J). (4.16) Это соотношение получается, если в левой части (4.12) вместо αI(J) записать αI(J1)αI(J2), т.к. αI(J1)αI(J2) = αI(J1)+I(J2) ≡ αI(J) -известное условие незави-симости двух множеств признаков в рамках маргинального распределения призна-ков J. Необходимым и достаточным условием взаимной независимости всех призна-ков, входящих в множество J служит следующее соотношение: αI =J αij (j). (4.17) Это соотношение-результат завершения процесса дробления множеств при-знаков, который начат переходом от (4.12) к (4.16). Соотношения (4.12-4.14, 4.16-4.17) являются теоретическими. Оцененные по выборочной совокупности параметры совместных распределений, даже если со-ответствующие множества признаков независимы друг от друга, не могут обеспе-чить точное выполнение этих соотношений, поскольку они (параметры эмпириче-ских распределений) являются случайными величинами. Критерий независимости строится как определенный показатель (статистика), характеризующий степень нарушения равенств в указанных соотношениях. Использование этого критерия осуществляется как проверка статистической гипотезы (нулевая гипотеза: призна-ки данных групп не зависимы), логика которой описана в конце пункта 2.4. Данный критерий входит в группу критериев согласия и называется критерием Пирсона, или χ2 (критерием хи-квадрат). Показатели (статистики) этого критерия- χ2c l ("c"-calculated, "l"-ко-личество множеств признаков),- называемые иногда выборочными среднеквад-ратическими сопряженностями признаков, рассчитываются на основе (4.12), (4.16), (4.17) следующим образом: χ2c 2 = NJ, ¯ J αI(J)+I( ¯ J) − αI( ¯ J)αI(J)2 αI( ¯ J)αI(J) ,
140 Глава 4. Введение в анализ связей χ2c 3 = N J1,J2, ¯ J αI(J1)+I(J2)+I( ¯ J) − αI( ¯ J)αI(J1)αI(J2)2 αI( ¯ J)αI(J1)αI(J2) , χ2c n = NG αI −Jαij(j)2 Jαij (j) . Если признаки не зависимы, то соответствующая статистика критерия име-ет известное распределение, называемое χ2-распределением (см. Приложе-ние A.3.2). Данное распределение имеет один параметр-число степеней сво-боды df (degrees free), показывающее количество независимых случайных ве-личин, квадраты которых входят в сумму. Так, в статистику χ2c 2 входят квадраты K (K ¯ JKJ ) величин αI(J)+I( ¯ J) − αI( ¯ J)αI(J), но не все они независимы, т.к. удовле-творяют целому ряду линейных соотношений. Действительно, например: ¯ J (αI(J)+I( ¯ J) − αI( ¯ J)αI(J)) = 0KJ , где 0KJ- матричный нуль, имеющий размерность KJ. То есть KJ величин αI(J)+IK( ¯ J)−αIK( ¯ J)αI(J) линейно выражаются через другие величины.Пусть мно-жество этих величин обозначается χI(J). Аналогично, исходные величины αI(J)+I( ¯ J) − αI( ¯ J)αI(J) можно суммировать по J и установить, что K ¯ J величин αIK(J)+I(J¯) − αI( ¯ J)αIK(J) линейно выража-ются через остальные; их множество можно обозначить χI( ¯ J). Эти два множества χI(J) и χI( ¯ J) имеют один общий элемент: αIK(J)+IK( ¯ J) − −αIK( ¯ J)αIK(J). Таким образом, количество степеней свободы df2 (при l = 2) рав-но K −K ¯ J − KJ + 1 = (K ¯ J − 1)(KJ − 1).Аналогично рассуждая,можно устано-вить, что df3 = (K ¯ J − 1)(KJ1 − 1)(KJ2 − 1), dfL =J(kj − 1). Итак, чтобы ответить на вопрос, являются ли независимыми изучаемые множе-ства признаков, необходимо расчетное значение статистики χ2c l сравнить со зна-чением 95-процентного квантиля χ2dfl-распределения (в п. 2.4 отмечалось, что в статистике вполне приемлемым считается 95-процентный уровень доверия), ко-торый обозначается χ2dfl, 0.95 (это-односторонний квантиль, так как плотность χ2-распределения расположена в положительной области значений случайной ве-личины и не симметрична). Значения этих квантилей находят в соответствующих статистических таблицах и называют теоретическими, или табличными. Если рас-четное значение не превышает табличное (т.е. является достаточно малым), то ну-левая гипотеза не отвергается и данные множества признаков считаются незави-
4.2. Регрессионный анализ 141 симыми. Если расчетное значение больше табличного, то множества признаков определяются как зависимые между собой с уровнем ошибки 5%. Современные пакеты прикладных статистических программ избавляют от необ-ходимости пользоваться статистическими таблицами, т.к. расчет статистики кри-терия сопровождается оценкой уровня его значимости sl (significance level). Для некоторых критериев этот показатель называется значением вероятности pv (probability value). Уровень значимости sl -это такое число, что χ2c l = χ2dfl,1−sl. То есть нулевая гипотеза отвергается с вероятностью ошибки 0.05, если sl < 0.05. В случае 2-х признаков среднеквадратичная сопряженность имеет следующий вид(з десь и ниже используется 1-й способ обозначений): χ2c 2 = N i1,i2 (αi1i2 − αi1∗α∗i2)2 αi1∗α∗i2 , а соответствующее ей χ2-распределение имеет (k1−1)(k2−1) степеней свободы; множество χi1∗ образовано величинами αi1k2 − αi1∗α∗k2, i1 = 1, . . . , k1, множе-ство χ∗i2 -величинами αk1i2−αk1∗α∗i2 , i2 = 1, . . . , k2, общим для них является элемент αk1k2 − αk1∗α∗k2 . Далее в этой главе рассматривается в основном случай двух признаков. 4.2. Регрессионный анализ В качестве значений признаков xi1∗ и x∗i2 на полуинтервалах, как и преж-де, принимаются середины этих полуинтервалов. Средние и дисперсии признаков рассчитываются по известным формулам: ¯x1 =xi1∗αi1∗, ¯x2 =x∗i2α∗i2 ; s21=(xi1∗−¯x1)2 αi1∗, s22=(x∗i2 − ¯x2)2 α∗i2 или, более компактно, s21=ˆx2i1∗αi1∗, s22=ˆx2∗ i2α∗i2 . Важной характеристикой совместного распределения двух признаков является ковариация-совместный центральный момент 2-го порядка: m12 =ˆxi1∗ˆx∗i2αi1i2 . Дисперсия-частный случай ковариации (ковариация признака с самим со-бой), поэтому для обозначения дисперсии j-го признака часто используется mjj .
142 Глава 4. Введение в анализ связей В случае независимости признаков, когда αi1i2 = αi1∗α∗i2 , как несложно убе-диться, ковариация равна нулю. Равенство ковариации нулю2 является необходи-мым, но не достаточным условием независимости признаков, т.к. ковариация- характеристика только линейной связи. Если ковариация равна нулю, признаки линейно независимы, но какая-то другая форма зависимости между ними может существовать. Меройлинейной зависимости является относительная ковариация, называемая коэффициентом корреляции: r12 = √ m12 m11m22 . Этот коэффициент по абсолютной величине не превышает единицу (этот факт доказывается ниже). Если его значение близко к нулю, то признаки линейно неза-висимы, если близко к плюс единице-между признаками существует прямая линейная зависимость, если близко к минус единице-существует обратная ли-нейная зависимость. В частности, легко убедиться в том, что если ˆxi1∗ = ±a12ˆx∗i2 (т.е. между признаками имеет место линейная зависимость), то r12 = ±1. Значения ковариаций и коэффициентов корреляции симметричны: m12 = m21, r12 = r21. В дальнейшем рассуждения проводятся так, как будто 1-й признак зависит от 2-го (хотя с тем же успехом можно было бы говорить о зависимости 2-го при-знака от 1-го). В таком случае переменная x1 (значения 1-го признака) называется объясняемой, моделируемой, эндогенной, а переменная x2 (значения 2-го при-знака)-объясняющей, факторной, экзогенной. Наряду с общей средней 1-го признака ¯x1 полезно рассчитать условные сред-ние ¯x1 | ∗i2 3-средние 1-го признака при условии, что 2-й признак зафиксирован на определенном уровне i2. При расчете таких средних усреднение значений при-знака на полуинтервалах проводится по относительным частотам не маргинального (αi1∗), а соответствующих условных распределений (αi1∗ | ∗i2 ): ¯x1 | ∗i2 =xi1∗αi1∗ | ∗i2 . Усреднение этих величин по весам маргинального распределения 2-го признака дает общее среднее: ¯x1 =i1 xi1∗αi1∗ =i2 i1 xi1∗αi1i2 =i2 i1 xi1∗αi1∗ | ∗i2α∗i2 =i2 ¯x1 | ∗i2α∗i2 . 2Равенство или неравенство нулю понимается в статистическом смысле: не отвергается или от-вергается соответствующая нулевая гипотеза. 3В общем случае вектор условных средних признаков ¯ J обозначается ¯x ¯ J/I(J) .
4.2. Регрессионный анализ 143 В непрерывном случае эти формулы принимают вид: E(x1|x2) = ∞ −∞ x1f (x1|x2) dx1, E(x1) = ∞ −∞ E(x1|x2) f (x2) dx2. (Об условных и маргинальных распределениях см. Приложение A.3.1.) Условные дисперсии признака рассчитываются следующим образом: s21| ∗i2 =xi1∗ − ¯x1 |∗ i22 αi1∗ | ∗i2 . Отклонения фактических значений признака от условных средних ei1∗ | ∗i2 = xi1∗ − ¯x1 | ∗i2 обладают, по определению, следующими свойствами: а) их средние равны нулю:ei1∗ | ∗i2αi1∗ | ∗i2 = 0, б) их дисперсии, совпадающие с условными дисперсиями признака, минималь-ны (суммы их квадратов минимальны среди сумм квадратов отклонений от каких-либо фиксированных значений признака-наличие этого свойства у дисперсий доказывалось в п. 2.4): s2e1 | ∗i2 =e2i1∗ | ∗i2αi1∗ | ∗i2 = s21| ∗i2 = min c (xi1∗ − c)2 αi1∗ | ∗i2 . Общая дисперсия связана с условными дисперсиями более сложно: s21=ˆx2i1∗αi1∗ =i1 i2 ˆx2i1∗αi1i2 = =i1 i2 xi1∗ − ¯x1 | ∗i2+ ¯x1 | ∗i2 − ¯x12 αi1i2 = =i1 i2 xi1∗ − ¯x1 | ∗i22 αi1i2 + 2i1 i2 xi1∗ − ¯x1 | ∗i2¯x1 | ∗i2 − ¯x1αi1i2 + +i1 i2 ¯x1 | ∗i2 − ¯x12 αi1i2 =
144 Глава 4. Введение в анализ связей = i2 α∗i2 i1 xi1∗ − ¯x1 | ∗i22 αi1 ←−−∗|−∗→i2 αi1i2 α∗i2 ←−−−−−−−−−−−−−−−−−−−→ s2e1 |∗ i2 + + 2i2 α∗i2 ¯x1 | ∗i2 − ¯x1←−−−−−−−−−=−0−−−−−−−−→ i1 xi1∗ − ¯x1 | ∗i2αi1∗ | ∗i2 ←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→ =0 + +i2 ¯x1 | ∗i2 − ¯x12 i1 αi1i2 ←−−−−→ α∗i2 = s2e1 + s2q1. Равенство нулю среднего слагаемого в этой сумме означает, что отклонения фактических значений 1-го признака от условных средних не коррелированы (ли-нейно не связаны) с самими условными средними. В терминах регрессионного анализа s2q1 -объясненная дисперсия, т.е. та дисперсия 1-го признака, которая объясняется вариацией 2-го признака (в частности, когда признаки независимы и условные распределения 1-го признака одинаковы при всех уровнях 2-го призна-ка, то условные средние не варьируют и объясненная дисперсия равна нулю); s2e1 -остаточная дисперсия. Чем выше объясненная дисперсия по сравнению с остаточной, тем вероятнее, что 2-й признак влияет на 1-й. Количественную меру того, насколько объяснен-ная дисперсия должна быть больше остаточной, чтобы это влияние можно было признать существенным (значимым), дает критерий Фишера, или F-критерий. Статистика этого критерия Fc рассчитывается следующим образом: Fc = s2q1k2 (k1 − 1) s2e1 (k2 − 1) . В случае если влияние 2-го признака на 1-й не существенно, эта величи-на имеет F-распределение (см. Приложение A.3.2). Такое распределение имеет случайная величина, полученная отношением двух случайных величин, имеющих χ2-распределение, деленных на количество своих степеней свободы: Fdf1, df2 = χ2df1df2 χ2df2df1 . Количество степеней свободы в числителе (df1) и знаменателе (df2) относится к параметрам F-распределения.
4.2. Регрессионный анализ 145 Рассуждая аналогично тому, как это сделано в конце предыдущего пункта, мож-но установить, что объясненная дисперсия (в числителе F-статистики) имеет k2−1 степеней свободы, а остаточная дисперсия (в знаменателе)- k2(k1−1) степеней свободы. Это объясняет указанный способ расчета данной статистики. Чтобы проверить гипотезу о наличии влияния 2-го признака на 1-й, необходимо сравнить расчетное значение статистики Fc с теоретическим-взятым из соот-ветствующей статистической таблицы 95-процентным квантилем (односторонним) F-распределения с k2−1 и k2(k1−1) степенями свободы Fk2−1,k2(k1−1), 0.95. Если расчетное значение не превышает теоретическое, то нулевая гипотеза не отвер-гается, и влияние считается не существенным. В противном случае (объясненная дисперсия достаточно велика по сравнению с остаточной) нулевая гипотеза от-вергается и данное влияние принимается значимым. Современные статистические пакеты прикладных программ дают уровень значимости расчетной статистики, на-зываемый в данном случае значением вероятности pv: Fc = Fk2−1, k2(k1−1), 1−pv. Если pv < 0.05, то нулевая гипотеза отвергается с вероятностью ошибки 5%. Линия, соединяющая точки x∗i2 , ¯x1 | ∗i2в пространстве значений признаков (абсцисса-2-й признак, ордината-1-й) называется линией регрессии, она по-казывает зависимость 1-го признака от 2-го. Условные средние, образующие эту линию, являются расчетными (модельными) или объясненными этой зависимостью значениями 1-го признака. Объясненная дисперсия показывает вариацию значе-ний 1-го признака, которые расположены на этой линии, остаточная дисперсия- вариацию фактических значений признака вокруг этой линии. Линию регрессии можно провести непосредственно в таблице сопряженности. Это линия, которая соединяет клетки с максимальными в столбцах плотностями относительных частот.Понятно, что о такой линии имеет смысл говорить, если име-ются явные концентрации плотностей относительных частот в отдельных клетках таблицы сопряженности. Критерием наличия таких концентраций как раз и явля-ется F-критерий. В непрерывном случае уравнение x1 = E(x1|x2) называют уравнением регрессии x1 по x2, т.е. уравнением статистической зави-симости 1-го признака от 2-го (о свойствах условного математического ожидания см.Приложение A.3.1). Это уравнение выражает статистическую зависимость, по-скольку показывает наиболее вероятное значение, которое принимает 1-й признак при том или ином уровне 2-го признака. В случае если 2-й признак является един-ственным существенно влияющим на 1-й признак, т.е. это уравнение выражает
146 Глава 4. Введение в анализ связей теоретическую, истинную зависимость, эти наиболее вероятные значения назы-вают теоретическими, а отклонения от них фактических значений-случайными ошибками измерения. Для фактических значений x1 это уравнение записывают со стохастическим членом, т.е. со случайной ошибкой, остатками, отклонением фактических значений от теоретических: x1 = E(x1|x2) + ε1. Случайные ошибки по построению уравнения регрессии имеют нулевое ма-тематическое ожидание и минимальную дисперсию при любом значении x2, они взаимно независимы со значениями x2. Эти факты обсуждались выше для эмпи-рического распределения. В рассмотренной схеме регрессионного анализа уравнение регрессии можно построить лишь теоретически. На практике получают линию регрессии, по виду которой можно лишь делать предположения о форме и, тем более, о параметрах зависимости. В эконометрии обычно используется другая схема регрессионного анализа. В этой схеме используют исходные значения признаков xi1, xi2, i = 1, . . . , N без предварительной группировки и построения таблицы сопряженности, выдвигают гипотезу о форме зависимости f : x1 = f (x2,A), гд еA -параметры зависимо-сти, и находят эти параметры так, чтобы была минимальной остаточная дисперсия s2e1 = 1 N i(xi1 − f (xi2,A))2. Такой методназ ывается методом наименьших квадратов (МНК). Ковариация и коэффициент корреляции непосредственно по данным выборки рассчитываются следующим образом: mjj= 1 N (xij − ¯xj) xij− ¯xj, rjj= mjj'mjjmjj, j,j = 1, 2. Далее в этом пункте рассматривается случай линейной регрессии, т.е. случай, когда x1 = α12x2 + β1 + ε1, (4.18) где α12, β1, ε1 -истинные значения параметров регрессии и остатков. Следует иметь в виду, что регрессия линейна, если форма зависимости при-знаков линейна относительно оцениваемых параметров, а не самих признаков,
4.2. Регрессионный анализ 147 и уравнения x1 = α12√x2 + β1 + ε1, x1 = α12 1 x2 + β1 + ε1, ln x1 = α12 ln x2 + lnβ1 + lnε1 (x1 = xα12 2 β1ε1) , и т.д. также относятся к линейной регрессии. Во всех этих случаях метод наи-меньших квадратов применяется одинаковым образом. Поэтому можно считать, что в записи (4.18) x1 и x2 являются результатом какого-либо функционального преобразования исходных значений. Оценки параметров регрессии и остатков обозначаются соответствующими буквами латинского алфавита, и уравнение регрессии, записанное по наблюде-ниям i, имеет следующий вид: xi1 = a12xi2 + b1 + ei1, i= 1, . . . , N, (4.19) а в матричной форме: X1 = X2a12 + 1Nb1 + e1, (4.20) где X1, X2 -вектор-столбцы наблюдений размерности N, соответственно, за 1-м и 2-м признаками, e1 -вектор-столбец остатков; 1N -вектор-столбец размерности N, состоящий из единиц. <45' x1 x2 Рис. 4.1 Прежде чем переходить к оценке параметров регрессии (применению метода наименьших квадратов), имеет смысл объяснить проис-хождение термина "регрессия".Этот термин введен английским статистикомФ. Гальтоном в последней четверти XIX века при изучении зависимости роста сыновей от роста отцов. Оказалось, что если по оси абсцисс распо-ложить рост отцов (x2), а по оси ординат- рост сыновей (x1), то точки, соответствую-щие проведенным наблюдениям (облако то-чек наблюдений), расположатся вокруг неко-торой прямой (рис. 4.1). Это означает, что зависимость между ростом сыновей и отцов существует, и эта зависимость близка к линейной. Но угол наклона соответствующей прямой мень-ше 45*. Другими словами, имеет место "возврат"-регрессия-роста сыновей к некоторому среднему росту. Для этой зависимости и был предложен термин "ре-грессия". Со временем он закрепился за любыми зависимостями статистического характера, т.е. такими, которые выполняются "по математическому ожиданию", с погрешностью.
148 Глава 4. Введение в анализ связей Остаточная дисперсия из (4.19) получает следующее выражение: s2e1 = 1 N i (xi1 − a12xi2 − b1)2, или в матричной форме: s2e1 = 1 N e1e1, где e1 = X1 − X2a12 − 1Nb1, -остатки регрессии, штрих-знак транспонирования. Величина e1e1 называется суммой квадратов остатков. Для минимизации этой дисперсии ее производные по искомым параметрам (сначала по b1, потом по a12) приравниваются к нулю. ∂s2e1 ∂b1 = − 2N (xi1 − a12xi2 − b1) = 0, откуда: ei1 = 0, b1 = ¯x1 − a12¯x2. (4.21) Это означает, что ¯e1 = 0, т.е. сумма остатков равна нулю, а также, что линия регрессии проходит через точку средних. После подстановки полученной оценки свободного члена форма уравнения ре-грессии и остаточной дисперсии упрощается: ˆxi1 = a12ˆxi2 + ei1, i= 1, . . . , N, (4.22) ˆX1 = ˆX2a12 + e1, -сокращенная запись уравнения регрессии, (4.23) s2e1 = 1N (ˆxi1 − a12ˆxi2)2. (4.24) Далее: ∂s2e1 ∂a12 = − 2 N ˆxi2←−−−−ei−1−−−→ (ˆxi1 − a12ˆxi2) = 0. (4.25) Отсюда следует, во-первых, то, что вектора e1 и X2 ортогональны, т.к. ковари-ация между ними равна нулю (ˆxi2ei1 = 0); во-вторых-выражение для оценки углового коэффициента: a12 = m12 m22 . (4.26)
4.2. Регрессионный анализ 149 Матрица вторых производных остаточной дисперсии в найденной точке равна 2 ⎡⎢⎣ 1 ¯x2 ¯x2 m022⎤⎥⎦, где m022 -2-й начальный (а не центральный, как m22) момент д ля x2. Тот же результат можно получить, если не переходить к сокращенной записи уравнения регрессии передд ифференцированием остаточной дисперсии по a12. Эта матрица положительно определена (ее определитель равен 2m22, то есть всегда неотрицателен), поэтому найденная точка является действительно точкой минимума остаточной дисперсии. Таким образом, построен оператор МНК-оценивания (4.21, 4.26) и выявлены свойства МНК-остатков: они ортогональны факторной переменной x2, стоящей в правой части уравнения регрессии, и их среднее по наблюдениям равно нулю. "Теоретические" значения моделируемой переменной x1, лежащие на линии оцененной регрессии: xc i1 = a12xi2 + b1, (4.27) ˆxc i1 = a12ˆxi2, где " c "-calculated, часто называют расчетными, или объясненными. Это- математические ожидания моделируемой переменной. Вторую часть оператора МНК-оценивания (4.26) можно получить, используя другую логику рассуждений, часто применяемую в регрессионном анализе. Обе части уравнения регрессии, записанного в сокращенной матричной форме (4.23) умножаются слева на транспонированный вектор . X2 и д елятся на N: 1 N . X2 . X1 = 1N . X2 . X2a12 + 1 N . X2e1. Второе слагаемое правой части полученного уравнения отбрасывается, так как в силу отмеченных свойств МНК-остатков оно равно нулю, и получается искомое выражение: m12 = m22a12. Пользуясь этой логикой, оператор МНК-оценивания можно получить и в пол-ном формате. Для этого используют запись регрессионного уравнения в форме без свободного члена (со скрытым свободным членом): X1 = / X2/ a12 + e1, (4.28) где / X2 -матрица [X2, 1N] размерности N × 2, ˜a12 -вектор 0 a12 b11.
150 Глава 4. Введение в анализ связей Как и прежде, обе части этого уравнения умножаются слева на транспониро-ваннуюматрицу / X2 и д елятсяна N, второе слагаемое правой части отбрасывается по тем же причинам. Получается выражение для оператора МНК-оценивания: / m12 = 2M22/ a12, т.е. / a12 = 2M−1 22 / m12, (4.29) где / m12 = 1 N / X2X1, 2M22 = 1 N / X2 / X2. Это выражение эквивалентно полученному выше. Действительно, учитывая, что Xj = ˆXj + 1N ¯xj , 1N ˆXj = 0, j= 1, 2, ˜m12 = 1N ⎡⎢⎣ X2X1 1NX1 ⎤⎥⎦= ⎡⎢⎣ m12 + ¯x1¯x2 ¯x1 ⎤⎥⎦, ˜M22 = 1 N ⎡⎢⎣ X2X2 X21N 1NX2 1N1N ⎤⎥⎦= m022 ⎡⎢⎣ ←−−−−→ m22 + ¯x22¯x2 ¯x2 1 ⎤⎥⎦. Тогда матричное уравнение (4.29) переписывается следующим образом: m12 + ¯x1¯x2 = m22a12 + ¯x22a12 + ¯x2b1, ¯x1 = ¯x2a12 + b1. Из 2-го уравнения сразу следует (4.21), а после подстановки b12 в 1-е уравне-ние оно преобразуется к (4.26). Что и требовалось доказать. Таким образом, выражение (4.29) представляет собой компактную запись опе-ратораМНК-оценивания. Из проведенных рассуждений полезно, в частности, запомнить, что уравнение регрессии может быть представлено в трех формах: в исходной-(4.19, 4.20), сокращенной-(4.22, 4.23) и со скрытым свободным членом-(4.28). Третья форма имеет только матричное выражение. Оцененное уравнение линейной регрессии "наследует" в определенном смысле свойства линии регрессии, введенной в начале этого пункта по данным совместного распределения двух признаков: минимальность остаточной дисперсии, равенство нулю средних остатков и ортогональность остатков к объясняющей переменной- в данном случае к значениям второго признака. (Последнее для регрессии, постро-енной по данным совместного распределения, звучало как линейная независимость отклонений от условных средних и самих условных средних.) Отличие в том, что теперь линия регрессии является прямой, условными средними являются расчет-ные значения моделируемой переменной, а условными дисперсиями-остаточная
4.2. Регрессионный анализ 151 дисперсия, которая принимается при такомметоде оценивания одинаковой для всех наблюдений. Теперь рассматривается остаточная дисперсия (4.24) в точке минимума: s2e1 = 1 N ˆx2 i1 − 2ˆxi1ˆxi2a12 + ˆx2 i2a212(4.26) = m11 − m212 m22 . (4.30) Поскольку остаточная дисперсия неотрицательна, m11 m212 m22 , т.е. r212 1. Это доказывает ранее сделанное утверждение о том, что коэффициент корре-ляции по абсолютной величине не превышает единицу. Второе слагаемое (взятое с плюсом) правой части соотношения (4.30) является дисперсией расчетных значений моделируемой переменной (var -обозначение дисперсии): var (xc1) = 1N (xc i1 − ¯xc1)2 ¯e=0 = 1N (xc i1 − ¯x1)2 (4.27) = = 1 N (a12ˆxi2)2 = a212m22 (4.26) = m212 m22 . (4.31) Эту дисперсию, как и в регрессии, построенной по данным совместного рас-пределения признаков, естественно назвать объясненной и обозначить s2q1. Тогд а из (4.30) следует, что общая дисперсия моделируемого признака, как и прежде, распадается на две части-объясненную и остаточную дисперсии: s21= m11 = s2q1 + s2e1. Доля объясненной дисперсии в общей называется коэффициентом детерми-нации, который обозначается R2. Такое обозначение не случайно, поскольку этот коэффициент равен квадрату коэффициента корреляции: R2 = s2q1 s21= m212 m11m22 . Коэффициент детерминации является показателем точности аппроксимации фактических значений признаков линией регрессии: чем ближе он к единице, тем точнее аппроксимация.При прочих равных его значениебудет расти с уменьшением числа наблюдений. Так, если наблюдений всего два, этот коэффициент всегда будет равен единице, т.к. через две точкиможно провести единственную прямую. Поэтому
152 Глава 4. Введение в анализ связей данный коэффициент выражает скорее "алгебраическое" качество построенного уравнения регрессии. Показатель статистической значимости оцененного уравнения дает статистика Фишера-как и для регрессии, построенной по данным совместного распреде-ления признаков. В данном случае остаточная дисперсия имеет N − 2 степени свободы, а объясненная-одну степень свободы (доказательство этого факта да-ется во II части книги): Fc = s2q1 (N − 2) s2e1 = R2 (N − 2) (1 − R2) . Если переменные не зависят друг от друга, т.е. α12 = 0 (нулевая гипотеза), то эта статистика имеет распределение Фишера с одной степенью свободы в чис-лителе и N−2 степенями свободы в знаменателе. Логика использования этой ста-тистики описана выше. Статистическая значимость (качество) полученного урав-нения тем выше, чем ниже значение показателя pv для расчетного значения данной статистики Fc. Оценки параметров α12, β1 и остатков εi1 можно получить иначе, из регрессии x2 по x1: ˆxi2 = a21ˆxi1 + ei2, i= 1, . . . , N. В соответствии с (4.26) оценка углового коэффициента получается делением ковариации переменных, стоящих в левой и правой частях уравнения, на дисперсию факторной переменной, стоящей в правой части уравнения: a21 = m21 m11 . Поскольку ˆxi1 = 1 a21 ˆxi2 − 1 a21 ei2, a12 (2) = 1 a21 = m11 m21 , (4.32) b1 (2) = ¯x1 − a12 (2) ¯x2, ei1 (2) = a12 (2) ei2, i= 1, . . . , N. Это-новые оценки параметров. Легко убедиться в том, что a12 (2) совпадает с a12 (а вследза ним b1 (2) совпадает с b1 и ei1 (2) -с ei1) тогда и только тогда, когда коэффициент корреляции r12 равен единице, т.е. зависимость имеет функциональный характер и все остатки равны нулю. При оценке параметров α12, β1 и остатков ei1 регрессия x1 по x2 иногда называется прямой, регрессия x1 по x2 -обратной.
4.2. Регрессионный анализ 153 AB D C r 0 1 F E x1 x2 Рис. 4.2 На рисунке 4.2 в плоскости (в про-странстве) переменных x1, x2 при-менение прямой регрессии означает минимизацию суммы квадратов рас-стояний от точек облака наблюдений до линии регрессии, измеренных па-раллельно оси x1. При применении обратной регрессии эти расстояния измеряются параллельно оси x2. lr -линия регрессии, OA -вектор-строка i-го на-блюдения ˆxi = (ˆxi1, ˆxi2), AB -расстояние до линии ре-грессии, измеренное параллельно оси ˆx1, равное величине ei1, AC -расстояние, измеренное параллельно оси ˆx2, равное величине ei2, AD -расстояние, измеренное перпендикулярно линии регрессии, равное ei, OE -вектор-строка aпараметров ортогональной регрессии. Очевидно, что оценить параметры регрессии можно, измеряя расстояния до ли-нии регрессии перпендикулярно самой этой линии (на рисунке-отрезок AD). Такая регрессия называется ортогональной. В уравнении такой регрессии обе пе-ременные остаются в левой части с коэффициентами, сумма квадратов которых должна равняться единице (длина вектора параметров регрессии должна равнять-ся единице): a1ˆxi1 + a2ˆxi2 = ei, i= 1, . . . , N (4.33) a21+ a22= 1. В матричной форме: Xˆa = e, (4.34) aa = 1, где ˆX-матрица наблюдений за переменными, размерности N ∗ 2, a -вектор-столбец параметров регрессии. Само уравнение регрессии можно записать еще и так: ˆxia = ei, i= 1, . . . , N. (4.35) Чтобы убедиться в том, что такая регрессия является ортогональной, достаточ-но вспомнить из линейной алгебры, что скалярное произведение вектора на вектор
154 Глава 4. Введение в анализ связей единичной длины равно длине проекции этого вектора на единичный вектор. В ле-вой части (4.35) как раз и фигурирует такое скалярное произведение. На рисунке вектором параметров a является OE, проекцией вектора наблюдений OA(ˆxi) на этот вектор-отрезок OF, длина которого (ˆxia) в точности равна расстоянию от точки облака наблюдений до линии регрессии, измеренному перпендикулярно этой линии (ei). Следует иметь в виду, что и в "обычной" регрессии, в левой части которой остается одна переменная, коэффициент при этой переменной принимается рав-ным единице, т.е. фактически используется аналогичное ортогональной регрессии требование: вектор параметров при переменных в левой части уравнения должен иметь единичную длину. Впротивоположность ортогональной "обычные" регрессии называют просты-ми. В отечественной литературе простой часто называют "обычную" регрессию с одной факторной переменной. А регрессию с несколькими факторными перемен-ными называют множественной. Теперь остаточную дисперсию в матричной форме можно записать следующим образом: s2e= 1N ee = 1N aˆXXˆa = aMa, где M = 1 N ˆXˆX-матрица ковариации переменных, равная ⎡⎢⎣ m11 m12 m21 m22 ⎤⎥⎦. Для минимизации остаточной дисперсии при ограничении на длину вектора параметров регрессии строится функция Лагранжа: L(a, λ) = aMa − λaa, где λ -множитель Лагранжа (оценка ограничения). Далее находятся производные этой функции по параметрам регрессии, и эти производные приравниваются к нулю. Результат таких операций в матричной фор-ме представляется следующим образом (поскольку M -симметричная матрица: M= M): (M − λI) a = 0. (4.36) Таким образом, множитель Лагранжа есть собственное число матрицы кова-риации M, а вектор оценок параметров регрессии-соответствующий правый собственный вектор этой матрицы (см. Приложение A.1.2). Матрица M является вещественной, симметричной и положительно полу-определенной (см. Приложение A.1.2).
4.2. Регрессионный анализ 155 Последнее справедливо, т.к. квадратичная форма μMμ при любом векторе μ неот-рицательна. Действительно, эту квадратичную форму всегда можно представить как сумму квадратов компонент вектора η = √1N Xˆμ: μMμ = 1 N μˆXˆ Xμ = ηη 0. Из линейной алгебры известно, что все собственные числа такой матрицы ве-щественны и неотрицательны, следовательно λ неотрицательно. После умножения обеих частей уравнения (4.36) слева на aиз него следует, что s2e= aMa = λaa aa=1 = λ, т.е. минимизации остаточной дисперсии соответствует поиск минимального соб-ственного числа матрицы ковариации переменных M. Соответствующий этому собственному числу правый собственный вектор этой матрицы есть вектор оце-нок параметров ортогональной регрессии a (см. Приложение A.1.2). Кроме того, в соответствии со свойствами матрицы M, сумма ее собственных чисел равна сумме ее диагональных элементов (следу матрицы), и, т.к. λ -меньшее из двух собственных чисел, то λ < 12 (m11 + m12) (случай двух одинаковых собственных чисел не рассматривается, т.к. он имеет место, когда связь между переменными отсутствует, и m12 = 0). Оценка свободного члена b, как и прежде, получается из условия прохождения линии регрессии через точку средних: b = ¯xa, гд е ¯x -вектор-строка средних значений переменных. Расчетное значение ˆxi дает вектор OD (см. рис. 4.2), который равен разности векторов OA и OF, т.е. (в матричной форме): ˆXc = ˆX− ea. Теперь можно дать еще одну оценку параметров уравнения (4.18): a12 (⊥) = −a2 a1 , b1 (⊥) = ¯x1 − a12 (⊥) ¯x2, ei1 (⊥) = 1 a1 ei. Полученная оценка углового коэффициента a12 (⊥) лежитмежду его оценками по прямой и обратной регрессиям. Действительно, из (4.36) следует, что a12 (⊥) = −a2 a1 = m12 m22 − λ = m11 − λ m12 .
156 Глава 4. Введение в анализ связей 90' >90' x x1 1 x2 kx2 Рис. 4.3 Отсюда, в частности, следует, что величины m11 − λ и m22 − λ имеют один знак, и, т.к. λ < 12 (m11 + m12), то обе эти величины положительны. Поэтому, если m12 0, то m11 m12 (4.32) = a12 (2) > a12 (⊥) > a12 (4.26) = m12 m22 , а если m12 0, то a12 (2) < a12 (⊥) < a12. Понятно, что эти 3 оценки совпадают тогда и только тогда, когда λ = s2e= 0, т.е. зависимость функциональна. В действительности любое число, лежащее на отрезке с концами a12, a12 (2) (т.е. либо [a12, a12 (2)], если m12 0, либо [a12 (2) , a12], если m12 0), может являться МНК-оценкой параметра α12, т.е. оценкой этого параметра является γ1a12 + γ2a12 (2) при любых γ1 и γ2, таких что γ1 0, γ2 0, γ1 + γ2 = 1. Каждая из этих оценок может быть получена, если расстояния от точек облака наблюдения до линии регрессии измерять подопред еленным углом, что достигается с помощью предварительного преобразования в пространстве переменных. Убедиться в этом можно, рассуждая следующим образом. Пусть получена оценка углового коэффициента по ортогональной регрессии (рис. 4.3, слева). Теперь проводится преобразование в пространстве переменных: ˆx2 умножается на некоторое число k > 1, и снова дается оценка этого коэффициента по ортогональной регрессии (рис. 4.3, справа). После возвращения в исходное про-странство получается новая оценка углового коэффициента, сопоставимая со старой (возвращение в исходное пространство осуществляется умножением оценки коэф-фициента, полученной в преобразованном пространстве, на число k). Этот рисунок не вполне корректен, т.к. переходв новое пространство переменных и возвращение в исходное пространство ведет к смещению линии регрессии. Однако
4.2. Регрессионный анализ 157 смысл происходящего он поясняет достаточно наглядно: новая оценка получена так, как будто расстояния от точек облака наблюдений до линии регрессии измеряются поду глом, не равным 90*. Должно быть понятно, что в пределе, при k → ∞, расстояния до линии регрессии будут измеряться параллельно оси ˆx1 и полученная оценка углового коэффициента совпадет с a12. Наоборот, в пределе при k → 0 эта оценка совпадет с a12 (2). Выбор оценок параметров регрессии на имеющемся множестве зависит от ха-рактера распределения ошибок измерения переменных. Это-предмет изучения во II части книги. Пока можно предложить некоторые эмпирические критерии. Например, следующий. Общая совокупность (множество наблюдений) делится на две части: обуча-ющую и контрольную. Оценка параметров производится по обучающей совокуп-ности. На контрольной совокупности определяется сумма квадратов отклонений фактических значений переменных от расчетных. Выбирается та оценка, которая дает минимум этой суммы. В заключение выбранную оценку можно дать по всей совокупности. Рассмотренный случай двух переменных легко обобщить на n переменных (без доказательств: они даются во II части книги). Основное уравнение регрессии запи-сывается следующим образом: x1 = x−1α−1 + β1 + ε1, где x−1 = [x2, . . . , xn] - вектор-строка всех переменных кроме первой, вектор факторных переменных, α−1 = ⎡⎢⎢⎢⎢⎢⎣ α12 ... α1n ⎤⎥⎥⎥⎥⎥⎦ -вектор-столбец параметров регрессии при факторных переменных, а в матрич-ной форме: ˆX1 = ˆX−1a−1 + e1, гд е ˆX−1 -матрица размерности N × (n − 1) наблюдений за факторными переменными. По аналогии с (4.21, 4.26): a−1 = M−1 −1m−1, (4.37) b1 = ¯x1 − ¯x−1a−1, где M−1 = 1 N ˆX − 1 ˆX−1 -матрица ковариации факторных переменных между со-бой,m−1 = 1 N ˆX − 1 ˆX1 -вектор-столбец ковариации факторных переменных с мо-делируемой переменной,
158 Глава 4. Введение в анализ связей ¯x−1 = 1 N 1N ˆX−1 -вектор-строка средних значений факторных переменных. Расчетные значения моделируемой переменной, т.е. ее математические ожида-ния, есть ˆXc1 = ˆX−1a−1. Как и в случае двух переменных объясненной дисперсией является дисперсия расчетных значений моделируемой переменной: s2q1 = 1 N a−1 ˆX − 1 ˆX−1a−1 = a−1M−1a−1 (4.37) = a−1m−1 (4.37) = m−1M−1 −1m−1. (4.38) Коэффициентмножественной корреляции r1,−1 есть коэффициент корреляции между моделируемой переменной и ее расчетным значением (cov -обозначение ковариации): cov (xc1, x1) = 1 N a−1 ˆX − 1 ˆX1 = a−1m−1 (4.38) = s2q1, r1,−1 = cov (xc1, x1) 'var (xc1) var (x1) = s2q1 sq1s1 = sq1 s1 , Коэффициент детерминации, равный квадрату коэффициента множественной корреляции: R2 = s2q1 s21, показывает долю объясненной дисперсии в общей. Если связь отсутствует и α−1 = 0 (нулевая гипотеза), то расчетная статистика Фишера Fc = R2 (N − n) (1 − R2) (n − 1) имеет F-распределение с n − 1 степенями свободы в числителе и N − n степе-нями свободы в знаменателе- Fn−1,N−n. Логика использования этой статистики сохраняется прежней. Прииспользовании в общемслучае записи уравнения регрессии вформесо скры-тым свободным членом X1 = ˜X−1˜a−1 + e,
4.2. Регрессионный анализ 159 где ˜X−1 -матрица [X−1, 1N] размерности N×(n+1), ˜a−1 -вектор ⎡⎢⎣ a−1 b1 ⎤⎥⎦, оператор МНК-оценивания записывается как ˜a−1 = ˜M−1 −1 ˜m−1, (4.39) где ˜m−1 = 1 N ˜X − 1X1, ˜M−1 = 1 N ˜X − 1 ˜X−1. Достаточнопростые алгебраические преобразования показывают, что этот опе-ратор эквивалентен (4.37). C B A O Рис. 4.4 Полезной является еще одна геометрическая иллюстрация регрессии-в пространстве наблю-дений (см. рис. 4.4 и 4.5). При n = 2 (n -количество переменных), OA -вектор ˆx1, OB -вектор ˆx2, OC - вектор проекции ˆx1 на ˆx2, равный расчетному значению ˆxc1, CA -вектор остатков e1, так что: ˆx1 = a12ˆx2 + e1. Косинус угла между OA и OB равен коэффициенту корреляции. B C E O F AD Рис. 4.5 При n = 3, OA -вектор ˆx1, OB -вектор ˆx2, OC -вектор ˆx3, OD -вектор проекции ˆx1 на плоскость, определяемую ˆx2 и ˆx3, равный расчетному значению ˆxc1, DA -вектор остатков e1, OE -вектор проекции ˆxc1 на ˆx2, равный a12ˆx2, OF -вектор проекции ˆxc1 на ˆx3, равный a13ˆx3, так что ˆx1 = a12ˆx2 + a13ˆx3 + e1. Косинус угла между OA и плоскостью, определенной ˆx2 и ˆx3, (т.е.межд уOA и OD) равен коэффициенту множественной корреляции. Кроме оценки a−1 можно получить оценки a−1 (j), j = 2, . . . , n, последовательно переводя в левую часть уравнения переменные ˆxj, приме-няяМНК и алгебраически возвращаясь к оценкам исходной формы уравнения. Для представления ортогональной регрессии в общем случае подходят формулы (4.34, 4.36) и другие матричные выражения, приведенные выше при описании ортогональной регрессии. Необходимо только при определении векторов и матриц, входящих в эти выражения, заменить "2" на "n". Спомощью преобразований в пространстве переменных передиспользованием ортогональной регрессии и последующего возвращения в исходное пространство
160 Глава 4. Введение в анализ связей в качестве оценок a−1 можно получить любой вектор из множества (симплекса) γ1a−1 + n j=2 γja−1 (j), γj 0, j= 1, . . . , n, n j=1 λj = 1. Это-подмножество всех возможныхМНК-оценок истинных параметров α−1. 4.3. Дисперсионный анализ Дисперсионный анализ заключается в представлении (разложении) дисперсии изучаемых признаков по факторам и использовании F-критерия для сопоставле-ния факторных "частей" общей дисперсии с целью определения степени влияния факторов на изучаемые признаки. Примеры использования дисперсионного ана-лиза даны в предыдущем пункте при рассмотрении общей дисперсии моделируемой переменной как суммы объясненной и остаточной дисперсии. Дисперсионный анализ может быть одномерным или многомерным. В первом случае имеется только один изучаемый (моделируемый) признак, во втором случае их несколько. В данном курсе рассматривается только первый случай. Примене-ние методов этого анализа основывается на определенной группировке исходной совокупности (см. п. 1.9). В качестве факторных выступают группирующие при-знаки. То есть изучается влияние группирующих признаков на моделируемый. Если группирующий (факторный) признак один, то речь идет об однофакторном дис-персионном анализе, если этих признаков несколько-о многофакторном ана-лизе. Если в группировке для каждого сочетания уровней факторов имеется строго одно наблюдение (численность всех конечных групп в точности равна единице), говорят о дисперсионном анализе без повторений; если конечные группы могут иметь любые численности-с повторениями. Многофакторный дисперсионный анализ может быть полным или частичным. В первом случае исследуется вли-яние всех возможных сочетаний факторов (смысл этой фразы станет понятным ниже). Во втором случае принимаются во внимание лишь некоторые сочетания факторов. В этом пункте рассматриваются две модели: однофакторный дисперсионный анализ с повторениями и полный многофакторный анализ без повторений. Пусть исходная совокупность xi, i = 1, . . . , N сгруппирована по одному фак-тору, т.е. она разделена на k групп: xill -значение изучаемого признака в il-м наблюдении (il = 1, . . . , Nl) в l-й группе (l = 1, . . . , k); Nl = N.
4.3. Дисперсионный анализ 161 Рассчитываются общая средняя и средние по группам: ¯x = 1N k l=1 Nl il=1 xill = 1N k l=1 Nl¯xl, ¯xl = 1 Nl Nl il=1 xill, общая дисперсия, дисперсии по группам и межгрупповая дисперсия (s2q): s2 = 1N k l=1 Nl il=1 (xill − ¯x)2, s2l= 1 Nl Nl il=1 (xill − ¯xl)2, s2q= 1N k l=1 Nl (¯xl − ¯x)2. Общую дисперсию можно разложить на групповые и межгрупповую дисперсии: s2 = 1 N k l=1 Nl il=1 ((xill − ¯xl) + (¯xl − ¯x))2 = = 1 N k l=1 Nl il=1 (xill − ¯xl)2 + 2 N k l=1 Nl il=1 (xill − ¯xl) (¯xl − ¯x) + 1 N k l=1 Nl il=1 (¯xl − ¯x)2 = = 1 N k l=1Nl 1 Nl Nl il=1(xill − ¯xl)2 + 2 N k l=1(¯xl − ¯x) Nl il=1(xill − ¯xl) ←−−−−−−−−→ ←−−−−−−−−−−−−−−=−0−−−→ =0 + 1 N k l=1Nl (¯xl − ¯x)2 = = 1N k l=1 Nls2l+ s2q= s2e+ s2q. Данное представление общей дисперсии изучаемого признака аналогично полу-ченному в начале предыдущего пункта при рассмотрении регрессии, построенной по данным совместного эмпирического распределения признаков. В том случае "группами" выступали значения первого признака при тех или иных значениях второго признака. В данном случае (в терминах дисперсионного анализа) s2e-внутригрупповая дисперсия; s2q-межгрупповая дисперсия.
162 Глава 4. Введение в анализ связей Тот факт, что среднее слагаемое в вышеприведенном выражении равно нулю, означает линейную независимость внутригрупповой и межгрупповой дисперсий. Чем выше межгрупповая дисперсия по сравнению с внутригрупповой, тем ве-роятнее, что группирующий (факторный) признак влияет на изучаемый признак. Степень возможного влияния оценивается с помощью F-статистики: Fc = s2q(N − k) s2e(k − 1) . В случае если влияние отсутствует (нулевая гипотеза), эта статистика име-ет распределение Fk−1,N−k (межгрупповая дисперсия имеет k − 1 степеней свободы, внутригрупповая- N − k), что объясняет указанный способ расчета F-статистики. Логика проверки нулевой гипотезы та же, что и в предыдущих слу-чаях. Рассмотрение модели однофакторного дисперсионного анализа с повторениями завершено. Пусть теперь имеется группировка исходной совокупности xi, i = 1, . . . , N по n факторам; j-й фактор может принимать kj уровней, j = 1, . . . , n. Все численности конечных групп равны единице: NI = 1, для любого I. Такая сово-купность может быть получена по результатам проведения управляемого экспе-римента. В экономических исследованиях она может быть образована в расчетах по математической модели изучаемой переменной: для каждого сочетания уровней факторов проводится один расчет по модели. В этом случае N = n j=1 kj =G kj , где через G, как и в пункте 1.9, обозначено полное множество факторов J = {12 . . .n}, xI -значение изучаемого признака при сочетании уровней фак-торов I = {i1i2 . . . in}. Общая сред няя изучаемого признака: b0 = ¯x = 1N I xI . Каждый j-й фактор делит исходную совокупность на kj групп по Nkj эле-ментов. Для каждого из уровней ij j-го фактора (для каждой из таких групп) рассчитывается среднее значение изучаемого признака: xij(j) = kj N I−ij(j) xI ,
4.3. Дисперсионный анализ 163 где I−ij(j) означает суммирование по всем наблюдениям, в которых j-й фактор находится на уровне ij . Если бытот факт, что j-й фактор находится на уровне ij , не влиял на изучаемый признак, означало бы, что xij(j) = b0. Потому bij (j) = xij(j) − b0 -коэффициент влияния на изучаемый признак то-го, что j-й фактор находится на уровне ij. Это-главные эффекты, илиэффекты 1-го порядка. Очевидно, что kj ij=1 bij (j) = 0 и дисперсия, определенная влиянием j-го фактора, равна s2j= 1 kj kj ij=1 bij(j)2. Каждые два фактора j и jделят совокупность на Kjj= kjkjгрупп по N Kjjэлементов. Для каждой из таких групп рассчитывается среднее изучаемого приз-нака: xij ij(jj) = KjjN I−ij ij(jj) xI , где I−ij ij(jj) означает суммирование по всем наблюдениям, в которых j-й фактор находится на уровне ij, а j-й фактор-на уровне ij. Если бы тот факт, что одновременно j-й фактор находится на уровне ij, а j-й фактор-на уровне ij, не влиял на изучаемый признак, то это означало бы, что xjjij ij(jj) = b0 + bij (j) + bij(j). Поэтому bij ij(jj) = xij ij(jj) − b0 + bij (j) + bij(j)
164 Глава 4. Введение в анализ связей -коэффициент влияния на изучаемый признак того, что одновременно j-й фактор находится на уровне ij, а j-й фактор-на уровне ij. Это эффекты взаимодей-ствия (или сочетания) факторов j и j, парные эффекты, или эффекты 2-го порядка. Легко убедиться в том, что kj ij=1 bij ij(jj) = kjij=1 bij ij(jj) = 0, и тогда s2jj= 1 Kjjij ,ijbij ij(jj)2 -дисперсия, определенная совместным влиянием факторов j и j. Рассмотрим общий случай. Факторы J = {j1j2 . . . jn}, nn делят совокупность на KJ =Jkj групп по N KJ элементов (выделяют группы класса J порядка n).Мультииндексом таких групп является I (J) = 3i1i2 . . . in4 3j1j2 . . . jn4= 3ij1ij2 . . . ijn4; конкретно данный мультииндекс именует группу, в которой фактор j1 находится на уровне ij1 и т.д. По каждой такой группе рассчитывается среднее изучаемого признака: xI(J) = KJ N I−I(J) xI , где I−I(J) -означает суммирование по всем наблюдениям, в которых фактор j1 находится на уровне ij1 и т.д . Как и в двух предыдущих случаях: bI(J) = xI(J) − ⎛⎝b0 + ¯ J∈J− bI( ¯ J) ⎞⎠ (4.40) -эффекты взаимодействия (или сочетания) факторов J, эффекты порядка n. Здесь ¯ J∈J− -суммирование по всем подмножествам множества J без самого множества J . Суммирование этих коэффициентов по всем значениям любого индекса, входя-щего в мультииндекс I(J) дает нуль. s2J = 1 KJ I(J) b2I(J)
4.3. Дисперсионный анализ 165 -дисперсия, определенная совместным влиянием факторов J. При определении эффектов наивысшего порядка J = G, xI(G) = xI, KG = N. Из способа получения коэффициентов эффектов должно быть понятно, что xI = b0 + G J=1 bI(J). Все факторные дисперсии взаимно независимы и общая дисперсия изучаемого признака в точности раскладывается по всем возможным сочетаниям факторов: s2 = G J=1 s2J. (4.41) Данное выражение называют дисперсионным представлением, или тожде-ством. Этот факт доказывается в IV части книги. Пока можно его только проверить, например, при n = 2. Используя 1-й способ обозначений (см. п. 4.1): b0 = 1 k1k2 i1,i2 xi1i2 , xi1∗ = 1 k2i2 xi1i2, bi1∗ = xi1∗ − b0, s21= 1 k1i1 b2i1∗, x∗i2 = 1 k1i1 xi1i2, b∗i2 = x∗i2 − b0, s22= 1 k2i2 b2∗i2 , bi1i2 = xi1i2 − b0 − bi1∗ − b∗i2, s212 = 1 k1k2 i1,i2 b2i1i2 . Теперь, раскрывая скобки в выражении для s212 и учитывая, что ˆxi1i2 = xi1i2 − b0, получаем: s212 = 1 k1k2 i1,i2 ˆx2i1i2 + 1 k1i1 b2i1∗ + 1 k2i2 b2∗i2 − 2 k1k2i1 bi1∗ i2 ˆxi1i2 ←−−−−→ =k2bi1∗ − − 2 k1k2i2 b∗i2 i1 ˆxi1i2 ←−−−−→ =k1b∗i2 + 2 k1k2i1 bi1∗ ←−−−→ =0 i2 b∗i2 ←−−−→ ←−−−−−−−−−−−=−0−→ =0 = s2 − s21− s22. Т.е. s2 = s21+ s22+ s212, что и требовалось показать.
166 Глава 4. Введение в анализ связей В силу взаимной независимости эффектов оценки коэффициентов и дисперсий эффектов остаются одинаковыми в любой модели частичного анализа (в котором рассматривается лишь часть всех возможных сочетаний факторов) и совпадают с оценками полного анализа. Дисперсия s2J имеет K−J степеней свободы: K−J =J (kj − 1) . Сумма этих величин по всем J от 1 до G равна N −1. В этом легко убедиться, если раскрыть скобки в следующем тождестве: N =G ((kj − 1) + 1). Процедура определения степени влияния факторов на изучаемый признак мо-жет быть следующей. На 1-м шаге выбирается сочетание факторов J1, оказывающих наибольшее влияние на изучаемый признак. Этими факторами будут такие, для которых мини-мума достигает показатель pv статистики Фишера Fc 1 = s2J1 N − KJ1 − − 1s2 − s2J1KJ1 − . На 2-м шаге выбирается сочетание факторов J2, для которого минимума до-стигает показатель pv статистики Фишера Fc 2 = s2J1 + s2J2N − KJ1 − − KJ2 − − 1s2 − s2J1 − s2J2KJ1 − + KJ2 − . И так далее. Процесс прекращается, как только показатель pv достигнет за-данного уровня ошибки, например, 0.05. Пусть этим шагом будет t-й. Оставши-еся сочетания факторов формируют остаточную дисперсию. Как правило, в таком процессе сначала выбираются главные эффекты, затем парные и т.д., так что оста-точную дисперсию образуют эффекты высоких порядков. Расчетные значения изучаемого признака определяются по следующей формуле: xcI = b0 + t l=1 bI(Jl). Этим завершается рассмотрение модели полного многофакторного дисперси-онного анализа без повторений.
4.4. Анализ временных рядов 167 Несколько слов можно сказать о многофакторном дисперсионном анализе с повто-рениями. Если все NI 1, можно попытаться свести этот случай к предыдущему. Для каждой конечной группы рассчитываются среднее ¯xI и дисперсия s2I. Исполь-зуя приведенные выше формулы можно рассчитать коэффициенты и дисперсии всех эффектов, заменяя xI на ¯xI . К сожалению, в общем случае эффекты перестают быть взаимно независимыми, и в представлении общей дисперсии (4.41) кроме дис-персий эффектов различных сочетаний факторов появляются слагаемые с нижним индексом J ¯ J. Возникает неопределенность результатов и зависимость их от того набора сочетаний факторов, которые включены в анализ. Поэтому разные модели частичного анализа дают разные результаты, отличные от полного анализа. Имеется несколько частных случаев, в которых "хорошие" свойства оценок сохра-няются. Один из них-случай, когда все численности конечных групп одинаковы. Тогда дисперсионное тождество записывается следующим образом: s2 = G J=1 s2J + IK I=I1 s2I ←−−−→ s2e , причемпоследнее слагаемое-остаточная, или внутригрупповая дисперсия-име-ет N − K−G − 1 степеней свободы. 4.4. Анализ временных рядов Временным или динамическим рядом называется совокупность наблюдений xi в последовательные моменты времени i = 1, . . . , N (обычно для индексации вре-менных рядов используется t, в этом пункте для целостности изложения материала сохранено i). Задача анализа временного ряда заключается в выделении и модели-ровании 3-х его основных компонент: xi = δi + γi + εi, i= 1, . . . , N, или в оценках: xi = di + ci + ei, i= 1, . . . , N, где δi, di -тренд, долговременная тенденция, γi, ci -цикл, циклическая составляющая, εi, ei -случайная компонента, с целью последующего использования построенных моделей в прикладном эконо-мическом анализе и прогнозировании.
168 Глава 4. Введение в анализ связей Для выявления долгосрочной тенденции используют различные методы. Наиболее распространено использование полиномиального тренда. Такой трендстроится как регрессия xi на полином определенной степени относительно времени: xi = a1i + a2i2 + . . . + b + ei, i= 1, . . . , N. Для выбора степени полинома можно использовать F-критерий: оценивают тренд как полином, последовательно увеличивая его степень до тех пор, пока уда-ется отвергнуть нулевую гипотезу. Трендм ожет быть экспоненциальным.Он строится как регрессия ln xi на по-лином от времени, так что после оценки параметров регрессии его можно записать в следующем виде: xi = ea1i+a2i2+...+b+ei, i= 1, . . . , N. Иногда тренд строится как сплайн, т.е. как некоторая "гладкая" композиция разных функций от времени на разных подпериодах. Пусть, например, на двух подпериодах [1, . . . , N1] и [N1 + 1, . . . , N] трендвы -ражается разными квадратическими функциями от времени (в момент времени N1 происходит смена тенденции): xi = a1i + a2i2 + b1 + ei1, i= 1, . . . , N1, xi = a3i + a4i2 + b2 + ei2, i= N1 + 1, . . . , N. Для того чтобы общий тренд был "гладким" требуют совпадения самих значений и значений первых производных двух полиномов в точке "перелома" тенденции: a1N1 + a2N2 1 + b1 = a3N1 + a4N2 1 + b2, a1 + 2a2N1 = a3 + 2a4N1. Отсюда выражают, например, a3 и b2 через остальные параметры и подставляют полученные выражения в исходное уравнение регрессии. После несложных преоб-разований уравнение приобретает следующий вид: xi = a1i + a2i2 + b1 + ei1, i= 1, . . . , N1, xi = a1i + a2 i2 − (i − N1)2+ b1 + a4 (i − N1)2 + ei2, i= N1 + 1, . . . , N. Параметры полученного уравнения оцениваются, и, тем самым, завершается по-строение тренда как полиномиального сплайна.
4.4. Анализ временных рядов 169 Для выявления долговременной тенденции применяют также различные прие-мы сглаживания динамических рядов с помощью скользящего среднего. Одиниз подходов к расчету скользящей средней заключается в следующем: в ка-честве сглаженного значения xi, которое по аналогии с расчетным значением мож-но обозначить через xci, принимается среднее значений xi−p, . . . , xi, . . . , xi+p, где p -полупериодсглажив ания. Сам процесс сглаживания заключается в по-следовательном расчете (скольжении средней) xcp+1, . . . , xcN−p. При этом часто теряются первые и последние p значений исходного временного ряда. Для сглаживания могут использоваться различные средние. Так, например, при полиномиальном сглаживании средние рассчитываются следующим образом. Пусть сглаживающим является полином q-й степени. Оценивается регрессия вида: xi+l = a1l + a2l2 + . . . + aqlq + b + ei+l, l= −p, . . . , p, и в качестве сглаженного значения xciпринимается b (расчетное значение при l = 0).Так, при q = 2 и p = 2 уравнение регрессии принимает следующий вид (исключая i как текущий индекс): ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x−2 x−1 x0 x1 x2 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ −2 4 1 −1 1 1 0 0 1 1 1 1 2 4 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎣ a1 a2 b ⎤⎥⎥⎥⎥⎦ + ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ e−2 e−1 e0 e1 e2 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . По аналогии с (4.29), можно записать: ⎡⎢⎢⎢⎢⎣ a1 a2 b ⎤⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ ⎡⎢⎢⎢⎢⎣ −2 −1 0 1 2 4 1 0 1 4 1 1 1 1 1⎤⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ −2 4 1 −1 1 1 0 0 1 1 1 1 2 4 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ −1 ⎡⎢⎢⎢⎢⎣ −2 −1 0 1 2 4 1 0 1 4 1 1 1 1 1⎤⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x−2 x−1 x0 x1 x2 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ =
170 Глава 4. Введение в анализ связей = 1 70 ⎡⎢⎢⎢⎢⎣ −14 −7 0 7 14 10 −5 −10 −5 10 −6 24 34 24 −6 ⎤⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x−2 x−1 x0 x1 x2 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Таким образом, в данном случае веса скользящей средней принимаются равными 1 35 [−3, 12, 17, 12, −3] . При полиномиальном сглаживании потеря первых и последних p наблюдений в сглаженном динамическом ряду не является неизбежной; их можно взять как рас-четные значения соответствующих наблюдений по первому и последнему полиному (в последовательности скольжения средней). Так, в рассмотренном примере при p = q = 2: ⎡⎢⎣ xc1 xc2 ⎤⎥⎦= ⎡⎢⎣ −2a1 + 4a2 + b −a1 + a2 + b ⎤⎥⎦= 1 35 ⎡⎢⎣ 31 9 −3 −5 3 9 13 12 6 −5 ⎤⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x1 x2 x3 x4 x5 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ , ⎡⎢⎣ xcN−1 xcN ⎤⎥⎦= ⎡⎢⎣ a1 + a2 + b 2a1 + 4a2 + b ⎤⎥⎦= 1 35 ⎡⎢⎣ −5 6 12 13 9 3 −5 −3 9 31 ⎤⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ xN−4 xN−3 xN−2 xN−1 xN ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Как видно, все эти расчетные значения являются средними взвешенными величина-ми с несимметричными весами. Для выбора параметров сглаживания p и q можно воспользоваться F-критерием (применение этого критерия в данном случае носит эвристический
4.4. Анализ временных рядов 171 характер). Для каждой проверяемой пары p и q рассчитывается сначала остаточ-ная дисперсия: s2e= 1N Ni=1 (xi − xci)2, а затем F-статистика: Fc = s2x − s2e(2p − q) s2eq , где s2x -полная дисперсия ряда. Выбираются такие параметрысглаживания, при которых эта статистика (q сте-пеней свободы в числителе и 2p − q степеней свободы в знаменателе) имеет наи-меньший показатель pv. Другой способ сглаживания называется экспоненциальным.Притакомспосо-бе в качестве сглаженного (расчетного) значения принимается среднее всех преды-дущих наблюдений с экспоненциально возрастающими весами: xc i+1 = (1 − a) ∞l=0 alxi−l, где 0 < a < 1 -параметр экспоненциального сглаживания (xciявляется на самом деле средней, т.к. ∞l=0 al = 1 1 − a). В такой форме процедура сглаживания неоперациональна, поскольку требует знания всей предыстории-до минус бесконечности. Но если из xc i+1 вычесть axci, то весь "хвост" предыстории взаимно сократится: xc i+1 − axci= (1 − a)xi + (1 − a) ∞l=1 alxi−l ←−−−−−−−−−−→− (1 − a) ∞l=0 al+1xi−1−l ←−−−−−−−−−−−−−−→ ↑ ↑ ←−−−−−−−−−−−−−−−−−−→ = . Отсюда получается правило экспоненциального сглаживания: xc i+1 = (1 − a)xi + axci, в соответствии с которым сглаженное значение в следующий момент времени по-лучается как среднее фактического и сглаженного значений в текущий момент времени. Для того чтобы сгладить временной ряд, используя это правило, необходимо задать не только a, но и xc1. Эти два параметра выбираются так, чтобы миниму-ма достигла остаточная дисперсия. Минимизация остаточной дисперсии в данном
172 Глава 4. Введение в анализ связей случае является достаточно сложной задачей, поскольку относительно a она (оста-точная дисперсия) является полиномом степени 2(N −1) (по xc1 -квадратичной функцией). Пусть долговременная тенденция выявлена. На ее основе можно попытаться сразу дать прогноз моделируемой переменной (прогноз, по-видимому, будет точнее, если в нем учесть все компоненты временного ряда). В случае тренда как аналитической функции от времени i, прогнозом является расчетное значение переменной в моменты времени N + 1, ,N +2, . . . . Процедура экспоненциального сглаживания дает прогноз на один момент вре-мени вперед: xcN+1 = (1 − a) xN + axcN. Последующие значения "прогноза" не будут меняться, т.к. отсутствуют основания для определения ошибки eN+1 и т.д. и, соответственно, для наблюдения различий между xcN+1 и xN+1 и т.д . При полиномиальном сглаживании расчет xcN+1 проводится по последнему по-линому (в последовательности скольжения средней) и оказывается равным неко-торой средней последних 2p +1 наблюдений во временном ряду. В приведенном выше примере (p = q = 2): xcN+1 = (b + 3a1 + 9a2) = 1 35 21 −21 −28 0 63 ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ xN−4 xN−3 xN−2 xN−1 xN ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Определение циклической и случайной составляющей временного ряда дается во II части книги. 4.5. Упражнения и задачи Упражнение 1 На основании информации о весе и росте студентов вашего курса: 1.1. Сгруппируйте студентов по росту и весу (юношей и девушек отдельно).
4.5. Упражнения и задачи 173 1.2. Дайте табличное и графическое изображение полученных совместных рас-пределений частот, сделайте выводы о наличии связи между признаками. 1.3. С помощью критерия Пирсона проверьте нулевую гипотезу о независимости роста и веса студентов. 1.4. С помощью дисперсионного анализа установите, существенно ли влияние роста на их вес. 1.5. На основе построенной таблицы сопряженности рассчитайте средние и дис-персии роста и веса, а также абсолютнуюи относительнуюковариациюмежду ними. 1.6. На основе исходных данных, без предварительной группировки (для юношей и девушек отдельно): : Оцените с помощью МНК параметры линейного регрессионного урав-нения, предположив, что переменная "рост" объясняется переменной "вес". Дайте интерпретацию полученным коэффициентам уравнения регрессии. : Повторите задание, предположив, что переменная "вес" объясняется переменной "рост". : Оцените с помощью МНК параметры ортогональной регрессии. : Изобразите диаграмму рассеяния признаков роста и веса и все три линии регрессии. Объясните почему, если поменять экзогенные и эндогенные переменные местами, получаются различные уравнения. : Для регрессионной зависимости роста от фактора веса вычислите объ-ясненную и остаточную дисперсию, рассчитайте коэффициент детер-минации и с помощью статистики Фишера проверьте статистическую значимость полученного уравнения. Упражнение 2 Дана таблица (табл. 4.1, индекс Доу-Джонса средних курсов на акции ряда промышленных компаний). 2.1. Изобразить данные, представленные в таблице, графически. 2.2. Найти оценки параметров линейного тренда. Вычислить и изобразить графи-чески остатки от оценки линейного тренда. 2.3. На основе данных таблицы
174 Глава 4. Введение в анализ связей Таблица 4.1 Год Индекс Год Индекс Год Индекс 1897 45.5 1903 55.5 1909 92.8 1898 52.8 1904 55.1 1910 84.3 1899 71.6 1905 80.3 1911 82.4 1900 61.4 1906 93.9 1912 88.7 1901 69.9 1907 74.9 1913 79.2 1902 65.4 1908 75.6 : произвести сглаживание ряда с помощью процедуры, основывающейся на q = 1 и p = 3 (q -степень полинома, p -полупериодсглажив а-ния); : произвести сглаживание ряда с помощью процедуры, основывающейся на q = 2 и p = 2. 2.4. Сравнить сглаженный ряд с трендом, подобранным в упражнении 2.2. Задачи 1. Используя интенсивность цвета для обозначения степени концентрации эле-ментов в группах, дайте графическое изображение совокупности, характери-зующейся: а) однородностью и прямой зависимостью признаков (x1, x2) ; б) однородностью и обратной зависимостью признаков (x1, x2) ; в) неоднородностью и прямой зависимостью признаков (x1, x2) ; г) неоднородностью и обратной зависимостью признаков (x1, x2) ; д) неоднородностью и отсутствием связи между признаками (x1, x2) . 2. Пусть заданы значения (x1, x2). Объясните, какие приемы следует приме-нять для оценки параметров следующих уравнений, используя обычныйметод наименьших квадратов: а) x1 = βxα2; б) x2 = βex1α; в) x1 = β + α ln(x2); г) x1 = x2/(β + αx2); д) x1 = β + α/(π − x2).
4.5. Упражнения и задачи 175 3. Может ли матрица а) ⎡⎢⎣ 2 3 3 4 ⎤⎥⎦б) ⎡⎢⎣ 4 3 2 3 ⎤⎥⎦ являться ковариационной матрицей переменных, для которых строятся урав-нения регрессии? Ответ обосновать. 4. Наблюдения трех пар (x1, x2) дали следующие результаты: i x2 i1 = 41, i x2 i2 = 14, i xi1xi2 = 23, i xi1 = 9, i xi2 = 6. Оценить уравнения прямой, обратной и ортогональной регрессии. 5. Построить уравнения прямой, обратной и ортогональной регрессии, если а) X1 = (1, 2, 3), X2 = (1, 0, 5); б) X1 = (0, 2, 0, 2), X2 = (0, 0, 2, 2); в) X1 = (0, 1, 1, 2), X2 = (1, 0, 2, 1). Нарисовать на графике в пространстве переменных облако наблюдений и линии прямой, обратной и ортогональной регрессии. Вычислить объяснен-ную, остаточную дисперсию и коэффициент детерминации для каждого из построенных уравнений регрессии. 6. Какая из двух оценок коэффициента зависимости баллов, полученных на экзамене, от количества пропущенных занятий больше другой: по прямой или по обратной регрессии. 7. В регрессии x1 = a12x2 + 1Nb1 + e1, фактор x1 равен (1, 3, 7, 1). Пара-метры регрессии найдены по МНК. Могут ли остатки быть равными: а) (1, −2, 2, 1); б) (1, −2, 1, −1). 8. Для рядов наблюдений x1 и x2 известны средние значения, которые равны соответственно 10 и 5. Коэффициент детерминации в уравнениях регрессии x1 на x2 равен нулю. Найти значения параметров простой регрессии x1 по x2. 9. В регрессии x1 = a12x2 +14b1 +e1, гд е x2 = (5, 3, 7, 1), получены оценки a12 = 2, b1 = 1, а коэффициент детерминации оказался равным 100%. Найти вектор фактических значений x1.
176 Глава 4. Введение в анализ связей 10. Изобразите на графике в пространстве двух переменных облако наблюдений и линию прямой регрессии, если коэффициент корреляции между перемен-ными: а) положительный; б) равен единице; в) отрицательный; г) равен минус единице; д) равен нулю. 11. Существенна ли связь между зарплатой и производительностью труда по вы-борке из 12 наблюдений, если матрица ковариаций для этих показателей имеет вид ⎡⎢⎣ 9 6 6 16 ⎤⎥⎦. 12. Оцените параметры ортогональной регрессии и рассчитайте остаточную дис-персию и коэффициент детерминации для переменных, у которых матрица ковариаций равна ⎡⎢⎣ 9 6 6 16 ⎤⎥⎦, а средние значения равны 3 и 4. 13. Имеются данные об объемах производства по четырем предприятиям двух отраслей, расположенным в двух регионах (млн. руб): Отрасль Регион 1 2 1 48 60 2 20 40 Рассчитать эффекты взаимодействия, факторную и общую дисперсии. 14. Имеются данные об инвестициях на предприятиях двух отраслей: Предприятие Инвестиции (млн. руб.) 1 50 Отрасль 1 2 60 3 40 4 110 Отрасль 2 5 160 6 150 
4.5. Упражнения и задачи 177 Рассчитать групповую, межгрупповую и общую дисперсии. 15. Имеются данные об урожайности культуры (в ц/га) в зависимости от способа обработки земли и сорта семян: Способы обработки земли (B) Сорт семян (A) 1 2 3 4 1 16 18 20 21 2 20 21 23 25 3 23 24 26 27 С помощью двухфакторного дисперсионного анализа оценить, зависит ли урожайность культуры от сорта семян (A) или от способа обработки земли. 16. Запишите систему нормальных уравнений оценивания параметров полино-миального тренда первой, второй и третей степеней. 17. Перенесите систему отсчета времени в середину ряда, т.е. i = . . .−3; −2; −1; 0; 1; 2; 3 . . ., и перепишите систему нормальных уравнений для поли-номиального тренда первой, второй и третей степеней. Как изменится вид системы? Найдите оценку параметров многочленов в явном виде из получен-ной системы уравнений. 18. По данным о выручке за 3 месяца: 11, 14, 15 -оцените параметры поли-номиального тренда первой степени и сделайте прогноз выручки на четвертый месяц. 19. Имеются данные об ежедневных объемах производства (млн. руб.): День 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Объем 9 12 27 15 33 14 10 26 18 24 38 28 45 32 41 Проведите сглаживание временного ряда, используя различные приемы скользящего среднего: а) используя полиномиальное сглаживание; б) используя экспоненциальное сглаживание. 20. Оценена регрессия xi = β + αs sin(ωi) + αc cos(ωi) + εi для частоты π/2. При этом αs = 4 и αc = 3. Найти значения амплитуды, фазы и периода. 21. Что называется гармоническими частотами? Записать формулу с расшиф-ровкой обозначений. 22. Что такое частота Найквиста? Записать одним числом или символом.
178 Глава 4. Введение в анализ связей 23. Строится регрессия с циклическими компонентами: xi = β + k j=1 (αsj sin(ωji) + αcj cos(ωj i)) + εi, i= 1, . . . , 5, k= 2. Запишите матрицу ковариаций факторов для данной регрессии. Рекомендуемая литература 1. Доугерти К. Введение в эконометрику.-М.: "Инфра-М", 1997. (Гл. 2). 2. КендэлМ.Временныеряды.-М.:"Финансыи статистика",1981. (Гл. 3-5,8). 3. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 2).
Часть II Эконометрия-I: Регрессионный анализ 179
Это пустая страница
В этой части развиваются положения 4-й главы "Введение в анализ связей" I-й части книги. Предполагается, что читатель знаком с основными разделами тео-рии вероятностей и математической статистики (функции распределения случай-ных величин, оценивание и свойства оценок, проверка статистических гипотез), линейной алгебры (свойства матриц и квадратичных форм, собственные числа и вектора). Некоторые положения этих теорий в порядке напоминания приводятся в тексте. В частности, в силу особой значимости здесь дается краткий обзор функций распре-деления, используемых в классической эконометрии (см. также Приложение A.3.2). Пусть ε -случайная величина, имеющая нормальное распределение с нулевымма-тематическиможиданиеми единичной дисперсией ( ε ∼ N(0, 1)).Функция плотности этого распределения прямо пропорциональна e−ε2 2 ; 95-процентный двусторонний квантиль ˆε0.95 равен 1.96, 99-процентный квантиль- 2.57. Пусть теперь имеется k таких взаимно независимых величин εl ∼ N(0, 1), l = 1, . . . , k. Сумма их квадратов kl=1 ε2lявляется случайной величиной, имею-щей распределение χ2 c k степенями свободы (обозначается χ2k).Математическое ожидание этой величины равно k, а отношение χ2k/k при k → ∞ стремится к 1, т.е. в пределе χ2 становится детерминированной величиной. 95-процентный (одно-сторонний) квантиль ˆχ2k,0.95 при k = 1 равен 3.84 (квадрат 1.96), при k = 5 - 11.1, при k = 20 - 31.4, при k = 100 - 124.3 (видно, что отношение χ2k,0.95/k приближается к 1). Если две случайные величины ε и χ2k независимы друг от друга, то случайная величина tk = ε 'χ2k/k имеет распределение t-Стьюдента с k степенями свободы. Ее функция распределения пропорциональна 1 + t2k k −k+1 2 ; в пределе при k→∞ она становится нормально распределенной. 95-процентный двусторонний кван-тиль ˆtk, 0.95 при k = 1 равен 12.7, при k = 5 - 2.57, при k = 20 - 2.09, при k = 100 - 1.98, т.е. стремится к ˆε0.95. Если две случайные величины χ2k1 и χ2k2 не зависят друг от друга, то случайная величина Fk1,k2 = χ2k1 /k1 χ2k2 /k2 имеет распределение F -Фишера с k1 и k2 степенями свободы (соответственно, в числителе и знаменателе). При k2 →∞ эта случайная величина стремится к χ2k1/k1, т.е. k1Fk1,∞ = χ2k1 .Очевидно также, что F1,k2 = t2k2 . 95-процентный (односторонний) квантиль ˆ F1,k2,0.95 при k2 = 1 равен 161, при k2 = 5 - 6.61, при k2 = 20 - 4.35, при k2 = 100 - 3.94 (квадраты соответ-ствующих tk,0.95); квантиль ˆ F2,k2,0.95 при k2 = 1 равен 200, при k2 = 5 - 5.79, при k2 = 20 - 3.49, при k2 = 100 - 3.09; квантиль ˆ Fk1,20,0.95 при k1 = 3равен 3.10, при k1 = 4 - 2.87, при k1 = 5 - 2.71, при k1 = 6 - 2.60.
Глава 5 Случайные ошибки Задачей регрессионного анализа является построение зависимости изучаемой случайной величины x от факторов z : x = f(z,A) + ε, где A -параметры зависимости. Если z -истинный набор факторов, полностью определяющий значение x, а f -истинная форма зависимости, то ε -случайные ошибки измерения x. Однако в экономике весьма ограничены возможности построения таких истинных моделей, прежде всего потому, что факторов, влияющих на изучаемую величи-ну, слишком много. В конкретных моделях в лучшем случае наборы z включают лишь несколько наиболее значимых факторов, и влияние остальных, неучтенных, факторов определяет ε. Поэтому ε называют просто случайными ошибками или остатками. В любом случае считают, что ε -случайные величины с нулевым математиче-ским ожиданием и, как правило, нормальным распределением. Последнее следует из центральной предельной теоремы теории вероятностей, поскольку ε по своему смыслу является результатом (суммой) действия многих мелких малозначимых по отдельности факторов случайного характера. Действительно, в соответствии с этой теоремой, случайная величина, являющаяся суммой большого количества других случайных величин, которые могут иметь раз-личные распределения, но взаимно независимы и не слишком различаются между собой, имеет асимптотически нормальное распределение, т.е. чем больше случайных величин, тем ближе распределение их суммы к нормальному.
5.1. Первичные измерения 183 5.1. Первичные измерения Пусть имеется N измерений xi, i = 1, . . . , N, случайной величины x, т.е. N наблюдений за случайной величиной. Предполагается, что измерения проведены в неизменных условиях (факторы, влияющие на x, не меняют своих значений), и систематические ошибки измерений исключены. Тогда различия в результатах отдельных наблюдений (измерений) связаны только с наличием случайных ошибок измерения: xi = β + εi, i= 1, . . . , N, (5.1) где β -истинное значение x, εi -случайная ошибка в i-м наблюдении. Такой набор наблюдений называется выборкой. Понятно, что это-идеальная модель, которая может иметь место в естествен-нонаучных дисциплинах (в управляемом эксперименте). В экономике возможности измерения одной и той же величины в неизменных условиях практически отсут-ствуют. Определенные аналогии с этой моделью возникают в случае, когда неко-торая экономическая величина измеряется разными методами (например, ВВП- по производству или по использованию), и наблюдениями выступают результаты измерения, осуществленные этими разными методами. Однако эта аналогия до-статочно отдаленная, хотя бы потому, что в модели N предполагается достаточно большим, а разных методов расчета экономической величины может быть в луч-шем случае два-три. Тем не менее, эта модель полезна для понимания случайных ошибок. Если X и ε -вектор-столбцы с компонентами, соответственно, xi и εi, а 1N - N-мерный вектор-столбец, состоящий из единиц, то данную модель мож-но записать в матричной форме: X = 1Nβ + ε. (5.2) Предполагается, что ошибки по наблюдениям имеют нулевое математическое ожидание в каждом наблюдении: E(εi) = 0, i = 1, . . . , N; линейно не зависят друг от друга: cov (εi, εj) = 0, i = j; а их дисперсии по наблюдениям одинаковы: var (εi) = σ2, i = 1, . . . , N или, в матричной форме: E(εε) = INσ2, гд е σ2 - дисперсия случайныхошибок или остаточная дисперсия, IN -единичнаяматрица размерности N. Это-обычные гипотезы относительно случайных ошибок. Требуется найти b и ei -оценки, соответственно, β и εi. Для этого исполь-зуется методнаименьш их квадратов (МНК), т.е. искомые оценки определяются так, чтобы Ni=1 (xi − b)2 = Ni=1 e2i= ee → min!, гд е e вектор-столбец оценок ei.
184 Глава 5. Случайные ошибки В результате, b = ¯x = 1N Ni=1 xi = 1 N 1NX, e = X − 1Nb, т.к. dee db = −2(xi − b) = 0. Кроме того, d2ee db2 = 2N >0, следовательно, в дан-ной точке достигаетсяминимум, т.е.МНК-оценкой истинного значения измеряемой величины является, как и следовало ожидать, среднее арифметическое по наблю-дениям, а среднееМНК-оценок остатков равно нулю: ¯e = 1 N 1N (X − 1Nb) = ¯x − b = 0. Оценка b относится к классу линейных, поскольку линейно зависит от наблюдений за случайной величиной. Полученная оценка истинного значения является несмещенной (т.е. ее мате-матическое ожидание равно истинному значению оцениваемого параметра), что можно легко показать. Действительно: b = 1 N xi (5.1) = 1 N (β + εi) = β + 1 N εi, (5.3) E(b) β-детер-минировано = β + 1 N E(εi) E(εi)=0 = β. Что и требовалось доказать. Однако несмещенной оценкой β является и любое наблюдение xi, т.к. из (5.1) следует, что E(xi) = β. Легко установить, что оценка b лучше, чем xi, т.к. имеет меньшую дисперсию (меньшую ошибку), то есть является эффективной. Более того, b -наилучшая в этом смысле оценка во множестве всех возможных линейных несмещенных оце-нок. Ее дисперсия минимальна в классе линейных несмещенных оценок и опреде-ляется следующим образом: σ2b = 1 N σ2, (5.4) т.е. она в N раз меньше, чем дисперсия xi, которая, как это следует из (5.1), равна σ2.
5.1. Первичные измерения 185 Действительно, множество всех линейных оценок по определению представляется следующим образом: b∗ = N i=1 dixi, где di -любые детерминированные числа. Из требования несмещенности, E(b∗) = β, следует, что di = 1, т.к. E(b∗) = Edixidi-детер-минировано = di E(xi) ←−−→ β = βdi. Таким образом, множество всех линейных несмещенных оценок описывается так: b∗ = N i=1 dixi, N i=1 di = 1. В этом множестве надо найти такую оценку (такие di ), которая имеет наименьшую дисперсию, b∗ =dixi (5.1) = β di ←−−→ =1 +diεi, откуда b∗ − β = diεi , и можно рассчитать дисперсию b∗ : var(b∗) = σ2b∗ = E(b∗ − E(b∗)2= = Ediεi2E(εiεi)=0 = d2iE(ε2i) E(ε2i )=σ2 = σ2d2i. Минимум d2iпри ограничении di = 1 достигается, если все di одинаковы и равны 1N , т.е. если b∗ = b. Отсюда, в частности, следует, что σ2b = 1N σ2. Что и требовалось доказать. Такие оценки относятся к классу BLUE-Best Linear Unbiased Estimators. Кроме того, оценка b состоятельна (стремится при N → ∞ к истинному значению параметра), т.к. она несмещена и ее дисперсия, как это следует из (5.4), при N →∞ стремится к 0.
186 Глава 5. Случайные ошибки Чтобы завершить рассмотрение данного случая, осталось дать оценку остаточ-ной дисперсии. Естественный "кандидат" на эту "роль"-дисперсия x : s2 = 1 N (xi − b)2 = 1 N ei = 1N ee, -дает смещенную оценку. Для получения несмещенной оценки остаточной дис-персии сумму квадратов остатков надо делить не на N, а на N − 1 : ˆs2 = 1 N − 1ee, (5.5) поскольку в векторе остатков e и, соответственно, в сумме квадратов остатков ee линейно независимых элементов только N − 1 (т.к. 1Ne = 0). Этот факт можно доказать строго. Если просуммировать по i соотношения (5.1) и поделить обе части полученного выражения на N, то окажется, что b = β + 1N εi . Кроме того, известно, что xi = β + εi = b + ei .Объединяя эти два факта можно получить следующее выраже-ние: ei = εi − 1 N εi, (5.6) (т.е. оценки остатков равны центрированным значениям истинных случайных оши-бок), и далее получить ee =εi − 1 N εi2 =ε2i− 2 N εi2 + 1 N εi2 = =ε2i− 1 N εi2 . Наконец: E(ee) E(ε2i )=σ2, E(εiεi)=0 = (N − 1) σ2, т.е. E1 N − 1 ee= σ2. Что и требовалось доказать. Теперь относительно случайных ошибок вводится дополнительное предполо-жение: они взаимно независимы (а не только линейно независимы) и распре-делены нормально: εi ∼ NID 0, σ2. NID расшифровывается как normally and
5.1. Первичные измерения 187 independently distributed (нормально и независимо распределенные случайные величины). Тогда становится известной функция плотности вероятности εi : f (εi) = (2π)−12 σ−1e− 1 2σ2 ε2i = (2π)−12 σ−1e− 1 2σ2 (xi−β)2 , и функция совместной плотности вероятности (произведение отдельных функ-ций плотности, так как случайные ошибки по наблюдениям взаимно независи-мы) (см. Приложение A.3.2): f (ε1, . . . , εN) = (2π)−N2 σ−Ne− 1 2σ2 (xi−β)2 . Эта функция рассматривается как функция правдоподобия L(σ, β), значения которой показывают "вероятность" (правдоподобность) появления наблюдаемых xi, i = 1, . . . , N, при тех или иных значениях σ и β. Имея такую функцию, можно воспользоваться для оценки параметров σ и β методом максимального правдоподобия(ММП): в качестве оценок принять такие значения σ и β, которые доставляют максимум функции правдоподобия (фактически предполагая, что, раз конкретные xi, i = 1, . . . , N реально наблюдаются, то вероятность их появления должна быть максимальной). Обычно ищется максимум не непосредственно функции правдоподобия, а ее логарифма (значения этой функции при конкретных xi и конечных σ положи-тельны, и их можно логарифмировать; эта операция, естественно, не меняет точки экстремума), что проще аналитически. lnL(σ, β) = −N2 ln 2π − N ln σ − 1 2σ2 (xi − β)2. Ищутся производные этой функции по σ и β, приравниваются нулю и опреде-ляются искомые оценки: ∂ lnL ∂β = 1 σ2(xi − β) = 0 ⇒ β = ¯x = b, ∂ lnL ∂σ = −Nσ + 1 σ3e2i= 0 ⇒ σ2 = 1 N e2i= s2. Это точка минимума, поскольку матрица 2-х производных −Ns2 ⎡⎢⎣ 1 0 0 2⎤⎥⎦ в ней отрицательно определена.
188 Глава 5. Случайные ошибки Таким образом, ММП-оценки β и εi совпадают с МНК-оценками, но ММП-оценка σ2 равна не ˆs2, а s2, т.е. является смещенной. Тем не менее, эта оценка состоятельна, т.к. при N →∞ различия между ˆs2 и s2 исчезают. Известно, что метод максимального правдоподобия гарантирует оценкам со-стоятельность и эффективность, т.е. они обладают минимально возможными дис-персиями (вообще, а не только в классе линейных несмещенных, как оценки класса BLUE). Врамках гипотезы о нормальности ошибок ε можно построить доверительный интервал для истинного значения параметра, т.е. интервал, в который это значение попадает с определенной вероятностью 1− θ, гд е θ -уровень ошибки (аналоги-чен величинам sl и pv, введенным во 2-й и 4-й главах I части книги; в прикладных исследованиях уровень ошибки принимается обычно равным 0.05). Он называется (1−θ)100-процентным (например, при θ = 0.05 - 95-процентным) доверитель-ным интервалом. Следствием нормальности ε является нормальность b : b ∼ N β, σ2 N . По-этому (b − β)√N σ ∼ N (0, 1), (5.7) и, по определению двустороннего квантиля (см. п. 2.3), (b − β)√N σ ˆε1−θ, где ˆε1−θ - (1 − θ)100-процентный двусторонний квантиль нормального распре-деления. Откуда β ∈ b ± √σN ˆε1−θ(5.8) -искомый (1 − θ)100-процентный доверительный интервал. К сожалению, на практике этой формулой доверительного интервала восполь-зоваться невозможно, т.к. она предполагает знание остаточной дисперсии σ2. Из-вестна же только ее оценка ˆs2. Простая замена в (5.8) σ на ˆs будет приводить к систематическим ошибкам- к преуменьшению доверительного интервала, т.е. к преувеличению точности рас-чета. Чтобы получить правильную формулу расчета, необходимо провести дополни-тельные рассуждения.
5.1. Первичные измерения 189 Прежде всего, доказывается, что ee σ2 ∼ χ2N−1. (5.9) Справедливость этого утверждения достаточно очевидна, поскольку, как было по-казано выше, сумма квадратов ee имеет N − 1 степень свободы, но может быть доказана строго. В матричной форме выражение (5.6) записывается следующим образом: e = Bε, (5.10) где B = IN − 1 N 1N1N. Матрица B размерности N × N : а) вещественна и симметрична ( B= B), поэтому она имеет N вещественных корней, которые можно "собрать" в диагональной матрице Λ, и N взаимно орто-гональных вещественных собственных векторов, образующих по столбцам матрицу Y . Пусть проведена надлежащая нормировка и длины этих собственных векторов равны 1. Тогд а:Y Y = IN, Y= Y −1, BY = Y Λ, B= Y ΛY ; (5.11) б) вырождена и имеет ранг N − 1. Действительно, имеется один и только один (с точностью до нормировки) вектор ξ = 0, который дает равенство Bξ = 0. Все компоненты этого единственного вектора одинаковы, т.к., как было показано выше, Bξ -центрированный ξ. В частности, B1N = 0. (5.12) Это и означает, что ранг B равен N − 1; в) идемпотентна, т.е. B2 = B (см. Приложение A.3.2): B2 = IN − 1 N 1N1NIN − 1 N 1N1N= = IN − 1 N 1N1N − 1 N 1N1N + 1 N2 1N 1N1 ←−−→N =N 1N ←−−−−−−−−−−−−−−−−−−→ =0 = B. Далее, пусть u = 1σ Y ε, uj = 1σ Y j ε, (5.13) где Yj - j-й собственный вектор матрицы B.
190 Глава 5. Случайные ошибки Очевидно, что E(uj) = 0, дисперсии uj одинаковы и равны 1: Eu2j= 1 σ2EY j εεYjE(εε)=σ2IN = Y j Yj (5.11) = 1, и uj взаимно независимы (при j = j): E(ujuj) аналогично = Y j Yj(5.11) = 0. Тогдаee σ2 (5.10) = 1 σ2 εBBε B=B, B2=B = 1 σ2 εBε (5.11) = 1 σ2 εY ΛY ε (5.13) = uΛu. (5.14) Собственные числа матрицы B, как и любой другой идемпотентной матрицы, равны либо 1, либо 0 ( λ -любое собственное число, ξ-соответствующий собственный вектор): Bξ = λξ, Bξ = B2ξ = Bξλ = λ2ξ ⇒ λ2 = λ ⇒ λ = 0, 1. и, поскольку ранг матрицы B равен N − 1, среди ее собственных чисел имеется N − 1, равных 1, и одно, равное 0. Поэтому (5.14), в соответствии с определе-нием случайной величины, имеющей распределение χ2, дает требуемый результат (см. также Приложение A.3.2). Случайные величины, определенные соотношениями (5.7, 5.9), некоррелиро-ваны, а, следовательно, и взаимно независимы по свойствам многомерного нор-мального распределения (см. Приложение A.3.2). Действительно: b − β (5.3) = 1 N 1Nε, cov(e, b) = E(e (b − β)) (5.10) = E(Bεε1N 1 N ) E(εε)=σ2IN = σ2 N B1N (5.12) = 0. Что и требовалось доказать. Поэтому, в соответствии с определением случайной величины, имеющей t-распределение (см. также Приложение A.3.2): (b − β)√N σ 5!ee σ2 /(N − 1) ∼ tN−1,
5.1. Первичные измерения 191 и после элементарных преобразований (сокращения σ и замены (5.5)) получается следующий результат: (b − β)√N ˆs ∼ tN−1. Откуда: β ∈ b ± √sˆN ˆtN−1,1−θ, (5.15) где ˆtN−1,1−θ - (1 − θ)100-процентный двусторонний квантиль tN−1-распреде-ления. Это-операциональная (допускающая расчет) форма доверительного интер-вала для β. Как видно, для ее получения в (5.8) надо заменить не только σ на ˆs, но и ˆε1−θ на ˆtN−1,1−θ. Т. к. ˆtN−1,1−θ > ˆε1−θ, использование (5.8) с простой заменой σ на ˆs действительно преуменьшает доверительный интервал (преувеличивает точность расчета). Но по мере роста N (объема информации), в соответствии со свойствами t-распределения, доверительный интервал сужается (растет точность расчета), и в пределе при N →∞ он совпадает с доверительным интервалом (5.8) (с простой заменой σ на ˆs). Важнымявляется вопрос содержательной интерпретации доверительных ин-тервалов. Понятно, что в рамках подхода объективной вероятности непосредственно утверждения (5.8, 5.15) не могут считаться корректными. Величина β -детерми-нирована и не может с какой-либо вероятностью 0 < 1 −θ < 1 принадлежать кон-кретному интервалу. Она может либо принадлежать, либо не принадлежать этому интервалу, т.е. вероятность равна либо 1, либо 0. Потому в рамках этого подхода интерпретация может быть следующей: если процедуру построения доверитель-ного интервала повторять многократно, то (1 − θ) - 100 процентов полученных интервалов будут содержать истинное значение измеряемой величины. Непосредственно утверждения (5.8, 5.15) справедливы в рамках подхода субъ-ективной вероятности. Рассмотренная модель (5.1) чрезвычайно идеализирует ситуацию: в экономике условия, в которых измеряются величины, постоянно меняются. Эти условия пред-ставляются некоторым набором факторов zj , j = 1, . . . , n, и модель "измерения" записывается следующим образом: xi = n j=1 zijαj + β + εi, i= 1, . . . , N, где zij -наблюдения за значениями факторов, αj , j = 1, . . . , n, β -оценива-емые параметры.
192 Глава 5. Случайные ошибки Такая модель-это предмет регрессионного анализа. Рассмотренная же мо-дель (5.1) является ее частным случаем: формально-при n = 0, по существу- при неизменных по наблюдениям значениях факторов zij = zcj ( c -const), так что оцениваемый в (5.1) параметр β в действительности равен zcjαj + β. Прежде чем переходить к изучению этой более общей модели, будут рассмот-рены проблемы "распространения" ошибок первичных измерений (в этой гла-ве) и решены алгебраические вопросы оценки параметров регрессии (следующая глава). 5.2. Производные измерения Измеренные первично величины используются в различных расчетах (в произ-водных измерениях), и результаты этих расчетов содержат ошибки, являющиеся следствием ошибок первичных измерений. В этом пункте изучается связь между ошибками первичных и производных измерений, или проблема "распространения" ошибок первичных измерений. Возможна и более общая трактовка проблемы: вли-яние ошибок в исходной информации на результаты расчетов. Пусть xj , j = 1, . . . , n,-выборочные (фактические) значения (наблюдения, измерения) n различных случайных величин, βj -их истинные значения, εj - ошибки измерений. Если x, β, ε -соответствующие n-компонентные вектор-строки, то x = β + ε. Предполагается, что E(ε) = 0 и ковариационная матрица ошибок E(εε) равна Ω. Пусть величина y рассчитывается как f(x). Требуется найти дисперсию σ2y ошибки εy = y − f(β) измерения (расчета) этой величины. Разложение функции f в рядТэйлора в фактической точке x по направлению β − x (= −ε), если в нем оставить только члены 1-го порядка, имеет вид: f(β) = = y − εg (заменяя "≈" на "=") или εy = εg, гд е g -град иент f в точке x (вектор-столбец с компонентами gj = ∂f ∂xj (x)). Откуда E(εy) = 0 и σ2y = Eε2y= EgεεgE(εε)=Ω = gΩg. (5.16) Это-общая формула, частным случаем которой являются известные форму-лы для дисперсии среднего, суммы, разности, произведения, частного от деления и д р. Пусть n = 2, Ω = ⎡⎢⎣ σ21 ω ω σ22 ⎤⎥⎦.
5.2. Производные измерения 193 а) если y = x1 ± x2, то: g = ⎡⎢⎣ 1 ±1 ⎤⎥⎦, σ2y = σ21 + σ22 ± 2ω. б) если y = x1x2, то: g = ⎡⎢⎣ x2 x1 ⎤⎥⎦, σ2y = x22σ21 + x21σ22 + 2x1x2ω или σ2y y2 = σ21 x21+ + σ22 x22+ 2 ω x1x2 . в) если y = x1 x2 , то: g = ⎡⎢⎣ 1 x2 −x1 x22 ⎤⎥⎦, σ2y = 1 x22σ21 + x21 x42σ22 − 2 x21 x32ω или σ2y y2 = σ21 x21+ σ22 x22− − 2 ω x1x2 . Случаи (б) и (в) можно объединить: если y = x1x±1 2 , то σ2y y2 = σ21 x21+ σ22 x22± 2 ω x1x2 Можно назвать σy, σ1, σ2 абсолютными, а σy y , σ1 x1 , σ2 x2 -относительными ошибка-ми, и, как только что показано, сделать следующие утверждения. Если ошибки аргументов не коррелированы ( ω = 0), то квадрат абсолютной ошиб-ки суммы или разности равен сумме квадратов абсолютных ошибок аргументов, а квадрат относительной ошибки произведения или частного от деления равен сумме квадратов относительных ошибок аргументов. Если ошибки аргументов коррелированы положительно (ω > 0), то ошибка сум-мы или произведения возрастает (предполагается, что x1x2 > 0), а разности или частного от деления-сокращается. Влияние отрицательной корреляции ошибок аргументов противоположное. Выражение (5.4), которое фактически дает формулу ошибки среднего, также явля-ется частным случаем (5.16). Действительно, в данном случае y = 1N Ni=1 xi , Ω = σ2IN , и поскольку g = 1 N ⎡⎢⎢⎢⎢⎣ 1... 1 ⎤⎥⎥⎥⎥⎦ , то σ2y = 1 N σ2. В случае, если ошибки величин xj не коррелированы друг с другом и имеют одинаковую дисперсию σ2 (Ω = σ2In), то σ2y = σ2gg, (5.17) т.е. чем резче меняется значение функции в точке расчета, тем в большей сте-пени ошибки исходной информации влияют на результат расчета. Возможны си-туации, когда результат расчета практически полностью определяется ошибками "на входе".
194 Глава 5. Случайные ошибки В случае, если известны дисперсии ошибок εj , а информация о их ковариациях отсутствует, можно воспользоваться формулой, дающей верхнюю оценку ошибки результата вычислений: σy n j=1 |σjgj | = Δy, где σj -среднеквадратическое отклонение εj . Пусть в данном случае σ -диагональная матрица {σj}, тогд аΩ = σRσ, гд е R - корреляционная матрица ( rjj= ωjjσjσj). Тогда (5.16) преобразуется к виду: σ2y = gσRσg. Пусть далее |σg| -вектор-столбец {|σjgj|}, а W -диагональная матрица {±1} такая, что σg = W |σg|. Тогда σ2y = |gσ|WRW |σg| . (5.18) По сравнению с R в матрице WRW лишь поменяли знаки некоторые недиаго-нальные элементы, и поэтому все ее элементы, как и в матрице R, не превышают единицы: WRW 1n1n. Умножение обеих частей этого матричного неравенства справа на вектор-столбец |σg| и слева на вектор строку |gσ| сохранит знак " ", т.к. эти векторы, по опре-делению, неотрицательны. Следовательно: |gσ|WRW |σg| (5.18) = σ2y |gσ| 1n1n |σg| = |σjgj |2 . Что и требовалось доказать. 5.3. Упражнения и задачи Упражнение 1 Дана модель xi = β + εi = 12+εi, i = 1, . . . , N. Используя нормальное распределение, в котором каждое значение ошибки εi независимо, имеет среднее 0 и дисперсию 2, получите 100 выборок вектора ε размерности (N × 1), k = = 1, . . . , 100 , гд еN = 10 (в каждой выборке по 10 наблюдений). Прибавив к каждому элементу этой выборки число 12 получите 100 выборок вектора x.
5.3. Упражнения и задачи 195 1.1. Используйте 20 из 100 выборок, чтобы получить выборочную оценку bk для β (bk = 1 10 10 i=1 xik, k = 1, . . . , 20). 1.2. Вычислите среднее и дисперсию для 20 выборок оценок параметра β b = 1 20 20 k=1 bk, s2 = 1 20−1 20 k=1 (bk − b)2.Сравните эти средние значения с ис-тинными параметрами. 1.3. Для каждой из 20 выборк оцените дисперсию, используя формулу ˆs2 = 1 N − 1 Ni=1 (xi − b)2. Пусть ˆs2k -это оценка σ2 в выборке k. Рассчитайте 1 20 20 k=1 ˆs2k и сравните с истинным значением. 1.4. Объедините 20 выборок по 10 наблюдений каждая в 10 выборок по 20 на-блюдений и повторите упражнение 1.1-1.3. Сделайте выводы о результатах увеличения объема выборки. 1.5. Повторите упражнение 1.1-1.3 для всех 100 и д ля 50 выборок и проанали-зируйте разницу в результатах. 1.6. Постройте распределения частот для оценок, полученных в упражнении 1.5, сравните и прокомментируйте результаты. 1.7. Постройте 95%доверительный интервал для параметра β в каждой выбор-ке, сначала предполагая, что σ2 известно, а потом при условии, что истинное значение σ2 неизвестно. Сравните результаты. Задачи 1. При каких условиях средний за ряд лет темп инфляции будет несмещенной оценкой истинного значения темпа инфляции? 2. В каком случае средняя за ряд лет склонность населения к сбережению будет несмещенной оценкой истинного значения склонности к сбережению? 3. Пусть x1, x2, . . . , xN -независимые случайные величины, распределен-ные нормально с математическим ожиданием β и дисперсией σ2. Пусть b∗ = Ni=1 ixi Ni=1 i -это оценка β,
196 Глава 5. Случайные ошибки - покажите, что b∗ -относится к классу несмещенных линейных оценок; - рассчитайте дисперсию b∗; - проверьте b∗ на состоятельность; - сравните b∗ с простой средней b = 1 N Ni=1 xi; 4. Случайная величина измерена три раза в неизменных условиях. Получены значения: 99, 100, 101. Дать оценку истинного значения этой величины и стандартную ошибку данной оценки. 5. Измерения веса студента Иванова на четырех весах дали следующие резуль-таты: 80.5 кг, 80 кг, 78.5 кг, 81 кг. Дайте оценку веса с указанием ошибки измерения. 6. Пусть β -величина ВВП в России в 1998 г. Несколько различных экспер-тов рассчитали оценкиВВП xi. Какие условия для ошибок этих оценок xi−β должны выполнятся, чтобы среднее xi было несмещенной и эффективной оценкой β? 7. Проведено пять измерений некоторой величины. Результаты этих измерений следующие: 5.83, 5.87, 5.86, 5.82, 5.87 . Как бы вы оценили истинное значе-ние этой величины при доверительной вероятности 0.95 ? А при вероятности 0.99 ? 8. Предположим, что исследователь, упоминавшийся в задаче 7, полагает, что истинное стандартное отклонение измеряемой величины равно 0.02. Сколько независимых измерений он должен сделать, чтобы получить оценку значения величины, отличающуюся от истинного значения не более чем на 0.01: а) при 95%-ном доверительном уровне? б) при 99%-ном доверительном уровне? 9. Случайная величина измерена три раза в неизменных условиях. Получена оценка истинного значения этой величины 5.0 и стандартная ошибка этой оценки √13. Каким мог быть исходный ряд? 10. Пусть имеется 25 наблюдений за величиной x, и по этим данным построен 95%-ный доверительный интервал для x: [1.968; 4.032]. Найдите по этим данным среднее значение и дисперсию ряда. 11. Пусть xi -продолжительность жизни i-го человека ( i = 1, . . . , N), x - средняя продолжительность жизни, элементы выборки случайны и незави-симы. Ошибка измерения исходного показателя для всех i составляет 5%,
5.3. Упражнения и задачи 197 какова ошибка x ? Вывести формулу σ2¯x, рассчитать коэффициент вариации для x, если x1 = 50, x2 = 60, x3 = 70. 12. Пусть объем экспорта равен 8 условных единиц, а импорта- 7 условных единиц. Показатели некоррелированы, их дисперсии одинаковы и равны 1 условной единице. На каком уровне доверия можно утверждать, что сальдо экспорта-импорта положительно? 13. Средние рентабельности двух разных фирм равны соответственно 0.4 и 0.2, стандартные отклонения одинаковы и составляют 0.2.Действительно ли пер-вая фирма рентабельнее и почему? 14. Наблюдаемое значение некоторой величины в предыдущий и данный мо-мент времени одинаково и равно 10. Ошибки наблюдений не коррелированы и имеют одинаковую дисперсию. Какова относительная ошибка темпа роста? 15. Пусть величина ВНП в I и II квартале составляла соответственно 550 и 560 млрд. долларов. Ошибки при расчетах ВНП в I и II квартале не коррели-рованы и составляют 1%. Какова относительная ошибка темпа прироста ВНП во II квартале? К каким последствиям в расчетах темпов роста и темпов прироста приведут ошибки измерения ВНП, равные 5%? 16. Стандартная ошибка измерения показателя труда и показателя капитала составляет 1%, ошибки измерений не коррелированы. Найти относитель-ную ошибку объема продукции, рассчитанного по производственной функции Кобба-Дугласа: Y = CKαLβ. 17. Доля бюджетного дефицита в ВВП вычисляется по формуле (R−E)/Y , гд е R = 600 условных единиц-доходы бюджета, E = 500 условных единиц- расходы, Y = 1000 условных единиц-ВВП. Известно, что дисперсии R и E равна 100, дисперсия Y равна 25. Оценить сверху дисперсию доли дефицита. Рекомендуемая литература 1. Венецкий И.Г., Венецкая В.И. Основные математико-статистические по-нятия и формулы в экономическом анализе.-М.: "Статистика", 1979. (Разд. 7). 2. ЕзекиэлМ., Фокс К. Методы анализа корреляций и регрессий.-М.: "Ста-тистика", 1966. (Гл. 2).
198 Глава 5. Случайные ошибки 3. Кейн Э. Экономическая статистика и эконометрия.-М.: "Статистика", 1977. Вып. 1. (Гл. 8, 9). 4. Моргенштерн О.Оточности экономико-статистических наблюдений.-М.: "Статистика", 1968. (Гл. 2, 6). 5. Тинтер Г. Введение в эконометрию.-М.: "Статистика", 1965. (Гл. 1). 6. Frees Edward W. Data Analysis Using Regression Models: The Business Perspective, Prentice Hall, 1996. (Ch. 2). 7. (*) Judge G.G., Hill R.C., Griffiths W.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch. 3, 5). 8. William E.Griffiths, R. Carter Hill., George G. Judge Learning and Practicing econometrics, N 9 John Wiley & Sons, Inc., 1993. (Ch. 14).
Глава 6 Алгебра линейной регрессии 6.1. Линейная регрессия В этой главе предполагается, что между переменными xj , j = 1, . . . , n суще-ствует линейная зависимость: n j=1 xjαj = β + ε, (6.1) где αj , j = 1, . . . , n, β (угловые коэффициенты и свободный член)-параметры (коэффициенты) регрессии (их истинные значения), ε -случайная ошибка; или в векторной форме: xα = β + ε, (6.2) где x и α -соответственно вектор-строка переменных и вектор-столбец пара-метров регрессии. Как уже отмечалось в пункте 4.2, регрессия называется линейной, если ее уравнение линейно относительно параметров регрессии, а не переменных.Поэтому предполагается, что xj , j = 1, . . . , n, могут являться результатом каких-либо функциональных преобразований исходных значений переменных. Для получения оценок aj , j = 1, . . . , n, b , e, соответственно, параметров регрессии αj , j = 1, . . . , n, β и случайных ошибок ε используется N наблюде-ний за переменными x, i = 1, . . . , N, которые образуют матрицу наблюдений X
200 Глава 6. Алгебра линейной регрессии размерности N × n (столбцы-переменные, строки-наблюдения). Уравнение регрессии по наблюдениям записывается следующим образом: Xα = 1Nβ + ε, (6.3) где, как и прежде, 1N -вектор-столбец размерности N, состоящий из еди-ниц, ε -вектор-столбец размерности N случайных ошибок по наблюдениям; или в оценках: Xa = 1Nb + e. (6.4) Собственно уравнение регрессии (без случайных ошибок) xα = β или xa = b определяет, соответственно, истинную или расчетную гиперплоскость (линию, плоскость, . . . ) регрессии. Далее применяется методнаименьших квадратов: оценкипараметроврегрессии находятся так, чтобы минимального значения достигла остаточная дисперсия: s2e= 1 N ee = 1 N aX− b1N(Xa − 1Nb) . Из равенства нулю производной остаточной дисперсии по свободному члену b следует, что ¯xa = b (6.5) и 1Ne = 0. (6.6) Действительно, ∂s2e ∂b = − 2 N 1N (Xa − 1Nb) =⎧⎨ ⎩ − 2 (¯xa − b) , − 2 N 1Ne. Вторая производная по b равна 2, т.е. в найденной точке достигается минимум. Здесь и ниже используются следующие правила матричной записи результатов диф-ференцирования линейных и квадратичных форм. Пусть x, a -вектор-столбцы, α -скаляр, а M -симметричная матрица. То-гда: dxα dα = x, ∂xa ∂x = a, ∂xM ∂x = M, ∂xMx ∂x = 2Mx. (См. Приложение A.2.2.)
6.2. Простая регрессия 201 Этот результат означает, что точка средних значений переменных лежит на расчетной гиперплоскости регрессии. В результате подстановки выражения b из (6.5) через a в (6.4) получается другая форма записи уравнения регрессии: Xˆa = e, (6.7) где ˆX= X − 1N ¯x -матрица центрированных значений наблюдений. (6.3, 6.4)-исходная, (6.7)-сокращенная запись уравнения регрессии. Минимизация остаточной дисперсии по a без дополнительных условий приве-дет к тривиальному результату: a = 0. Чтобы получать нетривиальные решения, на вектор параметров α и их оценок a необходимо наложить некоторые огра-ничения. В зависимости от формы этих ограничений возникает регрессия разного вида-простая или ортогональная. 6.2. Простая регрессия В случае, когда ограничения на вектор a (α) имеют вид aj = 1 ( αj = 1), возникают простые регрессии. В таких регрессиях в левой части уравнения оста-ется одна переменная (в данном случае j-я), а остальные переменные переносятся в правую часть, и уравнение в исходной форме приобретает вид(регрессия j-й переменной по остальным, j-я регрессия): Xj = X−ja−j + 1Nbj + ej , (6.8) где Xj -вектор-столбец наблюдений за j-й переменной-объясняемой, X−j -матрица наблюдений размерности N × (n − 1) за остальными перемен-ными-объясняющими (композиция Xj и X−j образует матрицу X), a−j - вектор a без j-го элемента (равного 1), взятый с обратным знаком (компози-ция 1 и −a−j образует вектор a), bj и ej -соответственно свободный член и вектор-столбец остатков в j-й регрессии. В сокращенной форме: ˆXj = ˆX−ja−j + ej . (6.9) В таких регрессиях ошибки eij -расстояния от гиперплоскости регрессии до точек облака наблюдения-измеряются параллельно оси xj . Остаточная дисперсия приобретает следующую форму: s2ej = 1 N ejej = 1N ˆXj − a−j ˆX− jˆXj − ˆX−ja−j. (6.10)
202 Глава 6. Алгебра линейной регрессии Из равенства нулю ее производных по параметрам a−j определяется, что a−j = M−1 −j m−j , (6.11) где M−j = 1 N ˆX − j ˆX−j -матрица ковариации объясняющих переменных x−j между собой, m−j = 1 N ˆX − j ˆXj -вектор-столбец ковариации объясняющих пе-ременных с объясняемой переменной xj; и cov (X−j, ej) = 1 N ˆX − jej = 0. (6.12) Действительно, ∂s2ej ∂a−j = − 2 N ˆX − j ˆXj − ˆX−ja−j=⎧⎪⎨ ⎪⎩ −2(m−j −M−ja−j), − 2 N ˆX − jej . Кроме того, очевидно, что матрица вторых производных равна 2M−j, и она, как всякая ковариационная матрица, положительно полуопределена. Следовательно, в найденной точке достигается минимум остаточной дисперсии. Справедливость утверждения о том, что любая матрица ковариации (теоретическая или ее оценка) положительно полуопределена, а если переменные линейно незави-симы, то-положительно определена, можно доказать в общем случае. Пусть x -случайный вектор-столбец с нулевым математическим ожиданием. Его теоретическая матрица ковариации по определению равна E(xx). Пусть ξ = 0 - детерминированный вектор-столбец. Квадратичная форма ξE(xx)ξ = E(ξxxξ) = E(ξx)20, т.е. матрица положительно полуопределена. Если не существует такого ξ = 0, что ξx = 0, т.е. переменные вектора x линейно не зависят друг от друга, то неравенство выполняется строго, и соответствующая матрица положительно определена. Пусть X -матрица N наблюдений за переменными x. Оценкой матрицы ко-вариации этих переменных является 1 N ˆXˆX. Квадратичная форма 1 N ξˆXXˆξ = = 1 N uu 0, гд е u = Xˆ ξ, т.е. матрица положительно полуопределена. Если не существует такого ξ = 0, что Xˆ ξ = 0, т.е. переменные x линейно не зависят друг от друга, то неравенство выполняется строго, и соответствующая матрица положи-тельно определена. ОператорМНК-оценивания образуется соотношениями (6.11) и (6.5), которые в данном случае записываются следующим образом: bj = ¯xj − ¯x−ja−j (6.13)
6.2. Простая регрессия 203 (соотношения МНК-оценивания (4.37), данные в пункте 4.2 без доказательства, являются частным случаем этого оператора). Уравнения m−j = M−ja−j , (6.14) решение которых дает первую часть оператора МНК-оценивания (6.11), называ-ется системой нормальных уравнений. МНК-оценки остатков имеют нулевую среднюю (6.6) и не коррелированы (ор-тогональны) с объясняющими переменными уравнения (6.12). Систему нормальных уравнений можно вывести, используя иную логику. Если обе части уравнения регрессии (6.9) умножить слева на ˆX − j и разделить на N, то получится условие m−j = M−ja−j + 1 N ˆX − jej , из которого получается искомая система при требованиях ¯ej = 0 и cov(X−j, ej) = 0, следующих из полученных свойств МНК-оценок остатков. Такаяже логика используется в методе инструментальных переменных.Пусть имеется матрица Z размерности N ×(n−1) наблюдений за некоторыми величи-нами z, называемыми инструментальными переменными, относительно которых известно, что они линейно не зависят от εj и коррелированы с переменными X−j . Умножение обеих частей уравнения регрессии слева на ˆ Zи деление их на N да-ет условие 1 N ˆ ZˆXj = 1 N ˆ ZˆX−ja−j + 1 N ˆ Zej , из которого-после отбрасывания второго члена правой части в силу сделанных предположений-следует система нормальных уравнений метода инструментальных переменных: mz−j = Mz− jaz−j , (6.15) где mz−j = cov (z, xj), Mz− j = cov (z, x−j). Значения j-й (объясняемой) переменной, лежащие на гиперплоскости регрес-сии, называются расчетными (по модели регрессии): Xcj = X−ja−j + 1Nbj , (6.16) ˆXcj = ˆX−ja−j . (6.17) Их дисперсия называется объясненной (дисперсия, объясненная регрессией) и может быть представлена в различных вариантах: s2qj = 1 N ˆXc j ˆXcj (6.17) = a−jM−ja−j (6.11) = a−jm−j = m−ja−j = m−jM−1 −j m−j . (6.18)
204 Глава 6. Алгебра линейной регрессии Если раскрыть скобки в выражении остаточной дисперсии (6.10) и прове-сти преобразования в соответствии с (6.11, 6.18), то получается s2ej = s2j− s2qj , где s2j-дисперсия j-й (объясняемой) переменной, или s2j= s2qj + s2ej. (6.19) Это-дисперсионное тождество, показывающее разложение общей диспер-сии объясняемой переменной на две части-объясненную (регрессией) и оста-точную. Доля объясненной дисперсии в общей называется коэффициентом детерми-нации: R2j = s2qj s2j= 1− s2ej s2j, (6.20) который является показателем точности аппроксимации исходных значений объ-ясняемой переменной гиперплоскостью регрессии (объясняющими переменными). Он является квадратом коэффициента множественной корреляции между объ-ясняемой и объясняющими переменными rj,−j , который, по определению, равен коэффициенту парной корреляции между исходными и расчетными значениями объясняемой переменной: rj,−j = cov xj, xcjsjsqj = 1 N ˆXj ˆXcj sjsqj (6.17) = 1 N ˆXj ˆX−ja−j sjsqj = = m−ja−j sjsqj (6.18) = s2qj sjsqj (6.20) = 'R2j . Из(6.19) следует, что коэффициент корреляциипо абсолютнойвеличине не пре-вышает единицы. Эти утверждения, начиная с (6.16), обобщают положения, представленные в конце пункта 4.2. Композиция 1 и −aj обозначается a(j) и является одной из оценок вектора α. Всего таких оценок имеется n -по числу простых регрессий, в левой части уравнения которых по очереди остаются переменные xj , j = 1, . . . , n.Эти вектор-столбцы образуют матрицу A. По построению ее диагональные элементы равны единице ( ajj = 1 вследз а aj (j) = 1). Все эти оценки в общем случае различны, т.е. одну из другой нельзя получить алгебраическим преобразованием соответствующих уравнений регрессии: a (j) = 1 aj (j)a j, j= j. (6.21)
6.3. Ортогональная регрессия 205 Это утверждение доказывалось в пункте 4.2 при n = 2. В данном случае спра-ведливо утверждение, что соотношение (6.21) может (при некоторых j, j) вы-полняться как равенство в том и только том случае, если среди переменных xj , j = 1, . . . , n существуют линейно зависимые. Достаточность этого утверждения очевидна.Действительно,пусть переменныенеко-торого подмножества J линейно зависимы, т.е. существует такой вектор ξ, в кото-ром ξj = 0 при j ∈ J и ξj = 0 при j ∈/J, и Xˆξ = 0. Тогда для любого j ∈ J справедливо: a(j) = 1 ξj ξ, причем aj(j) = 0 при j/∈ J, и ej = 0, т.е. некоторые соотношения (6.21) выполняются как равенства. Для доказательства необходимости утверждения предполагается, что существует такой ξ = 0, что Aξ = 0 (6.22) (т.е., в частности, некоторые соотношения из (6.21) выполняются как равенства). Сначала следует обратить внимание на то, что вслед за (6.14) все компоненты век-тора Ma(j) (M -матрица ковариации всех переменных x: M = 1N ˆXˆX), кроме j-й, равны нулю, а j-я компонента этого вектора в силу (6.18, 6.19) равна s2ej, т.е. MA = S2e , (6.23) где S2e -диагональная матрица 3s2ej4. Теперь, после умножения обеих частей полученного матричного соотношения справа на вектор ξ, определенный в (6.22), получается соотношение: 0 = S2e ξ, которое означает, что для всех j, таких, что ξj = 0, s2ej = 0, т.е. переменные xj линейно зависят друг от друга. Что и требовалось доказать. Все возможные геометрические иллюстрации простых регрессий в простран-стве наблюдений и переменных даны в пункте 4.2. 6.3. Ортогональная регрессия В случае, когда ограничения на вектор a (или α) состоят в требовании равен-ства единице длины этого вектора aa = 1 (αα = 1), (6.24) и все переменные остаются в левой части уравнения, получается ортогональная регрессия, в которой расстояния от точек облака наблюдений до гиперплоскости регрессии измеряются перпендикулярно этой гиперплоскости. Разъяснения этому факту давались в пункте 4.2.
206 Глава 6. Алгебра линейной регрессии Оценка параметров регрессии производится из условия минимизации остаточ-ной дисперсии: s2e(6.7) = 1N aˆXXˆa = aMa → min!, где M = 1 N ˆXˆX-ковариационная матрица переменных регрессии, при условии (6.24). Из требования равенства нулю производной по a соответствующей функции Лагранжа следует, что (M − λIn) a = 0, (6.25) где λ -множитель Лагранжа ограничения (6.24), причем λ = s2e. (6.26) Действительно, функция Лагранжа имеет вид: L (a, λ) = aMa − λaa, а вектор ее производ ных по a: ∂L ∂a = 2(Ma − λa) . Откуда получается соотношение (6.25). А если обе части этого соотношения умно-жить слева на aи учесть (6.24), то получается (6.26). Таким образом, применение МНК сводится к поиску минимального собствен-ного числа λ ковариационной матрицы M и соответствующего ему собствен-ного (правого) вектора a (см. также Приложение A.1.2). Благодаря свойствам данной матрицы (вещественность, симметричность и положительная полуопреде-ленность), искомые величины существуют, они вещественны, а собственное чис-ло неотрицательно (предполагается, что оно единственно). Пусть эти оценки по-лучены. В ортогональной регрессии все переменные x выступают объясняемыми, или моделируемыми, их расчетные значения определяются по формуле: ˆXc = ˆX− ea. (6.27)
6.3. Ортогональная регрессия 207 Действительно: ˆXca = Xˆa ←−→e − e a←→a 1 = 0, т.е. вектор-строки ˆxci, соответствующие наблюдениям, лежат на гиперплоскости регрессии и являются проекциями на нее вектор-строк фактических наблюдений ˆxi (вектор a по построению ортогонален гиперплоскости регрессии, а eia-вектор нормали ˆxciна ˆxi ), а аналогом коэф-фициента детерминации выступает величина 1− λ s2Σ , гд е s2Σ = n j=1 s2j-суммарная дисперсия переменных x, равная следу матрицы M. Таким образом, к n оценкам вектора a простой регрессии добавляется оценка этого вектора ортогональной регрессии, и общее количество этих оценок стано-вится равным n + 1. Задачу простой и ортогональной регрессии можно записать в единой, обобщен-ной форме: (M − λW) a = 0, aWa = 1, λ→ min!, (6.28) где W -диагональная n×n-матрица, на диагонали которой могут стоять 0 или 1. В случае, если в матрице W имеется единственный ненулевой элемент wjj = 1, то это-задача простой регрессии xj по x−j (действительно, это следу-ет из соотношения (6.23)); если W является единичной матрицей, то это-задача ортогональной регрессии. Очевидно, что возможны и все промежуточные случаи, когда некоторое количество n1, 1 < n1 < n, переменных остается в левой части уравнения, а остальные n2 переменных переносятся в правую часть уравнения регрессии: ˆX1a1 = ˆX2a2 + e1, a1a1 = 1. Если J -множество переменных, оставленных в левой части уравнения, то в записи (6.28) такой регрессии wjj = 1 для j ∈ J и wjj = 0 для остальных j. Оценка параметров регрессии производится следующим образом: a2 = M−1 22 M21a1, M11 −M12M−1 22 M21 − λIn1a1 = 0 ( a1 находится как правый собственный вектор, соответствующий минимальному собственному числу матрицы M11 −M12M−1 22 M21), где M11 = 1 N ˆX1ˆX1, M12 = M21 = 1 N ˆX1ˆX2, M22 = 1 N ˆX2ˆX2
208 Глава 6. Алгебра линейной регрессии -соответствующие ковариационные матрицы. Таким образом, общее количество оценок регрессии- (2n − 1). В рамках любой из этих оценок λ в (6.28) является остаточной дисперсией. Задача ортогональной регрессии легко обобщается на случай нескольких урав-нений и альтернативного представления расчетных значений изучаемых перемен-ных.Матрица M, как уже отмечалось, имеет n вещественных неотрицательных собственных чисел, сумма которых равна s2Σ, и n соответствующих им веществен-ных взаимноортогональных собственных векторов, дающих ортонормированный базис в пространстве наблюдений (см. также Приложение A.1.2). Пусть собствен-ные числа, упорядоченные по возрастанию, образуют диагональную матрицу Λ, а соответствующие им собственные вектора (столбцы)-матрицу A. Тогд а AA = In, MA= AΛ. (6.29) Собственные вектора, если их рассматривать по убыванию соответствующих им собственных чисел, есть главные компоненты облака наблюдений, которые по-казывают направления наибольшей "вытянутости" (наибольшей дисперсии) этого облака. Количественную оценку степени этой "вытянутости" (дисперсии) дают соответствующие им собственные числа. Пусть первые k собственных чисел "малы". s2E -сумма этих собственных чисел; AE -часть матрицы A, соответствующая им (ее первые k стоблцов); это- коэффициенты по k уравнениям регрессии или k младших главных компонент; AQ -остальная часть матрицы A, это- n − k старших главных компонент или собственно главных компонент; A = [AE,AQ]; xAE = 0 -гиперплоскость ортогональной регрессии размерности n − k; [E,Q] = ˆXAE,AQ-координаты облака наблюдений в базисе главных компонент; E -матрица размерности N × k остатков по уравнениям регрессии; Q -матрица размерности N × (n − k), столбцы которой есть значения так называемых главных факторов. Поскольку A= A−1, можно записать ˆX= E AE+ QAQ. Откуд а получается два возможных представления расчетных значений переменных: ˆXc (1) = (6.27) ˆX− E AE(2) = QAQ. (6.30)
6.3. Ортогональная регрессия 209 Первое из них-по уравнениям ортогональной регрессии, второе (альтерна-тивное)-по главным факторам (факторная модель). 1 − s2Es2Σ -аналог коэффициента детерминации, дающий оценку качества обеих этих моделей. A B D G C r 0 1 F E x1 x2 Рис. 6.1 Факторная модель представляет n переменных через n − k факто-ров и, тем самым, "сжимает" ин-формацию, содержащуюся в исход-ных переменных. В конкретном ис-следовании, если k мало, то предпо-чтительнее использовать ортогональ-ные регрессии, если k велико (со-ответственно n − k мало), целе-сообразно применить факторную мо-дель. При этом надо иметь в ви-ду следующее: главные факторы- расчетные величины, и содержатель-ная интерпретация их является, как правило, достаточно сложной зада-чей. Сделанные утвержденияможнопроиллюстрироватьнапримере n = 2,предполагая, что λ1 λ2, и упрощая обозначения (введенные выше матрицы являются в данном случае векторами): a1 = AE-вектор параметров ортогональной регрессии, a2 = AQ -вектор первой (в данном случае-единственной) главной компоненты, e = E -остатки в уравнении ортогональной регрессии, q = Q -значения первого (в данном случае-единственного) главного фактора. На рисунке: OA -вектор-строка i-го наблюдения ˆxi = (ˆxi1, ˆxi2), OD - вектор-строка расчетных значений ˆxci, д лина OC - ˆxi1, д лина OB - ˆxi2, OE -вектор-строка a1, OG -вектор-строка a2, д лина OF - ei, д лина OD - qi . Как видно из рисунка 6.1, квадрат длины вектора ˆxi равен (из прямоугольных тре-угольников OAC и OAD) ˆx2 i1+ˆx2 i2 = e2i+q2i , и если сложить все эти уравнения по i и разд елить на N, то получится s21+s22= s2e+s2q. Понятно, что s2e= λ1, s2q= λ2, и это равенство означает, что следмат рицы ковариации равен сумме ее собственных чисел.Кроме того, как видно из рисунка, s21показывает дисперсию облака наблюде-ний (суммарную дисперсию переменных регрессии) в направлении a1 наименьшей "вытянутости" облака, s22-дисперсию облака наблюдений в направлении a2 его наибольшей "вытянутости".
210 Глава 6. Алгебра линейной регрессии Вектор OF есть eia1, а вектор OD - qia2, и рисунок наглядно иллюстрирует выполнение соотношения (6.30): ˆxci= ˆxi − eia1 = qia2. Пусть теперь n = 3, и λ1, λ2, λ3, a1, a2, a3 -собственные числа и вектора ковариационной матрицы переменных. 1) Если λ1 ≈ λ2 ≈ λ3, то облако наблюдений не "растянуто" ни в одном из направ-лений. Зависимости между переменными отсутствуют. 2) Если λ1 λ2 ≈ λ3 и k = 1, то облако наблюдений имеет форму "блина". Плоскость, в которой лежит этот "блин", является плоскостью ортогональной ре-грессии, которуюописывает уравнение ˆxa1 = 0, а собственно уравнением регрессии является Xˆa1 = e. Эту же плоскость представляют вектора a2 и a3, являясь ее осями координат. В этих осях координат можно выразить любую точку данной плоскости, в том числе все точки расчетных значений переменных (6.30): ˆXc = q1 q2 ⎡⎢⎣ a2 a3 ⎤⎥⎦= q1a2 + q2a3, где q1 = Xˆa2, q2 = Xˆa3 -вектора значений главных факторов или вектора координат расчетных значений переменных в осях a2, a3. 3) Если λ1 ≈ λ2 λ3 и k = 2, то облако наблюдений имеет форму "веретена". Ось этого "веретена" является линией регрессии, образованной пересечением двух плоскостей ˆxa1 = 0 и ˆxa2 = 0. И уравнений ортогональной регрессии в данном случае два: Xˆa1 = e1 и Xˆa2 = e2. Данную линию регрессии представляет вектор a3, и через него можно выразить все расчетные значения переменных: ˆXc = qa3, где q = Xˆa3 -вектор значений главного фактора. 6.4. Многообразие оценок регрессии Множество оценок регрессии не исчерпывается 2n − 1 отмеченными выше элементами. Передтем как получать любую из этих оценок, можно провести пре-образование в пространстве наблюдений или переменных. Преобразование в пространстве наблюдений проводится с помощью матрицы D размерности N× N, NN. Обе части исходного уравнения (6.3) умножа-ются слева на эту матрицу: DXα = D1Nβ + Dε, (6.31)
6.4. Многообразие оценок регрессии 211 после чего проводится оценка параметров любым из указанных 2n − 1 способов. Понятно, что полученные оценки будут новыми, если только DD = cIN, гд е c - любая константа. В результате такого преобразования β может перестать являться свободным членом, если только D1N = c1N( c -любая константа). Но, главное, меняется распределение ошибок по наблюдениям. Именно с целью изменить это распре-деление в нужную сторону (с помощью подбора матрицы D) и проводятся такие преобразования (см. гл. 8). Преобразование в пространстве переменных осуществляется с помощью квадратной невырожденной матрицы C размерности n × n: Y = XC -пре-образованные значения переменных регрессии. И затем оцениваются параметры регрессии в новом пространстве: Y f = 1Ng + u. Это преобразование можно проводить в пространстве центрированных пере-менных, т.к. Yˆ = XˆC. Действительно: XˆC = IN − 1N 1N1NXC = IN − 1N 1N1NY = ˆ Y . То есть исходное уравнение регрессии (6.7) после преобразования приобретает вид: ˆY f = u. (6.32) Оценки f являются новыми, если после "возвращения" их в исходное про-странство, которое производится умножением f слева на C, они не совпадут с оценками a, полученными в исходном пространстве, т.е. если a = Cf. Справед -ливость этого утверждения становится очевидной после следующего алгебраически эквивалентного преобразования исходного уравнения (6.7): XˆC ←−→ ˆ Y C−1a ←−−→ f = e. (6.33) Понятно, что МНК-оценка f совсем не обязательно совпадет с C−1a -и тогда это будет новая оценка. После преобразования меняется распределение ошибок в переменных регрес-сии. И именно для того, чтобы изменить это распределение в нужную сторону, осуществляются такие преобразования (см. гл. 8). Результаты преобразований в пространстве переменных различны для простых и ортогональной регрессий. В случае простой регрессии xj по x−j это преобразование не приводит к по-лучению новых оценок, если j-я строка матрицы C является ортом, т.е. в объ-ясняющие переменные правой части не "попадает"-после преобразования- объясняемая переменная.
212 Глава 6. Алгебра линейной регрессии Действительно, пусть для определенности j = 1 и C = ⎡⎢⎣1 0 c−1 C−1 ⎤⎥⎦(первая строка является ортом), C−1 = ⎡⎢⎣1 0 −C−1 −1 c−1 C−1 −1 ⎤⎥⎦. Уравнение (6.33) записывается следующим образом: ˆX1 + ˆX−1c−1 ˆX−1C−1 ←−−−−−−−−−−−−−−−−−−−−→ ˆ Y ⎡⎢⎣ 1 −C−1 −1 c−1 − C−1 −1a−1 ⎤⎥⎦ ←−−−−−−−−−−−−−−−−−→ f = e1 или, после переноса переменных в правую часть: ˆX1 + ˆX−1c−1←−−−−−−−−−−→ ˆ Y1 = ˆX−1C−1 ←−−−−→ ˆ Y−1 7C−1 −1 c−1 + C−1 −1a−18 ←−−−−−−−−−−−−−→ f−1 +e1. Система нормальных уравнений для оценки f−1 имеет следующий вид: 1 N C− 1 ˆX − 1 ←−−−−→ ˆ Y − 1 ˆX1 + ˆX−1c−1←−−−−−−−−−−→ ˆ Y1 = 1 N C− 1 ˆX − 1 ←−−−−→ ˆ Y − 1 ˆX−1C−1 ←−−−−→ ˆ Y−1 7C−1 −1 c−1 + C−1 −1a−18 ←−−−−−−−−−−−−−→ f−1 или, раскрыв скобки: C− 1m−1 + C− 1M−1c−1 = C− 1M−1c−1 + C− 1M−1a−1. После взаимного сокращения одинаковых слагаемых в полученном матричном урав-нении (2-го в левой части и 1-го в правой) и умножения обеих частей слева на C−1 −1 получается система нормальных уравнений для оценки a−1: m−1 = M−1a−1. Это означает, что f−1 после "возвращения" в исходное пространство совпадает с a−1, т.е. проведенное преобразование в пространстве переменных новых оценок регрессии не дает. Верно и обратное утверждение: если j-я строка матрицы C не является ортом, то a и f совпадают с точностью до обратного преобразования только тогда, когда связь функциональна и e = 0.
6.4. Многообразие оценок регрессии 213 Пусть теперь C = ⎡⎢⎣ 1 c−1 0 In−1 ⎤⎥⎦(т.е. первая строка не является ортом), C−1 = ⎡⎢⎣ 1 −c−1 0 In−1 ⎤⎥⎦. Тогда уравнение (6.33) приобретает следующую форму: ˆX1 ˆX−1 + ˆX1c−1 ←−−−−−−−−→ ˆ Y−1 ⎡⎢⎣ 1 + c−1a−1 −a−1 ⎤⎥⎦ ←−−−−−−−−−−→ f = e1, (6.34) или ˆX1 1 + c−1a−1= ˆ Y−1a−1 + e1, и ˆX1 = ˆ Y−1 a−1 1 + c−1a−1+ 1 1 + c−1a−1e1. Таким образом, условием совпадения a и f с точностью до обратного преобразо-вания является следующее: f−1 = a−1 1 + c−1a−1. (6.35) Система нормальных уравнений для оценки f−1 имеет вид: 1 N ˆ Y − 1 ˆX1 = 1 N ˆY − 1 ˆ Y−1f−1, или, учтя зависимость Y от X из (6.34) и раскрыв скобки: m−1 + c−1m11 = M−1 + m−1c−1 + c−1m−1 + m11c−1c−1f−1. Это равенство с учетом (6.35) и (6.11) принимает вид: (m−1 + c−1m11) 1 + c−1M−1 −1m−1= = M−1 + m−1c−1 + c−1m−1 + m11c−1c−1M−1 −1m−1. Раскрыв скобки и приведя подобные, можно получить следующее выражение: c−1m11 = c−1m−1M−1 −1m−1,
214 Глава 6. Алгебра линейной регрессии которое выполняется как равенство, только если m11 = m−1M−1 −1m−1, т.е. если (в соответствии с (6.18)) m11 = s2q1. Таким образом, a и f совпадают с точностью до обратного преобразования только тогда, когда полная дисперсия равна объясненной, т. е. связьфункциональна и e = 0. Что и требовалось доказать. Итак, преобразования в пространстве переменных в простых регрессиях лишь в особых случаях приводят к получению новых оценок, обычно меняются толь-ко шкалы измерения. Некоторые из этих шкал находят применение в прикладном анализе. Такой пример дает стандартизированная шкала, которая возникает, ес-ли C = S−1, гд е S -диагональная матрица среднеквадратических отклонений переменных. Оценки параметров регрессии после преобразования оказываются измерен-ными в единицах среднеквадратических отклонений переменных от своих средних, и они становятся сопоставимыми межд у собой и с параметрами д ругих регрес-сий.В этом случае система нормальных уравнений формируется коэффициентами корреляции, а не ковариации, и f−j = R−1 −j r−j, гд еR−j -матрица коэффици-ентов корреляции объясняющих переменных между собой, r−j -вектор столбец коэффициентов корреляции объясняющих переменных с объясняемой перемен-ной. Действительно (предполагается, что j = 1), соотношения (6.33) при указанной матрице C имеют следующую форму: ˆX1 1 s1 ←−−→ ˆ Y1 ˆX−1S−1 −1 ←−−−−−→ ˆ Y−1 ⎡⎢⎣ s1 −S−1a−1 ⎤⎥⎦= e1. (6.36) Для того чтобы вектор параметров приобрел необходимую для простой регрессии форму, его над о разд елить на s1. Тогд аи e делится на s1 (т.е. на s1 делятся обе части уравнения (6.36)). После переноса объясняющих переменных в правую часть получается следующее уравнение регрессии: ˆ Y1 = ˆ Y−1f−1 + 1 s1 e1, где f−1 = S−1a−1 1 s1 . Система нормальных уравнений для f−1 имеет следующий вид: 1 N ˆ Y − 1 ˆY1 = 1 N ˆY − 1 ˆ Y−1f−1,
6.4. Многообразие оценок регрессии 215 или, учитывая зависимость Y от X из (6.36), S−1 −1m−1 1 s1 ←−−−−−−→ r−1 = S−1 −1M−1S−1 −1 ←−−−−−−−→ R−1 f−1. Что и требовалось доказать. Преобразование в пространстве переменных в ортогональной регрессии при использовании любой квадратной и невырожденной матрицы C = In приводит к получению новых оценок параметров. В пункте 4.2 при n = 2 этот факт графически иллюстрировался в случае, когда C = ⎡⎢⎣ 1 0 0 k ⎤⎥⎦. В общем случае верно утверждение, состоящее в том, что в результате пре-образования и "возвращения" в исходное пространство для получения оценок a надо решить следующую задачу: (M − λΩ) a = 0, aΩa = 1, (6.37) где Ω = C−1C−1. Действительно: После преобразования в пространстве переменных задача ортогональной регрессии записывается следующим образом (6.24, 6.25): (MY − λIn) f = 0, ff = 1, (6.38) где, учитывая (6.33), MY = CMC, f = C−1a. Выражение (6.37) получается в результате элементарных преобразований (6.38). Понятно, что решение задачи (6.37) будет давать новую оценку параметрам a при любой квадратной и невырожденной матрице C = In. Такую регрессию иногда называют регрессией в метрике Ω−1.
216 Глава 6. Алгебра линейной регрессии 6.5. Упражнения и задачи Упражнение 1 Таблица 6.1 X1 X−1 X2 X3 0.58 1.00 1.00 1.10 2.00 4.00 1.20 3.00 9.00 1.30 4.00 16.00 1.95 5.00 25.00 2.55 6.00 36.00 2.60 7.00 49.00 2.90 8.00 64.00 3.45 9.00 81.00 3.50 10.00 100.00 3.60 11.00 121.00 4.10 12.00 144.00 4.35 13.00 169.00 4.40 14.00 196.00 4.50 15.00 225.00 По наблюдениям из таблицы 6.1: 1.1. Вычислите M−1 = 1 N ˆX − 1 ˆX−1, m−1 = 1 N ˆX − 1 ˆX1 и для регрессии X1 = X−1a−1+1Nb1+e1 найдите оценки a−1 и b1. 1.2. Рассчитайте вектор Xc1 = X−1a−1 + 1Nb1 и век-тор e1 = X1 − Xc1. Убедитесь, что 1Ne1 = 0 и cov(X−1, e) = 1 N ˆX − 1e1 = 0. 1.3. Вычислите объясненную дисперсию различными способами: s2q1 = 1 N ˆXc 1 ˆXc1; s2q1 = a−1m−1; s2q1 = m−1M−1 −1m−1. 1.4. Вычислите остаточную дисперсию различными способами: s2e1 = 1 N e1e1; s2e1 = s21− s2q1 = 1N ˆX1 ˆX1 − s2q1. 1.5. Вычислите коэффициент детерминации различны-ми способами: R21 = s2q1 s21; R21 = cov(x1, xc1) s1sq1 2 . 1.6. Оцените параметры и коэффициент детерминации для ортогональной ре-грессии xα = β + ε.
6.5. Упражнения и задачи 217 - сравните эти оценки с оценками линии регрессии, полученными в 1.1; - рассчитайте расчетные значения переменных. 1.7. Оцените матрицу оценок и значений главных компонент (AQ и Q), а также расчетное значение переменных. 1.8. Пусть единицы измерения x1 увеличились в 100 раз.Как в этомслучае долж-на выглядеть матрица преобразования D? Как изменятся оценки уравнения прямой и ортогональной регрессий? Задачи 1. Может ли матрица а) ⎡⎢⎢⎢⎢⎢⎣ 9.2 −3.8 −2 −3.8 2 0.6 −2 0.5 2 ⎤⎥⎥⎥⎥⎥⎦ б) ⎡⎢⎢⎢⎢⎢⎣ 5.2 −3.8 −2 −3.8 2 0.6 −2 0.6 2 ⎤⎥⎥⎥⎥⎥⎦ являться ковариационной матрицей переменных, для которых строится урав-нение регрессии? Ответ обосновать. 2. Для x = (x1, x2) = ⎛⎜⎜⎜⎜⎜⎝ 1 1 2 2 6 3⎞⎟⎟⎟⎟⎟⎠ найдите оценки ковариаций переменных x, оценки параметров уравнения прямой (x1 = a12x2 + 1Nb1 + e1) и обратной регрессии (x2 = a21x1 + 1Nb2 + e2). Покажите, что a12 = 1 a21 . Рассчитайте вектор-столбец остатков по прямой и обратной регрессии. Убедитесь, что сумма остатков равна нулю, вектора остатков и x2 ортогональны при пря-мой регрессии, вектора остатков и x1 ортогональны при обратной регрес-сии. Найдите объясненную и остаточную дисперсии различными способами, а также коэффициент детерминации. 3. Предположим, что мы, используя модель регрессии x1 = x−1a−1 + 1Nb1 + + e1, из условия минимизации e1e1 получили следующую систему линейных уравнений: ⎧⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎩ b1 +2a12 + a13 = 3, 2b1 + 5a12 + a13 = 9, b1 + a12 + 6a13 = −8.
218 Глава 6. Алгебра линейной регрессии Запишите условия задачи в матрично-векторнойформе,решите ее, используя метод, указанный в приложении для обратных матриц, и найдите оценки параметров регрессии. 4. Оцените регрессию x1 = a12x2 + a13x3 + 1Nb1 + e1 и рассчитайте: - оценку остаточной дисперсии, - объясненную дисперсию, - коэффициент детерминации, если a) матрица наблюдений имеет вид: X = (X1, X2, X3) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 51 −2 0 −4 12345 31524 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , б) X1X1 = 96, X2X2 = 55, X3X3 = 129, X1X2 = 72, X1X3 = 107, X2X3 = 81, X11N = 20, X21N = 15, X31N = 25, N = 5. 5. Дисперсии двух переменных совпадают, корреляция отсутствует. Изобра-зить на графике-в пространстве переменных-линии прямой, обратной и ортогональной регрессий. Ответ обосновать. 6. Дисперсии выпуска продукции и количества занятых по предприятиям равны, соответственно, 10 и 20 , их ковариация равна 12 .Чему равен коэффициент детерминации в регрессии выпуска по занятым, коэффициент зависимости выпуска от занятых по прямой, обратной и ортогональной регрессии? 7. Дисперсии временных рядов индекса денежной массы и сводного индекса цен равны, соответственно, 150 и 200, их ковариация равна 100. Чему равен параметр влияния денежной массы на цены по модели прямой регрессии и доля объясненной дисперсии в дисперсии индекса цен? 8. По заданной матрице ковариации двух переменных ⎡⎢⎣ 143 53 53 23⎤⎥⎦найти оста-точную дисперсию уравнения регрессии первой переменной по второй.
6.5. Упражнения и задачи 219 9. В регрессии x1 = a12x2 + 1Nb1 + e1, гд е x1 = (5, 3, 7, 1) коэффициент детерминации оказался равным 50%. Найдите сумму квадратов остатков. 10. Оцените модель x1 = a12x2 + 1Nb1 + e1, используя следующие данные: (x1, x2) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 3 3 1 1 8 5 3 2 5 5⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Вычислите остатки (ei) и покажите, что 5 i=1 ei = 0, 5 i=1 x2iei = 0. 11. Две парные регрессии построены на одних и тех же данных: x1 = a12x2 + +1Nb1 +e1 и x2 = a21x1 +1Nb2 +e2. R21 -коэффициент детерминации в первой регрессии, R22 -во второй. Запишите соотношение между R21 и R22. Ответ обосновать. 12. Возможна ли ситуация, когда угловые коэффициенты в уравнениях прямой и обратной регрессии построены на одних и тех же данных, соответственно равны 0.5 и 3.0. Почему? 13. Что геометрически означает R2 = 0 и R2 = 1? 14. Регрессия x1 = α12x2 + β1 + ε1 оценивается по двум наблюдениям. Чему равен коэффициент детерминации? 15. Для x = (x1, x2) = ⎛⎜⎜⎜⎜⎜⎝ 1 1 2 2 6 3⎞⎟⎟⎟⎟⎟⎠ оцените параметры ортогональной регрессии и коэффициент детерминации. Покажите, что линия ортогональной регрес-сии находится между линиями прямой и обратной регрессии. 16. Какая из двух оценок коэффициента зависимости выпуска продукции от ко-личества занятых в производстве больше: по прямой или по ортогональной регрессии? Ответ обосновать. 17. Какая из двух оценок коэффициента зависимости спроса от цены больше: по прямой или по ортогональной регрессии? Ответ обосновать.
220 Глава 6. Алгебра линейной регрессии 18. Какой видимеет уравнение ортогональной регрессии для переменных x1 и x2 с одинаковыми значениями дисперсий и средних, а также имеющих положительную корреляцию равную ρ? 19. Покажите, что решение задачи ⎛⎜⎝ ⎛⎜⎝ m11 m12 m12 m22 ⎞⎟⎠− λ⎛⎜⎝ 1 0 0 0 ⎞⎟⎠ ⎞⎟⎠ ⎛⎜⎝ 1 −a12 ⎞⎟⎠= 0, λ→ min! эквивалентно решению задачи прямой регрессии x1 = a12x2 +1Nb1 + e1. 20. Пусть x1 и x2 -центрированные переменные. Уравнение ортогональной регрессии, поcтроенные по множеству наблюдений над величинами x1 и x2, есть x1 − x2 = 0. Запишите вектор первой главной компоненты. 21. Оценка парной регрессии ведется в стандартизированной шкале. Как связан коэффициент детерминации и коэффициент регрессии (угловой)? 22. Была оценена регрессия x1 = α12x2 + β1 + ε1, гд е x1 измеряется в рублях, а x2 -в килограммах. Затем ту же регрессию оценили, изменив единицы измерения на тысячи рублей и тонны. Как при этом поменялись следующие величины: а) оценка коэффициента α12; б) коэффициент детерминации? Как в этом случае должна выглядеть матрица преобразования D? 23. Пусть в ортогональной регрессии, построенной для переменных x1 и x2, из-за деноминации рубля единица измерения x2 изменилась в 1000 раз. Как в этом случае должна выглядеть матрица преобразования D? Изменятся ли оценки? Ответ обосновать. 24. Пусть в наблюдениях задачи 2 единица измерения x1 увеличилась в 10 раз. Как в этом случае должна выглядеть матрица преобразования D? Как изме-нятся оценки уравнения прямой и обратной регрессии? 25. В регрессии в метрике Ω1 матрица Ω равна ⎡⎢⎣ 9 0 0 4 ⎤⎥⎦. Как преобразовать исходные переменные, чтобы свести эту регрессию к ортогональной? Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т.2.-М.: "Юнити", 2001. (Гл. 2).
6.5. Упражнения и задачи 221 2. Болч Б., Хуань К.Дж. Многомерные статистические методы для экономи-ки.-М.: "Статистика", 1979. (Гл. 7). 3. Джонстон Дж. Эконометрические методы.-М.: "Статистика", 1980. (Гл. 2, 11). 4. Дрейпер Н., Смит Г. Прикладной регрессионный анализ: В 2-х кн. Кн.1.- М.: "Финансы и статистика", 1986. (Гл. 1, 2). 5. ЕзекиэлМ., Фокс К. Методы анализа корреляций и регрессий.-М.: "Ста-тистика", 1966. (Гл. 5, 7). 6. Кейн Э. Экономическая статистика и эконометрия. Вып. 2.-М.: "Стати-стика", 1977. (Гл. 10, 11). 7. Лизер С. Эконометрические методы и задачи.-М.: "Статистика", 1971. (Гл. 2). 8. (*) Маленво Э. Статистические методы эконометрии. Вып. 1.-М.: "Ста-тистика", 1975. (Гл. 1). 9. Judge G.G., Hill R.C., GriffithsW.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch. 5). 10. William E., Griffiths R., Carter H., George G. Judge Learning and Practicing econometrics, N 9 John Wiley & Sons, Inc., 1993. (Ch. 3).
Глава 7 Основная модель линейной регрессии 7.1. Различные формы уравнения регрессии Основная модель линейной регрессии относится к классу простых регрессий, в левой части уравнения которых находится одна переменная: объясняемая, моде-лируемая, эндогенная, а в правой-несколько переменных: объясняющих, фак-торных, независимых, экзогенных. Объясняющие переменные называют также факторами, регрессорами. Для объясняемой переменной сохраняется прежнее обозначение- x. А век-тор-строка размерности n объясняющих переменных будет теперь обозначаться через z, поскольку свойства этих переменных в основной модели регрессии суще-ственно отличаются от свойств объясняемой переменной. Через X и Z обозна-чаются, соответственно, вектор-столбец размерности N наблюдений за объясня-емой переменной и матрица размерности N × n наблюдений за объясняющими переменными. Обозначения параметров регрессии и остатков по наблюдениям со-храняются прежними (отличие в том, что теперь вектор-столбцы α и a имеют размерность n). Уравнения регрессии в исходной форме имеют следующий вид: X = Zα + 1Nβ + ε, (7.1)
7.1. Различные формы уравнения регрессии 223 или в оценках X = Za + 1Nb + e. (7.2) В сокращенной форме: ˆX= ˆ Zα + ε, (7.3) или ˆX= ˆ Za + e. (7.4) Оператор МНК-оценивания ((6.11, 6.13) в п. 6.2) принимает теперь вид: a = M−1m, b = ¯x − ¯za, (7.5) где M = 1N ˆ Zˆ Z -ковариационная матрица переменных z между собой, m = 1N ˆ ZˆX-вектор ковариаций переменных z с переменной x.Первую часть оператора (7.5) часто записывают в форме: a = ˆ Zˆ Z−1 ˆ ZXˆ. (7.6) МНК-оценки e обладают следующими свойствами ((6.6, 6.12) в предыдущей главе): ¯e = 1N 1Ne = 0, cov (Z, e) = 1 N ˆ Ze = 0. (7.7) Коэффициент детерминации рассчитывается следующим образом (см. (6.20)): R2 = s2q s2x = 1− s2e s2x , (7.8) где s2x -дисперсия объясняемой переменной, s2e-остаточная дисперсия, s2q(6.2.6) = aMa = am = ma = mM−1m (7.9) -объясненная дисперсия. Уравнение регрессии часто записывают вформе со скрытымсвободным членом: X = / Z/ α+ ε, (7.10) X = / Z/ a+ e, (7.11)
224 Глава 7. Основная модель линейной регрессии где / Z = [Z 1N], / α= ⎡⎢⎣ αβ ⎤⎥⎦, / a= ⎡⎢⎣ ab ⎤⎥⎦. При таком представлении уравнения регрессии оператор МНК-оценивания записывается следующим, более компактным, чем (7.5), образом: / a= 2M−1 / m, (7.12) где 2M = 1 N / Z/ Z, / m= 1N / ZX, или / a= / Z/ Z−1 / ZX. (7.13) 2M и / mтакже, как M и m, являются матрицей и вектором вторых моментов, но не центральных, а начальных. Кроме того, их размерность на единицу больше. Оператор (7.12) дает, естественно, такой же результат, что и оператор (7.5). Этот факт доказывался в п. 4.2 для случая одной переменной в правой части урав-нения.В общем случае этот факт доказывается следующим образом. Учитывая, что X = ˆX+ 1N ¯x, (7.14) / Z = ˆ Z + 1N ¯z 1N , (7.15) можно установить, что 2M = ⎡⎢⎣ M + ¯z¯z ¯z¯z 1 ⎤⎥⎦, (7.16) / m= ⎡⎢⎣ m+ ¯z¯x ¯x ⎤⎥⎦, и записать систему нормальных уравнений, решением которой является (7.12) в сле-дующей форме: ⎡⎢⎣ M + ¯z¯z ¯z¯z 1 ⎤⎥⎦ ⎡⎢⎣ ab ⎤⎥⎦= ⎡⎢⎣ m+ ¯z¯x ¯x ⎤⎥⎦.
7.1. Различные формы уравнения регрессии 225 Вторая (нижняя) часть этой матричной системы уравнений эквивалентна второй части оператора (7.5). После подстановки b, выраженного через a, в первую(верх-нюю) часть данной матричной системы уравнений она приобретает следующий вид: Ma+ ¯z¯za+ ¯z¯x − ¯z¯za = m+ ¯z¯x, и после приведения подобных становится очевидной ее эквивалентность первой ча-сти оператора (7.5). Что и требовалось доказать. Кроме того, можно доказать, что 2M−1 = ⎡⎢⎣ M−1 −M−1¯z−¯zM−1 1 + ¯zM−1¯z⎤⎥⎦. (7.17) Этот факт потребуется ниже. (Правило обращения блочных матриц см. в Прило-жении A.1.2.) Справедливость этого утверждения проверяется умножением 2M−1 из (7.17) на 2M из (7.16). В результате получается единичная матрица. МНК-оценки вектора e ортогональны столбцам матрицы / Z: / Ze = 0. (7.18) Доказательствоналичия этого свойстваполучается как побочныйрезультатпри вы-воде оператора оценивания (7.12) путем приравнивания нулю производных оста-точной дисперсии по параметрам регрессии, как это делалось в п. 6.2. Поскольку последним столбцом матрицы / Z является 1N, из (7.18) следует, что 1Ne = 0, (7.19) т.е. ¯e = 0. Из остальной части (7.18):Ze = 0, (7.20) что в данном случае означает, что cov(Z, e) = 0. Действительно, раскрывая (7.20): Ze (7.15) = ˆ Ze+ ¯z1Ne ←−−→ =0 = ˆ Ze = 0.
226 Глава 7. Основная модель линейной регрессии Таким образом, (7.18) эквивалентно (7.7). Однако уравнения (7.10) допускают и иную интерпретацию. Если последним в / Z является не 1N, а столбец "обычной" переменной, то это-регрессия без свободного члена.Втакомслучае из (7.18) не следует (7.19), и свойства (7.7) не вы-полняются. Кроме того, для такой регрессии, очевидно, не возможна сокращенная запись уравнения. Этот случай в дальнейшем не рассматривается. В дальнейшем будет применяться в основном форма записи уравнения со скры-тым свободным членом, но чтобы не загромождать изложение материала, символ "∼" будет опускаться, т.е. соотношения (7.10, 7.11, 7.12, 7.13, 7.18) будут исполь-зоваться в форме X = Zα + ε, (7.21) X = Za + e, (7.22) a = M−1m, (7.23) a = ZZ−1 ZX, (7.24) Ze = 0. (7.25) Случаи, когда a, Z, m, M означают не / a, / Z, / m, 2M, а собственно a, Z, m, M, будут оговариваться специально. 7.2. Основные гипотезы, свойства оценок Применение основной модели линейной регрессии корректно, если выполня-ются следующие гипотезы: g1. Между переменными x и z существует линейная зависимость, и (7.10) является истинной моделью, т.е., в частности,правильно определеннаборфакторов z -модель верно специфицирована. g2. Переменные z детерминированы, наблюдаются без ошибок и линейно независимы. g3. E(ε) = 0. g4. E(εε) = σ2IN. Гипотеза g2 является слишкомжесткой и в экономике чаще всего нарушается. Возможности ослабления этого требования рассматриваются в следующей главе. Здесь можно заметить следующее: в тех разделах математической статистики, в ко-торых рассматривается более общий случай, и z также случайны, предполагается, что ε не зависит от этих переменных-регрессоров.
7.2. Основные гипотезы, свойства оценок 227 В этих предположениях a относится к классу линейных оценок, поскольку a = LX, (7.26) где L(7.13) = (ZZ)−1 Z-детерминированная матрица размерности (n + 1) × N, и доказывается ряд утверждений о свойствах этихМНК-оценок. 1) a -несмещенная оценка α. Действительно: a (7.26), g1 = L (Zα + ε) = LZα + Lε LZ=In+1 = α + Lε (7.27) и E(a) g3 = α. 2) Ее матрица ковариации Ma удовлетворяет следующему соотношению: Ma = 1N σ2M−1, (7.28) в частности, σ2aj = σ2 N m−1 jj, j= 1, . . . , n + 1 (σ2an+1 ≡ σ2b ), где m−1 jj - j-й диагональный элемент матрицы M−1. Действительно: Ma = E((a − α)(a − α)) (7.27) = E(LεεL) g4 = σ2LL= σ2 (ZZ)−1= 1 N σ2M−1. Этот результат при n = 1 означает, что σ2a = σ2 N 1 s2z, и его можно получить, исполь-зуя формулу (5.17) распространения ошибок первичных измерений. Действительно, a = di (xi − ¯x), гд е di = zi − ¯z (zi − ¯z)2 . Тогд а ∂a ∂xi = − 1 N N l=1 dl ←−−−−−→ =0 +di = di и в соответствии с указанной формулой: σ2a = σ2d2i= σ2 (zi − ¯z)2 (zi − ¯z)22 = σ2 (zi − ¯z)2 = σ2 N 1 s2z.
228 Глава 7. Основная модель линейной регрессии Здесь важно отметить следующее. Данная формула верна и в случае использования исходной или сокращенной за-писи уравнения регрессии, когда M -матрица ковариации регрессоров. Это сле-дует из (7.17). Но в такой ситуации она (эта формула) определяет матрицу ковариа-ции только оценок коэффициентов регрессии при объясняющих переменных, а дис-персию оценки свободного члена можно определить по формуле σ2 N 1 + ¯zM−1¯z, как это следует также из (7.17). Следует также обратить внимание на то, что несмещенность оценок при учете только что полученной зависимости их дисперсий от N свидетельствует о состоя-тельности этих оценок. Иногда формулу (7.28) используют в другой форме: Ma = σ2 ZZ−1 . (7.29) 3) Несмещенной оценкой остаточной дисперсии σ2 является ˆs2e= N N − n − 1s2e= 1 N − n − 1ee. (7.30) Для доказательства этого факта сначала устанавливается зависимостьМНК-оценок ошибок от их истинных значений, аналогично (5.10): e = X − Zag1, (7.27) = Zα + ε − Z (α + Lε) = (IN − ZL) ε = Bε, (7.31) и устанавливаются свойства матрицы B (аналогично тому, как это делалось в п. 5.1) B = IN − ZL = IN − Z (ZZ)−1 Z= IN − 1 NZM−1Z. (7.32) Эта матрица: а) вещественна и симметрична: B= B, б) вырождена и имеет ранг N −n−1, т.к. при любом ξ = 0 выполняется BZξ = 0 (поскольку BZ (7.32) = 0), а в множестве Zξ в соответствии с g2 имеется точно n+1 линейно независимых векторов, в) идемпотентна: B2 = B, г) положительно полуопределена в силу симметричности и идемпотентности: ξBξ = ξB2ξ = ξBBξ 0. Теперь исследуется зависимость остаточной дисперсии от σ2: s2e= 1 N ee (7.31) = 1 N εBBε = 1 N εBε, Es2e= 1 N E(εBε) g4 = σ2 N tr (B) ←−−→ bii , (7.33)
7.2. Основные гипотезы, свойства оценок 229 где tr(-)-операция следа матрицы, результатом которой является сумма ее диаго-нальных элементов. Далее, в силу коммутативности операции следа матрицы tr (B) = tr (IN) − tr (ZL) = N − tr (LZ) ←−→ In+1 = N − n − 1. (См. Приложение A.1.2.) Таким образом, Es2e= N − n − 1 N σ2, и E1 N − n − 1ee= σ2. Что и требовалось доказать. Тогда оценкой матрицы ковариации Ma является (в разных вариантах расчета) ˆs2e NM−1 = ee N (N − n − 1)M−1 = ee N − n − 1 ZZ−1 , (7.34) и, соответственно, несмещенными оценками дисперсий (квадратовошибок) оценок параметров регрессии: ˆs2aj = ee N (N − n − 1)m−1 jj, j= 1, . . . , n + 1 (s2an+1 ≡ s2b). (7.35) 4) Дисперсии a являются наименьшими в классе линейных несмещенных оце-нок, т.е. оценки a относятся к классу BLUE (см. п. 5.1). Это утверждение называ-ется теоремой Гаусса-Маркова. Доказательство этого факта будет проведено для оценки величины cα, гд е c - любой детерминированный вектор-столбец размерности n + 1. Если в качестве c выбирать орты, данный факт будет относиться к отдельным параметрам регрессии. МНК-оценка этой величины есть ca (7.26) = cLX, она линейна, не смещена, т.к. E(ca) = cα, и ее дисперсия определяется следующим образом: var (ca) (7.28) = σ2 N cM−1c. (7.36) Пусть dX -любаялинейная оценка cα, где d -некоторыйдетерминированный вектор-столбец размерности N. E(dX) g1 = E(dZα + dε) g3 = dZα, (7.37) и д ля того, чтобыэта оценка была несмещенной, т.е. чтобы dZα = cα, необход имо dZ = c. (7.38)
230 Глава 7. Основная модель линейной регрессии Из (7.37) следует, что dX = E(dX) + dε, и тогд а var (dX) = E((dX − E(d←−−−−−−−−X→) dε )2) = E(dεεd) g4 = σ2dd. (7.39) И, наконец, в силу положительной полуопределенности матрицы B (из (7.32)): var (dX) − var (ca) (7.36,7.40) = σ2dd − σ2 N cM−1c (7.38) = = σ2dIN − 1 N ZM−1Zd (7.32) = σ2dBd 0, т.е. дисперсия МНК-оценки меньше либо равна дисперсии любой другой оценки в классе линейных несмещенных. Что и требовалось доказать. Теперь вводится еще одна гипотеза: g5. Ошибки ε имеют многомерное нормальное распределение: ε ∼ N 0, σ2IN. (Поскольку по предположению g4 они некоррелированы, то по свойству мно-гомерного нормального распределения они независимы). Тогда оценки a будут также иметь нормальное распределение: a ∼ N (α,Ma) , (7.40) в частности, aj ∼ N αj, σ2aj, j= 1, . . . , n + 1 (an+1 ≡ b, αn+1 ≡ β), они совпадут с оценками максимального правдоподобия, что гарантирует их со-стоятельность и эффективность (а не только эффективность в классе линейных несмещенных оценок). Применение метода максимального правдоподобия в линейной регрессии рас-сматривается в IV-й части книги. Здесь внимание сосредоточивается на других важных следствиях нормальности ошибок. Поскольку aj − αj σaj N (0, 1), (7.41) для αj можно построить (1 − θ)100-процентный доверительный интервал: αj ∈ aj ± σaj ˆε1−θ. (7.42)
7.2. Основные гипотезы, свойства оценок 231 Чтобы воспользоваться этой формулой, необходимо знать истинное значение остаточной дисперсии σ2, но известна только ее оценка. Для получения соответ-ствующей формулы в операциональной форме, как и в п. 5.1, проводятся следую-щие действия. Сначала доказывается, что ee σ2 ∼ χ2N−n−1. (7.43) Это доказательство проводится так же, как и в пункте 5.1 для (5.9). Только теперь матрица B, связывающая в (7.31) оценки ошибок с их истинными значениями, имеет ранг N −n − 1 (см. свойства матрицы B, следующие из (7.32)), а не N −1, как аналогичная матрица в (5.10). Затем обращается внимание на то, что e и a не коррелированы, а значит, не коррелированы случайные величины в (7.41, 7.43). Действительно (как и в 5.1): a − α (7.27) = Lε и cov (a, e) = E((a − α)e) (7.31) = E(LεεB) g4 = σ2 (ZZ)−1 Z←−B→ =0 = 0. Что и требовалось доказать. Поэтому по определению случайной величины, имеющей t-распределение: (aj − αj)√N σ'm−1 jj 9!ee σ2 /(N − n − 1) (7.35) = aj − αj ˆsaj ∼ tN−n−1. (7.44) Таким образом, для получения операциональной формы доверительного интер-вала в (7.42) необходимо заменить σaj на ˆsaj и ˆε1−θ на ˆtN−n−1,1−θ: αj ∈ aj ± ˆsaj ˆtN−n−1,1−θ. (7.45) Полезно заметить, что данный в этом пункте материал обобщает результаты, полученные в п. 5.1. Так, многие приведенные здесь формулы при n = 0 пре-образуются в соответствующие формулы п. 5.1. Полученные результаты можно использовать также и для проверки гипотезы о том, что αj = 0 (нулевая гипотеза).
232 Глава 7. Основная модель линейной регрессии Рассчитывается t-статистика tcj= aj ˆsaj , (7.46) которая в рамках нулевой гипотезы, как это следует из (7.44), имеет t-распреде-ление. Проверка нулевой гипотезы осуществляется по схеме, неоднократно применя-емой в I части книги. В частности, если уровень значимости t-статистики sl (напо-минание: sl таково, что tcj= tN−n−1,sl) не превышает θ (обычно 0.05), то нулевая гипотеза отвергается с ошибкой (1-го рода) θ и принимается, что αj = 0. В про-тивном случае, если нулевую гипотезу не удалось отвергнуть, считается, что j-й фактор не значим, и его не следует вводить в модель. Операции построения доверительного интервала и проверки нулевой гипоте-зы в данном случае в определенном смысле эквивалентны. Так, если построенный доверительный интервал содержит нуль, то нулевая гипотеза не отвергается, и на-оборот. Гипотеза о нормальности ошибок позволяет проверить еще один тип нулевой гипотезы: αj = 0, j = 1, . . . , n, т.е. гипотезы о том, что модель некорректна и все факторы введены в нее ошибочно. При построении критерия проверки данной гипотезы уравнение регрессии ис-пользуется в сокращенной форме, и условие (7.40) записывается в следующей форме: a ∼ N α, σ2 N M−1, (7.47) где a и α -вектора коэффициентов при факторных переменных размерности n, M -матрица ковариации факторных переменных. Тогда Nσ2 a− αM (a − α) ∼ χ2n. (7.48) Действительно: Матрица M−1 вследз а M является вещественной, симметричной и положительно полуопределенной, поэтому ее всегда можно представить в виде: M−1 = CC, (7.49) где C -квадратная неособенная матрица. Чтобы убедиться в этом, достаточно вспомнить (6.29) и записать аналогичные со-отношения: M−1Y = Y Λ, Y Y = Y Y = In, Λ 0, гд е Y -матрица, столбцы
7.2. Основные гипотезы, свойства оценок 233 которой есть собственные вектора M−1, Λ -диагональная матрица соответству-ющих собственных чисел. Тогда M−1 = Y ΛY = Y Λ0.5 ←−−−→ C Λ0.5Y ←−−−→ C(см. Приложение A.1.2). Вектор случайных величин u = √N σ C−1(a−α) обладает следующими свойствами: по построению E(u) = 0, и в силу того, что E((a − α)(a − α)) (7.47) = σ2 N M−1, cov(u) = E(uu) = Nσ2 C−1E((a − α)(a − α))C−1 = C−1M−1C−1 (7.49) = In. Следовательно, по определению χ2 случайная величина uu = Nσ2 (a− α) C−1C−1 ←−−−−−→ M (a − α) имеет указанное распределение (см. Приложение A.3.2). Как было показано выше, e и a не коррелированы, поэтому не коррелированы случайные величины, определенные в (7.43, 7.48), и в соответствии с определением случайной величины, имеющей F-распределение: Nσ2 a− αM (a − α) (N − n − 1)9ee σ2 n ∼ Fn,N−n−1. Отсюда следует, что при нулевой гипотезе α = 0 aMa(N − n − 1) (ee)Nn (7.9) = s2q(N − n − 1) s2en ∼ Fn,N−n−1, или R2 (N − n − 1) (1 − R2) n = Fc ∼ Fn,N−n−1. (7.50) Сама проверка нулевой гипотезы проводится по обычной схеме. Так, если зна-чение вероятности pv статистики Fc (величина, аналогичная sl для t-статистики) не превышает θ (например, 0.05), нулевая гипотеза отвергается с вероятностью ошибки θ, и модель считается корректной. В противном случае нулевая гипотеза не отвергается, и модель следует пересмотреть.
234 Глава 7. Основная модель линейной регрессии 7.3. Независимые факторы: спецификация модели В этом пункте используется модель линейной регрессии в сокращенной фор-ме, поэтому переменные берутся в центрированной форме, а m и M -вектор и матрица соответствующих коэффициентов ковариации переменных. Под спецификацией модели в данном случае понимается процесс и результат определения набора независимых факторов. При построении эконометрической модели этот набор должен обосновываться экономической теорией.Но это удается не во всех случаях. Во-первых, не все факторы, важные с теоретической точки зрения, удается количественно выразить. Во-вторых, эмпирический анализ часто предшествует попыткам построения теоретической модели, и этот набор просто неизвестен. Потому важную роль играют и методы формального отбора факторов, также рассматриваемые в этом пункте. В соответствии с гипотезой g2 факторные переменные не должны быть ли-нейно зависимыми. Иначе матрица M в оператореМНК-оценивания будет необ-ратима. Тогда оценкиМНК по формуле a = M−1m невозможно будет рассчитать, но их можно найти, решая систему нормальных уравнений (6.14): Ma = m. Решений такой системы нормальных уравнений (в случае необратимости матри-цы M) будет бесконечно много. Следовательно, оценки нельзя найти однозначно, т.е. уравнение регрессии невозможно идентифицировать. Действительно, пусть оценено уравнение ˆx = ˆz1a1 + e, (7.51) где ˆz1 -вектор-строка факторных переменных размерности n1, a1 -вектор-столбец соответствующих коэффициентов регрессии, и пусть в это уравнение вво-дится дополнительный фактор ˆz2, линейно зависимый от ˆz1, т.е. ˆz2 = ˆz1c21. Тогда оценка нового уравнения ˆx = ˆz1a∗1 + ˆz2a2 + e∗ (7.52) ("звездочкой" помечены новые оценки "старых" величин) эквивалентна оценке уравнения ˆx = ˆz1 (a∗1 + a2c21) + e∗.Очевидно, что a1 = a∗1+a2c21, e = e∗, и, про-извольно задавая a2, можно получать множество новых оценок a∗1 = a1 − a2c21. Логичнее всего положить a2 = 0, т.е. не вводить фактор ˆz2. Хотя, если из со-держательных соображений этот фактор следует все-таки ввести, то тогда надо исключить из уравнения какой-либо ранее введенный фактор, входящий в ˆz1. Та-ким образом, вводить в модель факторы, линейно зависимые от уже введенных, бессмысленно.
7.3. Независимые факторы: спецификация модели 235 B C A O Рис. 7.1 Случаи, когда на факторных переменных су-ществуют точные линейные зависимости, встре-чаются редко. Гораздо более распространена си-туация, в которой зависимости между фактор-ными переменными приближаются к линейным. Такая ситуация называется мультиколлинеарно-стью. Она чревата высокими ошибками получа-емых оценок и высокой чувствительностью ре-зультатов оценивания к ошибкам в факторных переменных, которые, несмотря на гипотезу g2, обычно присутствуют в эмпирическом анализе. Действительно, в такой ситуации матрица M плохо обусловлена и диагональные элементы M−1, определяющие дисперсии оценок, могут принимать очень большие значения. Кроме того, даже небольшие изменения в M, связанные с ошибками в факторных переменных, могут повлечь существенные изменения в M−1 и, как следствие,- в оценках a. Последнеенаглядноиллюстрируется рисунком(рис.7.1) в пространственаблюдений при n = 2. На этом рисунке: OA - ˆx, OB - ˆz1, OC - ˆz2. Видно, что факторные переменные сильно коррелированы (угол между соответству-ющими векторами мал). Поэтому даже небольшие колебания этих векторов, связанные с ошибками, зна-чительно меняют положение плоскости, которую они определяют, и, соответствен-но,-нормали на эту плоскость. Изрисунка видно, что оценки параметров регрессии"с легкостью"меняют не только свою величину, но и знак. По этим причинам стараются избегать ситуации мультиколлинеарности. Для этого в уравнение регрессии не включают факторы, сильно коррелирован-ные с другими. Можно попытаться определить такие факторы, анализируя матрицу коэффи-циентов корреляции факторных переменных S−1MS−1, гд е S -диагональная матрица среднеквадратических отклонений. Если коэффициент sjjэтой матри-цы достаточно большой, например, выше 0.75, то один из пары факторов j и jне следует вводить в уравнение. Однако такого элементарного "парного" анализа может оказаться не достаточно. Надежнее построить все регрессии на множестве факторных переменных, последовательно оставляя в левой части уравнения эти переменные по отдельности. И не вводить в уравнение специфицируемой моде-ли (с x в левой части) те факторы, уравнения регрессии для которых достаточно значимы по F-критерию (например, значение pv не превышает 0.05).
236 Глава 7. Основная модель линейной регрессии B C D O A Рис. 7.2 Однако в эмпирических исследованиях могут возникать ситуации, когда только введение сильно коррелированных факторов может привести к по-строению значимой модели. Это утверждение можно проиллюстрировать ри-сунком (рис. 7.2) в пространстве наблюдений при n = 2. На этом рисунке: OA - ˆx, OB - ˆz1, OC - ˆz2, AD -нормаль на плоскость, определяе-мую векторами OB и OC, OD -проекция OA на эту плоскость. Из рисунка видно, что ˆz1 и ˆz2 по отдельности не объясняют ˆx (углымежду соответствующими векторами близки к 90*), но вместе они определяют плоскость, угол между которой и вектором OA очень мал, т.е. коэффициент детерминации в регрессии ˆx на ˆz1, ˆz2 близок к единице. Рисунок также показывает, что такая ситуация возможна только еслифакторы силь-но коррелированы. В таких случаях особое внимание должно уделяться точности измерения фак-торов. Далее определяются последствия введения в уравнение дополнительного фак-тора. Для этого сравниваются оценки уравнений (7.51, 7.52) в предположении, что ˆz2 линейно независим от ˆz1. В этом анализе доказываются два утверждения. 1) Введение дополнительного фактора не может привести к сокращению ко-эффициента детерминации, в большинстве случаев он растет (растет объясненная дисперсия). Коэффициент детерминации остается неизменным тогда и только то-гда, когда вводимый фактор ортогонален остаткам в исходной регрессии (линейно независим от остатков), т.е. когдаm2e = 1 N ˆ Z2e = 0 (7.53) (понятно, что коэффициент детерминации не меняется и в случае линейной зависи-мости ˆz2 от ˆz1, но такой случай исключен сделанным предположением о линейной независимости этих факторов; в дальнейшем это напоминание не делается). Для доказательства этого факта проводятся следующие действия. Записываются системы нормальных уравнений для оценки регрессий (7.51, 7.52): m1 = M11a1, (7.54)
7.3. Независимые факторы: спецификация модели 237 ⎡⎢⎣ m1 m2⎤⎥⎦= ⎡⎢⎣ M11 m12 m21 m22⎤⎥⎦ ⎡⎢⎣ a∗1 a2⎤⎥⎦, (7.55) где m1 = 1 N ˆ Z1 ˆX, m2 = 1 N ˆ Z2 ˆX, M11 = 1 N ˆ Z1 ˆ Z1, m12 = m21 = 1 N ˆ Z1 ˆ Z2, m22 = 1 N ˆ Z2 ˆ Z2. Далее, с помощью умножения обеих частей уравнения (7.51), расписанного по на-блюдениям, слева на 1 N ˆ Z2, устанавливается, что m2 − m21a1 (7.53) = m2e, (7.56) а из регрессии ˆ Z2 = ˆ Z1a21 + e21, в которой по предположению e21 = 0, наход ится остаточная дисперсия: s2 e21 = 1 N e21e21 (7.9) = m22 − m21M−1 11 m12 > 0. (7.57) Из первой (верхней) части системы уравнений (7.55) определяется: M11a∗1 + m12a2 = m1 (7.54) = M11a1, и д алее a∗1 = a1 −M−1 11 m12a2. (7.58) Из второй (нижней) части системы уравнений (7.55) определяется: m22a2 = m2 − m21a∗1 (7.58) = m2 − m21 a1 −M−1 11 m12a2. Откуда m22 − m21M−1 11 m12a2 = m2 − m21a1 и, учитывая (7.56, 7.57), s2 e21a2 = m2e. (7.59) Наконец, определяется объясненная дисперсия после введения дополнительного фактора: s2∗ q (7.9) = m1a∗1 + m2a2 (7.58) = m1a ←−−→1 s2q +⎛⎜⎝m2 − m1M−1 ←−−−−11→ a1 m12⎞⎟⎠a2 (7.56) = s2q+ m2ea2, (7.60)
238 Глава 7. Основная модель линейной регрессии т.е. s2∗ q (7.59) = s2q+ m2 2e s2 e21 . Что и требовалось доказать. Это утверждение легко проиллюстрировать рисунком 7.3 в пространстве наблюде-ний при n1 = 1. На этом рисунке: OA - ˆx, OB - ˆz1, OC - ˆz2, AD -нормаль ˆx на ˆz1 ( DA -вектор e). Рисунок показывает, что если ˆz2 ортогонален e, то нормаль ˆx на плоскость, опре-деляемую ˆz1 и ˆz2, совпад ает с AD, т.е. угол между этой плоскостью и ˆx совпадает с углом межд у ˆx и ˆz1, введение в уравнение нового фактора ˆz2 не меняет коэффи-циент детерминации. Понятно также и то, что во всех остальных случаях (когда ˆz2 не ортогонален e) этот угол уменьшается и коэффициент детерминации растет. B C D O A Рис. 7.3 После введения дополнительного фактора ˆz2 в уравнение максимально коэффициент детерми-нации может увеличиться до единицы. Это про-изойдет, если ˆz2 является линейной комбинацией ˆx и ˆz1. Рост коэффициента детерминации с увеличе-нием количества факторов-свойство коэффи-циента детерминации, существенно снижающее его содержательное (статистическое) значение. Введение дополнительных факторов, даже если они по существу не влияют на моделируемую пе-ременную, приводит к росту этого коэффициента. И, если таких факторов введено достаточно много, то он начнет приближаться к единице. Он обязательно достигнет единицы при n = N − 1. Более приемлем в роли критерия качества коэффициент детерминации, скорректированный на число степеней свободы: ˜R2 = 1− 1 − R2N − 1 N − n − 1 ( 1 − R2 -отношение остаточной дисперсии к объясненной, которые имеют, со-ответственно, N − n − 1 и N − 1 степеней свободы), этот коэффициент может снизиться после введения дополнительного фактора. Однако наиболее правильно при оценке качества уравнения ориентироваться на показатель pv статистики Fc. Скорректированный коэффициент детерминации построен так, что он, так сказать, штрафует за то, что в модели используется слишком большой набор факторов. На этом же принципе построено и большинство других критериев, используемых
7.3. Независимые факторы: спецификация модели 239 для выбора модели: на них положительно отражается уменьшение остаточной дис-персии s2e(z1) (здесь имеется в виду смещенная оценка дисперсии из регрессии по z1) и отрицательно - количество включенных факторов n1 (без константы). Укажемтолько три наиболее известных критерия (из огромного числа предложенных в литературе): Критерий Маллоуза: Cp = s2e(z1) + 2(n1 + 1) N ˆs2e(z), где ˆs2e(z) -несмещенная оценка дисперсии в регрессии с полным набором факто-ров. Информационный критерий Акаике: AIC = ln2πs2e(z1)+ 2(n1 + 1) N . Байесовский информационный критерий (критерийШварца): BIC = ln2πs2e(z1)+ ln(N)(n1 + 1) N . В тех же обозначениях скорректированный коэффициент детерминации имеет вид ˜R2 = 1− s2e(z1) s2e(∅) N − 1 N − n1 − 1 , где s2e(∅) -остаточная дисперсия из регрессии с одной константой. Регрессия тем лучше, чем ниже показатель Cp ( AIC, BIC ).Для ˜R2 используется противоположное правило-его следует максимизировать. Вместо ˜R2 при неиз-менном количестве наблюдений N можно использовать несмещенную остаточную дисперсию ˆs2e= ˆs2e(z1), которую уже следует минимизировать. В идеале выбор модели должен происходить при помощи полного перебора воз-можных регрессий. А именно, берутся все возможные подмножества факторов z1, для каждого из них оценивается регрессия и вычисляется критерий, а затем выби-рается набор z1, дающий наилучшее значение используемого критерия. Чем отличается поведение критериев ˜R2 ( ˆs2e), Cp, AIC , BIC при выборе моде-ли?Прежде всего, они отличаются по степени жесткости, то есть по тому, насколько велик штраф за большое количество факторов и насколько более "экономную" мо-дель они имеют тенденцию предлагать. ˜R2 является наиболее мягким критерием. Критерии Cp и AIC занимают промежуточное положение; при больших N они ве-дут себя очень похоже, но Cp несколькожестче AIC, особенно при малых N. BIC является наиболее жестким критерием, причем, как можно увидеть из приведенной формулы, в отличие от остальных критериев его жесткость возрастает с ростом N. Различие в жесткости проистекает из различия в целях. Критерии Cp и AIC на-правлены на достижение высокой точности прогноза: Cp направлен на миними-зацию дисперсии ошибки прогноза (о ней речь пойдет в следующем параграфе),
240 Глава 7. Основная модель линейной регрессии а AIC -на минимизацию расхождения между плотностью распределения по ис-тинной модели и по выбранной модели. В основе BIC лежит цель максимизации вероятности выбора истинной модели. 2) Оценки коэффициентов регрессии при факторах, ранее введенных в уравне-ние, как правило, меняются после введения дополнительного фактора. Они оста-ются прежними в двух и только двух случаях: а) если неизменным остается ко-эффициент детерминации и выполняется условие (7.53) (в этом случае уравнение в целом остается прежним, т.к. a2 = 0); б) если новый фактор ортогонален старым B C D E F O A Рис. 7.4 ( ˆz1 и ˆz2 линейно не зависят друг от друга), т.е. m12 = 1 N ˆ Z1 ˆ Z2 = 0 (7.61) (в этом случае объясненная дисперсия равна сумме дисперсий, объясненных факторами ˆz1 и ˆz2 по от-дельности). Действительно, в соотношении (7.58) M−1 11 m12 не может равняться нулю при m12 = 0, т.к. M11 невырожденная матрица. Поэтому из данного со-отношения следует, что оценки a1 не меняются, если a2 = 0 (случай "а") или/и m12 = 0 (случай "б"). Случай "а", как это следует из (7.59), возникает, когда выполняется (7.53). В случае "б" соотношение (7.60) переписывается следующим образом: s2∗ q (7.9) = m1a∗1 + m2a2 a∗1=a1 = m1a1 + m2a2, т.к. вторая (нижняя) часть системы (7.55) означает в этом случае, что m22a2 = m2, т.е. a2 -оценка параметра в регрессии ˆx по ˆz2: ˆx = ˆz2a2 + e2 = s2q+ s2q2, (7.62) где s2q2 -дисперсия ˆx, объясненная только ˆz2. Что и требовалось доказать. Иллюстрация случая "а" при n1 = 1 достаточно очевидна и дана выше. Рисунок 7.4 иллюстрирует случай "б". На этом рисунке: OA - ˆx, OB - ˆz1, OC - ˆz2, EA - e, нормаль ˆx на ˆz1, FA - e2, нормаль ˆx на ˆz2, DA - e∗, нормаль ˆx на плоскость, определенную ˆz1 и ˆz2, ED -нормаль к ˆz1, FD -нормаль к ˆz2. Понятно (геометрически), что такая ситуация, когда точка E является одновре-менно началом нормалей EA и ED, а точка F -началом нормалей FA и FD, возможна только в случае, если угол COB равен 90*.
7.3. Независимые факторы: спецификация модели 241 Но именно этот случай означает (как это следует из рисунка) одновременное вы-полнение соотношений регрессий (7.51) (OE + EA = OA), (7.52) (при a∗1 = a1) ( OE+OF +DA = OA) и (7.62) ( OF +FA = OA), т.е. что введение нового фактора не меняет оценку при "старом" факторе, а "новая" объясненная дисперсия равна сумме дисперсий, объясненных "старым" и "новым" факторами по отдельности (сумма квадратов длин векторов OE и OF равна квадрату длины вектора OD). BD C O A Рис. 7.5 На основании сделанных утверждений можно сформулировать такое правило введения новых факторов в уравнение регрессии: вводить в ре-грессию следует такие факторы, которые имеют высокую корреляцию с остатками по уже введен-ным факторам и низкую корреляцию с этими уже введенными факторами. В этом процессе следует пользоваться F-критерием: вводить новые фак-торы до тех пор, пока уменьшается показатель pv F-статистики. В таком процессе добавления новых факторов в регрессионную модель некоторые из ранее вве-денных факторов могут перестать быть значимыми, и их следует выводить из урав-нения.Эту возможность иллюстрирует рисунок 7.5 в пространстве наблюдений при n1 = 1. На этом рисунке: OA - ˆx, OB- ˆz1, OC - ˆz2, AD -нормаль ˆx на плос-кость, определенную ˆz1 и ˆz2. Рисунок показывает, что нормаль AD "легла" на вектор вновь введенного фактора. Следовательно, "старый" фактор входит в "новую" регрессию с нулевым коэффи-циентом. Это-крайний случай, когда "старый" фактор автоматически выводится из уравне-ния. Чаще встречается ситуация, в которой коэффициенты при некоторых "старых" факторах оказываются слишком низкими и статистически незначимыми. Процесс, в котором оценивается целесообразность введения новых факторов и выведения ранее введенных факторов, называется шаговой регрессией. В раз-витой форме этот процесс можно организовать следующим образом. Пусть z -полный набор факторов, потенциально влияющих на x. Рассмат-ривается процесс обращения матрицы ковариации переменных x, z, в начале ко-торого рядом с этой матрицей записывается единичная матрица. С этой парой мат-риц производятся одновременные линейные преобразования. Известно, что если первую матрицу привести таким образом к единичной, то на месте второй будет по-лучена матрица, обратная к матрице ковариации. Пусть этот процесс не завершен,
242 Глава 7. Основная модель линейной регрессии и только n1 строк первой матрицы, начиная с ее второй строки (т.е. со стро-ки первого фактора), преобразованы в орты; z1 -множество факторов, строки которых преобразованы в орты, z2 -остальные факторы. Это-ситуация на те-кущем шаге процесса. В начале процесса пара преобразуемых матриц имеет вид(надматрицами по-казаны переменные, которые соответствуют их столбцам): x z1 z2 ⎡⎢⎢⎢⎢⎢⎣ mxx m1 m2 m1 M11 M12 m2 M12 M22⎤⎥⎥⎥⎥⎥⎦ и x z1 z2 ⎡⎢⎢⎢⎢⎢⎣ 1 0 0 0 I1 0 0 0 I2⎤⎥⎥⎥⎥⎥⎦ , где mxx = 1 N ˆXˆX-дисперсия x , m1 = 1 N ˆ Z1 ˆX-вектор-столбец коэффициентов ковариации z1 и x , m2 = 1 N ˆ Z2 ˆX-вектор-столбец коэффициентов ковариации z2 и x , M11 = 1 N ˆ Z1 ˆ Z1 -матрица коэффициентов ковариации z1 между собой, M12 = 1 N ˆ Z1 ˆ Z2 -матрица коэффициентов ковариации z1 и z2 , M22 = 1 N ˆ Z2 ˆ Z2 -матрица коэффициентов ковариации z2 между собой. На текущем шаге эти матрицы преобразуются к виду: x z1 z2 ⎡⎢⎢⎢⎢⎢⎢⎢⎣ mxx −m1M−1 1 m1 m1M−1 ←−−−−1→ a1 m2 − m1M−1 ←−−−−−−−−1−−M−−1→2 ce2 0 I1 0 m2 −M12M−1 1 m1 M12M−1 1 M2 −M12M−1 1 M12⎤⎥⎥⎥⎥⎥⎥⎥⎦ и x z1 z2 ⎡⎢⎢⎢⎢⎢⎣ 1 0 0 −M−1 1 m1 M−1 1 −M−1 1 M12 0 0 I2 ⎤⎥⎥⎥⎥⎥⎦ .
7.3. Независимые факторы: спецификация модели 243 Информация, используемая в шаговой регрессии, расположена в 1-й строке первой матрицы: остаточная дисперсия в текущей регрессии (в столбце x), коэф-фициенты a1 текущей регрессии при переменных z1 (в столбцах z1), коэффи-циенты ce2 ковариации текущих остатков e с переменными z2, не включенными в текущую регрессию (в столбцах z2). Для введения очередного фактора в регрессию (шаг вперед) следует его строку в первой матрице преобразовать в орт, для исключения фактора из регрессии (шаг назад) следует преобразовать в орт его строку во второй матрице. Шаг вперед увеличивает количество элементов в векторе z1 на единицу и сокращает на единицу количество элементов в векторе z2. Шаг назадпривод ит к обратным изменениям. Последствия любого из этих шагов можно оценить по F-критерию, рассчитав показатель pv Fc-статистики (информацию для такого расчета дает остаточная дисперсия-первый элемент первой строки первой матрицы). На текущем шаге процесса проверяются последствия введения всех ранее не введенных факторов z2 и исключения всех введенных факторов z1. Выби-рается тот вариант, который дает минимальное значение показателя pv. Процесс заканчивается, как только этот показатель перестает падать. В результате опреде-ляется наилучшая регрессия. Такой процесс не приводит, как правило, к включению в регрессию сильно коррелированных факторов, т.е. позволяет решить проблему мультиколлинеарности. Если бы расчеты проводились в стандартизированной шкале (по коэффици-ентам корреляции, а не ковариации), "кандидатом" на введение был бы фактор с максимальным значением показателя в множестве ce2 (как было показано вы-ше), а на исключение-фактор с минимальным значением показателя в множе-стве a1. Но даже в этом случае для окончательного выбора (вводить-исключать) и решения вопроса о завершении процесса требуется использование F-критерия. При "работе" с коэффициентами ковариации использование F-критерия необ-ходимо. На последних шагах процесса, при приближении к минимуму критериального показателя pv, его величина меняется, как правило, весьма незначительно.Поэто-му один из возможных подходов к использованию шаговой регрессии заключается в определении некоторого множества регрессий, получаемых на последних шагах процесса, которые практически одинаковы по своему качеству. И на этом мно-жестве следует делать окончательный выбор, пользуясь содержательными крите-риями. Иногда процесс шаговой регрессии предлагают строить на основе t-критерия: фактор вводится в уравнение, если его t-статистика больше некоторой заданной величины t1, выводится из уравнения, если эта статистика меньше заданной вели-чины t2; как правило, t1 > t2. Такой процесс не гарантирует получение наилучшей
244 Глава 7. Основная модель линейной регрессии регрессии, его использовали в то время, когда вычислительные возможности были еще слабо развиты, и, в частности, точные значения показателя pv было трудно определить. 7.4. Прогнозирование Пусть получены оценки параметров уравнения (7.11). Задача прогнозирования заключается в определении возможного значения (прогноза) переменной x, объ-ясняемой этой моделью, при некоторых заданных значениях факторов z, которые не совпадают ни с одним из наблюдений в матрице Z. Более того, как прави-ло, z лежит вне области, представляемой матрицей Z. При этом предполагается, что гипотезы g1−g3 по-прежнему выполняются. Обычно термин "прогнозирование" используется в случае, когда наблюдения i = 1, . . . , N в матрице Z даны по последовательным моментам (периодам) вре-мени, и заданные значения факторов z, для которых требуется определить прогноз x, относятся к какому-то будущему моменту времени, большему N (т.е. z лежит вне области, представляемой матрицей Z). Методы прогнозирования могут быть различными. Если применяются отно-сительно простые статистические методы, как в данном случае, то часто исполь-зуют термин "экстраполирование". Если аналогичная задача решается для z, лежащих внутри области, представляемой наблюдениями в матрице Z (например, для "пропущенных" по каким-то причинам наблюдений), то используют термин "интерполирование". Процедуры экстраполирования и интерполирования с ис-пользованием модели (7.11) с формальной точки зрения одинаковы. Итак, задан некоторый zr = [zr1 - - - zrn 1], который отличается от всех zi, i = 1, . . . , N (если i -обозначает момент времени, то r > N). xr = zrα + εr -истинное значение искомой величины, x0r= zrα -ожидаемое значение, xpr= zra -искомый (точечный) прогноз. Предполагаем, что гипотезы g1−g4 выполнены как для i = 1, . . . , N, так и для r > N. Это линейный (относительно случайных величин X) прогноз: xpr(7.26) = zrLX, он не смещен относительно ожидаемого значения вслед за несмещенностью a: E(xpr) = x0r. Его ошибка εpr= xr − xprимеет нулевое математическое ожидание и дисперсию σ2p = σ2 1 +zr ZZ−1 zr, (7.63)
7.4. Прогнозирование 245 которая минимальна на множестве всех возможных линейных несмещенных про-гнозов. Действительно: εpr= zr (α − a) + εr. Поскольку случайные величины a и εr не зависят друг от друга, σ2p = E(εpr)2= E(zr(α − a)(α − a)zr) + Eε2r= = zrMazr + σ2 (7.29) = σ2 1 + zr (ZZ)−1 zr. Эта дисперсия минимальна среди всех возможных дисперсий линейных несмещен-ных прогнозов вследза аналогичным свойством оценок a. Это является прямым следствием того, что оценки МНК относятся к классу BLUE. Для того чтобы в этом убедиться, достаточно в доказательстве данного свойства оценок a, которое приве-дено в п. 7.2, заменить cна zr . Следует иметь в виду, что ошибка любого расчетного по модели значения xci, являясь формально такой же: εci= xi − xci, имеет также нулевое математическое ожидание, но принципиально другую, существенно меньшую, дисперсию: σ2i = σ2 1 − zi ZZ−1 zi. Видно, что эта дисперсия даже меньше остаточной. Действительно, как и прежде: εci= zi (α − a) + εi . Но теперь случайные величины a и εi коррелированы и поэтому: σ2i = σ2 1 + zi (ZZ)−1 zi+ 2ziE((α − a) ←−−−→ (7.27) = −Lεεi) E(εεi) g4 = σ2oi, где oi-i-й орт = = σ2 1 + zi (ZZ)−1 zi− 2σ2zi (ZZ)−1 zi = σ2 1 − zi (ZZ)−1 zi. Величины 1−zi (ZZ)−1 zi (i = 1, . . . , N), естественно, неотрицательны, посколь-ку они являются диагональными элементами матрицы B из (7.32), которая поло-жительно полуопределена. Структуру дисперсии ошибки прогноза (7.63) можно пояснить на примере n = 1. В этом случае (используются обозначения исходной формы уравнения ре-грессии, и все z -одномерные величины): σ2p = σ2 1 + 1 N + (zr − ¯z)2 ˆz2i . (7.64)
246 Глава 7. Основная модель линейной регрессии В этом легко убедиться, если перейти к обозначениям исходной формы урав-нения регрессии, подставить в (7.63) вместо zr и Z, соответственно, 7zr 18 и 7Z 1N8 и сделать необходимые преобразования (правило обращения матрицы (2 × 2) см. в Приложении A.1.2), учитывая, что ⎡⎢⎣ ξ1 ξ2 ξ3 ξ4⎤⎥⎦ −1 = 1 ξ1ξ4 − ξ2ξ3 ⎡⎢⎣ ξ4 −ξ2 −ξ3 ξ1 ⎤⎥⎦и ZZ =ˆz2i + N¯z2 : σ2p = σ2 ⎛⎜⎝1 + zr 1⎡⎢⎣ ZZ N¯z N¯z N⎤⎥⎦ −1 ⎡⎢⎣ zr 1 ⎤⎥⎦ ⎞⎟⎠= = σ2 ⎛⎜⎝1 + 1 ZZ − N¯z zr 1⎡⎢⎣ 1 −¯z −¯z 1N ZZ⎤⎥⎦ ⎡⎢⎣ zr 1 ⎤⎥⎦ ⎞⎟⎠= = σ2 1 + z2r − 2¯zzr + 1N ˆz2i + N¯z2ˆz2i = σ2 1 + 1 N + (zr − ¯z)2 ˆz2i . Что и требовалось доказать. Это выражение показывает "вклады" в дисперсию ошибки прогноза собствен-но остаточной дисперсии, ошибки оценки свободного члена и ошибки оценки угло-вого коэффициента.Первые две составляющие постоянны и не зависят от горизон-та прогнозирования, т.е. от того, насколько сильно условия прогноза (в частности, значение zr) отличаются от условий, в которых построена модель (в частности, значение ¯z). Третья составляющая-ошибка оценки углового коэффициента- определяет расширяющийся конус ошибки прогноза. Мы рассмотрели точечный прогноз. Если дополнительно к гипотезам g1−g4 предположить выполнение гипотезы g5 для i = 1, . . . , N и д ля r > N, то можно построить также интервальный прогноз. По формуле (7.27) ошибка прогноза имеет вид: εpr= zr(α − a) + εr = zrLε + εr. Таким образом, она имеет нормальное распределение: εpr= xr − xpr∼ N(0, σ2p). Если бы дисперсия ошибки σ2 была известна, то на основе того, что xr − xpr σp ∼ N(0, 1),
7.5. Упражнения и задачи 247 Таблица 7.1 X Z1 Z2 65.7 26.8 541 74.2 25.3 616 74 25.3 610 66.8 31.1 636 64.1 33.3 651 67.7 31.2 645 70.9 29.5 653 69.6 30.3 682 67 29.1 604 68.4 23.7 515 70.7 15.6 390 69.6 13.9 364 63.1 18.8 411 48.4 27.4 459 55.1 26.9 517 55.8 27.7 551 58.2 24.5 506 64.7 22.2 538 73.5 19.3 576 68.4 24.7 697 для xr можно было бы построить (1 − θ)100-процентный прогнозный интервал: xr ∈ [xpr± σpˆε1−θ] . Вместо неизвестной дисперсии σ2p = σ2(1+zr(ZZ)−1zr) берется несмещенная оценка s2p = ˆs2e(1 + zr(ZZ)−1zr). По аналогии с (7.44) можно вывести, что xr − xpr sp ∼ tN−n−1. Тогда в приведенной формуле прогнозного интервала необ-ходимо заменить σp на sp и ˆε1−θ на ˆtN−n−1, 1−θ: xr ∈ xpr± spˆtN−n−1, 1−θ. 7.5. Упражнения и задачи Упражнение 1 По наблюдениям за объясняемой переменной X и за объясняющими переменными Z = (Z1, Z2) из таблицы 7.1: 1.1. Вычислите ковариационную матрицу переменных z (M = 1 N ˆ Zˆ Z), вектор ковариаций переменных z с пе-ременной x (m = 1N ˆ ZˆX), дисперсию объясняемой переменной s2x. Для регрессии X = Za+1Nb+e най-дите оценки a и b, объясненную дисперсию s2q= ma и остаточную дисперсию s2e= s2x − s2q, а также коэф-фициент детерминации R2. 1.2. Запишите для данной модели уравнение регрессии в форме со скрытым сво-бодным членом X = / Z/ a+e. Рассчитайте для переменных начальные момен-ты второго порядка двумя способами: а) 2M = 1 N / Z/ Z и / m= 1N / ZX
248 Глава 7. Основная модель линейной регрессии б) 2M = ⎡⎢⎣ M + ¯z¯z ¯z¯z 1⎤⎥⎦и / m= ⎡⎢⎣ m+ ¯z¯x ¯x ⎤⎥⎦. 1.3. Найдите оценку / a, рассчитайте s2x = 1 NXX − ¯x2 и s2q= / m/ a− ¯x2 и убе-дитесь, что результат совпадает с результатом пункта 1 упражнения 1. 1.4. Рассчитайте несмещенную оценку остаточной дисперсии ˆs2e= N N − n − 1s2e и оцените матрицу ковариации параметров уравнения регрессии Ma = ˆs2e N 2M−1. 1.5. Используя уровень значимости θ = 0.05, вычислите доверительные интер-валы для коэффициентов уравнения регрессии и проверьте значимость фак-торов. 1.6. Рассчитайте статистику Fc = R2(N − n − 1) (1 − R2)n и, используя уровень значи-мости θ = 0.05, проверьте гипотезу о том, что модель некорректна и все факторы введены в нее ошибочно. 1.7. Рассчитайте коэффициент детерминации, скорректированный на число сте-пеней свободы ˜R2. 1.8. По найденному уравнению регрессии и значениям а) z = (minZ1, minZ2); б) z = ( ¯ Z1, ¯ Z2); в) z = (maxZ1,maxZ2); вычислите предсказанное значение для x и соответствующую интервальную оценку при θ = 0.05. Упражнение 2 Дано уравнение регрессии: X = / Z/ α+ε = −1.410z1 +0.080z2 +56.962 120+ε, где X- вектор-столбец 20 наблюдений за объясняемой переменной (20 × 1), ε -вектор-столбец случайных ошибок (20 × 1) с нулевым средним и ковариа-ционной матрицей σ2I20 = 21.611I20 и / Z -матрица размерности (20 × 3) на-блюдений за объясняющими переменными. Используя нормальное распределение
7.5. Упражнения и задачи 249 с независимыми наблюдениями, со средним 0 и ковариационной матрицей σ2I20 = = 21.611I20, получите 100 выборок вектора ε (N × 1), k = 1, . . . , 100, гд еN = = 20. Эти случайные векторы потом используйте вместе с известным вектором / α= (−1.410, 0.080, 56.962) и матрицей / Z = (Z1, Z2, 1) из таблицы 7.1. Снача-ла получите ожидаемое значения X0 = / Z/ α, затем, чтобы получить 100 выборок вектора X (20 × 1), добавьте случайные ошибки: X0 + ε = X. 2.1. Используйте 10 из 100 выборок, чтобыполучить выборочныеоценки для α1, α2, β , σ и R2. 2.2. Вычислитематрицу ковариаций параметров уравнения регрессии Ma для каж-дого элемента выборки и сравните с истинным значением ковариационной матрицы:σ2 / Z/ Z−1 = ⎛⎜⎜⎜⎜⎜⎝ 0.099813 −0.004112 −0.233234 −0.004112 0.000290 −0.057857 −0.233234 −0.057857 39.278158 ⎞⎟⎟⎟⎟⎟⎠ . Дайте интерпретацию диагональных элементов ковариационных матриц. 2.3. Вычислите среднее и дисперсию для 10 выборок для каждого из параметров, полученных в упражнении 2.1, и сравните эти средние значения с истинными параметрами. Обратите внимание, подтвердилась ли ожидаемые теоретиче-ские результаты. 2.4. Используя уровень значимости θ = 0.05, вычислите и сравните интерваль-ные оценки для α1, α2, β и σ для 10 выборок. 2.5. Объедините 10 выборок, по 20 наблюдений каждая, в 5 выборок по 40 на-блюдений и повторите упражнения 2.1 и 2.2. Сделайте выводы о результатах увеличения объема выборки. 2.6. Повторите упражнения 2.1 и 2.5 для всех 100 и д ля 50 выборок и проана-лизируйте разницу в результатах. 2.7. Постройте распределения частот для оценок, полученных в упражнении 2.6, сравните и прокомментируйте результаты.
250 Глава 7. Основная модель линейной регрессии Задачи 1. В регрессии X = Za + 1Nb + e матрица вторых начальных моментов ре-грессоров равна ⎛⎜⎝ 9 2 2 1⎞⎟⎠. Найдите дисперсию объясняющей переменной. 2. На основании ежегодных данных за 10 лет с помощью МНК была сделана оценка параметров производственной функции типа Кобба-Дугласа. Чему равна несмещенная оценка дисперсии ошибки, если сумма квадратов остат-ков равна 32? 3. В регрессии X = Za + 1Nb + e с факторами Z= (1, 2, 3) сумма квадра-тов остатков равна 6. Найдите ковариационную матрицу оценок параметров регрессии. 4. Какие свойства МНК-оценок коэффициентов регрессии теряются, если ошибки по наблюдениям коррелированы и/или имеют разные дисперсии? 5. Что обеспечивает гипотеза о нормальности распределения ошибок при по-строения уравнения регрессии? Ответ обоснуйте. 6. Какие ограничения на параметры уравнения проверяются с помощью t-кри-терия (написать ограничения с расшифровкой обозначений)? 7. Четырехфакторное уравнение регрессии оценено по 20-ти наблюдениям. В каком случае отношение оценки коэффициента регрессии к ее стандарт-ной ошибке имеет распределение t-Стьюдента? Сколько степенией свободы в этом случае имеет эта статистика? 8. Оценки МНК в регрессии по 20-ти наблюдениям равны (2, −1), а ковариа-ционная матрица этих оценок равна ⎛⎜⎝ 9 2 2 1⎞⎟⎠. Найти статистики t-Стьюдента для этих коэффициентов. 9. По 10 наблюдениям дана оценка 4 одному из коэффициентов двухфакторной регрессии. Дисперсия его ошибки равна 4.Построить 99%-ный доверитель-ный интервал для этого коэффициента. 10. МНК-оценка параметра регрессии, полученная по 16 наблюдениям, рав-на 4, оценка его стандартной ошибки равна 1. Можно ли утверждать с веро-ятностью ошибки не более 5%,что истинное значение параметра равно 5.93? Объяснить почему.
7.5. Упражнения и задачи 251 11. Оценка углового коэффициента регрессии равна 4, а дисперсия этой оценки равна 4. Значим ли этот коэффициент, если табличные значения: tN−n−1, 0.95 = 2.4, tN−n−1, 0.90 = 1.9? 12. В результате оценивания регрессии x = zα + 1Nβ + ε на основе N = 30 наблюдений получены следующие результаты: x= 1.2z1+ 1.0z2− 0.5z3+ 25.1 Стандартные ошибки оценок ( ) (1.3) (0.06) (2.1) t-статистика (0.8) ( ) ( ) ( ) 95%доверительные интервалы (−1.88; 4.28) ( ) ( ) ( ) Заполните пропуски в скобках. 13. На основе годовых отчетов за 1973-1992 годы о затратах на продукты пи-тания Q, располагаемом доходе Y , индексе цен на продукты питания PF и индексе цен на непродовольственные товары PNF, группа исследовате-лей получила различные регрессионные уравнения для функции спроса на продукты питания: lnQ = 3.87 − 1.34 lnPF (1.45) (−4.54) R2 = 0.56 lnQ = 2.83 − 0.92 lnPF + 1.23 ln Y (1.25) (−2.70) (2.99) R2 = 0.76 lnQ = 2.35 − 0.52 lnPF + 0.95 ln Y + 1.54 lnPNF (1.54) (−1.80) (0.79) (2.45) R2 = 0.84 В скобках приведены значения t-статистики. Прокомментируйте полученные оценки коэффициентов и t-статистики, объ-ясните, почему значения могут различаться в трех уравнениях.Можете ли вы предложить решение проблемы статистической незначимости коэффициен-тов в последнем уравнении?
252 Глава 7. Основная модель линейной регрессии 14. Используя приведенные ниже данные, оцените параметры модели xt = β + + α1z1t + α2z2t + εt и, делая все необходимые предположения, проверьте статистическую значимость коэффициента α1 . а) ˆz21t = 10, ˆz22t = 8, ˆz1tˆz2t = 8, ˆz1tˆxt = −10, ˆz2tˆxt = −8, ˆx2t= 20, t = 1, . . . , 5; б)z21t = 55, z22t = 28, z1tz2t = 38, z1txt = 35, z2txt = 22, xt = 15, z1 = 15, z2 = 10, N = 5, x2 = 65. 15. Анализ годовых данных (21 наблюдение) о спросе на некоторый товар привел к следующим результатам: Средние Стандартные отклонения Парные коэффициенты корреляции ¯z = 51.843 sz = 9.205 rxz = 0.9158 ¯x = 8.313 sx = 1.780 rxt = 0.8696 ¯t = 0 st = 6.055 rzt = 0.9304 z -потребление на душу населения, x -цена с учетом дефлятора, t - время (годы). а) Найдите коэффициент при времени в оцененной регрессии x по z и t. б) Проверьте, будет ли этот коэффициент значимо отличен от нуля. в) Кратко объясните экономический смысл включения в регрессию вре-мени в качестве объясняющей переменной. 16. Какие ограничения на параметры уравнения можно проверить с помощью F-критерия? Написать ограничения с расшифровкой обозначений. 17. Пяти-факторное уравнение линейной регрессии для переменной x оценено по 31 наблюдению. При этом объясненная и смещенная остаточная дис-персии соответственно равны 8 и 2. Вычислить коэффициент детерминации и расчетное значение F-статистики. 18. Врегрессии x = z1α1+z2α2+β+ε по 5-ти наблюдениям смещенная оценка остаточной дисперсии равна 1, а дисперсия зависимой переменной равна 2. Значима ли эта зависимость? 19. По 10 наблюдениям оценено двухфакторное уравнение линейной регрессии, коэффициент детерминации составляет 90%. При каком уровне доверия это уравнение статистически значимо? Записать уравнение для нахождения этого уровня значимости.
7.5. Упражнения и задачи 253 20. Используя следующие данные: X = (5, 1, −2, 5, −4), Z = (1, 2, 3, 4, 5), и делая все необходимые предположения а) для X = Zα+1Nβ +ε оценить 95-процентные доверительные интер-валы для параметров регрессии; б) проверить значимость коэффициентов регрессии и оценить качество регрессии с вероятностью ошибки 5%. 21. Пусть X = α1Z1 + α2Z2 + ε, X = (4, −2, 4, 0), Z1 = (1, 1, 2, 2)и Z2 = 2Z1. Постройте систему нормальных уравнений и покажите, что существует бесконечное множество решений для a1 и a2. Выберите любые два решения, покажите, что они дают одинаковые расчетные значения X и, таким образом, одинаковые значения суммы квадратов ошибок. 22. Для уравнения регрессии X = Zα + 15β + ε имеются следующие данные: X = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 48 5.5 5.8 7.0⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , Z= (Z1 Z2 Z3) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1.03 2.08 0.41 1.46 2.80 2.03 1.14 2.30 0.98 1.71 3.05 0.81 1.06 2.17 1.17⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . а) Являются ли факторы линейно зависимыми? б) Найти матрицу коэффициентов корреляции факторных переменных, рассчитать определитель данной матрицы и сделать вывод о мульти-коллинеарности факторов. в) Рассчитать определитель матрицы коэффициентов корреляциифактор-ных переменных в случае, если из уравнения выводится фактор Z2. г) Учесть дополнительную внешнюю информацию: α1 = 1.5α2 (с помо-щьюподстановки в уравнение регрессии) и найти определительматрицы коэффициентов корреляции факторных переменных. д) Построить точечный прогноз x (xpr) для значений экзогенных перемен-ных zr = (z1r, z2r, z3r) = (0.8, 1.6, 0.6): - при использовании исходного уравнения; - при исключении из уравнения фактора Z2;
254 Глава 7. Основная модель линейной регрессии - при использовании внешней информации из пункта (г). 23. Пусть цены сильно коррелируют с денежной массой и неплатежами. Ко-эффициент корреляции между денежной массой и неплатежами равен 0.975 R2 = 0.95.Имеет ли смысл строить регрессию цен на эти два (сильно мультиколлинеарных) фактора? 24. Модель x = α1z1 + α2z2 + β + ε (1) была оценена по МНК, и был получен коэффициент детерминации R21, а д ля преобразованной модели x = α1z1 + α2z2 + α3z3 + β + ε (2) был получен коэффициент детерминации R22. а) Объясните, почему R21не может быть больше, чем R22. При каких условиях они равны? б) Объясните последствия оценки модели (1), если верной является мо-дель (2). 25. В регрессии x = α1z1 + β + ε остатки равны (−2, 1, 0, 1). Оценивается регрессия x = α1z1 +α2z2 +β +ε. Привести пример переменной z2, чтобы коэффициенты детерминации в обеих регрессиях совпадали. 26. В регрессию x = α1z1 + β + ε добавили переменную z2. Переменная z2 оказалась совершенно незначимой. Как изменились обычный и скорректи-рованный коэффициенты детерминации? 27. Коэффициент детерминации в регрессии выпуска продукции по численности занятых в производстве, оцененной по 12 наблюдениям, равен 0.8. После введения в регрессию дополнительного фактора-основного капитала- он вырос до 0.819. Имело ли смысл вводить этот дополнительный фактор? Ответ обосновать без применения статистических критериев. 28. Дана модель регрессии xi = α1zi + β + εi. а) Как оценивается точечный прогноз xN+1, если известно, что β = 0? Покажите, что дисперсияошибок прогнозабудетравна σ21 + z2N+1 Ni=1 z2i .
7.5. Упражнения и задачи 255 б) Как оценивается точечный прогноз xN+1, если известно, что α = 0? Покажите, что дисперсия ошибок прогноза будет равна σ2 1 + 1 N . 29. Почему ошибки прогнозирования по линейной регрессии увеличиваются с ростом горизонта прогноза? 30. Была оценена регрессия x = α1z + β + ε по 50 наблюдениям. Делается прогноз x в точке z51. При каком значении z51 доверительный интервал прогноза будет самым узким? 31. Вычислите предсказанное значение для x и соответствующую интервальную оценку прогноза при θ = 0.05 в точке z26 = 14, если регрессионная модель x = 3z +220+e построена по 25 наблюдениям, остаточная дисперсия равна 25 и сред няя по z равна 14. Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т.2.-М.:Юнити, 2001. (Гл. 2). 2. Демиденко Е.З. Линейная и нелинейная регрессия.-М.: "Финансы и ста-тистика", 1981. (Гл. 1, 2, 6). 3. Джонстон Дж. Эконометрические методы.-М.: "Статистика", 1980. (Гл. 2, 5). 4. Дрейпер Н., Смит Г. Прикладной регрессионный анализ: В 2-х книгах. Кн.1-М.: "Финансы и статистика", 1986, (Гл. 1, 2). 5. Кейн Э. Экономическая статистика и эконометрия. Вып. 2.-М.: "Стати-стика", 1977. (Гл. 10, 11, 14). 6. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 3, 4, 8). 7. (*) Маленво Э. Статистические методы эконометрии. Вып. 1.-М.: "Ста-тистика", 1975. (Гл. 3, 6). 8. Себер Дж. Линейный регрессионый анализ.-М.: Мир, 1980. 9. Тинтер Г. Введение в эконометрию.-М.: "Статистика", 1965. (Гл. 5). 10. Davidson, Russel, Mackinnon, James. Estimation and Inference in Econometrics, N 9, Oxford University Press, 1993. (Ch. 2).
256 Глава 7. Основная модель линейной регрессии 11. GreeneW.H. Econometric Analysis, Prentice-Hall, 2000. (Ch. 6, 7). 12. Judge G.G., Hill R.C., GriffithsW.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch. 5, 21). 13. (*) William E., Griffiths R., Carter H., George G. Judge. Learning and Practicing econometrics, N 9 JohnWiley & Sons, Inc., 1993. (Ch. 8).
Глава 8 Нарушение гипотез основной линейной модели 8.1. Обобщенный метод наименьших квадратов (взвешенная регрессия) Пусть нарушена гипотеза g4 и матрица ковариации ошибок по наблюдени-ям равна не σ2IN, а σ2Ω, гд еΩ -вещественная симметричная положительно полуопределенная матрица (см. Приложение A.1.2), т.е. ошибки могут быть кор-релированы по наблюдениям и иметь разную дисперсию. В этом случае обычные МНК-оценки параметров регрессии (7.26) остаются несмещенными и состоятель-ными, но перестают быть эффективными в классе линейных несмещенных оценок. Ковариационная матрица оценок МНК в этом случае приобретает вид Ma = σ2 ZZ−1 ZΩZ ZZ−1 . Действительно, a − E(a) = a − α = (ZZ)−1 Zε, поэтому E7(a − E(a)) (a − E(a))8 = (ZZ)−1 ZE(εε)Z (ZZ)−1 = = σ2 (ZZ)−1 ZΩZ (ZZ)−1 . (Ср. с выводом формулы (7.28), где Ω = σ2I.)
258 Глава 8. Нарушение гипотез основной линейной модели Обычная оценка ковариационной матрицы s2e(ZZ)−1 при этом является сме-щенной и несостоятельной. Как следствие, смещенными и несостоятельными ока-зываются оценки стандартных ошибок оценок параметров (7.35): чаще всего они преуменьшаются (т.к. ошибки по наблюдениям обычно коррелированы положи-тельно), и заключения о качестве построенной регрессии оказываются неоправ-данно оптимистичными. По этим причинам желательно применять обобщенныйМНК (ОМНК), заклю-чающийся в минимизации обобщенной остаточной дисперсии 1 N eΩ−1e. В обобщенной остаточной дисперсии остатки взвешиваются в соответствии со структурой ковариационной матрицы ошибок. Минимизация приводит к полу-чению следующего оператора ОМНК-оценивания (ср. с (7.13), где Ω = IN): a = (ZΩ−1Z)−1ZΩ−1X. (8.1) Для обоснования ОМНК проводится преобразование в пространстве наблю-дений (см. параграф 6.4) с помощью невырожденной матрицы D размерности N × N, такой, что D−1D−1 = Ω (такое представление допускает любая ве-щественная симметричная положительно определенная матрица, см. Приложение A.1.2): DX = DZα + Dε. (8.2) Такое преобразование возвращаетмодель в"штатную"ситуацию,посколькуновые остатки удовлетворяют гипотезе g4: E(DεεD) = Dσ2ΩD= σ2DD−1D−1D= σ2IN. Остаточная дисперсия теперь записывается как 1N eDDe, а оператор оцени-вания-как a = (ZDDZ)−1ZDDX. Что и требовалось доказать, поскольку DD = Ω−1. Обычно ни дисперсии, ни тем более ковариации ошибок по наблюдениям не из-вестны. В классической эконометрии рассматриваются два частных случая. 8.2. Гетероскедастичность ошибок Пусть ошибки не коррелированы по наблюдениям, и матрица Ω (а вследза ней и матрица D) диагональна. Если эта матрица единична, т.е. дисперсии ошибок
8.2. Гетероскедастичность ошибок 259 одинаковы по наблюдениям (гипотеза g4 не нарушена), то имеет место гомос-кедастичность или однородность ошибок по дисперсии-"штатная" ситуация. В противном случае констатируют гетероскедастичность ошибок или их неодно-родность по дисперсии. Пусть var(εi) = σ2i -дисперсия ошибки i-го наблюдения. Гомоскедастич-ность означает, что все числа σ2i одинаковы, а гетероскедастичность-что среди них есть несовпадающие. Факт неоднородности остатков по дисперсии мало сказывается на качестве оце-нок регрессии, если эти дисперсии не коррелированы с независимыми факторами. Это-случай гетероскедастичности "без негативных последствий". Данное утверждение можно проиллюстрировать в случае, когда в матрице Z все-го один столбец, т.е. n = 1 и свободный член отсутствует. Тогда формула (7.33) приобретает вид: E(s2e) = 1 N ⎛⎝i σ2i −iσ2i z2i iz2i ⎞⎠. Если ситуацияштатная и σ2i = σ2, топравая часть этойформулыпреобразуется к ви-ду N − 1 N σ2, и N N − 1s2eоказывается несмещенной оценкой σ2, как и было пока-зано в параграфе 7.2. Если σi и zi не коррелированы, то, обозначив σ2 = 1 Niσ2i , можно утверждать, чтоiσ2i z2i5iz2i ≈ σ2iz2i5iz2i = σ2, т.е. ситуация остается прежней. И только если σi и zi положительно (или отрица-тельно) коррелированы, факт гетероскедастичности имеет негативные последствия. Действительно, в случае положительной корреляции σ2i z2i z2i > σ2 и, следова-тельно, EN N − 1s2e< σ2. Обычная "несмещенная" оценка остаточной диспер-сии оказывается по математическому ожиданию меньше действительного значе-ния остаточной дисперсии, т.е. она (оценка остаточной дисперсии) дает основания для неоправданно оптимистичных заключений о качестве полученной оценки модели. Следует заметить, что факт зависимости дисперсий ошибок от независимых факторов в экономике весьма распространен.Вэкономике одинаковыми по диспер-сии скорее являются относительные (εz ), а не абсолютные (ε) ошибки.Поэтому, когда оценивается модель на основе данных по предприятиям, которые могут иметь
260 Глава 8. Нарушение гипотез основной линейной модели и, как правило, имеют различные масштабы, гетероскедастичности с негативными последствиями просто не может не быть. Если имеет место гетероскедастичность, то, как правило, дисперсия ошибки связана с одной или несколькими переменными, в первую очередь-с факторами регрессии. Пусть, например, дисперсия может зависеть от некоторой перемен-ной yi, которая не является константой: σ2i = σ2(yi), i= 1, . . . , N. Как правило, в качестве переменной yi берется один из независимых факторов или математическое ожидание изучаемой переменной, т.е. x0 = Zα (в качестве его оценки используют расчетные значения изучаемой переменной Za). В этой ситуации желательно решить две задачи: во-первых, определить, имеет ли место предполагаемая зависимость, а во-вторых, если зависимость обнаружена, получить оценки с ее учетом. При этом могут использоваться три группы методов. Методы первой группы позволяют работать с гетероскедастичностью, которая за-дается произвольной непрерывной функцией σ2(-). Для методов второй группы функция σ2(-) должна быть монотонной. В методах третьей группы функция σ2(-) предполагается известной с точностью до конечного числа параметров. Примером метода из первой группы является критерий Бартлетта, который заключается в следующем. Пусть модель оценена и найдены остатки ei, i = 1, . . . , N. Для расчета bc - статистики, лежащей в основе применения этого критерия, все множество наблю-дений делится по какому-либо принципу на k непересекающихся подмножеств. В частности, если требуется выявить, имеется ли зависимость от некоторой пе-ременной yi, то все наблюдения упорядочиваются по возрастанию yi, а затем в соответствии с этим порядком делятся на подмножества. Пусть Nl -количество элементов в l-м подмножестве, k l=1Nl = N; s2l-оценка дисперсии остатков в l-м подмножестве, найденная на основе остатков ei; bs = 1 N kl=1Nls2l kl=1 s2Nl l 1/N -отношение средней арифметической дисперсий к сред-ней геометрической; это отношение в соответствии со свойством мажорантности средних (см. п. 2.2) больше или равно единице, и чем сильнее различаются диспер-сии по подмножествам, тем оно выше.
8.2. Гетероскедастичность ошибок 261 22 s 2i e 23 s 24 s 25 s i y 2 1 s Рис. 8.1 Тогда статистика Бартлетта равна bc = N 1 + kl=1 1 Nl − 1 N 3(k − 1) ln bs. При однородности наблюдений по дисперсии (нулевая гипотеза) эта статистика распределена как χ2k−1. Проверка нулевой гипотезы проводится по обычному ал-горитму. Если нулевую гипотезу отвергнуть не удалось, т.е. ситуация гомоскедастична, то исходная оценка модели удовлетворительна. Если же нулевая гипотеза отверг-нута, то ситуация гетероскедастична. Принцип построения статистики Бартлетта иллюстрирует рисунок 8.1. Классическийметод второй группы заключается в следующем.Все наблюдения упорядочиваются по возрастанию некоторой переменной yi. Затем оцениваются две вспомогательные регрессии: по K "малым" и по K "большим" наблюдениям (с целью повышения мощности критерия средние N − 2K наблюдения в расчете не участвуют, а K можно, например, выбрать равным приблизительно трети N). Пусть s21-остаточная дисперсия в первой из этих регрессий, а s22-во второй. Вслучае гомоскедастичностиошибок (нулевая гипотеза)отношение двух дисперсий распределено как s22 s21 ∼ FK−n−1,K−n−1. Здесь следует применять обычный F-критерий. Нулевая гипотеза о гомос-кедастичности принимается, если рассчитанная статистика превышает 95%-ный квантиль F-распределения.
262 Глава 8. Нарушение гипотез основной линейной модели 22 s 2i e i y 2 1 s Рис. 8.2 Такой подход применяется, если ожидается, что дисперсия может быть только по-ложительно коррелирована с переменной yi . Если неизвестно, положительно или отрицательно коррелирована дисперсия с рассматриваемым фактором, то следу-ет отклонять нулевую гипотезу как при больших, так и при малых значениях ста-тистики s22 s21. Можно применить следующий прием: рассчитать статистику как отношение максимальной из дисперсий s21и s22к минимальной. Такая статисти-ка будет иметь усеченное F -распределение, где усечение происходит на уровне медианы, и берется правая половина распределения. Отсюда следует, что для до-стижения, например, 5%-го уровня ошибки, следует взять табличную критиче-скую границу, соответствующую, 2.5%-му правому хвосту обычного (не усеченного) F -распределения. Если указанная статистика превышает данную границу, то нуле-вая гипотеза о гомоскедастичности отвергается. Данный методиз вестен подназ ванием метода Голдфельда-Квандта. Можно применять упрощенный вариант этого критерия, когда дисперсии s22и s22считаются на основе остатков из проверяемой регрессии. При этом s21и s22не будут независимы, и их отношение будет иметь F-распределение только прибли-женно. Этот методил люстрирует рисунок 8.2. Для того чтобы можно было применять методы третьей группы, требуется обладать конкретной информацией о том, какой именно вид имеет гетероскеда-стичность. Так, например, если остатки прямо пропорциональны значениям фактора (n = 1): x = zα + β + zε, и ε удовлетворяет необходимым гипотезам, то делением обеих частей уравнения на z ситуация возвращается в "штатную": xZ = α + 1Z β + ε,
8.2. Гетероскедастичность ошибок 263 22 s 2i e i y 2 1 s Рис. 8.3 в которой, правда, угловой коэффициент и свободный член меняются местами. Тем самым применяется преобразование в пространстве наблюдений такое, что диаго-нальные элементы матрицы D равны 1zi . Если зависимость дисперсии от других переменных известна не точно, а только с точностью до некоторых неизвестных параметров, то для проверки гомоскеда-стичности следует использовать вспомогательные регрессии. Так называемый метод Глейзера состоит в следующем. Строится регрессия модулей остатков |ei| на константу и те переменные, которые могут быть коррели-рованными с дисперсией (например, это может быть все множество независимых факторов или какое-то их подмножество). Если регрессия оказывается статисти-чески значимой, то гипотеза гомоскедастичности отвергается. Построение вспомогательной регрессии от некоторой переменной yi показано на рисунке 8.3. Другой метод( критерий Годфрея) использует аналогичную вспомогательную регрессию, в которой в качестве зависимой переменной используются квадраты остатков e2i. Если с помощью какого-либо из перечисленных критериев (или других анало-гичных критериев) проверены различные варианты возможной зависимости и ну-левая гипотеза во всех случаях не была отвергнута, то делается вывод, что ситуа-ция гомоскедастична или гетероскедастична без негативных последствий и что для оценки параметров модели можно использовать обычный МНК. Если же нуле-вая гипотеза отвергнута и поэтому, возможно, имеет место гетероскедастичность с негативными последствиями, то желательно получить более точные оценки, учи-тывающие гетероскедастичность. Это можно сделать, используя для оценивания обобщенныйМНК (см. уравне-ние (8.2)). Соответствующее преобразование в пространстве наблюдений состоит
264 Глава 8. Нарушение гипотез основной линейной модели в том, чтобы каждое наблюдение умножить на di, т.е. требуется оценить обычным методом наименьших квадратов преобразованную регрессию с переменными diXi и diZi. При этом не следует забывать, что если матрица факторов Z содержит свободный член, то его тоже нужно умножить на di, поэтому вместо свободного члена в регрессии появится переменная вида (d1, . . . , dN). Это приводит к тому, что стандартные статистические пакеты выдают неверные значения коэффициен-та детерминации и F-статистики. Чтобы этого не происходило, требуется поль-зоваться специализированными процедурами для расчета взвешенной регрессии. Описанный методполучил название взвешенногоМНК, поскольку он равнозначен минимизации взвешенной суммы квадратов остатков Ni=1 d2ie2i. Чтобы это можно было осуществить, необходимо каким-то образом получить оценку матрицы D, используемой для преобразования в пространстве наблюдений. Перечисленные в этом параграфе методы дают возможность не только проверить гипотезу об отсутствии гетероскедастичности, но и получить определенные оценки матрицы D (возможно, не очень хорошие). Если S2 -оценка матрицы σ2Ω, гд е S2 -диагональная матрица, состав-ленная из оценок дисперсий, то S−1 (матрица, обратная к ее квадратному кор-ню)-оценка матрицы σD. Так, после проверки гомоскедастичности методом Глейзера в качестве диа-гональных элементов матрицы S−1 можно взять 1|ei|c, гд е |ei|c-расчетные значения |ei|. Если используются критерии Бартлетта или Голдфельда-Квандта, то наблюдения разбиваются на группы, для каждой из которых есть оценка дис-персии, s2l. Тогда для этой группы наблюдений в качестве диагональных элементов матрицы S−1 можно взять 1sl . В методе Голдфельда-Квандта требуется дополнительно получить оценку дис-персии для пропущенной средней части наблюдений. Эту оценку можно получить непосредственно по остаткам пропущенных налюдений или как среднее (s21+s22)/2. Если точный вид гетероскедастичности неизвестен, и, как следствие, взвешенный МНК неприменим, то, по крайней мере, следует скорректировать оценку ковариа-ционной матрицы оценок параметров, оцененных обычным МНК, прежде чем про-верять гипотезы о значимости коэффициентов. (Хотя при использовании обычного МНКоценки будут менее точными, но как уже упоминалось, они будут несмещенны-ми и состоятельными.) Простейший методко ррекции состоит в замене неизвестной ковариационной матрицы ошибок σ2Ω на ее оценку S2, гд е S2 - диагональная матрица с типичным элементом e2i(т.е. квадраты остатков используются как оценки дисперсий). Тогда получается следующая скорректированная оценка ковариацион-ной матрицы a (оценка Уайта или устойчивая к гетероскедастичности оценка): (ZZ)−1 ZS2Z (ZZ)−1 .
8.3. Автокорреляция ошибок 265 8.3. Автокорреляция ошибок Если матрица ковариаций ошибок не является диагональной, то говорят об ав-токорреляции ошибок. Обычно при этом предполагают, что наблюдения однород-ны по дисперсии, и их последовательность имеет определенный смысл и жестко фиксирована. Как правило, такая ситуация имеет место, если наблюдения про-водятся в последовательные моменты времени. В этом случае можно говорить о зависимостях ошибок по наблюдениям, отстоящим друг от друга на 1, 2, 3 и т.д . момента времени. Обычно рассматривается частный случай автокорреляции, когда коэффициенты ковариации ошибок зависят только от расстояния во времени меж-ду наблюдениями; тогда возникает матрица ковариаций, в которой все элементы каждой диагонали (не только главной) одинаковы1. Поскольку действие причин, обуславливающих возникновение ошибок, доста-точно устойчиво во времени, автокорреляции ошибок, как правило, положительны. Это ведет к тому, что значения остаточной дисперсии, полученные по стандартным ("штатным") формулам, оказываются ниже их действительных значений. Что, как отмечалось и в предыдущем пункте, чревато ошибочными выводами о качестве получаемых моделей. Это утверждение иллюстрируется рисунком 8.4 (n = 1). На этом рисунке: a -линия истинной регрессии. Если в первый момент времени истинная ошибка отрицательна, то в силу положительной автокорреляции ошибок все облако наблю-дений сместится вниз, и линия оцененной регрессии займет положение b. Если в первый момент времени истинная ошибка положительна, то по темже причи-нам линия оцененной регрессии сместится вверх и займет положение c. Поскольку 1В теории временных рядов это называется слабой стационарностью. x c ab время Рис. 8.4
266 Глава 8. Нарушение гипотез основной линейной модели ошибки случайны и в первый момент времени они примерно с равной вероятно-стью могут оказаться положительными или отрицательными, то становится ясно, насколько увеличивается разброс оценок регрессии вокруг истинных по сравнению с ситуацией без (положительной) автокорреляции ошибок. Типичный случай автокорреляции ошибок, рассматриваемый в классической эконометрии,-это линейная авторегрессия ошибок первого порядка AR(1): εi = ρεi−1 + ηi, где η -остатки, удовлетворяющие обычным гипотезам; ρ -коэффициент авторегрессии первого порядка. Коэффициент ρ вляется также коэффициентом автокорреляции (первого по-рядка). Действительно, по определению, коэффициент авторегрессии равен (как МНК-оценка): ρ = cov(εi, εi−1) var(εi−1) , но, в силу гомоскедастичности, var(εi−1) = :var(εi)var(εi−1) и, следовательно, ρ, также по определению, является коэффициентом автокорреляции. Если ρ = 0, то εi = ηi и получаем "штатную" ситуацию. Таким образом, проверку того, что автокорреляция отсутствует, можно проводить как проверку нулевой гипотезы H0: ρ = 0 для процесса авторегрессии 1-го порядка в ошибках. Для проверки этой гипотезы можно использовать критерий Дарбина- Уотсона или DW-критерий. Проверяется нулевая гипотеза о том, что автокорре-ляция ошибок первого порядка отсутствует. (При автокорреляции второго и более высоких порядков его мощность может быть мала, и применение данного критерия становится ненадежным.) Пусть была оценена модель регрессии и найдены остатки ei, i = 1, . . . , N. Значение статистики Дарбина-Уотсона (отношения фон Неймана), или DW-ста-тистики, рассчитывается следующим образом: dc = Ni=2 (ei − ei−1)2 Ni=1 e2i . (8.3) Оно лежит в интервале от 0 до 4, в случае отсутствия автокорреляции ошибок приблизительно равно 2, при положительной автокорреляции смещается в мень-
8.3. Автокорреляция ошибок 267 dU 2 dL 4dU 4dL 0 4 Рис. 8.5 шую сторону, при отрицательной-в большую сторону. Эти факты подтвержда-ются тем, что при больших N справедливо следующее соотношение: dc ≈ 2(1 − r), (8.4) где r -оценка коэффициента авторегрессии. Минимального значения величина dc достигает, если коэффициент авторегрессии равен +1. В этом случае ei = e, i = 1, . . . , N, и dc = 0. Если коэффициент авторегрессии равен −1 и ei = (−1)ie, i = 1, . . . , N, то величина dc достигает значения 4N − 1 N (можно достичь и более высокого значения подбором остатков), которое с ростом N стремится к 4.Формула (8.4) следует непосредственно из (8.3) после элементарных преобразований: dc = Ni=2 e2i Ni=1 e2i − 2 Ni=2 ei−1ei Ni=1 e2i + Ni=2 e2i−1 Ni=1 e2i , поскольку первое и третье слагаемые при больших N близки к единице, а второе слагаемое является оценкой коэффициента автокорреляции (умноженной на −2). Известно распределение величины d, если ρ = 0 (это распределение близко к нормальному), но параметры этого распределения зависят не только от N и n, как для t-и F-статистик при нулевых гипотезах. Положение "колокола" функции плотности распределения этой величины зависит от характера Z. Тем не менее, Дарбин иУотсон показали, что это положение имеет две крайние позиции (рис. 8.5). Поэтому существует по два значения для каждого (двустороннего) квантиля, соответствующего определенным N и n: его нижняя dL и верхняя dU границы. Нулевая гипотеза H0: ρ = 0принимается, если dU dc 4−dU ; она отвергается в пользу гипотезы о положительной автокорреляции, если dc < dL, и в пользу
268 Глава 8. Нарушение гипотез основной линейной модели гипотезы об отрицательной автокорреляции, если dc > 4−dL. Если dL dc < dU или 4−dU < dc 4−dL, вопрос остается открытым(это-зона неопределенности DW-критерия). Пусть нулевая гипотеза отвергнута. Тогда необходимо дать оценку матрицы Ω. Оценка r параметра авторегрессии ρ может определяться из приближенного равенства, следующего из (8.4): r ≈ 1 − dc 2 , или рассчитываться непосредственно из регрессии e на него самого со сдвигом на одно наблюдение с принятием "круговой" гипотезы, которая заключается в том, что eN+1 = e1. Оценкой матрицы Ω является 1 1 − r2 ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 r r2 - - - rN−1 r 1 r - - - rN−2 r2 r 1 - - - rN−3 ... ... .... . . ... rN−1 rN−2 rN−3 - - - 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ , а матрица D преобразований в пространстве наблюдений равна ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ √1 − r2 0 0 - - - 0 −r 1 0 - - - 0 0 −r 1 - - - 0 ... ... .... . . ... 0 0 0 - - - 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Для преобразования в пространстве наблюдений, называемом в данном слу-чае авторегрессионным, используют обычно указанную матрицу без 1-й строки, что ведет к сокращению количества наблюдений на одно. В результате такого пре-образования из каждого наблюдения, начиная со 2-го, вычитается предыдущее, умноженное на r, теоретическими остатками становятся η , которые, по предпо-ложению, удовлетворяют гипотезе g4.
8.3. Автокорреляция ошибок 269 После этого преобразования снова оцениваются параметры регрессии. Если новое значение DW-статистики неудовлетворительно, то можно провести следую-щее авторегрессионное преобразование. Обобщает процедуру последовательных авторегрессионных преобразований метод Кочрена-Оркатта, который заключается в следующем. Для одновременной оценки r, a и b используется критерийОМНК (в обозна-чениях исходной формы уравнения регрессии): 1 N Ni=2 ((xi − rxi−1) − (zi − rzi−1)a − (1 − r)b)2 → min, где zi - n-вектор-строка значений независимых факторов в i-м наблюдении (i-строка матрицы Z). Поскольку производные функционала по искомым величинам нелинейны от-носительно них, применяется итеративная процедура, на каждом шаге которой сначала оцениваются a и b при фиксированном значении r предыдущего шага (на первом шаге обычно r = 0), а затем- r при полученных значениях a и b. Процесс, как правило, сходится. Как и в случае гетероскедастичности, можно не использовать модифицированные методы оценивания (тем более, что точный вид автокорреляции может быть неиз-вестен), а использовать обычный МНК и скорректировать оценку ковариационной матрицы параметров. Наиболее часто используемая оценка Ньюи-Уэста (устой-чивая к гетероскедастичности и автокорреляции) имеет следующий вид: (ZZ)−1 Q(ZZ)−1 , где Q = N i=1 e2i+ L k=1 N i=k+1 λkeiei−k(zizi−k + zi−kzi), а λk -понижающие коэффициенты, которые Ньюи и Уэст предложили рассчи-тывать по формуле λk = 1 − k L + 1. При k > L понижающие коэффициенты становятся равными нулю, т.е. более дальние корреляции не учитываются Обоснование этой оценки достаточно сложно2. Заметим только, что если заменить попарные произведения остатков соответствующими ковариациями и убрать пони-жающие коэффициенты, то получится формула ковариационной матрицы оценок МНК. Приведенная оценка зависит от выбора параметра отсечения L. В настоящее вре-мя не существует простых теоретически обоснованных методов для такого выбора. На практике можно ориентироваться на грубое правило L = 4 T1002/9 . 2Оно связано с оценкой спектральной плотности для многомерного временного ряда.
270 Глава 8. Нарушение гипотез основной линейной модели 8.4. Ошибки измерения факторов Пусть теперь нарушается гипотеза g2, и независимые факторы наблюдаются с ошибками. Предполагается, что изучаемая переменная зависит от истинных зна-чений факторов (далее в этом пункте используется сокращенная форма уравнения регрессии), ˆz0, а именно: ˆx = ˆz0α + ε, но истинные значения неизвестны, а вместо этого имеются наблюдения над неко-торыми связанными с ˆz0 переменными ˆz: ˆz = ˆz0 + εz, где εz -вектор-строка длиной n ошибок наблюдений. В разрезе наблюдений: ˆX= ˆ Z0α + ε, ˆ Z = ˆ Z0 + εz, где ˆ Z0 и εz -соответствующие N × n-матрицы значений этих величин по на-блюдениям (т.е., в зависимости от контекста, εz обозначает вектор или матрицу ошибок). Предполагается, что ошибки факторов по математическому ожиданию равны нулю, истинные значения регрессоров и ошибки независимы друг от друга (по край-ней мере не коррелированы друг с другом) и известны матрицы ковариации: E(εz) = 0, E(ˆz0, ε) = 0, E(ˆz0, εz) = 0, E(ˆz0, ˆz0) = M0, E(εz, εz) = Ω, E(εz, ε) = ω. (8.5) Важно отметить, что эти матрицы и вектора ковариации одинаковы во всех наблюдениях, а ошибки в разных наблюдениях не зависят друг от друга, т.е. речь, фактически, идет о "матричной" гомоскедастичности и отсутствии автокорреляции ошибок. Через наблюдаемые переменные ˆx и ˆz уравнение регрессии записывается в следующей форме: ˆx = ˆzα + ε − εzα. (8.6) В такой записи видно, что "новые" остатки не могут быть независимыми от факто-ров-регрессоров ˆz, т.е. гипотезы основной модели регрессии нарушены. В рамках
8.4. Ошибки измерения факторов 271 сделанных предположений можно доказать, что приближенно E(a) ≈ (M0 +Ω)−1(M0α + ω) = α + (M0 +Ω)−1(ω − Ωα), (8.7) т.е. МНК-оценки теряют в такой ситуации свойства состоятельности и несмещен-ности3, если ω = Ωα (в частности, когда ошибки регрессии и ошибки факторов не коррелированны, т.е. когда ω = 0, а Ω и α отличны от нуля). Для обоснования (8.7) перейдем к теоретическому аналогу системы нормальных уравнений, для чего обе части соотношения (8.6) умножаются на транспонирован-ную матрицу факторов: E(ˆzˆx) = E(ˆzˆz) α + E(ˆzε) − E(ˆzεz) α. Здесь, как несложно показать, пользуясь сделанными предположениями, E(ˆzˆz) = M0 +Ω, E(ˆzε) = ω, E(ˆzεz) = Ω, Поэтому E(ˆzˆx) = E(ˆzˆz) α + ω − Ωα или E(ˆzˆz)−1 E(ˆzˆx) = α + M0 +Ω−1 (ω − Ωα) . Левая часть приближенно равна E(a). Действительно, a = M−1m, гд еM = 1N ˆ Zˆ Z и m = 1N ˆ Zˆx. Выборочные ковари-ационные матрицы M и m по закону больших чисел с ростом числа наблюдений сходятся по вероятности к своим теоретическим аналогам: M p −→E(ˆzˆz) и m p −→E(ˆzˆx) . По свойствамсходимости по вероятности пределфункции равенфункции от предела, если функция непрерывна. Поэтому a = M−1m p −→ E(ˆzˆz)−1 E(ˆzˆx) = (M0 +Ω)−1(M0α + ω). Существуют разные подходы к оценке параметров регрессии в случае наличия ошибок измерения независимых факторов. Здесь приводятся два из них. 3Они смещены даже асимптотически, т.е. при стремлении количества наблюдений к бесконечно-сти смещение не стремится к нулю.
272 Глава 8. Нарушение гипотез основной линейной модели а) Простая регрессия. Если имеется оценка W ковариационной матрицы Ω и w -ковариационного вектора ω , то можно использовать следующий оператор оценивания: a = (M −W)−1(m − w), который обеспечивает состоятельность оценок и делает их менее смещенными. Это формула следует из E(ˆzˆx) = E(ˆzˆz) α + ω − Ωα заменой теоретических моментов на их оценки. Обычно предполагается, что W -диагональная матрица, а w = 0. б) Ортогональная регрессия. Поскольку z теперь такие же случайные пере-менные, наблюдаемые сошибками, как и x, имеет смысл вернуться к обозначениям 6-го раздела, где через x обозначался n-мерный вектор-строка всех переменных. Пусть ε -вектор их ошибок наблюдения, а x0 -вектор их истинных значений, то есть x = x0 + ε, X = X0 + ε. Предположения (8.5) записываются следующим образом: E(ˆx0, ε) = 0, E(ˆx0, ˆx0) = M0, E(ε, ε) = σ2Ω. Теперь через M0 обозначается матрица, которую в обозначениях, используемых в этом пункте выше, можно записать следующим образом: ⎡⎢⎢⎣ σ2x0 m0 m0M0 ⎤⎥⎥⎦, а через σ2Ω матрица ⎡⎢⎣ σ2 ω ωΩ ⎤⎥⎦. Поскольку речь идет о линейной регрессии, предполагается, что между истин-ными значениями переменных существует линейная зависимость: x0α = 0.
8.5. Метод инструментальных переменных 273 Это означает, что M0α = 0. Рассуждая также, как при доказательстве соотношения (8.7), легко установить, что E(M) = M0 + σ2Ω, (M -фактическая матрица ковариации X) т.е. (E(M) − σ2Ω)α = 0. Таким образом, если считать, что Ω известна, а σ2 -минимизируемый параметр (в соответствии с логикойМНК), то решение задачи (M − σ2Ω)a = 0, σ2 → min! даст несмещенную оценку вектора α . А это, как было показано в пункте 6.4, есть задача регрессии в метрике Ω−1 (см. (6.37)). Преобразованием в пространстве переменных она сводится к "обычной" ортогональной регрессии. Т.е. если для устранения последствий нарушения гипотезы g4 используется преобразование в пространстве наблюдений, то при нарушении гипотезы g2 надо "работать" с преобразованием в пространстве переменных. Несмотря на то, что методы ортогональной регрессии и регрессии в метрике Ω−1 в наибольшей степени соответствуютреалиям экономики (ошибки есть во всех переменных, стоящих как в левой, так и в правой частях уравнения регрессии), они мало используются в прикладных исследованиях. Основная причина этого заклю-чается в том, что в большинстве случаев невозможно получить надежные оценки матрицы Ω. Кроме того, ортогональная регрессия гораздо сложнее простой с вы-числительной точки зрения, и с теоретической точки зрения она существенно менее изящна и прозрачна. Вследующемпараграфе излагается еще один метод, который позволяет решить проблему ошибок в переменных (и в целом может использоваться при любых нарушениях гипотезы g2). 8.5. Метод инструментальных переменных Предполагаем, что в регрессии x = zα + ε переменные-факторы z являются случайными, и нарушена гипотезаg2 в обобщенной формулировке:ошибка ε зави-сит от факторов z, так что корреляция между z и ошибкой ε не равна нулю. Такую
274 Глава 8. Нарушение гипотез основной линейной модели регрессию можно оценить, имея набор вспомогательных переменных y, называ-емых инструментальными переменными. Часто инструментальные переменные называют просто инструментами. Для того, чтобы переменные y можно было использовать в качестве инстру-ментальных, нужно, чтобы они удовлетворяли следующим требованиям: 1) Инструменты y некоррелированы с ошибкой ε. (В противном случае метод даст несостоятельные оценки, как и МНК.) Если это условие не выполнено, то такие переменные называют негодными инструментами4. 2) Инструменты y достаточно сильно коррелированы с факторами z. Если данное условие не выполнено, то это так называемые "слабые" инструменты. Если инструменты слабые, то оценки по методу будут неточными и при малом количестве наблюдений сильно смещенными. Обычно z и y содержат общие переменные, т.е. частьфакторов используется в качестве инструментов. Например, типична ситуация, когда z содержит константу; тогда в y тоже следует включить константу. Пусть имеются N наблюдений, и X, Z и Y - соответствующие данные в матричном виде. Оценки по методу инструментальных переменных (сокращенно IV от англ. instrumental variables) вычисляются по следующей формуле: aIV = ZY Y Y −1 Y Z−1 ZY Y Y −1 Y X. (8.8) В случае, если количество инструментальных переменных в точности равно количеству факторов, ( rank Y = n + 1) получаем собственно классический ме-тодинстр ументальных переменных. При этом матрица Y Z квадратная и оценки вычисляются какaIV = Y Z−1 Y Y ZY −1 ZY Y Y −1 Y X. Средняя часть формулы сокращается, поэтому aIV = Y Z−1 Y X. (8.9) Рассмотрим выводк лассического метода инструментальных переменных, т.е. случай точной идентификации (ср. с (6.15) в главе 6): Умножим уравнение регрессии x = zα + ε слева на инструменты y (с транс-понированием). Получим следующее уравнение: yx = yzα + yε. 4В модели ошибок в переменных ошибка регрессии имеет вид ε − εzα, гд е ε - ошибка в исходном уравнении, а εz - ошибка измерения факторов z. Чтобы переменные y можно было использовать в качестве инструментов, достаточно, чтобы y были некоррелированы с ε и εz .
8.5. Метод инструментальных переменных 275 Если взять от обеих частей математическое ожидание, то получится E(yx) = E(yzα), где мы учли, что инструменты некоррелированы с ошибкой, E(yε) = 0. Заменяя теоретические моменты навыборочные, получимследующиенормаль-ные уравнения, задающие оценки a: Myx = Myza, где Myx = 1 N Y X и Myz = 1N Y Z. Очевидно, что эти оценки совпадут с (8.9). Фактически, мы применяем здесь метод моментов. Методинструментальных переменных можно рассматривать как так называе-мый двухшаговый метод наименьших квадратов. (О нем речь еще пойдет ниже в пункте 10.3.) 1-й шаг. Строим регрессию каждого фактора Zj на Y . Получим в этой ре-грессии расчетный значения Zcj . По формуле расчетных значений в регрессии Zcj = Y (Y Y )−1 Y Z. Заметим, что если Zj входит в число инструментов, то по этой формуле получим Zcj = Zj , т.е. эта переменная останется без изменений. Поэтому данную процедуру достаточно применять только к тем факторам, которые не являются инструментами (т.е. могут быть коррелированы с ошибкой). В целом для всей матрицы факторов можем записать Zc = Y (Y Y )−1 Y Z. 2-й шаг. В исходной регрессии используются Zc вместо Z. Смысл состоит в том, чтобы использовать факторы "очищенные от ошибок". Получаем следующие оценки: a2M = ZcZc−1 Zcx = = ZY Y Y −1 Y Y Y Y −1 Y Z−1 ZY Y Y −1 Y x = = ZY Y Y −1 Y Z−1 ZY Y Y −1 Y x = aIV . Видим, что оценки совпадают. Если записать оценки в виде aIV = (ZcZ)−1 Zcx, то видно, что обобщенный методинструментальных переменных можно рассматривать как простой методин-струментальных переменных с матрицей инструментов Zc. Такая запись позволяет обосновать обобщенный методинстр ументальных пе-ременных. Если исходных инструментов Y больше, чем факторов Z, и мы хотим построить на их основе меньшее количество инструментов, то имеет смысл сопо-ставить каждому фактору Zj в качестве инструмента такуюлинейнуюкомбинацию исходных инструментов, которая была бы наиболее сильно коррелирована с Zj . Этому требованию как раз и удовлетворяют расчетные значения Zcj .
276 Глава 8. Нарушение гипотез основной линейной модели Другое обоснование обобщенного метода инструментальных переменных со-стоит, как и выше для классического метода, в использовании уравнений E(yx) = = E(yzα). Заменой теоретических моментов выборочными получим уравнения Myx = Myza, число которых больше числа неизвестных. Идея состоит в том, чтобы невязки Myx −Myza были как можно меньшими. Это достигается минимизацией следующей квадратичной формы от невязок: (Myx −Myza)M−1 yy (Myx −Myza), где Myy = 1 N Y Y .Минимум достигается при a = MzyM−1 yy Myz−1MzyM−1 yy Myx. Видим, что эта формула совпадает с (8.8). Эти рассуждения представляют собой применение так называемого обобщенного метода моментов, в котором количе-ство условий на моменты может превышать количество неизвестных параметров. Чтобы можно было использовать методинстр ументальных переменных на практике, нужна оценка ковариационной матрицы, с помощью которой можно было бы вычислить стандартные ошибки коэффициентов и t-статистики. Такая оценка имеет вид MaIV = s2 ZcZc−1 . Здесь s2 - оценка дисперсии ошибок σ2, например s2 = ee/N или s2 = = ee/(N − 1). Остатки рассчитываются по обычной формуле e = x − ZaIV . (Здесь следует помнить, что остатки, получаемые на втором шаге тут не годят-ся, поскольку они равны x − ZcaIV . Если использовать их для расчета оценки дисперсии, то получим заниженную оценку дисперсии и ковариационной матрицы. Отсюда следует, что из регрессии второго шага можно использовать только оценки коэффициентов. Стандартные ошибки и t-статистики требуется пересчитывать.) Обсудим теперь более подробно проблему идентификации5. Чтобы можно было вычислить оценки (8.8), нужно, чтобы выполнялись следу-ющие условия: 1) Матрица инструментов должна иметь полный ранг по столбцам, иначе (Y Y )−1 не существует. 2) ZY (Y Y )−1 Y Z должна быть невырожденной. В частности, матрица ZY (Y Y )−1 Y Z необратима, когда rankY <rankZ. Предположим, чтоматрица факторов Z имеет полныйранг, т.е. rankZ = n+1. 5См. также обсуждение идентификации в контексте систем уравнений ниже в пункте 10.2.
8.5. Метод инструментальных переменных 277 Т.е. если rankY < n+ 1, то уравнение неидентифицируемо, т.е. невозмож-но вычислить оценки (8.8). Таким образом, количество инструментов (включая константу) должно быть не меньше n + 1 (количество регрессоров, включая кон-станту). Если rankY > n+ 1, то говорят, что уравнение сверхидентицировано. Если количество инструментов равно n + 1, то это точная идентификация. Если возможен случай сверхидентификации, то это обобщенный метод инстру-ментальных переменных. При точной идентификации ( rank Y = n + 1) получаем собственно классический методинструментальных переменных. Таким образом, необходимое условие идентификации имеет следующий вид: rank Y rankZ(= n + 1). Это так называемое порядковое условие идентификации, условие на размерность матриц. Словесная формулировка порядкового условия: Количество инструментов Y должно быть не меньше количества ре-грессоров Z (учитывая константу). Заметим, чтоможно сначала "вычеркнуть" общие переменные в Z и Y и смотреть только на количество оставшихся. Количество оставшихся инструментов должно быть не меньше количества оставшихся регрессоров. Почему это только необходимое условие? Пусть, например, некоторый фактор Zj ортогонален Y . Тогд а Zcj = 0, и невозможно получить оценки aIV , т.е. данное условие не является достаточным. Необходимое и достаточное условие идентификации формулируется следую-щим образом: Матрица Zc имеет полный ранг по столбцам: rankZc = n + 1. Это так называемое ранговое условие идентификации. Встречаются случаи, когда ранговое условие идентификации соблюдается, но матрица Zc близка к вырожденности, т.е. в Zc наблюдается мультиколли-неарность. Например, если инструмент Zj является слабым ( Zj и Y почти ор-тогональны), то Zc близка к вырожденности. Один из способов проверки того, является ли инструмент слабым, состоит в анализе коэффициентов детерминации и F-статистик в регрессиях на первом шаге.
278 Глава 8. Нарушение гипотез основной линейной модели 8.6. Упражнения и задачи Упражнение 1 Таблица 8.1 Z1 Z2 1 26.8 541 1 25.3 616 1 25.3 610 1 31.1 636 1 33.3 651 1 31.2 645 1 29.5 653 1 30.3 682 1 29.1 604 1 23.7 515 1 15.6 390 1 13.9 364 1 18.8 411 1 27.4 459 1 26.9 517 1 27.7 551 1 24.5 506 1 22.2 538 1 19.3 576 1 24.7 697 1 Дано уравнение регрессии X = Zα + ε = −1.410z1 + + 0.080z2 + 56.962 + ε, гд е ε -вектор-столбец нормальный случайных ошибок с нулевым средним и ковариационной мат-рицей Eεε= σ2Ω = = σ2 1 − ρ2 ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 ρ ρ2 - - - ρN−1 ρ 1 ρ - - - ρN−2 ρ2 ρ 1 - - - ρN−3 ... ... .... . . ... ρN−1 ρN−2 ρN−3 - - - 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ (8.10) с ρ = 0.9 и σ2 = 21.611. Используя нормальное распределение с незасисимыми на-блюдениями, средним 0 и ковариационной матрицей (8.10), получите 100 выборок вектора ε размерности (N × 1), k = 1, . . . , 100, гд еN = 20. Эти случайные векторы потом используйте вместе с известным вектором α= = (−1.410, 0.080, 56.962) и матрицей регрессоров (табл. 8.1). Сначала получите ожидаемое значения X0 = Zα, затем, чтобыполучить 100 выборок вектора X размерности (20 × 1), добавьте случайные ошибки: X0 + ε = X. 1.1. Рассчитайте невырожденную матрицу D такую, что D−1D−1 = Ω. 1.2. Найдите истинную матрицу ковариации для МНК-оценки (a = (ZZ)−1 ZX): E(a − α) (a − α)= = EZZ−1 ZεεZ ZZ−1= = σ2 ZZ−1 ZΩZ ZZ−1
8.6. Упражнения и задачи 279 и истинную матрицу ковариации для ОМНК-оценки (aомнк = ZΩ−1Z−1 ZΩ−1X): E(aомнк − α) (aомнк − α)= σ2 (ZDDZ) = σ2 ZΩ−1Z−1 . Результат поясните. 1.3. Используйте 10 из 100 выборок, чтобы посчитать по каждой выборке зна-чения следующих оценок: - МНК-оценки a = (ZZ)−1 ZX; - ОМНК-оценки aомнк = ZΩ−1Z−1 ZΩ−1X; - МНК-оценки остаточной дисперсии ˆs2e= (x − Za) (x − Za)N − n − 1 ; - ОМНК-оценки остаточной дисперсии ˆs2ej омнк = (x − Zaомнк)Ω−1 (x − Zaомнк)N − n − 1 . Объясните результаты. 1.4. Вычислите среднее и дисперсию для 10 выборок для каждого из параметров, полученных в упражнении 1.3 и сравните эти средние значения с истинными параметрами. 1.5. На основе упражнения 1.3 рассчитайте S2a1 омнк, который является первым диагональным элементом матрицы ˆs2eомнк ZΩ−1Z−1 и S2a1 , который явля-ется первым диагональным элементом матрицы ˆs2e(ZZ)−1. Сравните раз-личные оценки S2a1 и S2a1 омнк друг с другом и с соответствующими значени-ями из упражнения 1.2. 1.6. На основе результатов упражнений 1.3 и 1.5 рассчитайте значения t-статис-тики, которые могут быть использованы для проверки гипотез: H0 : α1 = 0. 1.7. Повторите упражнение 1.3 для всех 100 выборок, постройте распределения частот для оценок и прокомментируйте результаты.
280 Глава 8. Нарушение гипотез основной линейной модели Упражнение 2 Таблица 8.2 z1 z2 1N 13,9 364 1 15,6 390 1 18,8 411 1 27,4 459 1 24,5 506 1 23,7 515 1 26,9 517 1 22,2 538 1 26,8 541 1 27,7 551 1 19,3 576 1 29,1 604 1 25,3 610 1 25,3 616 1 31,1 636 1 31,2 645 1 33,3 651 1 29,5 653 1 30,3 682 1 24,7 697 1 Предположим, есть данные, состоящие из 100 выборок X, по 20 значений в каждой, сгенерированных при помощи моде-ли X = Zα + ε = α1z1 + α2z2 +1Nβ + ε, гд еεi -нормально и независимо распределенная случайная величина с E(εi) = 0, Eε2i= σ2i и σ2i = e(γ1zi2+γ2). Наблюдения за X были полу-чены с использованием следующих значений параметров: α = = (α1 α2 β)= (−1.410, 0.080, 56.962)и γ = (γ1 γ2)= = (0.25,−2), а матрица значений факторов, упорядоченных в соответствии с величиной z2, имеет следующий вид (табл. 8.2). 2.1. Найдите матрицу ковариации для - ОМНК-оценки aомнк = ZΩ−1Z−1 ZΩ−1X; - МНК-оценки a = (ZZ)−1 ZX. Что вы можете сказать об относительной эффективности этих оценок? 2.2. Используйте 10 из 100 выборок, чтобы посчитать по каждой выборке значения следующих оценок: - МНК-оценки a = (ZZ)−1 ZX; - оценки γ = Ni=1 yiyi−1 Ni=1 yi ln(e2i), гд еyi = = (zi2, 1) и ei = xi − zia; - ОМНК-оценки a, используя найденую оценку γ. Сравните все эти оценки друг с другом и с соответствую-щими истинными значениями. 2.3. На основе упражнения 2.2 рассчитайте S2a1 омнк, кото-рый является первым диагональным элементом матри-цы ˆs2eомнк(ZΩ−1Z)−1, S2a1 , который является первым диагональным элементом матрицы ˆs2e(ZZ)−1, а также S2a1 Уайта, который является первым диагональным эле-ментом скорректированной оценки ковариационной матрицы (оценка Уайта или устойчивая к гетероскедастичности оценка). Сравните различные оцен-ки S2a1, S2a1 омнк и S2a1 Уайта друг с другом и с соответствующими значениями из упражнения 2.1.
8.6. Упражнения и задачи 281 2.4. На основе результатов упражнений 2.1 и 2.3 рассчитайте значения t-статис-тики, которые могут быть использованы для проверки гипотез H0 : α1 = 0. 2.5. Возьмите те же выборки, что и в упражнении 2.2, и проведите проверку на гетероскедастичность с помощью: - критерия Бартлета; - метода второй группы (метод Голдфельда-Квандта) с пропуском 4-х значений в середине выборки; - метода третьей группы (методГлейзера). 2.6. Выполните упражнение 2.2 для всех 100 выборок и, используя результаты, оцените математическое ожидание и матрицу среднеквадратических ошибок для каждой оценки. Есть ли среди оценок смещенные? Что можно сказать об относительной эффективности МНК-оценки и ОМНК-оценки? Упражнение 3 Предположим, есть данные, состоящие из 100 выборок X, по 20 значений в каждой, сгенерированных при помощи модели X = Zα+ε = α1z1+α2z2+1Nβ+ + ε, гд е εi = ρεi−1 + ηi, и η -нормально распределенная случайная величина с E(ηi) = 0, Eη2i = σ2η . Наблюдения заX были получены с использованием следующих значений параметров: α= (α1 α2 β) = (−1.410, 0.080, 56.962), ρ = 0.8 и σ2η = 6.4, а матрица значений факторов взята из упражнения 1. 3.1. Найдите матрицу ковариации для: - ОМНК-оценки aомнк = ZΩ−1Z−1 ZΩ−1X; - МНК-оценки a = (ZZ)−1 ZX. Что вы можете сказать об относительной эффективности этих оценок? 3.2. Используйте 10 из 100 выборок, чтобы посчитать по каждой выборке зна-чения следующих оценок: - МНК-оценки a = (ZZ)−1 ZX; - оценку r = Ni=2 eiei−1 Ni=1 e2i ; - ОМНК-оценки, используя найденую оценку r.
282 Глава 8. Нарушение гипотез основной линейной модели Сравните все эти оценки друг с другом и с соответствующими истинными значениями. 3.3. Возьмите те же выборки, что и в упражнении 3.2, и проверьте гипотезу об ав-токорреляции ошибок. 3.4. Найдите скорректированную оценку ковариационной матрицы, устойчивую к гетероскедастичности и автокорреляции (оценку Ньюи-Уэста). 3.5. Выполните упражнение 3.2 для всех 100 выборок и, используя результаты, оцените математическое ожидание и матрицу среднеквадратических ошибок для каждой оценки. Есть ли среди оценок смещенные? Что можно сказать об относительной эффективности МНК-оценки и ОМНК-оценки? Упражнение 4 Для уравнения X = Zoα+ε = −1.410z01+0.080z02+1N56.962+ε, z1 = z01+εz1 , z2 = z02 + εz2 и при предположении, что εi ∼ N(0, 21.611), εz1 ∼ N(0, 21.700) и εz2 ∼ N(0, 21.800), были генерированы 20 значений выборки. Результаты при-ведены в таблице 8.3. Предполагая, что истинная матрица факторов Z0 неизвестна, выполните сле-дующие задания: 4.1. Найдите МНК-оценки a = (ZZ)−1 ZX параметров уравнения регрессии X = Zα + ε = α1z1 + α2z2 + 1Nβ + ε. 4.2. Рассчитайте ковариационную матрицу ошибок измерения факторов- W и ковариационный вектор- w и оцените параметры регрессии как a = (M −W)−1(m − w). 4.3. Найдите оценку через ортогональную регрессию. 4.4. Сравните эти все оценки друг с другом и с соответствующими истинными значениями. Задачи 1. Какие свойства МНК-оценок коэффициентов регрессии теряются, если ошибки по наблюдениям коррелированы и/или имеют разные дисперсии? 2. Как оцениваются параметры уравнения регрессии, если известна матрица ковариации ошибок и она не диагональна с равными элементами по диа-гонали?
8.6. Упражнения и задачи 283 Таблица 8.3 N ε εz1 εz2 z01 z02 z1 z2 X 1 26.19 1.96 37.94 13.9 364 15.86 401.94 92.67 2 6.94 -5.94 3.57 15.6 390 9.66 393.57 73.10 3 5.55 -13.85 -18.78 18.8 411 4.95 392.22 68.88 4 14.00 24.48 14.49 27.4 459 51.88 473.49 69.05 5 0.89 23.91 51.48 24.5 506 48.41 557.48 63.79 6 46.61 -32.80 10.99 23.7 515 -9.10 525.99 111.36 7 -20.52 13.27 11.07 26.9 517 40.17 528.07 39.87 8 10.15 -16.17 18.86 22.2 538 6.03 556.86 78.85 9 -13.95 -28.22 -18.57 26.8 541 -1.42 522.43 48.50 10 14.94 20.64 -10.89 27.7 551 48.34 540.11 76.92 11 19.38 -36.99 -0.91 19.3 576 -17.69 575.09 95.21 12 5.72 -32.44 -12.71 29.1 604 -3.34 591.29 69.97 13 1.08 25.91 7.70 25.3 610 51.21 617.70 71.17 14 11.07 10.90 9.24 25.3 616 36.20 625.24 81.64 15 5.81 -42.77 8.25 31.1 636 -11.67 644.25 69.80 16 27.21 25.63 -29.14 31.2 645 56.83 615.86 91.78 17 -11.63 -13.07 13.20 33.3 651 20.23 664.20 50.46 18 -4.24 10.27 -37.62 29.5 653 39.77 615.38 63.37 19 46.56 44.81 33.93 30.3 682 75.11 715.93 115.36 20 -7.57 -40.10 -6.34 24.7 697 -15.40 690.66 70.32
284 Глава 8. Нарушение гипотез основной линейной модели 3. Рассматривается регрессионная модель X = Zα + ε. Пустьα∗ = AX - это любая несмещенная оценка параметра α. Полагая, что E(εε) = σ2Ω, покажите, что матрица ковариации α∗ превышает матрицу ковариации αомнк = (ZΩ−1Z)−1ZΩ−1X на какую-то положительно полуопределенную матрицу. 4. Докажите, что σ2омнк = (x − zα)Ω−1 (x − zα) N − n − 1 есть оценка σ2. 5. Какое преобразование матрицы наблюдений перед оценкой регрессии полез-но сделать, если среднеквадратические отклонения ошибок регрессии про-порциональны какому-либо фактору? 6. Оценивается регрессия по 10 наблюдениям. Известно, что дисперсия оши-бок для первых 5 наблюдений в два раза больше, чем дисперсия ошибок остальных 5 наблюдений. Опишите процедуру оценивания этой регрессии. 7. Рассмотрите регрессию xt = α1t + β + εt, t = 1, . . . , 5, гд е E(εt) = 0, E(ε2t) = σ2t2, E(εtεs) = 0, при t = s. Пусть ε= (ε1, ε2, ε3, ε4, ε5) и E(εε) = σ2Ω. - определите Ω; - найдите Ω−1; - найдите матрицу ковариации МНК-оценки параметра α = ⎛⎜⎝ α1 β ⎞⎟⎠; - найдите матрицу ковариации ОМНК-оценки параметра α = ⎛⎜⎝ α1 β ⎞⎟⎠. 8. Рассмотрите регрессию xt = α1t + εt, t = 1, . . . , 5, где E(εt) = 0, E(ε2t) = σ2t2 , E(εtεs) = 0, t = s. Если x = (6, 4, 9, 8, 7): - определите оценкуМНК для α1 и ее дисперсию; - определите оценку ОМНК для α1 и ее дисперсию; - сравните эти оценки друг с другом и сделайте вывод. 9. Рассматривается модель X = Zα + ε, гд е εi -нормально и независимо распределенная случайная величина с E(εi) = 0 и Eε2i= σ2i = eyiγ .
8.6. Упражнения и задачи 285 В предположении, что X = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 48629⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , Z= ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 2 1 5 1 2 1 1 1 10 1⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , Y = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 2 1 3 1 1 1 0 1 2 1⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , - найдите МНК-оценки a = (ZZ)−1ZX; - найдите ОМНК-оценки aомнк = ZΩ−1Z−1 ZΩ−1X; - постройте два 95%-х доверительных интервала для α1: один непра-вильный, основанный на результатахМНК, а другой правильный, осно-ванный на результатах ОМНК; - проверьте гипотезу γ1 = 0. 10. Параметры трехфакторного уравнения регрессии оценены по 20 наблю-дениям. S1 и S2 -остаточные дисперсии по первой и второй половинам временного ряда. В каком случае гипотезы о гомоскедастичности следует отвергнуть? 11. Приведите примеры графиков зависимостей ошибки от времени в авторе-гресионной схеме первого порядка для случаев, когда модуль коэффициента авторегрессии превышает единицу. Что можно сказать об автокорреляции ошибок, если этот коэффициент равен нулю? 12. Ошибка в регрессии задана процессом εi = 0.6εi−1 + ηi, и η -нор-мально распределенная случайная величина с E(ηi) = 0, E(η2i) = σ2η и i = 1, . . . , 5. Как выглядит матрица преобразования в пространстве пе-ременных для ОМНК? 13. Проверьте, что DD = Ω−1, гд е D = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ √1 − r2 0 0 - - - 0 −r 1 0 - - - 0 0 −r 1 - - - 0 ... ... .... . . ... 0 0 0 - - - 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ,
286 Глава 8. Нарушение гипотез основной линейной модели Ω = 1 1 − r2 ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 r r2 - - - rN−1 r 1 r - - - rN−2 r2 r 1 - - - rN−3 ... ... .... . . ... rN−1 rN−2 rN−3 - - - 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . 14. Найдите D0D0, гд еD0 -это матрица размерности (N−1)×N, полученная из матрицы D путем удаления первой строки, и сравните ее с матрицей Ω−1. 15. Какое преобразование матрицы наблюдений перед оценкой регрессии полез-но сделать, если ошибки в каждом наблюдении имеют одинаковую дисперсию и коррелированы с ошибками в предыдущем наблюдении? 16. Почему при использовании критерия Дарбина-Уотсона требуется знать два критических значения для расчетной статистики? 17. Фактическое значение dc статистики Дарбина-Уотсона равно 0.5. Что это означает?Какое преобразование следует применить к этой модели (запишите формулу)? 18. В регрессионной модели X = Zα + ε существует автокорреляции ошибок первого порядка и ρ = 0.6. Предположим, что X = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 48629⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , Z= ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 2 1 5 1 2 1 1 1 10 1⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , - найдите преобразованные наблюдения Dx и Dz; - найдите ОМНК-оценки параметра α; - найдите фактическое значение dc статистики Дарбина-Уотсона по остаткам после применения ОМНК.
8.6. Упражнения и задачи 287 19. Положим, построили регрессию для N = 20 и n = 4 и нашли оценку z = Ni=2 eiei−1 Ni=1 e2i = 0.5, ee = 40, e21= 1, e2N = 4. Найдите фактическое значение dc статистики Дарбина-Уотсона и с ее по-мощью проведите тест на автокорреляцию. 20. На основе годовых данных 1959-1983 годов были оценены следующиефунк-ции спроса на продовольственные товары. lnQt = 2.83 − 0.47 lnPFt + 0.64 ln Yt, (6.69) (−3.94) (24.48) R2 = 0.987, DW= dc = 0.627, lnQt = 1.87 − 0.36 lnPFt + 0.38 ln Yt + 0.44Qt−1, (3.24) (−2.79) (3.20) (24.10) R2 = 0.990, DW= dc = 1.65, где Q -спрос на продукты питания, PF -цены на продукты питания, Y -доход, в скобках приведены значения t-статистики. Проверьте каждое уравнение на наличие автокорреляции первого порядка и дайте короткий комментарий результатов. 21. Пусть остатки в регрессии xi = α + βzi + εi равны (1, 2, 0, −1, −2). Опишите первый шаг метода Кочрена-Оркарта. 22. Денежная масса измеряется с ошибкой. Как смещен коэффициент зависимо-сти динамики цен от динамики денежной массы относительно его истинного значения? 23. Пусть в парной линейной регрессииошибки зависимой переменной ифактора независимы и имеют одинаковую дисперсию. Запишите задачу для нахожде-ния оценок коэффициентов данной регрессии (с объяснением обозначений). Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т.2.-М.:Юнити, 2001. (Гл. 2)
288 Глава 8. Нарушение гипотез основной линейной модели 2. Демиденко Е.З. Линейная и нелинейная регрессия.-М.: "Финансы и ста-тистика", 1981. (Гл. 1). 3. Джонстон Дж. Эконометрические методы.-М.: "Статистика", 1980. (Гл. 7, 8). 4. Доугерти К. Введение в эконометрику.-М.: "Инфра-М", 1997. (Гл. 7). 5. Дрейпер Н., Смит Г. Прикладной регрессионный анализ: В 2-х книгах. Кн.1-М.: "Финансы и статистика", 1986. (Гл. 2, 3). 6. Кейн Э. Экономическая статистика и эконометрия.-М.: "Статистика", 1977. Вып. 2. (Гл. 15). 7. Лизер С. Эконометрические методы и задачи.-М.: "Статистика", 1971. (Гл. 2). 8. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: Дело, 2000. (Гл. 6, 7, 9). 9. Маленво Э. Статистические методы эконометрии. Вып. 1.-М.: "Стати-стика", 1975. (Гл. 10). 10. Тинтер Г. Введение в эконометрию.-М.: "Статистика", 1965. (Гл. 6). 11. Baltagi, Badi H. Econometrics, 2nd edition, Springer, 1999. (Ch. 5). 12. Davidson, Russel, Mackinnon, James. Estimation and Inference in Econometrics, N 9, Oxford University Press, 1993. (Ch. 16). 13. William E., Griffiths R., Carter H., George G. Judge Learning and Practicing econometrics, N 9 John Wiley & Sons, Inc., 1993. (Ch. 9, 15, 16). 14. GreeneW.H. Econometric Analysis, Prentice-Hall, 2000. (Ch. 9, 12, 13). 15. Judge G.G., Hill R.C., GriffithsW.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch 8, 9). 16. Maddala G.S. Introduction to Econometrics, 2nd ed., Prentice Hall, 1992. (Ch. 5, 6, 7).
Глава 9 Целочисленные переменные в регрессии 9.1. Фиктивные переменные С помощью фиктивных или псевдопеременных, принимающих дискретные, обычно целые значения, в регрессию включают качественные факторы. Уточнение обозначений: Z - N × n-матрица наблюдений за "обычными" независимыми факторами; α - n-вектор-столбец параметров регрессии при этих факторах; Z0 = 1N; β0=β. В этих обозначениях уравнение регрессии записывается следующим образом: X = Zα + Z0β0 + ε. Пусть имеется один качественный фактор, принимающий два значения (напри-мер: "мужчина" и "женщина", если речь идет о модели некоторой характеристики отдельных людей, или "годы войны" и "годы мира"-в модели, построенной на временных рядах наблюдений, которые охватывают периоды войны и мира и т.д.). Ставится вопрос о том, влияет ли этот фактор на значение свободного члена ре-грессии.
290 Глава 9. Целочисленные переменные в регрессии / ZG = {zGij } - N×2-матрица наблюдений за качественным фактором (мат-рица фиктивных переменных): zGi1 равен единице, если фактор в i-м наблюдении принимает первое значение, и нулю в противном случае; zGi2 равен единице, если фактор в i-м наблюдении принимает второе значение, и нулю в противном случае. /β = ⎡⎢⎣ β1 β2⎤⎥⎦ -двухкомпонентный вектор-столбец параметров при фиктивных переменных. Исходная форма регрессии с фиктивными переменными: X = Zα + Z0β0 + / ZG / β + ε. Поскольку сумма столбцов матрицы равна Z0, оценка параметоров непосред-ственно по этому уравнению невозможна. Проводится преобразование фиктивных переменных одним из двух способов. а) В исходной форме регрессии исключается один из столбцов матрицы фик-тивных переменных, в данном случае-первый. ¯ ZG -матрица фиктивных переменных без первого столбца; ¯ C = ⎡⎢⎣ 1 1 0 0 −1 1⎤⎥⎦. Тогда эквивалентная исходной запись уравнения имеет вид: X = Zα + Z0, ¯ ZG¯ C ⎡⎢⎣ β0 / β ⎤⎥⎦+ ε, и после умножения матрицы ¯ C справа на вектор параметров получается за-пись уравнения регресии, в которой отсутствует линейная зависимость между факторами-регрессорами: X = Zα + Z0 ¯β0 + ¯ ZG ¯ β + ε, где ¯ β0 = β0 + β1, ¯ β = β2 − β1. После оценки этих параметров можно определить значения исходных пара-метров β0 и / β, предполагая, что сумма параметров при фиктивных переменных
9.1. Фиктивные переменные 291 (в данном случае β1+β2) равна нулю, т.е. влияние качественногофактора приводит к колебаниям вокруг общего уровня свободного члена: β2 = ¯β/2, β1 = −β2, β0 = ¯ β0 + β2. б) Предполагая, что сумма параметров при фиктивных переменных равна ну-лю, в исходной форме регрессии исключается один из этих параметров, в данном случае-первый. β -вектор-стобец параметров при фиктивных переменных без первого эле-мента; C = ⎡⎢⎣ −1 1 ⎤⎥⎦. Эквивалентная исходной запись уравнения принимает форму: X = Zα + Z0β0 + / ZGCβ + ε, и после умножения матрицы C слева на матрицу наблюдений за фиктивными переменными получается запись уравнения регрессии, в которой также отсутствует линейная зависимость между регрессорами: X = Zα + Z0β0 + ZGβ + ε. После оценки параметров этого уравнения недостающая оценка параметра β определяется из условия β1 = −β2. Качественный фактор может принимать больше двух значений. Так, в класси-ческой модели выделения сезонных колебаний он принимает 4 значения, в случае поквартальных наблюдений, и 12 значений, если наблюдения проводились по ме-сяцам. Матрица / ZG в этой модели имеет размерность, соответственно, N ×4 или N × 12. Пусть в общем случае качественный фактор принимает k значений. Тогда: матрица / ZG имеет размерность N × k, вектор-столбец / β -размерность k, матрицы ¯ ZG и ZG - N × (k − 1), вектор-столбцы ¯ β и β - (k − 1); k × (k + 1) матрица ¯ C = ⎡⎢⎣1 1 0 0 −1k−1 Ik−1⎤⎥⎦; k × (k − 1) матрица C = ⎡⎢⎣ −1k−1 Ik−1 ⎤⎥⎦;
292 Глава 9. Целочисленные переменные в регрессии 1k / β = 0, ¯ C ⎡⎢⎣ β0 /β ⎤⎥⎦= ⎡⎢⎣ ¯ β0 ¯ β ⎤⎥⎦, / ZGC = ZG. Можно показать, что⎡⎢⎣ 1 −1k−1 0 Ik−1 − 1k−1⎤⎥⎦ ⎡⎢⎣ β0 β ⎤⎥⎦= ⎡⎢⎣ ¯ β0 ¯ β ⎤⎥⎦, или ⎡⎢⎣ 1 1k−1(Ik−1 − 1k 1k−1) 0 Ik−1 − 1k 1k−1 ⎤⎥⎦ ⎡⎢⎣ ¯ β0 ¯ β ⎤⎥⎦= ⎡⎢⎣ β0 β ⎤⎥⎦, где 1k−1 = 1k−11k−1 - (k−1)×(k−1)-матрица, состоящая из единиц; и далее по-казать, что результаты оценки параметров уравнения с фиктивными переменными при использовании обоих указанных подходов к устранению линейной зависимости факторов-регрессоров одинаковы. В дальнейшем для устранения линейной зависимости столбцов значений фик-тивных переменных используется способ "б". После оценки регрессии можно применить t-критерий для проверки значимо-сти влияния качественного фактора на свободный член уравнения. Если k слишком велико и приближается к N, то на параметры при фиктив-ных переменных накладываются более жесткие ограничения (чем равенство нулю их суммы). Так, например, если наблюдения проведены в последовательные мо-менты времени, и вводится качественный фактор "время", принимающий особое значение в каждый момент времени, то / ZG = IN, и обычно предполагается, что значение параметра в каждый момент времени (при фиктивной переменной каж-дого момента времени) больше, чем в предыдущий момент времени на одну и ту же величину. Тогда роль матрицы C играет N-вектор-столбец T , состоящий из чи-сел натурального ряда, начиная с 1, и / β = TβT, гд е βT -скаляр. Уравнение регрессии с фактором времени имеет вид(эк вивалентная исходной форма уравне-ния при использовании способа "б" исключения линейной зависимости фиктивных переменных): X = Zα + Z0β0 + TβT + ε. Методфик тивных переменных можно использовать для проверки влияния ка-чественного фактора на коэффициент регрессии при любом обычном факторе. Ис-ходная форма уравнения, в которое вводится качественный фактор для параметра α, имеет следующий вид:X = Zα + Z0β0 + Zj ¯⊗ / ZG/ αj + ε,
9.1. Фиктивные переменные 293 где Zj - j-й столбец матрицы Z; / αj - k-вектор-столбец параметров влияния качественного фактора на αj; в векторе α j-я компонента теперь обозначает-ся α0j-средний уровень параметра αj ; ¯⊗ -операция прямого произведения столбцов матриц. Прямое произведение матриц A ⊗ B (произведение Кронекера, см. Приложе-ние A.1.2), имеющих размерность, соответственно, mA × nA и mB × nB, есть матрица размерности (mAmB) × (nAnB) следующей структуры: ⎡⎢⎢⎢⎢⎣ a11B - - - a1nAB .... . . ... amA1B - - - amAnAB⎤⎥⎥⎥⎥⎦ . Прямое произведение матриц обладает следующими свойствами: (A1 ⊗- - -⊗Am)(B1 ⊗- - -⊗B2) = (A1B1)⊗- - -⊗(AmBm), если, конечно, соответствующие матричные произведения имеют смысл: (A1 ⊗- - -⊗Am)= A1 ⊗- - -⊗Am, (A1 ⊗- - -⊗Am)−1 = A−1 1 ⊗- - -⊗A−1 m , если все матрицы A квадратны и неособенны. Прямое произведение столбцов матриц применимо к матрицам, имеющим одинако-вое число строк, и осуществляется путем проведения операции прямого произведе-ния последовательно с векторами-строками матриц: A¯⊗B = ⎡⎢⎢⎢⎢⎣ A1... Am⎤⎥⎥⎥⎥⎦ ¯⊗⎡⎢⎢⎢⎢⎣ B1 ... Bm⎤⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎣ A1 ⊗ B1 ... Am ⊗ Bm⎤⎥⎥⎥⎥⎦ . Эта операция обладает следующим важным свойством: (A1 ¯⊗- - - ¯⊗Am)(B1 ⊗- - -⊗B2) = (A1B1) ¯⊗- - - ¯⊗(AmBm). Приоритет прямого произведения матриц выше, чем обычного матричного произве-дения. При использовании способа "а" эквивалентная исходной форма уравнения имеет вид(форма "а"): X = Z−jα−j + Z0β0 + Zj ¯⊗Z0, ¯ ZG¯ C ⎡⎢⎣ α0j ˜αj⎤⎥⎦+ ε,
294 Глава 9. Целочисленные переменные в регрессии где Z−j -матрица Z без j-го столбца, α−j -вектор α без j-го элемента, и после устранения линейной зависимости фиктивных переменных: X = Zα + Z0β0 + Zj ¯⊗ / ZGCαj + ε. Все приведенные выше структуры матриц и соотношения между матрицами и векторами сохраняются. В уравнение регрессии можно включать более одного качественного фактора. В случае двух факторов, принимающих, соответственно, k1 и k2 значения, форма "б" уравнения записывается следующим образом: X = Zα + Z0β0 + Z1β1 + Z2β2 + ε, где вместо "G" в качестве индекса качественного фактора используется его номер. Это уравнение может включать фиктивные переменные совместного влияния качественныхфакторов (взаимодействияфакторов).Висходной форме компонента совместного влияния записывается следующим образом: / Z1 ¯⊗ / Z2 /β12, где / β12 = (β12 11 , . . . , β12 1k2, β12 21, . . . , β12 2k2, . . . , β12 k11, . . . , β12 k1k2)- k1×k2-вектор-столбец, а β12 i1i2 -параметр при фиктивной переменной, которая равна 1, если первый фактор принимает i1-е значение, а второй фактор- i2-е значение, и равна 0 в остальных случаях (вектор-столбцом наблюдений за этой переменной является (k1(i1 − 1) + i2)-й столбец матрицы / Z1 ¯⊗ / Z2). Как и прежде, вектор параметров, из которого исключены все компоненты, линейно выражаемые через остальные, обозначается β12. Он имеет размерность (k1 − 1) × (k2 − 1) и связан с исходным вектором параметров таким образом: /β12 = C1 ⊗ C2β12, где C1 и C2 -матрицы размерности k1 × (k1 − 1) и k2 × (k2 − 1), имеющие описанную выше структуру (матрица C). Теперь компоненту совместного влияния можно записать следующим образом: ( / Z1 ¯⊗ / Z2)(C1 ⊗ C2)β12 = (/ Z1C1) ¯⊗( / Z2C2)β12 = Z1 ¯⊗Z2β12 = Z12β12, а уравнение, включающее эту компоненту (форма "б")- X = Zα + Z0β0 + Z1β1 + Z2β2 + Z12β12 + ε. В общем случае имеется n качественных факторов, j-й фактор принимает kj значений, см. пункт 1.9.Пусть упорядоченное множество {1, . . . , n} обозначается
9.2. Модели с биномиальной зависимой переменной 295 G, а J -его подмножества.Общее их количество, включая пустое подмножество, равно 2n. Каждому такому подмножеству взаимно-однозначно соответствует чис-ло, например, в системе исчисления с основанием max j kj , и их можно упорядочить по возрастанию этих чисел. Если пустое подмножество обозначить 0, то можно записать:J = 0, 1, . . . , n, {1, 2}, . . . , {1, n}, {2, 3}, . . . , {1, 2, 3}, . . . , G. Тогда уравнение регрессии записывается следующим образом: X = Zα + G J=0 / ZJ / βJ + ε = Zα + G J=0 / ZJCJβJ + ε = Zα + G J=0ZJβJ + ε, где / ZJ = j∈J ¯⊗ ˜ Zj, CJ = j∈J ⊗Cj при j > 0, C0 = 1. Выражение j ∈ J подз на-ком произведения означает, что j принимает значения последовательно с первого по последний элемент подмножества J. Очевидно, что приведенная выше запись уравнения для n∗ = 2 является част-ным случаем данной записи. Если p(J) -количество элементов вподмножестве J, то / ZJ /βJ или ZJβJ - J-е эффекты, эффекты p(J)-го порядка; при p(J) = 1 -главные эффекты, при p(J) > 1 -эффекты взаимодействия, эффекты совместного влияния или совместные эффекты. /βJ или β -параметры соответствующих J-х эффектов или также сами эти эффекты. 9.2. Модели с биномиальной зависимой переменной Рассмотрим теперь модели, в которых зависимая переменная принимает толь-ко два значения, т.е. является фиктивной переменной. При этом придется отойти от модели линейной регрессии, о которой речь шла выше. Если изучается спрос на рынке некоторого товара длительного пользования, например, на рынке холодильников определенной марки, то спрос в целом воз-можно предсказывать с помощью стандартной регрессии. Однако, если изучать спрос на холодильники отдельной семьи, то изучаемая переменная должна быть либо дискретной (0 или 1), либо качественной (не покупать холодильник, купить холодильник марки A, купить холодильник марки B и т.д.). Аналогично, разные методы приходится применять при изучении рынка труда и при изучении решения
296 Глава 9. Целочисленные переменные в регрессии отдельного человека по поводу занятости (работать/не работать). Данные о том, произошло какое-либо событие или нет, также можно представить дискретной переменной вида 0 или 1. При этом не обязательно наличие ситуации выбора. Например, можно исследовать данные об экономических кризисах, банкротствах (произошел или не произошел кризис или банкротство). 9.2.1. Линейная модель вероятности, логит и пробит В биномиальную модель входит изучаемая переменная x, принимающая два значения, а также объясняющие переменные z, которые содержат факторы, опре-деляющие выбор одного из значений. Без потери общности будем предполагать, что x принимает значения 0 и 1. Предположим, что мы оценили на основе имеющихся наблюдений линейную регрессию x = zα + ε. Очевидно, что для почти всех значений z построенная линейная регрессия будет предсказывать абсурдные значения изучаемой переменной x - дробные, отрицательные и большие единицы, что делает ее не очень полезной на практике. Более того, линейная модель не может быть вполне корректной с формальной точки зрения. Поскольку у биномиальной зависимой переменной распределение будет распределением Бернулли (биномиальным распределением с одним испы-танием Бернулли), то оно полностью задается вероятностью получения единицы. В свою очередь, вероятность того, что x = 1, совпадает с математическим ожида-нием x, если эта переменная принимает значения 0 и 1: E(x) = Pr(x = 1) - 1 + Pr(x = 0) -1 = Pr(x = 1). С другой стороны, ожидание x при данной величине z для линейной модели равно E(x) = zα + E(ε) = zα. Отсюда следует, что обычная линейная регрессионная модель не совсем под-ходит для описания рассматриваемой ситуации, поскольку величина za, вообще говоря, не ограничена, в то время как вероятность всегда ограничена нулем и еди-ницей. Ожидаемое значение зависимой переменной, E(x), может описываться только нелинейной функцией. Желательно каким-то образом модифицировать модель, чтобы она, с одной стороны, принимала во внимание тот факт, что вероятность не может выходить
9.2. Модели с биномиальной зависимой переменной 297 за пределы отрезка [0; 1], и, с другой стороны, была почти такой же простой как линейная регрессия. Этим требованиям удовлетворяет модель, для которой Pr(x = 1) = F(zα), где F(-) -некоторая достаточно простая функция, преобразующая zα в число от нуля до единицы. Естественно выбрать в качестве F(-) какую-либо дифферен-цируемуюфункцию распределения, определенную на всей действительной прямой. В дальнейшем мы рассмотрим несколько удобных функций распределения, кото-рые удовлетворяют этим требованиям. Заметим, что если выбрать F(-), соответствующую равномерному распреде-лению на отрезке [0; 1], то окажется, что E(x) = Pr(x = 1) =⎧⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎩ 0, zα0, zα, 0 zα 1, 1, zα1. Таким образом, при zα ∈ [0; 1] получим "линейную регрессию". Это так назы-ваемая линейная модель вероятности. Однако, вообще говоря, такой выбор F(-) скорее не упрощает оценивание, а усложняет, поскольку в целом математическое ожидание зависимой переменной является здесь нелинейной функцией неизвест-ных параметров α (т.е. это нелинейная регрессия), причем эта функция недиффе-ренцируема. В то же время, если данные таковы, что можно быть уверенным, что величина zα далека от границ 0 и 1, то линейную модель вероятности можно использо-вать, оценивая ее как обычную линейную регрессию. То, что величина zα далека от границ 0 и 1, означает, что z плохо предсказывает x. Таким образом, линей-ная модель вероятности применима в случае, когда изучаемая зависимость слаба, и в имеющихся данных доля как нулей, так и единиц не слишком мала. Ее можно рассматривать как приближение для нелинейных моделей. Есть два удобных вида распределения, которые обычно используют для моде-лирования вероятности получения единицы в модели с биномиальной зависимой переменной. Оба распределения симметричны относительно нуля. 1) Логистическое распределение. Плотность логистического распределения равна λ(y) = ey (1 + ey)2 ,
298 Глава 9. Целочисленные переменные в регрессии а функция распределения равна Λ(y) = ey 1 + ey = 1 1 + e−y . Модель с биномиальной зависимой переменной с логистически распределен-ным отклонением называют логит. Для логита E(x) = Pr(x = 1) = Λ(zα) = ezα 1 +ezα = 1 1 + e−zα . 2) Нормальное распределение (см. Приложение A.3.2). Модель с нормально распределенным отклонением ε называют пробит. При этом используется стандартное нормальное распределение, т.е. нормальное рас-пределение с нулевым ожиданием и единичной дисперсией, N(0, 1). Для пробита E(x) = Pr(x = 1) = Φ(zα) = zα −∞ ϕ(t)dt = √12π zα −∞ e−t2/2dt, где Φ(-) -функция распределения стандартного нормального распределения, ϕ(-) -его плотность. Логистическое распределение похоже на нормальное с нулевым ожидани-ем и дисперсией π2/3 (дисперсия логистического распределения). В связи с этим оценки коэффициентов в моделях различаются примерно на множитель π/√3 ≈ 1.8. Если вероятности далеки от границ 0 и 1 (около 0,5), то более точ-ной оценкой множителя является величина ϕ(0)/λ(0) = :8/π ≈ 1.6. При малом количестве наблюдений из-за схожести распределений сложно решить, когда сле-дует применять логит, а когда-пробит. Различие наиболее сильно проявляется при вероятностях, близких к 0 и 1, поскольку логистическое распределение име-ет более длинные хвосты, чем нормальное (оно характеризуется положительным коэффициентом эксцесса). Можно использовать в модели и другие распределения, например, асиммет-ричные. 9.2.2. Оценивание моделей с биномиальной зависимой переменной Требуется по N наблюдениям (xi, zi), i = 1, . . . , N, получить оценки коэффи-циентов α. Здесь наблюдения xi независимы и имеют биномиальное распределе-ние с одним испытанием (т.е. распределение Бернулли) и вероятностью Pr(xi = 1) = F(ziα).
9.2. Модели с биномиальной зависимой переменной 299 -4 -3.1 -2.2 -1.3 -0.4 0.5 1.4 2.3 3.2 4.1 5 Логистическое распределение Распределение экстремального значения Нормальное распределение Рис. 9.1 Можно рассматривать модель с биномиальной зависимой переменной как мо-дель регрессии: xi = F(ziα) + ξi, где ошибки ξi = xi − F(ziα) имеют нулевое математическое ожидание и незави-симы. Каждая из ошибок ξi может принимать только два значения, и поэтому их распределение мало похоже на нормальное. Кроме того, имеет место гетероскеда-стичность. Обозначим pi = pi(α) = F(ziα). В этих обозначениях дисперсия ошибки ξi равна var(ξi) = E(xi − pi)2= E(x2i) − 2piE(xi) + p2i= pi(1 − pi). При выводе этой формулы мы воспользовались тем, что x2i= xi и E(xi) = pi. Несмотря на эти нарушения стандартных предположений, данную модель, ко-торая в общем случае представляет собой модель нелинейной регрессии, можно оценить нелинейным методом наименьших квадратов, минимизируя по α следую-щую сумму квадратов: Ni=1 (xi − pi(α))2. Для минимизации такой суммы квадратов требуется использовать какой-либо алгоритм нелинейной оптимизации. Этот методд ает состоятельные оценки ко-эффициентов α. Гетероскедастичность приводит к двум важным последствиям. Во-первых, оценки параметров будут неэффективными (не самыми точными). Во-вторых, чтоболее серьезно, ковариационнаяматрица коэффициентов, стандартные
300 Глава 9. Целочисленные переменные в регрессии ошибки коэффициентов и t-статистики будут вычисляться некорректно (если ис-пользовать стандартные процедуры оценивания нелинейной регрессии и получения в ней оценки ковариационной матрицы оценок параметров). В частном случае модели линейной вероятности имеем линейную регрессию с гетероскедастичными ошибками:xi = ziα + ξi. Для такой модели можно предложить следующую процедуру, делающую по-правку на гетероскедастичность: 1) Оцениваем модель обычнымМНК и получаем оценки a. 2) Находим оценки вероятностей:pi = zia. 3) Используем взвешенную регрессию и получаем оценки a∗. Чтобы оценить взвешенную регрессию, следует разделить каждое наблюде-ние исходной модели на корень из оценки дисперсии ошибки, т.е. на величину 'pi(1 − pi) = 'zia(1 − zia): xi :pi(1 − pi) = zi :pi(1 − pi)α + ξi :pi(1 − pi) , и далее применить к этой преобразованной регрессии обычный методнаименьш их квадратов. При использовании данного метода получим асимптотически эффек-тивные оценки a∗ и корректную ковариационную матрицу этих оценок, на основе которой можно рассчитать t -статистики. Те же идеи дают метод оценивания модели с произвольной гладкой функцией F(-). Для этого можно использовать линеаризацию в точке 0: F(ziα) ≈ F(0) + f(0)ziα, где f(-) -производная функции F(-) (плотность распределения). Тогда получим следующую приближенную модель: xi ≈ F(0) + f(0) ziα + ξi или xi ≈ ziα + ξi, где xi = xi − F(0) f(0) и ξ= ξi f(0) ,
9.2. Модели с биномиальной зависимой переменной 301 которую можно оценить с помощью только что описанной процедуры. Для симмет-ричных относительно нуля распределений F(0) = 0, 5. В случае логита, учитывая λ(0) = 14, получаем xi = 4xi − 2, а в случае пробита, учитывая φ(0) = 1√2π , получаем xi = √2π(xi − 0, 5). Таким образом, можно получить приближенные оценки для коэффициентов пробита и логита, используя в качестве зависимой переменной регрессии вместо переменной, принимающей значения 0 и 1, переменную, которая принимает зна-чения ±2 для логита и ±'π2 для пробита ('π2 ≈ 1, 25). Ясно, что это хорошее приближение только когда величины ziα близки к нулю, то есть когда модель плохо описывает данные. Приближенные оценки можно получить также по группированным наблюдени-ям. Предположим, что все наблюдения разбиты на несколько непересекающихся подгрупп, в пределах каждой из которых значения факторов zi примерно одинако-вы. Введем обозначения: ¯pj = 1 Nji∈Ij xi и ¯zj = 1 Nji∈Ij zi, где Ij -множество наблюдений, принадлежащих j-й группе, Nj -количество наблюдений в j-й группе. Величина ¯pj является оценкой вероятности получения единицы в случае, когда факторы принимают значение ¯zj, т.е. ¯pj ≈ F(¯zjα), откуда F−1(¯pj) ≈ ¯zjα. Получаем модель регрессии, в которой в качестве зависимой переменной вы-ступает F−1(¯pj), а в качестве факторов- ¯zj . В частном случае логистического распределения имеем: Λ−1(¯pj) = ln¯pj 1 − ¯pj ,
302 Глава 9. Целочисленные переменные в регрессии т.е. для логита зависимая переменная представляет собой логарифм так называе-мого "соотношения шансов". Чтобы такое приближение было хорошим, следует правильно сгруппировать наблюдения. При этом предъявляются два, вообще говоря, противоречивых тре-бования: - в пределах каждой группы значения факторов должны быть примерно одина-ковы (идеальный случай-когда в пределах групп zi совпадает, что вполне может случиться при анализе экспериментальных данных), - в каждой группе должно быть достаточно много наблюдений. Описанный метод лучше всего подходит тогда, когда в модели имеется один объясняющий фактор (и константа), поскольку в этом случае проще группировать наблюдения. В настоящее время в связи с развитием компьютерной техники для оценивания моделей с биномиальной зависимой переменной, как правило, используется метод максимального правдоподобия, рассмотрение которого выходит за рамки данной главы. 9.2.3. Интерпретация результатов оценивания моделей с биномиальной зависимой переменной Предположим, что каким-либо методом получен вектор оценок a. Как в этом случае можно интерпретировать результаты и судить о качестве модели? Для логита коэффициенты a описывают влияние факторов на логарифм соот-ношенияшансов. В общем случае по знаку коэффициентов можно судить о направ-лении зависимости, а по соответствующим t-статистикам-о наличии или отсут-ствии зависимости. Однако интерпретировать коэффициенты в содержательных терминах затруднительно. Поэтому помимо коэффициентов полезно рассмотреть, как влияют факторы на вероятность получения единицы: ∂F(za) ∂zj = f(za)aj . Эти величины называют маргинальными значениями.Ясно, чтомаргинальные значения зависят от точки z, в которой они рассматриваются. Обычно берут z на среднем уровне по имеющимся наблюдениям: z = ¯z. Другой распространенный подход состоит в том, чтобы вычислить маргинальные значения во всех точках zi, i = 1, . . . , N, и по ним вычислить средние маргинальные значения: 1 N Ni=1 f(zia)aj .
9.2. Модели с биномиальной зависимой переменной 303 Таблица 9.1 Предсказано 0 1 Сумма На самом 0 × × × деле 1 × × × Сумма × × × Величину xci= zia можно назвать по аналогии с линейной регрессией расчет-ными значениями. При za > 0 для логита и пробита предсказанная вероятность единицы, F(za), превосходит 12, поэтому для такого наблюдения более вероятно наблюдать 1, чем 0. Таким образом, уравнение za = 0 задает ту гиперплоскость, которой разделяются две группы точек-те точки, для которых предсказано x = 0, и те точки, для которых предсказано x = 1. Поэтому наглядно о качестве модели можно судить по диаграмме xi по xci: чем лучше разделены две группы точек, тем более качественна модель. О качестве модели можно судить также по гра-фику оценки E(x) по xc. Этот график в случае "хорошей" модели должен быть "крутым" в нуле. На этих двух графиках (рис. 9.2) слева внизу и справа вверху расположены правильно предсказанные точки, а слева вверху и справа внизу-неправильно. То же самое можно представить таблицей 9.1. Понятно, что "хорошая" модель должна давать высокий процент правильных предсказаний (в таблице они лежат на диагонали). 10 1 0 Хорошее качество модели Плохое качество модели Рис. 9.2
304 Глава 9. Целочисленные переменные в регрессии 9.3. Упражнения и задачи Упражнение 1 1.1. Пусть / ZG = {zGi1zGi2} -фиктивная переменная, где zGi1 равно единице, если фактор в i -м наблюдении относится к годам войны (1941, . . . , 1945), и нулю в противном случае. Как выглядит вектор zGi2? Оцените двумя способами модель X = Zα + Z0β0 + ˜ ZG ˜ β + ε с помощью искусственно созданных данных из табл. 9.2, рассмотрев в качестве X столбец X1: а) исключив столбец zG1 в исходной форме регрессии; б) исключив в исходной форме регрессии параметр при переменной zG1 . Убедитесь, что значения коэффициентов исходной регрессии по способам а) и б) совпадают. 1.2. Запишите модель регрессии, в которой качественный фактор влияет не толь-ко на значение свободного члена регрессии, но и на коэффициент регрессии при факторе Z1. Посчитайте матрицы Z1 ¯⊗ ˜ ZG и Z1 ¯⊗[Z0, ¯ ZG] . Оцените данную модель ре-грессии на данных таблицы 9.2, рассмотрев в качестве X столбец X2 спо-собами а) и б). Упражнение 2 Самостоятельно подберите ряды наблюдений и охарактеризуйте цены на рос-сийском вторичном рынкежилья в зависимости от жилой и нежилой площади, пло-щади кухни, местоположения квартиры по районам города, расположения на эта-жах, количество комнат, наличия телефона, балкона, лифта и т.д. Упражнение 3 В таблице 9.3 приводится данные о голосовании по поводу увеличения налогов на содержание школ в городе Троя штата Мичиган в 1973 г. Наблюдения отно-сятся к 95 индивидуумам: результаты голосования и различные характеристики индивидов. Pub= 1, если хотя бы один ребенок посещает государственнуюшколу, иначе 0, Priv = 1, если хотя бы один ребенок посещает частную школу, иначе 0, Years = срок проживания в данном районе, Teach = 1, если работает учителем, иначе 0,
9.3. Упражнения и задачи 305 Таблица 9.2 Годы X1 X2 Z1 Z2 Годы X1 X2 Z1 Z2 1935 2.81 2.81 117.10 9.70 1945 24.95 19.93 200.70 32.00 1936 10.66 10.66 201.60 10.40 1946 16.44 16.44 220.80 34.60 1937 4.16 4.16 280.30 11.80 1947 15.04 15.04 165.60 45.60 1938 8.30 8.30 204.00 15.60 1948 15.44 15.44 160.40 54.30 1939 16.94 16.94 225.60 17.20 1949 23.43 23.43 61.80 55.50 1940 5.01 5.01 213.20 18.60 1950 6.98 6.98 161.10 64.70 1941 35.49 30.90 183.40 22.10 1951 18.61 18.61 181.90 67.10 1942 26.76 22.79 158.80 28.80 1952 22.74 22.74 207.90 72.60 1943 34.88 30.50 174.90 32.00 1953 24.63 24.63 237.10 80.00 1944 35.27 31.06 168.70 32.10 1954 31.35 31.35 275.90 88.90 LnInc = логарифм годового дохода семьи в долларах, PropTax = логарифм налогов на имущество в долларах за год (заменяет плату за обучение-плата зависит от имущественного положения), Yes = 1, если человек проголосовал на референдуме "за", 0, если "против". Зависимая переменная-Yes. В модель включаются все перечисленные фак-торы, а также квадрат Years. 3.1. Получите приближенные оценки для логита и пробита с помощью линейной регрессии 3.2. Вычислите коэффициенты логита через коэффициенты пробита и сравните. 3.3. Для логита найдите маргинальные значения для Teach, LnInc и PropTax при среднем уровне факторов. 3.4. Постройте график вероятности голосования "за" в зависимости от Years при среднем уровне остальных факторов. 3.5. Постройте аналогичный график маргинального значения Years.
306 Глава 9. Целочисленные переменные в регрессии Таблица 9.3. (Источник: R. Pindyck andD. Rubinfeld, EconometricModels and Economic Forecasts, 1998, Fourth Edition, Table 11.8, p. 332) Номер Pub Priv Years Teach LnInc PropTax Yes 1 1 0 10 1 9.77 7.0475 1 2 1 0 8 0 10.021 7.0475 0 3 1 0 4 0 10.021 7.0475 0 4 1 0 13 0 9.4335 6.3969 0 5 1 0 3 1 10.021 7.2792 1 6 1 0 5 0 10.463 7.0475 0 7 0 0 4 0 10.021 7.0475 0 8 1 0 5 0 10.021 7.2793 1 9 1 0 10 0 10.222 7.0475 0 10 1 0 5 0 9.4335 7.0475 1 11 1 0 3 0 10.021 7.0475 1 12 1 0 30 0 9.77 6.3969 0 13 1 0 1 0 9.77 6.7452 1 14 1 0 3 0 10.021 7.0475 1 15 1 0 3 0 10.82 6.7452 1 16 1 0 42 0 9.77 6.7452 1 17 1 0 5 1 10.222 7.0475 1 18 1 0 10 0 10.021 7.0475 0 19 1 0 4 0 10.222 7.0475 1 20 1 1 4 0 10.222 6.7452 1 21 1 0 11 1 10.463 7.0475 1 22 0 0 5 0 10.222 7.0475 1 23 1 0 35 0 9.77 6.7452 1 24 1 0 3 0 10.463 7.2793 1 25 1 0 16 0 10.021 6.7452 1 26 0 1 7 0 10.463 7.0475 0 27 1 0 5 1 9.77 6.7452 1 28 1 0 11 0 9.77 7.0475 0 29 1 0 3 0 9.77 6.7452 0 30 1 1 2 0 10.222 7.0475 1 31 1 0 2 0 10.021 6.7452 1 32 1 0 2 0 9.4335 6.7452 0 33 1 0 2 1 8.294 7.0475 0 34 0 1 4 0 10.463 7.0475 1 
9.3. Упражнения и задачи 307 Таблица 9.3. (продолжение) Номер Pub Priv Years Teach LnInc PropTax Yes 35 1 0 2 0 10.021 7.0475 1 36 1 0 3 0 10.222 7.2793 0 37 1 0 3 0 10.222 7.0475 1 38 1 0 2 0 10.222 7.4955 1 39 1 0 10 0 10.021 7.0475 0 40 1 0 2 0 10.222 7.0475 1 41 1 0 2 0 10.021 7.0475 0 42 1 0 3 0 10.82 7.4955 0 43 1 0 3 0 10.021 7.0475 1 44 1 0 3 0 10.021 7.0475 1 45 1 0 6 0 10.021 6.7452 1 46 1 0 2 0 10.021 7.0475 1 47 1 0 26 0 9.77 6.7452 0 48 0 1 18 0 10.222 7.4955 0 49 0 0 4 0 9.77 6.7452 0 50 0 0 6 0 10.021 7.0475 0 51 0 0 12 0 10.021 6.7452 1 52 1 0 49 0 9.4335 6.7452 1 53 1 0 6 0 10.463 7.2793 1 54 0 1 18 0 9.77 7.0475 0 55 1 0 5 0 10.021 7.0475 1 56 1 0 6 0 9.77 5.9915 1 57 1 0 20 0 9.4335 7.0475 0 58 1 0 1 1 9.77 6.3969 1 59 1 0 3 0 10.021 6.7452 1 60 1 0 5 0 10.463 7.0475 0 61 1 0 2 0 10.021 7.0475 1 62 1 1 5 0 10.82 7.2793 0 63 1 0 18 0 9.4335 6.7452 0 64 1 0 20 1 9.77 5.9915 1 65 0 0 14 0 8.9227 6.3969 0 66 1 0 3 0 9.4335 7.4955 0 67 1 0 17 0 9.4335 6.7452 0 68 1 0 20 0 10.021 7.0475 0 
308 Глава 9. Целочисленные переменные в регрессии Таблица 9.3. (продолжение) Номер Pub Priv Years Teach LnInc PropTax Yes 69 1 1 3 0 10.021 7.0475 1 70 1 0 2 0 10.021 7.0475 1 71 0 0 5 0 10.222 7.0475 1 72 1 0 35 0 9.77 7.0475 1 73 1 0 10 0 10.021 7.2793 0 74 1 0 8 0 9.77 7.0475 1 75 1 0 12 0 9.77 7.0475 0 76 1 0 7 0 10.222 6.7452 1 77 1 0 3 0 10.463 6.7452 1 78 1 0 25 0 10.222 6.7452 0 79 1 0 5 1 9.77 6.7452 1 80 1 0 4 0 10.222 7.0475 1 81 1 0 2 0 10.021 7.2793 1 82 1 0 5 0 10.463 6.7452 1 83 1 0 3 0 9.77 7.0475 0 84 1 0 2 0 10.82 7.4955 1 85 0 1 6 0 8.9227 5.9915 0 86 1 1 3 0 9.77 7.0475 1 87 1 0 12 0 9.4335 6.3969 1 88 0 0 3 0 9.77 6.7452 1 89 1 0 3 0 10.021 7.0475 1 90 0 0 3 0 10.021 6.7452 1 91 1 0 3 0 10.222 7.2793 1 92 1 0 3 1 10.021 7.0475 1 93 1 0 5 0 10.021 7.0475 1 94 0 0 35 1 8.9277 5.9915 1 95 1 0 3 0 10.463 7.4955 0 
9.3. Упражнения и задачи 309 Задачи 1. Какие из перечисленных факторов учитываются в регрессии с помощью фик-тивных переменных: а) профессия; б) курс доллара; в) численность населе-ния; г) размер среднемесячных потребительских расходов? 2. В уравнение регрессии для доходов населения вводятся два качественных фактора: "пол" и "наличие судимости". Сколько фиктивных переменных (с учетом взаимодействия факторов) в исходной и преобразованной (после устранения линейных зависимостей) форме уравнения? 3. В уравнение регрессии для доходов населения вводятся три качественных фактора: пол ("муж.", "жен."), образование ("нач.", "сред.", "высш.") и место проживания ("гор.", "сел."). Сколько фиктивных переменных (с учетом всех взаимодействий факторов) в исходной и преобразованной (после устранения линейных зависимостей) форме уравнения? Как выглядят матри-цы преобразований ¯ C и C? 4. Известно, что котировки многих ценных бумаг зависят от того, в какой день рабочей недели (понедельник, вторник, среда, . . . ) проходят торги.Как учесть эту зависимость при построении регрессионной модели котировок? 5. Предположим, что оценивается зависимость спроса на лыжи от располагае-мого личного дохода, используя наблюдения по месяцам. Как ввести фик-тивную переменную для оценивания сезонных колебаний? Запишите со-ответствующие матрицы преобразований ¯ C и C для каждого фиктивного фактора. 6. Рассмотрим регрессионнуюмодель xt = α1zt1+α2zt2+β0+εt, t = 1, . . . , T. Пусть для наблюдений t = 1 и 2 параметры α1, α2 и β0 отличаются от остальных ( T − 2) наблюдений. Запишите регрессионную модель с фик-тивными переменными и опишите возникшие проблемы оценивания. 7. На основе данных о расходах на автомобили (X) и располагаемом личном доходе (Z) за период с 1963 по 1982 года получена модель: X = 0.77 + +0.035Z −4.7ZG1 , гд е ZF1 -фиктивная переменная, учитывающая нефтя-ной кризис 1974 года, равная 0 для периодов с 1963 по 1973 гг. и равной единице для периода с 1974 по 1982 гг. а) Схематично нарисуйте график регрессионной функции и дайте полную интерпретацию. б) Запишите модель, в которой качественный фактор zG не влияет на сво-бодный член, но влияет на наклон линии регрессии. Схематично нари-суйте график регрессионной функции.
310 Глава 9. Целочисленные переменные в регрессии 8. Как меняется коэффициент детерминации при добавлении в регрессионную модель фиктивной объясняющей переменной? 9. На основе опроса населения США Current Population Survey за 1985 г. изу-чаются факторы, определяющие зарплату: WAGE: зарплата (долларов за час)-изучаемая переменная, EDU: образование (лет), SOUTH: индикаторная переменная для Юга (1 = человек живет на Юге, 0 = человек живет в другом месте), SEX: индикаторная переменная для пола (1 =жен, 0 =муж), EXPER: стаж работы (лет), UNION: индикаторная переменная для членства впрофсоюзе (1=членпроф-союза, 0 = нет), AGE: возраст (лет), RACE: раса (1 = другое, 2 = "Hispanic", 3 = белый), OCCUP: профессиональная категория ( 1 = другое, 2 = Management, 3 = Sales, 4 = Clerical, 5 = Service, 6 = Professional), SECTOR: сектор экономики (0 = другое, 1 = промышленность, 2 = стро-ительство), MARR: семейное положение (0 = неженатый/незамужняя, 1 = женатый/замужняя). а) Какие из перечисленных переменных можно назвать фиктивными? Объясните. б) Объясните, в каком виде следует учитывать переменные RACE,OCCUP и SECTOR в регрессии. в) Для каждого фиктивного фактора запишите соответствующую матрицу преобразований C. г) Объясните, как будут выглядеть фиктивные переменные, соответству-ющие эффектам второго порядка для пола и расы. 10. Модель регрессии с биномиальной зависимой переменной можно предста-вить в виде: (зависимая переменная)=(математическое ожидание)+(ошиб-ка). Какие предположения классической линейной регрессии при этом будут нарушены?
9.3. Упражнения и задачи 311 11. Предположим, что с помощью обычного линейного МНК с биномиальной зависимой переменной были получены оценки a. Как на их основе получить приближенные оценки для модели пробит? 12. Логит-оценивание модели Pr(x = 1) = F(zα) дало результат x∗ = −5.89+ + 0.2z. Чему равна вероятность x = 1 при z = 50? 13. Пробит-оценивание модели Pr(x = 1) = F(zα) дало результат x∗ = = −2.85 + 0.092z. Чему равна вероятность x = 1 при z = 50? 14. Логит-оценивание модели Pr(x = 1) = F(zα) дало результат x∗ = −5.89+ +0.2z. Чему равно увеличение вероятности Pr (x = 1) при увеличении z на единицу, если z = 50? 15. Пробит-оценивание модели Pr(x = 1) = F(zα) дало результат x∗ = = −2.85 + 0.092z. Чему равно увеличение вероятности Pr (x = 1) при уве-личении z на единицу, если z = 50? 16. Логит-модель применили к выборке, в которой x = 1, если производитель-ность труда на предприятии выросла, и x = 0 в противном случае. z1 - доход предприятия в млн. руб. в год, zG1 = 1если предприятие относит-ся к области высоких технологий ( zG1 = 0в противном случае). Получена следующая модель: x = 0.5 + 0.1z1 + 0.4zG1 . Определите оценку вероятно-сти роста производительности труда для высокотехнологичного предприятия с доходом 100 млн. руб. в год и для предприятия, не относящегося к сфере высоких технологий, с доходом 150 млн. руб. в год. 17. Имеется выборка, состоящая из 600 наблюдений, в которой x = 1, если работник состоит в профсоюзе, и x = 0 в противном случае. Предпола-гается, что членство в профсоюзе зависит от образования, лет ( z1), стажа работы, лет ( z2) и пола ( z3). Выборочные средние равны ¯x = 0.2, ¯z1 = 14, ¯z2 = 18 и ¯z3 = 0.45. На основе выборочных данных получена следующая пробит-модель: x = −0.9 − 0.01z1 + 0.4z2 − 0.6z3. Определить, насколько снижается вероятность быть членом профсоюза в расчете на годд ополни-тельного образования. 18. Пусть переменная x, принимающая значения 0 или 1, зависит от одного фактора z. Модель включает также константу. Данные приведены в таблице: x 0 0 1 1 0 1 0 1 0 1 z 1 2 3 4 5 6 7 8 9 10
312 Глава 9. Целочисленные переменные в регрессии а) Получите приближенные оценки логита и пробита методом усреднения, разбив данные на две группы по 5 наблюдений. Каким будет процент правильных предсказаний по модели для этих данных? б) Ответьте на вопросы предыдущего пункта для метода приближенного оценивания логита и пробита с помощью линейной регрессии. в) Найдите маргинальное значение для фактора z в точке, соответствую-щей его среднему уровню. 19. Пусть переменная x, принимающая значения 0 или 1, зависит от фиктив-ной переменной z, принимающей значения 0 или 1.Модель включает также константу. Данные резюмируются следующей таблицей (в клетках стоят ко-личества соответствующих наблюдений): x = 0 x = 1 z = 0 N00 N01 z = 1 N10 N11 а) При каких условиях можно на основе этих данных оценить логит и пробит? б) Получите приближенные оценки логита и пробита методом усреднения. Чему они будут равны при N00 = 15, N01 = 5, N10 = 5, N11 = 15? Ка-ким будет процент правильных предсказаний по модели для этих данных? в) Ответьте на вопросы предыдущего пункта для метода приближенного оценивания логита и пробита с помощью линейной регрессии. Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т.2.-М.: "Юнити", 2001. (Гл. 2) 2. Доугерти К. Введение в эконометрику.-М.: "Инфра-М", 1997. (Гл. 9). 3. Дрейпер Н., Смит Г. Прикладной регрессионный анализ. В 2-х книгах. Кн. 2.-М.: "Финансы и статистика", 1986. (Гл. 9). 4. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 4). 5. Маленво Э. Статистические методы эконометрии.-М.: "Статистика". Вып. 1, 1975. (Гл. 8).
9.3. Упражнения и задачи 313 6. Baltagi, Badi H. Econometrics, 2nd edition, Springer, 1999. (Ch. 13). 7. Davidson, Russel, Mackinnon, James. Estimation and Inference in Econometrics, No. 9, Oxford University Press, 1993. (Ch. 7). 8. GreeneW.H. Econometric Analysis, Prentice-Hall, 2000. (Ch. 8). 9. Judge G.G., Hill R.C., Griffiths W.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch. 10). 10. Maddala G.S. Introduction to Econometrics, 2nd ed., Prentice Hall, 1992. (Ch. 8). 11. Ruud Paul A. An Introduction to Classical Econometric Theory, Oxford University Press, 2000. (Ch. 27). 12. Wooldridge JeffreyM. Introductory Econometrics:AModernApproach, 2nd ed., Thomson, 2003. (Ch. 7, 17).
Глава 10 Оценка параметров систем уравнений Пусть теперь имеется несколько изучаемых переменных, для каждой из которых существует свое уравнение регрессии. В совокупности эти уравнения образуют систему, которая является невзаимозависимой, если одни изучаемые переменные не выступают факторами-регрессорами для других изучаемых переменных. Если изучаемые переменные возникаютне только в левых, но и правых частях уравнений, то такие системы называются одновременными или взаимозависимыми. 10.1. Невзаимозависимые системы В этом пункте используется сокращенная форма записи уравнений регрессии: ˆX= ˆ ZA + ε, (10.1) где ˆX- N × k-матрица центрированных наблюдений за изучаемыми перемен-ными, ˆ Z - N × n-матрица центрированных наблюдений за факторными перемен-ными, A - n × k-матрица параметров уравнений регрессии, ε - N ×n-матрица ошибок изучаемых переменных (остатков по наблюдени-ям).
10.1. Невзаимозависимые системы 315 Относительно ошибок предполагается, что в каждом наблюдении их математи-ческое ожидание равно нулю, матрица ковариации размерности k × k одинакова и равна Ω ( Ω -вещественная, симметричная, положительно определенная мат-рица), и что они не коррелированы по наблюдениям. Оценивать параметры этой системы можно отдельно по каждому уравнению: A = M−1 ˜ m, (10.2) где M = 1 N ˆ Zˆ Z, ˜m = 1 N ˆ ZˆX, или через обычные операторы МНК-оценива-ния (8.1), записанные последовательно для всех уравнений системы al = M−1ml, l= 1, . . . , k. Т.е. факт коррелированности ошибок разных изучаемых переменных ( Ω = Ik) не со-здает дополнительных проблем. Действительно, преобразованием в пространстве изучаемых переменных легко пе-рейти в ситуацию, когда ошибки изучаемых переменных не коррелированы. Пусть матрица C такая, что Ω = C−1C−1 (такое представление допускает любая вещественная симметричная положительно определенная матрица, см. Приложе-ние A.1.2). Умножим обе части (10.1) справа на эту матрицу: XˆC = ZˆAC + εC. (10.3) Новые ошибки изучаемых переменных во всех наблюдениях оказываются не корре-лированными: E(CεiεiC) E(εiεi)=Ω = IN, где εi -вектор-строка ошибок в i-том наблюдении. Теперь уравнения системы не связаны между собой, и их можно оценить обыч-ным МНК по отдельности, что, очевидно, приводит к матричному оператору AC = M−1 ˜mC, который эквивалентен (10.2). Что и требовалось доказать. Ситуация резко усложняется, если для коэффициентов матрицы A имеются априорные ограничения. Пусть, например, эта матрица имеет следующую структуру: ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ a1 0 - - - 0 0 a2 - - - 0 ....... . . ... 0 0 - - - ak⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ,
316 Глава 10. Оценка параметров систем уравнений где al - nl-вектор-столбец коэффициентов в l-м уравнении (для l-й изучаемой переменной), kl=1 nl = n, т.е. многие элементы матрицы A априорно приравнены нулю. Фактически это означает, что для каждой изучаемой переменной имеется свой набор объясняющих факторов с N ×nl-матрицей наблюдений ˆ Zl ˆ Z=7 ˆ Z1- - - ˆ Zk8, и система уравнений (10.1) представляется как совокупность внешне не связанных между собой уравнений: ˆXl = ˆ Zlal + εl, l= 1, . . . , k. (10.4) Сразу можно заметить, что теперь оператор (10.2) применить невозможно, т.к. система нормальных уравнений, решением которой является этот оператор, записывается следующим образом: ⎡⎢⎢⎢⎢⎢⎣ M11a1 - - - M1kak .... . . ... Mk1a1 - - - Mkkak⎤⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎣ m11 - - - m1k .... . . ... mk1 - - - mkk⎤⎥⎥⎥⎥⎥⎦ , (10.5) где Mll= 1 N ˆ Zl ˆ Zl, mll= 1 N ˆ Zl ˆXl, т.е. вектор оценок параметров каждого урав-нения должен удовлетворять k взаимоисключающим, в общем случае, системам уравнений. Правильная оценка параметров регрессии дается решением следующих урав-нений: k l=1 ω−1 llMllal= k l=1 ω−1 llmll, l= 1, . . . , k, где ω−1 ll-элемент матрицы Ω−1. Или в матричной записи: ⎡⎢⎢⎢⎢⎢⎣ ω−1 11 M11a1+ - - - +ω−1 1k M1kak .... . . ... ω−1 k1 Mk1a1+ - - - +ω−1 kkMkkak⎤⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎣ ω−1 11 m11+ - - - +ω−1 1k m1k .... . . ... ω−1 k1 mk1+ - - - +ω−1 kk mkk⎤⎥⎥⎥⎥⎥⎦ , (10.6) которая при сравнении с (10.5) оказывается результатом умножения в (10.5) всех Mllи mllна ω−1 llи сложения столбцов в обеих частях этого выражения.
10.1. Невзаимозависимые системы 317 Для доказательства этого утверждения необходимо перегруппировать уравнения си-стемы так, чтобы ˜X= ⎡⎢⎢⎢⎢⎣ ˆX1 ˆX2 ... ⎤⎥⎥⎥⎥⎦ , ˜ Z = ⎡⎢⎢⎢⎢⎣ ˆ Z1 0 - - - 0 ˆ Z2 . . . .... . . . . .⎤⎥⎥⎥⎥⎦ , ˜a = ⎡⎢⎢⎢⎢⎣ a1 a2... ⎤⎥⎥⎥⎥⎦ , ˜ε = ⎡⎢⎢⎢⎢⎣ ε1 ε2... ⎤⎥⎥⎥⎥⎦ , т.е. если забыть об особой структуре матрицы ˜ Z,формально имеется одна изучаемая переменная, для которой имеется N - k "наблюдений". Теперь система (10.4) записывается следующим образом: ˜X= ˜ Z˜a+ ˜ε, и применение простого МНК приводит к получению обычных оценок уравнений в отдельности: al = M−1 ll mll. Однако такой подход неприемлем, надо применятьОМНК, поскольку остатки корре-лированы по "наблюдениям", ибо в соответствии со сделанными предположениями E(˜ε˜ε) = Ω⊗ IN, где ⊗ -операция прямого умножения матриц (см. Приложения A.1.1 и A.1.2). Из (8.1) следует, что система нормальных уравненийОМНКв данном случае выгля-дит так: ˜ ZΩ−1 ⊗ IN ˜ Z˜a = ˜ ZΩ−1 ⊗ INX˜ . (10.7) Легко убедиться, что˜ ZΩ−1 ⊗ IN = ⎡⎢⎢⎢⎢⎣ ω−1 11 ˆ Z11 ω−1 12 ˆ Z12 ... ω−1 21 ˆ Z21 ω−1 22 ˆ Z22 . . . .... . . . . . ⎤⎥⎥⎥⎥⎦ . Умножение этой матричной конструкции справа на ˜ Z и деление на N дает блочную матрицу {ω−1 llMll}, которая является матрицей системы (10.6), а умножение ее справа на ˜Xи деление на N -вектор lω−1 llmll;, являющийся правой частью системы (10.6). Таким образом, (10.7) эквивалентна (10.6). Что и требовалось доказать. Эта оценка совпадает с обычной МНК-оценкой al = M−1 ll mll, если матрица Ω диагональна, т.е. ошибки изучаемых переменных не коррелированы.
318 Глава 10. Оценка параметров систем уравнений 10.2. Взаимозависимые или одновременные уравнения. Проблема идентификации Далее в этом разделе уравнения регрессии записываются в форме со скрытым свободным членом. X - N × k-матрица наблюдений за изучаемыми переменными x; Z - N × (n + 1)-матрица наблюдений за независимыми факторами z; B - k × k-матрица параметров регрессии при изучаемых переменных; B = Ik, иначе система была бы невзаимозависимой; |B| = 0 и βll = 1 -усло-вия нормализации, т.е. предполагается, что, в конечном счете, в левой части l-го уравнения остается только l-я переменная, а остальные изучаемые переменные переносятся в правую часть; A - (n+1)×k-матрица параметров регрессии (последняя строка-свобод-ные члены в уравнениях); ε - N × k-матрица значений случайных ошибок по наблюдениям; XB = ZA + ε. (10.8) Такая запись одновременных уравнений называется структурной формой. Умножением справа обеих частей этой системы уравнений на B−1 она приводится к форме, описанной в предыдущем пункте. Это-приведенная форма системы: X = ZAB−1 + εB−1. D = AB−1 - (n + 1) × k-матрица параметров регрессии приведенной формы. Как показано в пункте 10.1, для их оценки можно использовать МНК: D = (ZZ)−1ZX. Таким образом, матрица D оценивается без проблем, и ее можно считать известной. Однако задача заключается в оценке параметров B и A системы в приведенной форме. Эти параметры, по определению, удовлетворяют следую-щим условиям: DB − A = 0 (10.9) или WH = 0, гд е W - (n + 1) × (n + k + 1)-матрица D In+1 , H - (n + k + 1) × k-матрица ⎡⎢⎣ B −A ⎤⎥⎦.
10.2 Взаимозависимые или одновременные уравнения 319 Это-условия для оценки параметров структурной формы. В общем случае эти условия достаточно бессмысленны, т.к. они одинаковы для параметров всех уравнений. Они описывают лишь множество допустимых значений параметров (одинаковое для всех уравнений), поскольку для n + k + 1 параметров каждо-го уравнения структурной формы имеется только n + 1 одинаковых уравнений. Необходимы дополнительные условия, специальные для каждого уравнения. Пусть для параметров l-го уравнения кроме требования WHl = 0 ((ZZ)−1ZXBl − Al = 0) (10.10) имеется дополнительно rl условий:RlHl = 0, (10.11) где Rl - rl × (n + k +1)-матрица дополнительных условий, Hl - (n + k + 1)-вектор-столбец ⎡⎢⎣ Bl −Al ⎤⎥⎦параметров l-го уравнения- l-й столбец матрицы H. ⎡⎢⎣ WRl⎤⎥⎦Hl = WlHl = 0 -общие условия для определения структурных пара-метров l-го уравнения, где Wl - (n + rl + 1) × (n + k + 1)-матрица. Они позволяют определить искомые параметры с точностью до постоянного множителя (при выполнении условий нормализации βl = 1 параметры определя-ются однозначно), если и только если ранг матрицы Wl равен n + k. Для этого необходимо, чтобы rl k − 1. (10.12) Однако, это условие не является достаточным. Имеется необходимое и доста-точное условие для определения параметров l-го уравнения (более операциональ-ное, чем требование равенства n + k ранга матрицы Wl): rank(RlH) = k − 1. (10.13) Доказательство данного утверждения опускается по причине сложности. Теперь вводятся определения, связанные с возможностью нахождения пара-метров уравнения структурной формы: l-е уравнение не идентифицировано, если rl < k − 1; оно точно идентифицировано, если rl = k − 1 и ранг Wl равен n + k; сверхидентифицировано, если rl > k − 1. В первом случае параметры не
320 Глава 10. Оценка параметров систем уравнений могут быть оценены, и, хотя формально, например, используя МНК, оценки можно получить, они никакого смысла не имеют; во втором случае параметры уравнения оцениваются однозначно; в третьем-имеется несколько вариантов оценок. Обычно строки матрицы Rl являются ортами, т.е. дополнительные ограни-чения исключают некоторые переменные из структурной формы. Тогда, если kl и nl -количества, соответственно, изучаемых переменных, включая l-ю, и неза-висимых факторов в l-м уравнении, то для его идентификации необходимо, чтобы kl + nl n + 1. (10.14) По определению, rl = n − nl + k − kl (10.12) k − 1 ⇒ nl + kl n + 1. В таком случае условие (10.13) означает, что матрица, составленная из ко-эффициентов во всех прочих уравнениях, кроме l-го, при переменных, которые исключены из l-го уравнения, должна быть не вырождена. При этом l-й столбец матрицы RlH из (10.13), равный нулю, как это следует из (10.11), исключается из рассмотрения. Дляиллюстрации введенныхпонятий используется элементарнаямодельравновесия спроса и предложения на рынке одного товара в предположении, что уравнения спроса и предложения линейны (в логарифмах): s = b21p + c1 + ε1 -предложение, d = −b22p + c2 + ε2 -спрос, где p -цена, b21, b22 -эластичности предложения и спроса по цене, s, d и p -логарифмы предложения, спроса и цены. Наблюдаемой переменной является фактический объем продаж x, и, пред положив, что в действительности рынок находится в равновесии: x = s = d, эту мод ель в структурной форме (10.8) можно записать следующим образом: [x p] ⎡⎢⎣1 1 −b21 b22⎤⎥⎦= [c1 c2 ] + [ ε1 ε2 ]. (10.15) В такой записи условия нормализации не выполнены, т.к. в левой части обоих урав-нений находится одна и таже переменная x; понятно, что принципиального значения эта особенность модели не имеет. Следует напомнить, что одной из главных гипотез применения статистических ме-тодов вообще и МНК в частности является g1: уравнения регрессии представляют истинные зависимости, и речь идет лишь об оценке параметров этих истинных зави-симостей. В данном случае это означает, что на спрос и предложение влияет только
10.2 Взаимозависимые или одновременные уравнения 321 x p s d Рис. 10.1 цена, и линии спроса и предложения в плоскости, абсциссой которой является цена, не меняют своего положения. Поэтому наблюдаемые пары (p, x) сконцентрированы вокруг единственной точки равновесия, облако наблюдений не имеет вытянутостей, и зависимости x от p статистически выявить невозможно (рис. 10.1). Статистически оба уравнения одинаковы, и нет оснований считать коэффициент регрессии, например, x по p, эластичностью спроса или предложения по цене. Более того, в данном случае эта регрессия будет не значима. Эти уравнения не идентифицированы. Действительно, k = 2, n = 0, r1 = r2 = 0, и необход имое условие идентификации (10.12) для обоих уравнений не выполнено. Пусть речь идет о товаре, имеющем сельскохозяйственное происхождение. Тогда его предложение зависит от погодных условий, и в модель следует ввести переменную z1 -некий индекс погоды в течение сельскохозяйственного цикла. В правую часть соотношения (10.15) вводится дополнительное слагаемое: z1 [ a11 0 ] . (10.16) Если модель (10.15, 10.16) истинна (гипотеза g3), то подвижной становится линия предложения (погодные условия в разные сельскохозяйственные циклы различны), и облако фактических наблюдений вытягивается вдоль линии спроса. Регрессия x на p дает оценку эластичности спроса по цене (рис. 10.2).Вэтой ситуации уравнение предложения по-прежнему не идентифицировано, но для уравнения спроса условия идентификации (10.12) выполнены, и это уравнение идентифицировано. x p s1 s2 s3 s4 s5 d Рис. 10.2
322 Глава 10. Оценка параметров систем уравнений x p s d1 d2 d3d4d5d6 Рис. 10.3 Действительно: k = 2, n = 1, r1 = 0, r2 = 1 и r1 < k − 1, r2 = k − 1. Более убедительно этот результат можно получить, используя необходимые и достаточные условия идентификации (10.13). Матрица H в этих условиях имеет следующий вид: H = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 1 −b21 b22 −a11 0 c1 c2 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Матрица R1 -пустая ( rl = 0), и условия (10.13) для первого уравнения не выпол-няются. Для второго уравнения R2=[0 0 1 0], и матрица R2H равна [−a11 0 ], т.е. ее ранг равен единице, и условие (10.13) выполнено. А матрица, составлен-ная из коэффициентов во всех прочих уравнениях, кроме второго, при переменных, которые исключены из второго уравнения, есть [−a11], т.е. она не вырождена. Теперь рассматривается другая возможность: изучаемый товар входит в потреби-тельскую корзину, и спрос на него зависит от доходов домашних хозяйств. В модель вводится переменная z2 доходов домашних хозяйств, т.е. в правую часть соотноше-ний (10.15) добавляется слагаемое z2 [ 0 a22 ] . (10.17) Если истинна модель (10.15, 10.17), то подвижной окажется линия спроса (раз-ные домашние хозяйства имеют разные доходы), и регрессия x на p даст оценку эластичности предложения по цене (рис. 10.3). В такой ситуации не идентифициро-вано уравнение спроса. Уравнение предложения идентифицировано: k = 2, n = 1, r1 = 1, r2 = 0 и r1 = k − 1, r2 < k − 1. Понятно, что можно говорить о модели, в которую входят обе отмеченные пере-менные: и z1 и z2. Это-модель (10.15, 10.16, 10.17). В правую часть (10.15)
10.2 Взаимозависимые или одновременные уравнения 323 x p d1d2 d3d4d5 d6 s2 s1 s3s4 s5 Рис. 10.4 добавляется слагаемое [ z1 z2 ] ⎡⎢⎣ a11 0 0 a22 ⎤⎥⎦. Вэтомслучае идентифицированыоба уравнения: k = 2, n = 1, r1 = r2 = 1 = k−1. Но поскольку подвижны обе линии-и спроса, и предложения-облако наблюде-ний не имеет вытянутостей (рис. 10.4), и регрессия x на p опять оказывается не значимой. Для оценки параметров регрессии требуется использовать специальные методы, рассматриваемые ниже. Впрочем, и в двух предыдущих случаях необходимо использование специальных методов оценки параметров взаимозависимых систем, т.к. обычный МНК дает смещенные и несостоятельные оценки. Пусть теперь на предложение товара влияет еще один фактор z3, показывающий, например, количество удобрений на единицу площади, с которой собирается продукт, принимающий в дальнейшем форму товара. Тогда в правой части уравнения (10.15) возникает слагаемое [ z1 z3 ] ⎡⎢⎣ a11 0 a31 0 ⎤⎥⎦, и первое уравнение по-прежнему остается не идентифицированным, а второе ока-зывается сверхидентифицированным. Далее ряд утверждений будет иллюстрироваться на примере модели (10.15, 10.16). В иллюстрациях эту модель удобнее записывать в сокращенном виде: [ ˆx ˆp ] ⎡⎢⎣1 1 −β21 β22 ⎤⎥⎦= ˆz1 [ α11 0 ]+[ ε1 ε2 ] . (10.18) Поскольку ⎡⎢⎣1 1 −β21 β22⎤⎥⎦ −1 = 1 β21 + β22 ⎡⎢⎣ β22 −1 β21 1 ⎤⎥⎦,
324 Глава 10. Оценка параметров систем уравнений приведенная форма модели имеет следующий вид: [ ˆx ˆp] = ˆz1 [ d11 d12 ] + [ η1 η2 ] = = 1 β21 + β22 (ˆz1 [ α11β22 − α11 ] + [ ε1β22 + ε2β21 ε2 − ε1 ]). (10.19) Из этого соотношения видно, как d и η связаны с β и ε. Дальнейшее изложение ведется в предположении, что строки матрицы Rl - орты. 10.3. Оценка параметров отдельного уравнения Вводятся дополнительные обозначения: Xl - N ×kl-матрица наблюдений за изучаемыми переменными xl, вход ящи-ми в l-е уравнение; Xl - N-вектор-столбец наблюдений за l-й переменной xl ; Xl−- N × (kl − 1)-матрица Xl без столбца Xl наблюдений за xl−; βl - kl-вектор-столбец параметров при изучаемых переменных в l-м урав-нении; βl - (kl − 1)-вектор-столбец βl с обратным знаком и без l-го элемента βll = 1; Zl - N×(nl+1)-матрица наблюденийза независимымифакторами zl,входя-щими в l-е уравнение, включая единичный столбец, соответствующий свободному члену; αl - (nl + 1)-вектор-столбец параметров при этих факторах вместе со сво-бодным членом; εl - N-вектор-столбец остатков в l-м уравнении по наблюдениям. Тогда l-е уравнение регрессии можно записать следующим образом: Xlβl = Zlαl + εl (10.20) или Xl = Xl−βl + Zlαl + εl. (10.21) Применение обычного МНК к этому уравнению дает в общем случае смещен-ные и несостоятельные оценки, прежде всего потому, что остатки εl скорее всего коррелированы с регрессорами Xl−, которые к тому же недетерминированы и на-блюдаются с ошибками (гипотеза g2 нарушена).
10.3. Оценка параметров отдельного уравнения 325 Для иллюстрации справедливости этого утверждения используется модель (10.15, 10.16). Пусть эта модель истинна, и тогда регрессия x на p даст оценку −β22: −bмнк 22 = ˆxi ˆpi ˆp2i . (10.22) Это выражение можно преобразовать, используя (10.18, 10.19) (чтобы не загро-мождать записи, 1 ˆp2i обозначено через P ): − bмнк 22 = Pˆxi ˆpi ˆxi=−β22 ˆpi+εi2 = − β22 + Pεi2 ˆpi ˆpi=ˆzi1d12+ηi2 = = −β22 + P d12ˆzi1εi2 +ηi2εi2ηi2= εi2−εi1 β21+β22 = − β22 + + P d12ˆzi1εi2 + 1 β21 + β22 ε2 i2 −εi1εi2. Очевидно, что −bмнк 22 по математическому ожиданию никак не может равняться − −β22, поскольку в правой части полученного выражения имеется ε2 i2 , т.е. д испер-сия (в математическом ожидании) остатка в уравнении по спросу, которая не равна нулю и к тому же не будет уменьшаться с ростом N. Эта оценка смещена и несосто-ятельна. Если данное уравнение точно идентифицировано, то для оценки его параметров можно использовать косвенный метод (КМ) наименьших квадратов: с помощью МНК оцениваются параметры приведенной формы системы уравнений, через ко-торые однозначно выражаются структурные параметры данного уравнения. В качестве примера можно использовать оценку параметров второго уравнения мо-дели (10.15, 10.16), которое точно идентифицировано. Действительно, параметры приведенной формы модели однозначно определяют оценку −β22, как это следует из (10.19): −bKM 22 = d11 d12 . (10.23) Поскольку d11 = ˆxiˆzi1 ˆz2i1 , d12 = ˆpiˆzi1 ˆz2i1 , то соотношение (10.23) означает, что −bKM 22 = ˆxiˆzi1 ˆpiˆzi1 , т.е. что (ср. с (10.22)) используется методи нструментальных переменных с z1 в ка-честве инструментальной переменной.
326 Глава 10. Оценка параметров систем уравнений Можно записать уравнения для оценки косвенным методом в общем случае. Сначала следует обратить внимание на то, что условия (10.11) эквивалентны требованиям TBl βl = Bl, TAl αl = Al, (10.24) где TBl - k × kl-матрица, полученная из Ik вычеркиванием столбцов, соответ-ствующих тем изучаемым переменным, которые исключены из l-го уравнения; TAl - аналогичная (n + 1) × (nl + 1)-матрица для Al . Bl и Al имеют нулевые компоненты, соответствующие исключенным из l-го уравнения переменным. Далее необходимо учесть, что параметры структурной формы, удовлетворяю-щие условиям (10.24), должны для своей идентификации еще удовлетворять со-отношениям (10.10). Тем самым получается система уравнений для нахождения параметров структурной формы: DTBl bl − TAl al = 0, или по определению матрицы TBl : Dlbl − TAl al = 0, где Dl - оценки параметров приведенной формы уравнений для изучаемых пере-менных, вошедших в l-е уравнение, или, наконец, Dl = Dl−bl + TAl al, (10.25) где Dl -оценки параметров l-го уравнения в приведенной форме, Dl−-оценки параметров приведенной формы уравнений для изучаемых пе-ременных, вошедших в правую часть l-го уравнения. Эти матрицы коэффициентов приведенной формы представляются следующим образом:Dl = (ZZ)−1ZXl, Dl = (ZZ)−1ZXl, Dl−= (ZZ)−1ZXl−. Система уравнений (10.25) может быть также получена умножением обеих частей системы (10.21) слева на (ZZ)−1Z, т.к. третье слагаемое правой части отбрасывается (МНК-остатки должны быть ортогональны регрессорам), а во 2-м слагаемом (ZZ)−1ZZl заменяется на TAl (т.к. по определению этой матрицы Zl = ZTAl ). Вобщем случае,матрица этой системы Dl−TAl имеет размерность (n + 1)× × (kl + nl). Первый ее блок имеет размерность (n + 1) × (kl − 1), второй- (n + 1) × (nl + 1).
10.3. Оценка параметров отдельного уравнения 327 В случае точной идентификации и строгого выполнения условий (10.14) эта матрица квадратна и не вырождена. Система (10.25) дает единственное реше-ние-оценку параметров структурной формы l-го уравнения косвенным методом наименьших квадратов. В структурной форме со скрытым свободным членом модель (10.15+10.16) записы-вается следующим образом: X P⎡⎢⎣1 1 −b21 b22 ⎤⎥⎦= [Z1 1N ] ⎡⎢⎣ a11 0 c1 c2 ⎤⎥⎦+ [e1 e2 ] , а ее второе, точно идентифицированное уравнение в форме (10.21)- X = P(−b22) + [Z1 1N ] ⎡⎢⎣ 0 c2 ⎤⎥⎦+ [e1 e2 ] . (10.26) Как это было показано выше, обе части (10.26) умножаются на матрицу ⎡⎢⎣ ⎡⎢⎣ Z1 1N⎤⎥⎦Z1 1N ⎤⎥⎦ −1 ⎡⎢⎣ Z1 1N⎤⎥⎦: ⎡⎢⎣ d11 d21⎤⎥⎦= ⎡⎢⎣ d12 d22⎤⎥⎦(−b22) + ⎡⎢⎣ 0 c2⎤⎥⎦, или D1 = D2(−b22) + T A2 c2, где T A2 = ⎡⎢⎣ 01⎤⎥⎦. Непосредственно в форме (10.25) при учете условий нормализации эта система записалась бы в виде: D2b22 = −D1 + T A2 c2. Из решения этой системы −bKM 22 получается таким же, как в (10.23), кроме того, получается оценка свободного члена: cKM 2 = d21 − d22 d11 d12 .
328 Глава 10. Оценка параметров систем уравнений Если уравнение не идентифицировано, переменных в системе (10.21) оказы-вается больше, чем уравнений, и эта система представляет бесконечное множе-ство значений параметров структурной формы. Чтобы выбрать из этого множе-ство какое-то решение, часть параметров структурной формы надо зафиксировать, т.е. сделать уравнение идентифицированным. Для сверхидентифицированного уравнения система (10.21) является переопре-деленной, и ее уравнения не могут выполняться как равенства. Различные методы оценки такого уравнения реализуют различные подходы к минимизации невязок по уравнениям этой системы. Одним из таких методов является двухшаговый метод (2М) наименьших квад-ратов. На первомшаге с помощьюМНКоцениваются параметры приведенной формы для переменных Xl−: Xl−= ZDl−+ V l, где V l - N×(kl−1)-матрица остатков по уравнениям; и определяются расчетные значения этих переменных уже без ошибок: Xlc − = ZDl−. На втором шаге с помощью МНК оцениваются искомые параметры структур-ной формы из уравнения: Xl = Xlc − bl + Zlal + el. (10.27) Для этого уравнения гипотеза g2 выполняется, т.к. регрессоры не имеют ошибок, и поэтому применим обычныйМНК. Можно определить единый оператор 2M-оценивания. Поскольку Xlc − = FXl−, где F = Z(ZZ)−1Z, уравнение (10.24) записывается как: Xl = FXl−Zl⎡⎢⎣ bl al⎤⎥⎦+ el, (10.28) а оператор, входящий в него, как: ⎡⎢⎣ bl al⎤⎥⎦= ⎡⎢⎣ Xl−FXl−Xl−Zl ZlXl−ZlZl⎤⎥⎦ −1 ⎡⎢⎣ Xl−FXl ZlXl ⎤⎥⎦. (10.29)
10.3. Оценка параметров отдельного уравнения 329 Оператор в такой форме получается как результат применения МНК к уравнению (10.25), т.е. результат умножения обеих частей этого уравнения слева на транспо-нированную матрицу регрессоров и отбрасывания компоненты остатков: ⎡⎢⎣ Xl−F Zl⎤⎥⎦Xl = ⎡⎢⎣ Xl−F Zl⎤⎥⎦7FXl−Zl8⎡⎢⎣ bl al⎤⎥⎦ l . (10.30) Откуда следует оператор 2М-оценивания в указанной форме, т.к. F -симметрич-ная идемпотентная матрица и FZl = FZTAl = ZTAl = Zl. Такой оператор оценивания сверхидентифицированного уравнения можно по-лучить, еслиМНК применить к системе (10.21) (в этом случае она переопределена и в ее уравнениях возникают невязки), умножив предварительно обе ее части слева на Z.Система нормальных уравнений для оценки (10.21), умноженной на Z, записыва-ется следующим образом: ⎡⎢⎣ Dl− T Al ⎤⎥⎦ZZDl = ⎡⎢⎣ Dl− T Al ⎤⎥⎦ZZ 7Dl−T Al 8 ⎡⎢⎣ bl al⎤⎥⎦, и, учитывая, чтоDl−ZZDl = Xl−FXl, TAl ZZDl = ZlXl и т.д., она преобразуется к виду (10.29). Отсюда, в частности, следует, что для точно идентифированного уравнения 2М-оценка совпадает с КМ-оценкой, т.к. параметры структурной формы урав-нения, однозначно определяемые соотношениями (10.21), удовлетворяют в этом случае и условиям (10.25). Соотношения (10.29)-первая форма записи оператора 2М-оценивания. Ес-ли в (10.24) учесть, что Xlc − = Xl−− V l, этот оператор можно записать в более прозрачной второй форме: ⎡⎢⎣ bl al⎤⎥⎦= ⎡⎢⎣ Xl−Xl−− V lV l Xl−Zl ZlXl−ZlZl⎤⎥⎦ −1 ⎡⎢⎣ (Xl− − V l)Xl ZlXl ⎤⎥⎦. (10.31)
330 Глава 10. Оценка параметров систем уравнений Это доказывается аналогично с учетом того, что остатки V l ортогональны регрес-сорам Z и, соответственно, ZV l = 0, Xl−V l = V lV l, Xlc − V l = 0. Попытка применить оператор 2М-оценивания для не идентифицированного уравнения не имеет смысла, т.к. обращаемая матрица в данном операторе вырож-дена. В этом легко убедиться, т.к. 7FXl−Zl8 = Z 7Dl−T Al 8 , т.е. матрица наблюдений за регрессорами в (10.25) получается умножением на Z слева матрицы системы (10.21). В последней, если уравнение не идентифицирова-но,-столбцов больше, чем строк. Следовательно, регрессоры в (10.25) линейно связаны между собой, а матрица системы нормальных уравнений (матрица операто-ра оценивания) вырождена. Для сверхидентифицированного уравнения можно использовать также метод наименьшего дисперсионного отношения (МНДО). Строгое обоснование его применимости вытекает из метода максимального правдоподобия. Пусть bl в уравнении (10.20) оценено, и Xlbl рассматривается как единая эндогенная переменная. В результате применения МНК определяются: al = (ZlZl)−1ZlXlbl, el = (IN − Fl)Xlbl, где Fl = Zl(ZlZl)−1Zl, elel = blWlbl, гдеWl = Xl(IN − Fl)Xl. (10.32) Теперь находится остаточная сумма квадратов при условии, что все экзогенные переменные входят в l-е уравнение. Она равна blWbl, гд еW = Xl(IN − F)Xl . Тогда bl должны были бы быть оценены так, чтобы λ = blWlbl blWbl → min! Иначе было бы трудно понять, почему в этом уравнении присутствуют не все экзо-генные переменные. Решение этой задачи приводит к следующим условиям: (Wl − λW)bl = 0. (10.33)
10.4. Оценка параметров системы идентифицированных уравнений 331 Действительно, из условия равенства нулю первой производной: ∂λ ∂bl = 2Wlbl(blWbl) − 2Wbl(blWlbl) (blWbl)2 = 2 blWbl (Wlbl − λWbl) = 0, сразу следует (10.33). Следовательно, λ находится как минимальный корень характеристического уравнения (см. Приложение A.1.2) Wl − λW= 0, а bl определяется из 10.33 с точностью до постоянного множителя, т.е. с точностью до нормировки bll = 1. В общем случае λmin > 1, но при правильной спецификации модели λmin −→ N→∞1. Оператор⎡⎢⎣ bl al⎤⎥⎦= ⎡⎢⎣ Xl−Xl−− kV lV l Xl−Zl ZlXl−ZlZl⎤⎥⎦ −1 ⎡⎢⎣ (Xl− − kV l)Xl ZlXl ⎤⎥⎦ позволяет получить так называемые оценки k-класса (не путать с k -количе-ством эндогенных переменных в системе). При k = 0, они являются обычными МНК-оценками для l-го уравнения, что легко проверяется; при k = 1, это-2М-оценки; при k = λmin -МНДО-оценки (принимается без доказательства). 2М-оценки занимают промежуточное положение междуМНК-иМНДО-оценками (т.к. λmin > 1). Исследования пока-зывают, что эффективные оценки получаются при k < 1. 10.4. Оценка параметров системы идентифицированных уравнений Из приведенной формы системы уравнений следует, что xε = (B−1)Azε + (B−1)εε. Как и прежде, в любом наблюдении E(ε) = 0, E(εε) = σ2Ω, и ошибки не корре-лированы по наблюдениям. Тогда E(xε) = (B−1)E(εε) = σ2(B−1)Ω,
332 Глава 10. Оценка параметров систем уравнений т.е. в общемслучае все эндогенные переменные коррелированы с ошибками во всех уравнениях. Это является основным препятствием для применения обычногоМНК ко всем уравнениям по отдельности. Но в случае, если в матрице B все элементы, расположенные ниже глав-ной диагонали, равны нулю, т.е. в правой части l-го уравнения могут появлять-ся только более младшие эндогенные переменные xl, l< l, и послед ней компонентой любого вектора xl является xl, а матрица Ω диагональна, то εl не коррелирует с переменными xl− при любом l. Это-рекурсивная систе-ма, и для оценки ее параметров можно применять МНК к отдельным уравне-ниям. Для оценки параметров всех идентифицированных уравнений системы можно применить трехшаговый метод (3М) наименьших квадратов. Первые два шага 3М совпадают с 2М, но представляются они по сравнению с предыдущим пунктом в несколько иной форме. Предполагается, что идентифицированы все k уравнений: Xl = Xl−βl + Zlαl + εl = Qlγl + εl, l= 1, . . . , k, где Ql = [Xl−, Zl], γl = [βl αl ]. Учитывая указанные выше свойства остатков: E(εlεl) = σ2ωllIN, E(εlεl) = σ2ωllIN. Теперь обе части l-го уравнения умножаются слева на Z: ZXl = ZQlγl + Zεl, (10.34) и ZXl рассматривается как вектор n + 1 наблюдений за одной эндогенной пе-ременной, а ZQl -как матрица n + 1 наблюдений за nl + kl экзогенными пе-ременными, включая свободный член. Так как все уравнения идентифицированы, и выполнено условие (10.14), во всех этих новых регрессиях количество наблю-дений не меньше количества оцениваемых параметров. Для сверхидентифициро-ванных уравнений количество наблюдений в новой регрессии будет превышать количество оцениваемых параметров. Это более естественный случай. Поэтому 3М-метод обычно применяют для всех сверхидентифицированных уравнений си-стемы. Матрица ковариации остатков по уравнению (10.34) равна σ2ωllZZ. Она от-лична от σ2IN, и для получения оценок cl параметров γl этого уравнения нужно использовать ОМНК: cl = (QlZ(ZZ)−1ZQl)−1QlZ(ZZ)−1ZXl, или cl = (QlFQl)−1QlFXl.
10.4. Оценка параметров системы идентифицированных уравнений 333 Сравнив полученное выражение с (10.29), легко убедится в том, что cl - 2М-оценка. Если 2М на этом заканчивается, то в 3М полученные оценки cl используются для того, чтобы оценить el , и затем получить оценки W матрицы σ2Ω: wll = 1 N elel, wll = 1 N elel. Теперь все уравнения (10.34) записываются в единой системе (подобная запись использовалась в п.10.1 при доказательстве одного из утверждений): ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ ZX1 ZX2 ... ZXk⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ ZQ1 0 - - - 0 0 ZQ2 - - - 0 ... .... . . ... 0 0 - - - ZQk⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ γ1 γ2... γk⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ + ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ Zε1 Zε2 ... Zεk⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ , (10.35) или Y = Qγ + η, где Y -соответствующий k - (n + 1)-вектор-столбец наблюдений за изучаемой переменной; Q - k(n + 1) × kl=1(kl + nl)-матрица наблюдений за экзогенными перемен-ными; γ - k l=1(kl + nl)-вектор-столбец параметров регрессии; η - k(n + 1)-вектор-столбец остатков по наблюдениям. Легко проверить, что матрица ковариации остатков η удовлетворяет следую-щему соотношению: E(ηη) = σ2Ω ⊗ (ZZ). Для нее имеется оценка: k(n+1)×(n+1)-матрица Σ = W ⊗(ZZ). Эта матрица отлична от σ2Ik(n+1), поэтому на третьем шаге 3М-оценивания к единой системе (10.35) применяется ОМНК и получается окончательная оценка c параметров γ: c = (QΣ−1Q)−1QΣ−1Y.
334 Глава 10. Оценка параметров систем уравнений 10.5. Упражнения и задачи Упражнение 1 Таблица 10.1 i c y η1 = η2 2.00 18.19 20.19 0.193 2.00 17.50 19.50 -0.504 2.20 16.48 18.68 -2.318 2.20 19.06 21.26 0.257 2.40 21.38 23.78 1.784 2.40 21.23 23.63 1.627 2.60 21.11 23.71 0.708 2.60 22.65 25.25 2.252 2.80 20.74 23.54 -0.462 2.80 19.85 22.65 -1.348 3.00 22.23 25.23 0.234 3.00 22.23 25.23 0.226 3.20 23.43 26.63 0.629 3.20 23.04 26.24 0.244 3.40 23.03 26.43 -0.569 3.40 24.45 27.85 0.853 3.60 26.63 30.23 2.227 3.60 24.47 28.07 0.074 3.80 24.67 28.47 -0.527 3.80 26.00 29.80 0.804 Рассматривается простая Кейнсианская модель: ⎧⎪⎨ ⎪⎩ c = α1N + βy + ε, y = c + i, где c, i и y -объем потребления, инве-стиции и доход соответственно, 1N -стол-бец, состоящий из единиц. Пусть каждый век-тор имеет размерность 20 × 1, E(ε) = 0 и E(εε) = σ2IN = 0.22I20. Система уравнений приведенной формы следующая: ⎧⎪⎨ ⎪⎩ c = α 1−β 1N + β 1−β i + 1 1−β ε, y = α 1−β 1N + 1 1−β i + 1 1−β ε, Ошибки в приведенной форме для c и y та-ковы: η1 = η2 = 1 1 − β ε = 1 1 − 0.8ε = 1 0.2ε, т.е. в модели в приведенной форме ошибки η1 и η2 распределены как N(0, I). В таблице 10.1 на основе заданных 20-ти гипотетических значений для i (первая колонка) и нормаль-но распределенных ошибок (последняя колон-ка) получены данные для c и y из уравнений приведенной формы, используя значения па-раметров α = 2 и β = 0.8. В реальной ситуации существуют только значения i, c и y. Значения ошибки в модели и значения α и β неизвестны. 1.1. Используя данные таблицы 10.1, оцените уравнения приведенной формы для объема потреблении и дохода.
10.5. Упражнения и задачи 335 1.2. Используя данные таблицы 10.1, посчитайте косвенныеМНК-оценки для α и β из а) уравнения приведенной формы для объема потребления и б) уравнения приведенной формы для дохода. Идентичны ли косвенныеМНК-оценки, полученные из обоих уравнений приведенной формы? 1.3. Используя данные таблицы 10.1, посчитайте простые МНК-оценки для α и β и сравните их с косвенными МНК-оценками из упражнениия 1.2. 1.4. Используя данные таблицы 10.1 для i и используя значения параметров α = 2 и β = 0.8 составьте 100 выборок для c и y. 1.5. Примените простой МНК к каждому структурному уравнению системы для 100 выборок. Посчитайте среднее 100 оценок α и β. Проверьте степень эмпирического смещения. 1.6. Посчитайте косвенныеМНК-оценки для α и β для 100 выборок. Посчитай-те среднее 100 оценок α и β. Посчитайте степень смещения в маленьких выборках-размером по 20 наблюдений. Сравните смещение косвенных МНК-оценок со смещением обычныхМНК-оценок. 1.7. Объедините пары выборок так, чтобы получились 50 выборок по 40 наблю-дений. Посчитайте косвенныеМНК-оценки для α и β для этих 50 выборок. Посчитайте среднее и проверьте смещение оценок. Будут ли эмпирические смещения в этом случае меньше, чем рассчитанные из 100 выборок по 20 на-блюдений? Упражнение 2 Таблица 10.2 соде ржит векторы наблюдений z1, z2, z3 , z4, z5 и x1, x2, x3 которые представляют выборку, полученную из модели: x1 = β12x2 + β13x3 + α11z1 + ε1, x2 = β21x1 + α21z1 + α22z2 + α23z3 + α24z4 + ε2, x3 = β32x2 + α31z1 + α32z2 + α35z5 + ε3,
336 Глава 10. Оценка параметров систем уравнений Таблица 10.2 z1 z2 z3 z4 z5 x1 x2 x3 1 3.06 1.34 8.48 28.00 359.27 102.96 578.49 1 3.19 1.44 9.16 35.00 415.76 114.38 650.86 1 3.30 1.54 9.90 37.00 435.11 118.23 684.87 1 3.40 1.71 11.02 36.00 440.17 120.45 680.47 1 3.48 1.89 11.64 29.00 410.66 116.25 642.19 1 3.60 1.99 12.73 47.00 530.33 140.27 787.41 1 3.68 2.22 13.88 50.00 557.15 143.84 818.06 1 3.72 2.43 14.50 35.00 472.80 128.20 712.16 1 3.92 2.43 15.47 33.00 471.76 126.65 722.23 1 4.15 2.31 16.61 40.00 538.30 141.05 811.44 1 4.35 2.39 17.40 38.00 547.76 143.71 816.36 1 4.37 2.63 18.83 37.00 539.00 142.37 807.78 1 4.59 2.69 20.62 56.00 677.60 173.13 983.53 1 5.23 3.35 23.76 88.00 943.85 223.21 1292.99 1 6.04 5.81 26.52 62.00 893.42 198.64 1179.64 1 6.36 6.38 27.45 51.00 871.00 191.89 1134.78 1 7.04 6.14 30.28 29.00 793.93 181.27 1053.16 1 7.81 6.14 25.40 22.00 850.36 180.56 1085.91 1 8.09 6.19 28.84 38.00 967.42 208.24 1246.99 1 9.24 6.69 34.36 41.00 1102.61 235.43 1401.94
10.5. Упражнения и задачи 337 или в матричной форме: XB = ZA + ε, гд е εi -нормально распределенные векторы с E(εi) = 0 и Eεε= E⎛⎜⎜⎜⎜⎜⎜⎝ ⎡⎢⎢⎢⎢⎢⎣ ε1 ε2 ε3 ⎤⎥⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎣ ε1 ε2 ε3 ⎤⎥⎥⎥⎥⎥⎦ ⎞⎟⎟⎟⎟⎟⎟⎠ = Σ⊗ IN. Гипотетические структурные матрицы коэффициентов B, A и ковариационная матрица Σ следующие: B = ⎡⎢⎢⎢⎢⎢⎣ 0 0.2 0 −10 −1 2 2.5 0 −1 ⎤⎥⎥⎥⎥⎥⎦ , A= ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 60 −40 10 0 4 −80 0 6 0 0 −1.5 0 0 0 −5 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ , Σ = ⎡⎢⎢⎢⎢⎢⎣ 227.55 8.91 −56.89 8.91 0.66 −1.88 −56.89 −1.88 15.76 ⎤⎥⎥⎥⎥⎥⎦ Матрица коэффициентов в приведенной форме для гипотетической модели следу-ющая: D = AB−1 = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ −142.50 11.50 13.00 110.00 18.00 116.00 15.00 −3.00 −6.00 −3.75 0.75 1.50 6.25 1.25 7.50 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ В реальной ситуации B, A, Σ, D были бы неизвестны, доступны были бы только наблюдения в таблице 10.2. 2.1. Используя данные таблицы 10.2, проверьте каждое структурное уравнение системы на идентифицируемость.
338 Глава 10. Оценка параметров систем уравнений 2.2. Оцените матрицу параметров приведенной формы D = (ZZ)−1ZX. 2.3. Примените простойМНКк каждому структурному уравнению системы и оце-ните матрицы B и A. 2.4. Рассчитайте ⎡⎢⎣ bl al⎤⎥⎦= ⎡⎢⎣ Xl−Xl−− kV lV l Xl−Zl ZlXl−ZlZl ⎤⎥⎦ −1 ⎡⎢⎣ (Xl− − kV l)Xl ZlXl ⎤⎥⎦(10.36) при k = 0 и сравните с результатом упражнения 2.3. 2.5. Используя косвенный МНК, оцените параметры второго строго идентифи-цированого уравнения. 2.6. Найдите b2 и a2, решая систему D2 = D2−b2 + TA2 a2, и сравните с резуль-татом упражнения 2.5. 2.7. Найдите минимальный корень λ из уравнения Wl − λW= 0 и, используя формулу метода наименьшего дисперсионного отношения (10.36) при k = λ, оцените параметры в каждом из трех структурных уравнений. 2.8. Используя формулу двухшагового метода наименьших квадратов (10.36) при k = 1, сравните оценки матрицы D, полученные на основе оценок простым МНК, МНДО и 2МНК, с исходными гипотетическими матрицами парамет-ров приведенной формы. 2.9. Используя формулу3МНК, оцените параметры первого и третьего структур-ных уравнений совместно. Упражнение 3 Имеем модель Клейна, в которой C = αP + β(W + V ) + χP−1 + δ + ε1 -функция потребления, I = ϕP + γP−1 + ηK−1 + π + ε2 -функция инвестиционного спроса, W = μ(Y + T − V ) + θ(Y−1 + T−1 − V−1) + ψt + ζ + ε3 -функция спроса на труд. Выполняются следующие макроэкономические соотношения: Y + T = C + I + G, Y = W + V + P, K = K−1 + I,
10.5. Упражнения и задачи 339 где C -потребительские расходы, I -инвестиционные расходы, G -госу-дарственные расходы, P -прибыль, W -спрос на труднегосу дарственного сектора, V -спрос на трудгосу дарственного сектора, K -капитал, T -на-логи, t -время, Y -чистый доход от налогов. На основе данных из таблицы 10.3 оценить параметры модели Клейна простым методом наименьших квадратов и двухшаговым методом наименьших квадратов. Показать величину смещения оценок. Задачи 1. Эконометрическая модель описана следующими уравнениями: ⎧⎪⎨ ⎪⎩ x1 = α10 + α11z1 + β12x2 + ε1, x2 = α20 + β21x1 + ε2, где x1 и x2 -эндогенные переменные, z1 -экзогенная переменная, ε1 и ε2 -случайные ошибки. Определите направление смещения оценки для β21, если для оценивания второго уравнения используется методнаи-меньших квадратов. 2. Дана следующая макроэкономическая модель: y = c + i + g -макроэкономическое тождество; c = α10 + β11y -функция потребления, i = α20 + β21y − β22r -функция инвестиций, (m/p) = β31y − β32r -уравнение денежного рынка, где эндогенными переменными являются доход y, потребление c, инвестиции i и процентная ставка r.Переменные g (государственные расходы) и (m/p) (реальная денежная масса)-экзогенные. Проверьте, является ли данная система идентифицируемой, и перепишите модель в приведенной форме. 3. Дана следующая модель краткосрочного равновесия для малой открытой эко-номики (модель Манделла-Флеминга): y = c + i + nx -макроэкономическое тождество, c = α11 + β11y + ε1 -функция потребления, i = α21 − α21r + β21y + ε2 -функция инвестиций, nx = α31 − β31y − β32ec + ε3 -функция чистого экспорта, (m/p) = β41y − α41r + ε4 -уравнение денежного рынка,
340 Глава 10. Оценка параметров систем уравнений Таблица 10.3. (Источник: G.S. Maddala(1977), Econometrics, p. 237) t C P W I K−1 V G T 1920 39.8 12.7 28.8 2.7 180.1 2.2 2.4 3.4 1921 41.9 12.4 25.5 -0.2 182.8 2.7 3.9 7.7 1922 45 16.9 29.3 1.9 182.6 2.9 3.2 3.9 1923 49.2 18.4 34.1 5.2 184.5 2.9 2.8 4.7 1924 50.6 19.4 33.9 3 189.7 3.1 3.5 3.8 1925 52.6 20.1 35.4 5.1 192.7 3.2 3.3 5.5 1926 55.1 19.6 37.4 5.6 197.8 3.3 3.3 7 1927 56.2 19.8 37.9 4.2 203.4 3.6 4 6.7 1928 57.3 21.1 39.2 3 207.6 3.7 4.2 4.2 1929 57.8 21.7 41.3 5.1 210.6 4 4.1 4 1930 55 15.6 37.9 1 215.7 4.2 5.2 7.7 1931 50.9 11.4 34.5 -3.4 216.7 4.8 5.9 7.5 1932 45.6 7 29 -6.2 213.3 5.3 4.9 8.3 1933 46.5 11.2 28.5 -5.1 207.1 5.6 3.7 5.4 1934 48.7 12.3 30.6 -3 202 6 4 6.8 1935 51.3 14 33.2 -1.3 199 6.1 4.4 7.2 1936 57.7 17.6 36.8 2.1 197.7 7.4 2.9 8.3 1937 58.7 17.3 41 2 199.8 6.7 4.3 6.7 1938 57.5 15.3 38.2 -1.9 201.8 7.7 5.3 7.4 1939 61.6 19 41.6 1.3 199.9 7.8 6.6 8.9 1940 65 21.1 45 3.3 201.2 8 7.4 9.6 1941 69.7 23.5 53.3 4.9 204.5 8.5 14 12
10.5. Упражнения и задачи 341 где эндогенными переменными являются доход y, потребление c, инвести-ции i, чистый экспорт nx и валютный курс ec. Переменные r (процентная ставка, значение которой формируется на общемировом уровне) и (m/p) (реальная денежная масса)-экзогенные; ε1, . . . , ε4 -случайные ошиб-ки. Запишите общие условия для определения структурных параметров каж-дого уравнения модели. Какие уравнения модели точно идентифицируемы? Перепишите модель Манделла-Флеминга в приведенной форме. 4. Приведите пример системы одновременных уравнений, к которойможно при-менить косвенный МНК (с объяснением обозначений). 5. Приведите пример сверхидентифицированной системы одновременных урав-нений (с объяснением обозначений). 6. Рассмотрите модель: x1t = β12x2t + α11z1t + α12z2t + α13z3t + α14z4t + ε1t, x2t = β21x1t + α21z1t + α22z2t + α23z3t + α24z4t + ε2t, где вектор z -экзогенные переменные, а вектор ε -случайные после-довательно некоррелированные ошибки с нулевыми средними. Используя исключающие ограничения (т.е. обращая в нуль некоторые коэффициенты), определите три альтернативные структуры, для которых простейшими состо-ятельными процедурами оценивания являются соответственно обыкновен-ный методнаименьш их квадратов, косвенный методнаименьш их квадратов и двухшаговый метод наименьших квадратов. 7. Имеется следующая макроэкономическая модель: c = α10 + β11y + ε1, i = α20 + β21y + β22y−1 + ε2, y = c + i + g, где c, i и y -объем потребления, инвестиции и доход, соответственно, а y−1 -доход предыдущего периода, g -государственные расходы. а) Определите типы структурных уравнений; б) классифицируйте типы переменных; в) представьте структурные уравнения в матричной форме; г) запишите модель в приведенной форме; д) проверьте идентифицируемость и метод оценки параметров каждого уравнения в структурной форме модели;
342 Глава 10. Оценка параметров систем уравнений 8. Пусть дана простая Кейнсианская модель: c = βy + ε, y = c + i, где c, i и y -объем потребления, инвестиции и доход, соответственно. Пусть каждый вектор имеет размерность N ×1, E(ε) = 0 и E(εε) = σ2IN. а) Запишите модель в приведенной форме; б) найдите оценку для параметра дохода для приведенной формы; в) получите косвенную МНК-оценку для β из результатов (б); г) найдите оценку для параметра потребления для приведенной формы; д) получите косвенную МНК-оценку для β из результатов (г); е) покажите, что результаты (в) и (д) совпадают; ж) определите направление смещения МНК-оценки для β. 9. Известны МНК-оценки параметров регрессии (угловые коэффициенты) аг-регированного объема продаж продовольственных товаров и цены на них от индекса погодных условий: а) 0.3 и −0.6; б) 0.3 и 0.6. Определить коэффициенты эластичности спроса и предложения от цены. 10. Пусть система одновременных уравнений имеет вид: x1 = α10 + β12x2 + α11z1 + ε1, x2 = α20 + β21x1 + α22z2 + ε2. Получены следующие оценки приведенной формы этой системы: x1 = 1+2z1 + 3z2, x2 = −2 + 1z1 + 4z2. Найдите оценки параметров исходной системы. 11. Рассматривается следующаямодель краткосрочного равновесия типаIS-LM: yt = ct + it + gt + nxt, ct = α11 + β11yt + ε1t, it = α21 + α21rt + ε2t, nxt = α31 + β31yt + β32rt + ε3t, mt = α40 + β41yt + α41rt + ε4t,
10.5. Упражнения и задачи 343 где эндогенными переменными являются валовой доход (выпуск) y, объем личных потребительских расходов c, объем инвестиций i, чистый экспорт nx и ставка процента r. Экзогенные переменные: g -совокупные госу-дарственные расходы и m -предложение денег. Опишите процедуру оце-нивания модели с помощью двухшагового метода наименьших квадратов. 12. Дано одно уравнение x1t = β12x2t+β13x3t+α11z1t+ε1t модели, состоящей из трех уравнений. В нее входят еще три экзогенные переменные z1, z2 и z3. Наблюдения заданы в виде следующих матриц: ZZ = ⎡⎢⎢⎢⎢⎢⎣ 20 15 −5 15 60 −45 −5 −45 −70 ⎤⎥⎥⎥⎥⎥⎦ , ZX = ⎡⎢⎢⎢⎢⎢⎣ 2 2 4 5 0 4 12 −5 0 −2 −12 10 ⎤⎥⎥⎥⎥⎥⎦ , XX = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 0 0 0 0 2 0 0 0 0 4 0 0 0 0 5⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Получите оценки двухшаговым методом наименьших квадратов для парамет-ров этого уравнения и оцените их стандартные ошибки. Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т.2.-М.: "Юнити", 2001. (Гл. 4). 2. Бриллинджер Д. Временные ряды. Обработка данных и теория.-М.: "Мир", 1980. (Гл. 10). 3. Джонстон Дж. Эконометрические методы.-М.: "Статистика", 1980. (Гл. 12). 4. Доугерти К. Введение в эконометрику.-М.: "Инфра-М", 1997. (Гл. 11). 5. Кейн Э. Экономическая статистика и эконометрия. Вып. 2.-М.: "Стати-стика", 1977. (Гл. 13).
344 Глава 10. Оценка параметров систем уравнений 6. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 10). 7. (*) Маленво Э. Статистические методы эконометрии. Вып. 2.-М., 1975. (Гл. 17-20). 8. Тинтер Г. Введение в эконометрию.-М.: "Статистика", 1965. (Гл. 6). 9. Baltagi, Badi H. Econometrics, 2nd edition, Springer, 1999. (Ch. 11). 10. Davidson, Russel, Mackinnon, James. Estimation and Inference in Econometrics, No. 9, Oxford University Press, 1993. (Ch. 7, 18). 11. GreeneW.H. Econometric Analysis, Prentice-Hall, 2000. (Ch. 15, 16). 12. Judge G.G., Hill R.C., Griffiths W.E., Luthepohl H., Lee T. Introduction to the Theory and Practice of Econometric. John Wiley & Sons, Inc., 1993. (Ch. 14, 15). 13. Maddala G.S. Introduction to Econometrics, 2nd ed., Prentice Hall, 1992. (Ch. 9). 14. RuudPaulA.AnIntroduction toClassicalEconometricTheory, Oxford University Press, 2000. (Ch. 26). 15. William E., Griffiths R., Carter H., George G. Judge Learning and Practicing econometrics, N 9 John Wiley & Sons, Inc., 1993. (Ch. 17).
Часть III Эконометрия-I: Анализ временных рядов 345
Это пустая страница
Глава 11 Основные понятия в анализе временных рядов 11.1. Введение В каждой сфере экономики встречаются явления, которые интересно и важ-но изучать в их развитии, т.к. они изменяются во времени. С течением времени изменяются цены, экономические условия, режим протекания того или иного про-изводственного процесса. Совокупность измерений подобного рода показателей в течение некоторого периода времени и представляет временной ряд. Цели анализа временных рядов могут быть различными. Можно, например, стремиться предсказать будущее на основании знаний прошлого, пытаться выяс-нить механизм, лежащий в основе процесса, и управлять им. Необходимо уметь освобождать временной ряд от компонент, которые затемняют его динамику. Часто требуется сжато представлять характерные особенности ряда. Временным рядом называют последовательность наблюдений, обычно упо-рядоченную во времени, хотя возможно упорядочение и по какому-либо другому параметру. Основной чертой, выделяющей анализ временных рядов среди других видов статистического анализа, является существенность порядка, в котором про-изводятся наблюдения. Различают два вида временных рядов.Измерение некоторых величин (темпера-туры, напряжения и т.д.) производится непрерывно, по крайней мере теоретически. При этом наблюдения можно фиксировать в виде графика. Но даже в том случае,
348 Глава 11. Основные понятия в анализе временных рядов когда изучаемые величины регистрируются непрерывно, практически при их об-работке используются только те значения, которые соответствуют дискретному множеству моментов времени. Следовательно, если время измеряется непрерыв-но, временной рядназ ывается непрерывным, если же время фиксируется дис-кретно, т.е. через фиксированный интервал времени, то временной ряд дискретен. В дальнейшем мы будем иметь дело только с дискретными временными рядами. Дискретные временные ряды получаются двумя способами: - выборкой из непрерывных временных рядов через регулярные промежутки времени (например, численность населения, величина собственного капита-ла фирмы, объем денежной массы, курс акции),-такие временные ряды называются моментными; - накоплением переменной в течение некоторого периода времени (например, объем производства какого-либо вида продукции, количество осадков, объем импорта),-в этом случае временные ряды называются интервальными. В эконометрии принято моделировать временной ряд как случайный про-цесс, называемый также стохастическим процессом, подк оторым понимается статистическое явление, развивающееся во времени согласно законам теории ве-роятностей. Случайный процесс-это случайная последовательность. Обычно предполагают, что эта последовательность идет от минус до плюс бесконечно-сти: {Xt}t=−∞, ...,+∞. Временной ряд- это лишь одна частная реализация тако-го теоретического стохастического процесса: x = {xt}t=1, ..., T = (x1, . . . , xT ), где T -длина временного ряда. Временной ряд x = (x1, . . . , xT )также часто неформально называют выборкой1. Обычно стоит задача по данному ряду сделать какие-то заключения о свойствах лежащего в его основе случайного процесса, оценить параметры, сделать прогнозы и т.п. В литературе по временным рядам существует некоторая неоднозначность, и иногда временным рядом называют сам случайный процесс {Xt}t=−∞, ...,+∞, либо его отрезок {xt}t=1, ..., T , а иногда ста-тистическуюмодель, которая порождает данный случайный процесс.Вдальнейшем мы не будем в явном виде посредством особых обозначений различать случайный процесс и его реализацию. Из контекста каждый раз будет ясно, о чем идет речь. Возможные значения временного ряда в данный момент времени t описы-ваются с помощью случайной величины xt и связанного с ней распределения вероятностей p(xt). Тогда наблюдаемое значение xt временного ряда в момент t рассматривается как одно из множества значений, которые могла бы принять случайная величина xt в этот момент времени. Следует отметить, однако, что, как правило, наблюдения временного ряда взаимосвязаны, и для корректного его описания следует рассматривать совместную вероятность p(x1, . . . , xT ). 1Хотя, по формальному определению, выборка должна состоять из независимых, одинаково рас-пределенных случайных величин.
11.1. Введение 349 Для удобства можно провести классификацию случайных процессов и соот-ветствующих им временных рядов на детерминированные и случайные процессы (временные ряды). Детерминированным называют процесс, который принимает заданное значение с вероятностью единица. Например, его значения могут точно определяться какой-либо математической функцией от момента времени t, как в следующем примере: xt = Rcos(2ωt−θ). Когда же мы будем говорить о случай-ном процессе и случайном временном ряде, то, как правило, будем подразумевать, что он не является детерминированным. Стохастические процессы подразделяются на стационарные и нестационар-ные. Стохастический процесс является стационарным, если он находится в опре-деленном смысле в статистическом равновесии, т.е. его свойства с вероятностной точки зрения не зависят от времени. Процесс нестационарен, если эти условия нарушаются. Важное теоретическое значение имеют гауссовские процессы . Это такие про-цессы, в которых любой набор наблюдений имеет совместное нормальное распре-деление. Как правило, термин "временной ряд" сам по себе подразумевает, что этот рядяв ляется одномерным (скалярным). Часто бывает важно рассмотреть совместную динамику набора временных рядов xt = (x1t, . . . , xkt), t = 1, . . . , T. Такой набор называют многомерным временным рядом, или векторным вре-менным рядом. Соответственно, говорят также о многомерных, или векторных, случайных процессах. При анализе экономических временных рядов традиционно различают разные виды эволюции (динамики). Эти виды динамики могут, вообще говоря, комбини-роваться. Тем самым задается разложение временного ряда на составляющие, которые с экономической точки зрения несут разную содержательную нагрузку. Перечислим наиболее важные: - тенденция-соответствует медленному изменению, происходящему в неко-тором направлении, которое сохраняется в течение значительного проме-жутка времени. Тенденцию называют также трендом или долговременным движением. - циклические колебания-это более быстрая, чем тенденция, квазиперио-дическая динамика, в которой есть фаза возрастания и фаза убывания. Наи-более часто цикл связан с флуктуациями экономической активности. - сезонные колебания-соответствуют изменениям, которые происходят ре-гулярно в течение года, недели или суток. Они связаны с сезонами и ритмами человеческой активности.
350 Глава 11. Основные понятия в анализе временных рядов - календарные эффекты-это отклонения, связанные с определенными предсказуемыми календарными событиями-такими, как праздничные дни, количество рабочих дней за месяц, високосность года и т.п. - случайные флуктуации-беспорядочные движения относительно большой частоты. Они порождаются влиянием разнородных событий на изучаемую величину (несистематический или случайный эффект). Часто такую состав-ляющуюназываютшумом(этот терминпришел из технических приложений). - выбросы-это аномальные движения временного ряда, связанные с редко происходящими событиями, которые резко, но лишь очень кратковременно отклоняют рядот общего закона, по которому он движется. - структурные сдвиги-это аномальные движения временного ряда, связан-ные с редко происходящими событиями, имеющие скачкообразный характер и меняющие тенденцию. Некоторые экономические ряды можно считать представляющими те или иные виды таких движений почти в чистом виде. Но бо´ льшая часть их имеет очень слож-ный вид. В них могут проявляться, например, как общая тенденция возрастания, так и сезонные изменения, на которые могут накладываться случайные флуктуа-ции. Часто для анализа временных рядов оказывается полезным изолированное рассмотрение отдельных компонент. Для того чтобы можно было разложить конкретный рядна эти состав-ляющие, требуется сделать какие-то допущения о том, какими свойствами они должны обладать. Желательно построить сначала формальную статисти-ческую модель, которая бы включала в себя в каком-то виде эти состав-ляющие, затем оценить ее, а после этого на основании полученных оце-нок вычленить составляющие. Однако построение формальной модели явля-ется сложной задачей. В частности, из содержательного описания не все-гда ясно, как моделировать те или иные компоненты. Например, тренд мо-жет быть детерминированным или стохастическим. Аналогично, сезонные ко-лебания можно комбинировать с помощью детерминированных переменных или с помощью стохастического процесса определенного вида. Компонен-ты временного ряда могут входить в него аддитивно или мультипликатив-но. Более того, далеко не все временные ряды имеют достаточно про-стую структуру, чтобы можно было разложить их на указанные составляю-щие. Существует два основных подхода к разложению временных рядов на компонен-ты. Первый подход основан на использовании множественных регрессий с факто-рами, являющимися функциями времени, второй основан на применении линейных фильтров.
11.2. Стационарность, автоковариации и автокорреляции 351 11.2. Стационарность, автоковариации и автокорреляции Статистический процесс называется строго стационарным, если взаимное рас-пределение вероятностей m наблюдений инвариантно по отношению к общему сдвигу временного аргумента, т.е. совместная плотность распределения случайных величин xt1, xt2, . . . , xtm такая же, как для величин xt1+k, xt2+k, . . . , xtm+k при любых целых значениях сдвига k. Когд аm = 1, из предположения стационарности следует, что безусловное распределение величины xt, p(xt), одинаково для всех t и может быть записано как p(x). Требование стационарности, определенное этими условиями, является доста-точно жестким. На практике при изучении случайных процессов ограничиваются моментами первого и второго порядка, и тогда говорят о слабой стационарно-сти или стационарности второго порядка2. В этом случае процесс имеет по-стоянные для всех t моменты первого и второго порядков: среднее значение μ = E(xt), определяющее уровень, относительно которого он флуктуирует, дис-персию σ2 = E(xt − μ)2 и автоковариацию γk = E(xt − μ)(xt+k − μ). Ко-вариация между xt и xt+k зависит только от величины сдвига k и не зависит от t. Автокорреляция k-го порядка стационарного процесса с ненулевой диспер-сией ρk = E(xt − μ)(xt+k − μ) :E(xt − μ)2E(xt+k − μ)2 сводится к простой формуле ρk = γk γ0 . Следует иметь в виду, что два процесса, имеющие одинаковые моменты первого и второго порядка, могут иметь разный характер распределения. Автоковариационной функцией стационарного процесса называют последо-вательность автоковариаций {γk}k=−∞, ...,+∞. Так как автоковариационная функ-ция симметрична относительно нуля: γk = γ−k, то достаточно рассматривать k = 0, 1, 2, 3, . . . Aвтокорреляционной функцией (АКФ) называют последовательность авто-корреляций {ρk}k=−∞, ...,+∞. Автокорреляционная функция также симметрична, причем ρ0 = 1, поэтому рассматривают k = 0, 1, 2, 3, . . . 2В русскоязычной литературе строгую стационарность также называют стационарностью в узком смысле, а слабую стационарность-стационарностью в широком смысле.
352 Глава 11. Основные понятия в анализе временных рядов Автоковариационная матрица ΓT для стационарного ряда x1, . . . , xT имеет вид: ΓT = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ γ0 γ1 - - - γT−1 γ1 γ0 - - - γT−2 ... .... . . ... γT−1 γT−2 - - - γ0 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ = γ0 ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 ρ1 - - - ρT−1 ρ1 1 - - - ρT−2 ... .... . . ... ρT−1 ρT−2 - - - 1 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , ΓT = γ0PT . Особенность автоковариационной матрицы ΓT и соответствующей автокор-реляционной матрицы PT в случае стационарности состоит в том, что они имеют одни и теже элементы на любой диагонали.Матрицытакого вида принято называть тёплицевыми матрицами. Как известно, любая ковариационная матрица является симметричной и по-ложительно полуопределенной. Кроме того, если компоненты рассматриваемого случайного вектора x линейно независимы в том смысле, что не существует нену-левой вектор коэффициентов λ, такой что λx -детерминированная величина, то ковариационнаяматрица является положительно определенной. Напомним, что, по определению (см. Приложение A.1.1), симметричная T × T матрица A назы-вается положительно полуопределенной, если для каждого вектора λ выполняет-ся неравенство λAλ 0; матрица A называется положительно определенной, если для каждого ненулевого вектора λ выполняется неравенство λAλ > 0. Автоковариационная и автокорреляционная матрица являются ковариационными матрицами, поэтому они обладают указанными свойствами. Сдругой стороны, если матрица обладает указанными свойствами, то она может быть автоковариационной матрицей некоторого временного ряда. Из этих рассуждений следует, что условие слабой стационарности процесса, компоненты которого линейно независимы в указанном выше смысле, налагает рядогр аничений на вид автокорреляционной и автоковариационной функций. Они вытекают из того, что главные минорыположительно определеннойматрицы, в том числе ее определитель, должны быть положительны. В частности, положительная определенность главного минора второго порядка дает 1 ρ1 ρ1 1 = 1− ρ21> 0, или − 1 < ρ1 < 1,
11.3. Основные описательные статистики для временных рядов 353 А для третьего порядка: 2ρ21− 1 < ρ2 < 1. Среди стационарных процессов в теории временных рядов особую роль игра-ют процессы типа белый шум. Это неавтокоррелированные слабо стационарные процессы { εt } с нулевым математическим ожиданием и постоянной дисперсией: μ = E(εt) = 0, γk =⎧⎪⎨ ⎪⎩ σ2 , k = 0 0 , k = 0 (11.1) Следовательно, для белого шума ΓT = σ2IT , гд е IT -единичная матрица поряд-ка T . Название "белый шум" связано с тем, что спектральная плотность такого про-цесса постоянна, то есть он содержит в одинаковом количестве все частоты, по-добно тому, как белый цвет содержит в себе все остальные цвета. Если белый шум имеет нормальное распределение, то его называют гауссовским белым шумом. Аналогичные определения стационарности можно дать и для векторного стоха-стического процесса {xt}. Слабо стационарный векторный процесс будет харак-теризоваться уже не скалярными автоковариациями γk и автокорреляциями ρk, а аналогичными по смыслу матрицами. Вне главной диагонали таких матриц стоят, соответственно, кросс-ковариации и кросс-корреляции. 11.3. Основные описательные статистики для временных рядов Предположим, у нас имеются некоторые данные в виде временного ряда {xt}t=1, ..., T . Среднее и дисперсия временного ряда рассчитываются по обычным формулам: x = T t=1 xt и s2 = 1T T t=1(xt − x)2. Выборочная автоковариация k-го порядка вычисляется как ck = 1T T−k t=1 (xt − x)(xt+k − x).
354 Глава 11. Основные понятия в анализе временных рядов Если временной рядслаб о стационарен, то эти описательные статистики являются оценками соответствующих теоретических величин и при некоторых предположе-ниях обладают свойством состоятельности. Заметим, что в теории временных рядов при расчете дисперсии и ковариаций принято сумму квадратов и, соответственно, произведения делить на T. Вместо этого при расчете дисперсии, например,можно было быделить на T−1, чтод алобы несмещенную оценку, а при расчете ковариации k-го порядка-на T −k по числу слагаемых. Оправданием данной формулы может служить простота расчетов и то, что в таком виде это выражение гарантирует положительную полуопределенность матрицы выборочных автоковариаций CT : CT = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ c0 c1 - - - cT−1 c1 c0 - - - cT−2 ... .... . . ... cT−1 cT−2 - - - c0 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Это отражает важное свойство соответствующей матрицы ΓT истинных автоко-вариаций. Любую положительно определенную матрицу B можно представить в виде B = AA, гд еA -некоторая матрица (см., например,Приложения A.1.2 и A.1.2). В нашем случае A = √1T . X, поскольку матрица CT выражается в виде произве-дения: CT = 1T . X. X, где . X- T -диагональная матрица, составленная из центрированных значений ряда . xt = xt − x: . X= ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ . x1 0 - - - 0 . x2 . x1 - - - 0 .... . . . . . ... . xT . xT−1 . . . . x1 0 . xT - - - . x2 ....... . . ... 0 0 - - - .xT ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ .
11.3. Основные описательные статистики для временных рядов 355 Статистической оценкой автокорреляции k-го порядка для стационарных про-цессов является выборочный коэффициент автокорреляции: rk = ckc0 . При ана-лизе изменения величин ck и rk в зависимости от значения k обычно пользуются выборочными автоковариационной и автокорреляционной функциями, определя-емыми как последовательности {ck} и {rk}, соответственно. Выборочная авто-корреляционная функция играет особую роль в анализе стационарных временных рядов, поскольку может быть использована в качестве инструмента для распозна-вания типа процесса. При этом обычно анализируют график автокорреляционной функции, называемый коррелограммой. Заметим, что по ряду длиной T можно вычислить автокорреляции вплоть до rT−1. Однако "дальние" автокорреляции вычисляются неточно. С ростом по-рядка k количество наблюдений, по которым вычисляется коэффициент автокор-реляции rk , уменьшается. Для расчета rT−1 используется два наблюдения. Таким образом, с ростом k выборочные автокорреляции rk становятся все менее надеж-ными оценками теоретических автокорреляций ρk. Таким образом, при анализе ряда следует принимать во внимание только самые "ближние" автокорреляции, например, первые [T/5] автокорреляций. По аналогии с автоковариациями и автокорреляциями для анализа совместной динамики нескольких рядов можно использовать выборочные кросс-ковариации и кросс-корреляции. Выборочная кросс-ковариация двух временных рядов, {xt} и {yt}, рассчиты-вается по формуле: δk = 1T T−k t=1 (xt+k − x)(yt − y). Она характеризует взаимосвязи двух рядов во времени с различной величиной сдвига k. Следует помнить, что, в отличие от автоковариации, кросс-ковариация не является симметричной по k, поэтому ее следует рассматривать и при положи-тельных, и при отрицательных k. Выборочная кросс-корреляция определяется как: T−k t=1 (xt+k − x)(yt − y) 'T t=1(xt − x)2T t=1(yt − y)2 .
356 Глава 11. Основные понятия в анализе временных рядов 11.4. Использование линейной регрессии с детерминированными факторами для моделирования временного ряда Сравнительно простой моделью временного ряда может служить модель вида: xt = μt + εt, t= 1, . . . , T, (11.2) где μt -полностью детерминированнаяпоследовательность или систематическая составляющая, εt -последовательность случайных влеичин, являющаяся белым шумом. Если μt зависит от вектора неизвестных параметров θ: μt = μt(θ), томо-дель (11.2) является моделью регрессии, и ее параметры можно оценить с помощью МНК. Детерминированная компонента μt, как правило, самамоделируется как состо-ящая из нескольких компонент. Например, можно рассмотреть аддитивную модель, в которой временной рядсод ержит три компоненты: тренд τt, сезонные движе-ния vt и случайные флуктуации εt: xt = τt + vt + εt. Зачастую изучаемый экономический ряд ведет себя так, что аддитивной схеме следует предпочесть мультипликативную схему: xt = τtvt exp(εt). Однако, если это выражение прологарифмировать, то получится аддитивный ва-риант: ln(xt) = ln(τt) + ln(vt) + εt = τ ∗ t + v∗t + εt, что позволяет оставаться в рамках линейной регрессии и значительно упрощает моделирование. 11.4.1. Тренды Существует три основных типа трендов. Первым и самим очевидным типом тренда представляется тренд среднего, ко-гда временной ряд выглядит как колебания около медленно возрастающей или убывающей величины. Второй тип трендов-это тренд дисперсии.Вэтом случае во временименяется амплитуда колебаний переменной. Иными словами, процесс гетероскедастичен.
11.4 Использование линейной регрессии 357 Часто экономические процессы с возрастающим средним имеют и возрастающую дисперсию. Третий и более тонкий тип тренда, визуально не всегда наблюдаемый,-изме-нение величины корреляции между текущим и предшествующим значениями ряда, т.е. тренд автоковариации и автокорреляции. Проводя разложение ряда на компоненты, мы, как правило, подразумеваем под трендом изменение среднего уровня переменной, то есть тренд среднего. В рамках анализа тренда среднего выделяют следующие основные способы аппроксимации временных рядов и соответствующие основные виды трендов сред-него. - Полиномиальный тренд:τt = a0 + a1t + . . . + aptp. (11.3) Для p = 1 имеем линейный тренд. - Экспоненциальный тренд: τt = ea0+a1t+...+aptp . (11.4) - Гармонический тренд: τt = Rcos(ωt + ϕ), (11.5) где R -амплитуда колебаний, ω -угловая частота, ϕ -фаза. - Тренд, выражаемый логистической функцией: τt = k 1 + be−at . (11.6) Оценивание параметров полиномиального и экспоненциального трендов (по-сле введения обозначения zi = ti, i = 1, . . . , p, -в первом случае и логарифми-рования функции во втором случае) производится с помощью обычного МНК. Гармонический тренд оправдан, когда в составе временного ряда отчетливо прослеживаются периодические колебания. При этом если частота ω известна (или ее можно оценить), то функцию (11.5) несложно представить в виде линейной комбинации синуса и косинуса: τt = α cos(ωt) + β sin(ωt) и, рассчитав векторы cos(ωt) и sin(ωt), также воспользоваться МНК для оцени-вания параметров α и β. Логистическая кривая нуждается в особом рассмотрении.
358 Глава 11. Основные понятия в анализе временных рядов 11.4.2. Оценка логистической функции Проанализируем логистическую функцию: τt = k 1 + be−at , (11.7) где a, b , k -параметры, подлежащие оцениванию. Функция ограничена и имеет горизонтальную асимптоту (рис. 11.1): lim t→∞τt = k. t k τt Рис. 11.1. Логистическая кривая Вэтом преимущество логистическойфунк-ции передпол иномиальной или экспонен-циальной функциями, которые по мере ро-ста t стремятся в бесконечность и, следо-вательно, не всегда годятся для прогнози-рования. Логистическая кривая наиболее часто используется при изучении социальных и, в частности, демографических процессов. Особенностью логистической кривой является нелинейность по оцениваемым параметрам a, b , k, поэтому система уравнений, получаемая с помощью МНК, нелинейна относительно неизвестных параметров и для ее решения могут приме-няться только итеративные численные методы. ГарольдГоттелинг (H. Hotteling) предложил интересный методд ля оценки этих параметров, основанный на использовании дифференциального уравнения логи-стической функции. Дифференцирование функции τt по времени t дает первую производную: dτt dt = kabe−at (1 + be−at)2 . Поскольку τ 2 t k = k (1 + be−at)2 и be−at = kτt − 1, то, подставляя эти выражения в формулу первой производной, получаем диффе-ренциальное уравнение, выражающее зависимость темпа прироста исследуемой
11.4 Использование линейной регрессии 359 переменной от абсолютного уровня показателя в момент времени t: dτt/dt τt = a − ak τt. (11.8) Исходя из этого соотношения, можно предположить, что в реальности абсо-лютный прирост показателя Δxt связан с фактическим его уровнем xt следующей статистической зависимостью: Δxt = axt + −akx2t+ ηt, где ηt -белыйшум. К этому уравнению теперь можно применить непосредственно метод наимень-ших квадратов, получить оценки параметров a и −ak и, следовательно, найти k. Оценка параметра b методом моментов впервые предложена Родсом. Так как be−at = kτt −1, то ln b = at+lnkτt − 1и с помощью метода моментов получаем: ln b = 1T a - T(T + 1) 2 + T t=1 ln kτt − 1, или фактически после замены τt на xt имеем: ln b = a(T +1) 2 + Tt=1 ln k xt − 1T . (11.9) Описанный выше методГоттелинга имеет ограниченнуюсферу применения, его использование оправдано лишь в том случае, если наблюдения в исходном времен-ном ряду представлены через равные промежутки времени (например, ежегодные или еженедельные данные). 11.4.3. Сезонные колебания Для моделирования сезонной составляющей τt можно использовать формулу: vt = λ1δ1t + . . . + λhδht, где δjt -сезонные фиктивные переменные, соответствующие h сезонам: δjt = 1, когда наблюдение относится к сезону j, и δjt = 0 в противном случае.
360 Глава 11. Основные понятия в анализе временных рядов Использование в линейной регрессии полного набора таких переменных свя-зано с одной особенностью. В сумме они дают единицу: δ1t + . . . + δht = 1. Поэтому, коль скоро в регрессии имеется константа, то будет иметь место ли-нейная зависимость, и λ1, . . . , λh нельзя будет оценить однозначно. Таким об-разом, требуется наложить на коэффициенты λ1, . . . , λh какое-либо нормирую-щее ограничение. В частности, можно положить один из коэффициентов равным нулю, что эквивалентно неиспользованию соответствующей переменной при по-строении регрессии. Однако более удачная нормировка состоит в том, чтобы по-ложить λ1 + . . . + λh = 0. При этом сезонная компонента центрируется, то есть в среднем влияние эффекта сезонности на уровень ряда оказывается равным нулю. Подставим это ограничение в сезоннуюкомпоненту, исключив коэффициент λ1: vt = −(λ2 + . . . + λh)δ1t + λ2δ2t + . . . + λhδht = = λ2(δ2t − δ1t) + . . . + λh(δht − δ1t). Новые переменные δ2t −δ1t, . . . , δht −δ1t будут уже линейно независимыми, и их можно использовать в линейной регрессии в качестве факторов, а также получить и оценку структуры сезонности λ1, . . . , λh. Трактовать ее следует так: в j-м сезоне сезонность приводит к отклонению от основной динамики ряда на величину λj . Если для описания тренда взять полиномиальную функцию, то, используя ад-дитивную схему, можно представить временной ряд в виде следующей линейной регрессии: xt = a0 + a1t + . . . + aptp + λ1δ1t + . . . + λhδht + εt, где λ1 + . . . + λh = 0. В этой регрессии ai и λj являются неизвестными коэффициентами. При-менение МНК дает оценки p + h + 1 неизвестных коэффициентов и приводит к выделению составляющих τt, vt и εt . 11.4.4. Аномальные наблюдения При моделировании временного ряда часто отбрасываются аномальные на-блюдения, резко отклоняющиеся от направления эволюции ряда. Такого рода выбросы, вместо исключения, можно моделировать с помощью фиктивных пе-ременных, соответствующих фиксированным моментам времени. Предположим,
11.5. Прогнозы по регрессии с детерминированными факторами 361 что в момент t∗ в экономике произошло какое-нибудь важное событие (напри-мер, отставка правительства). Тогда можно построить фиктивную переменную δt∗ t , которая равна нулю всегда, кроме момента t = t∗, когда она равна едини-це: δt∗ t = (0, . . . , 0, 1, 0, . . . , 0). Такая фиктивная переменная пригодна только для моделирования кратковре-менного отклонения временного ряда. Если же в экономике произошел структур-ный сдвиг, вызвавший скачок в динамике ряда, то следует использовать фиктив-ную переменную другого вида: (0, . . . , 0, 1, . . . , 1). Эта переменная равна нулю до некоторого фиксированного момента t∗, а после этого момента становится равной единице. Заметим, что последние два вида переменных нельзя использовать для про-гнозирования, поскольку они относятся к единичным непрогнозируемым собы-тиям. 11.5. Прогнозы по регрессии с детерминированными факторами. Экстраполирование тренда Предположим, что данные описываются линейной регрессией с детерминиро-ванными регрессорами, являющимися функциями t, и получены оценки парамет-ров регрессии на основе данных x = (x1, . . . , xT )и соответствующей матрицы факторов Z. Это позволяет построить прогноз на будущее, например на период T + k. Вообще говоря, прогноз в такой регрессии строится так же, как в любой классической линейной регрессии. Отличие состоит только в том, что значения факторов zT+k, необходимые для осуществления прогноза, в данном случае всегда известны. Рассмотрим прогнозирование на примере, когда временной ряд моделирует-ся по упрощенной схеме-трендпл юс шум: xt = τt + εt, гд е τt = ztα, zt - вектор-строка значения факторов регрессии в момент t, α -вектор-столбец ко-эффициентов регрессии. Такое моделирование имеет смысл, если циклические и сезонные компоненты отсутствуют или мало значимы. Тогда выявленный тренд τt может служить осно-вой для прогнозирования.Прогноз величины xT+k строится поформуле условного математического ожидания xT (k) = zT+ka, гд е a -оценки параметров, получен-ные с помощьюМНК, т.е. a = (ZZ)−1 Zx. Известно, что такой прогноз обладает свойством оптимальности. Предположим, что для описания тренда выбран многочлен: τt = α0 + α1t + α2t2 + . . . + αptp, t= 1, . . . , T.
362 Глава 11. Основные понятия в анализе временных рядов В такой модели матрица факторов имеет следующий вид: Z = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 10 11 - - - 1p 20 21 - - - 2p ... .... . . ... T0 T1 - - - Tp ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Вектор значений факторов на момент T + k известен определенно: zT+k = 1, (T + k), (T + k)2, . . . , (T + k)p. Точечный прогноз исследуемогопоказателя вмомент времени T на k шаговвперед равен: xT (k) = zT+ka = a0 + a1(T + k) + a2(T + k)2 + . . . + ap(T + k)p. Возвратимся к общей теории прогноза. Ошибка прогноза равна: d = xT+k − xT (k) = xT+k − zT+ka. Ее можно представить как сумму двух отдельных ошибок: d = (xT+k − zT+kα) + (zT+kα − zT+ka) = εT+k + zT+k(α − a). Первое слагаемое здесь-это будущая ошибка единичного наблюдения, а вто-рое-ошибка, обусловленная выборкой и связанная с тем, что вместо неизвест-ных истинных параметров α используются оценки a. Прогноз будет несмещенным, поскольку E(d) = E(εT+k) + zT+kE(α − a) = 0. Величина xT (k) представляет собой точечный прогноз. Поскольку точечный прогноз всегда связан с ошибкой, то важно иметь оценку точности этого прогно-за. Кроме того, вокруг точечного прогноза желательно построить доверительный интервал и, тем самым, получить интервальный прогноз. Точность прогноза измеряется, как правило, средним квадратом ошибки про-гноза, т.е. величиной E(d2), или корнем из нее-среднеквадратической ошибкой прогноза.Поскольку E(d) = 0, то средний квадрат ошибки прогноза равен диспер-сии ошибки прогноза. Полезным показателем точности является корень из этой
11.5. Прогнозы по регрессии с детерминированными факторами 363 дисперсии-стандартная ошибка прогноза.Впредположении отсутствия автокор-реляции ошибок εt дисперсия ошибки прогноза, подобно самой ошибке прогноза, является суммой двух дисперсий: дисперсии εT+k и дисперсии zT+k(α − a), а именно: σ2d = var(d) = var(εT+k) + var (zT+k(α − a)) . Найдем эту дисперсию, исходя из того, что ошибки гомоскедастичны: σ2d = σ2 + zT+kvar(α − a)zT+k = σ2 + zT+kvar(a)zT+k. Как известно, при отсутствии автокорреляции и гетероскедастичности, оценки МНК имеют дисперсию var(a) = σ2 ZZ−1 . Поэтому σ2d = σ2 1 + zT+k ZZ−1 zT+k. Для того чтобы построить доверительный интервал прогноза, следует пред-положить нормальность ошибок. Более конкретно, предполагаем, что ошибки регрессии, включая ошибку наблюдения, для которого делается прогноз, имеют многомерное нормальное распределение с нулевым математическим ожиданием и ковариационной матрицей σ2I. При таком предположении ошибка прогноза имеет нормальное распределение с нулевым математическим ожиданием и диспер-сией σ2d: d ∼ N(0, σ2d). Приводя к стандартному нормальному распределению, получим d σd ∼ N(0, 1). Однако, эта формула еще не дает возможности построить доверительный ин-тервал, поскольку истинная дисперсия прогноза σ2d неизвестна.Вместо нее следует использовать оценку s2d = ˆs2e1 + zT+k ZZ−1 zT+k, где ˆs2e-несмещенная оценка дисперсии ошибок регрессии, или остаточная дис-персия.
364 Глава 11. Основные понятия в анализе временных рядов Оказывается, что получающаяся величина dsd имеет распределение Стью-дента с (T−p−1) степенями свободы (см.ПриложениеA.3.2), где p -количество факторов в регрессии (без учета константы): dsd ∼ tT−p−1. Построим на основе этого вокруг прогноза xT (k) доверительный интервал для xT+k, учитывая, что d = xT+k − xT (k): xT (k) − sdtT−p−1,1−q; xT (k) + sdtT−p−1,1−q, где tT−p−1,1−q - (1 − q)-квантиль t-распределения Стьюдента с (T − p − 1) степенями свободы. Рассмотрим прогнозирование на примере линейного тренда. В этом случае zT+k = (1, T + k) , С учетом того, что Z= ⎡⎢⎣ 1 1 - - - 1 1 2 - - - T ⎤⎥⎦, произведение ZZ имеет вид: ZZ = ⎛⎜⎝ T Tt=1 t Tt=1 t Tt=1 t2 ⎞⎟⎠, (ZZ)−1 = 1 T Tt=1 t2 − Tt=1 t2 ⎛⎜⎝Tt=1 t2 −Tt=1 t −Tt=1t T ⎞⎟⎠, zT+k (ZZ)−1 zT+k = Tt=1 t2 − 2(T + k)Tt=1 t + T (T + k)2 T Tt=1 t2 − Tt=1 t2 = = T (T + k)2 − 2(T + k)¯t + ¯t2 +Tt=1 t2 T − ¯t2T Tt=1(t − ¯t)2 = = T ((T + k) − ¯t)2 T Tt=1(t − ¯t)2 + Tt=1 t2 − T ¯t2 T Tt=1(t − ¯t)2 = ((T + k) − ¯t)2 Tt=1(t − ¯t)2 + 1T . Тогда: σ2d = σ2 1 + 1T + ((T + k) − ¯t)2 Tt=1(t − ¯t)2 .
11.6. Критерии, используемые в анализе временных рядов 365 Соответственно, sd = ˆse!1 + 1T + ((T + k) − ¯t)2 Tt=1(t − ¯t)2 . Из этой формулывидно, что чем больше горизонт прогноза k, тем больше дисперсия прогноза и шире прогнозный интервал. 11.6. Критерии, используемые в анализе временных рядов В анализе временных рядов наиболее разработанными критериями являют-ся критерии случайности, которые призваны определить, является ли ряд чисто случайным, либо в его поведении проявляются определенные закономерности, которые позволяют делать предсказания. "Чисто случайный ряд" - это в дан-ном случае неформальный термин, подчеркивающий отсутствие закономерностей. Здесь может, например, подразумеваться ряд, состоящий из независимых и одина-ково распределенных наблюдений (что соответствует понятию выборки в обычной статистике), либо белый шум, в том смысле, который указан ранее. Среди экономических временных рядов редко встречаются такие, которые под-ходят под это описание3. Типичный экономический рядх арактеризуется сильной положительной корреляцией. Очень часто экономические ряды содержат тенден-цию, сезонность и т.д. В связи с этим применение критериев случайности по пря-мому назначению не имеет особого смысла. Тем не менее, критерии случайности играют очень важную роль в анализе временных рядов, и существуют различные способы их использования: 1)Критерийможет быть чувствительнымк определеннымотклонениям от "слу-чайности". Тогда большое значение соответствующей статистики может указывать на наличие именно такого отклонения. Таким образом, статистика критерия мо-жет использоваться просто как описательная статистика. При этом формальная проверка гипотезы не производится. Так, например, автокорреляционная функция, о которой речь пойдет ниже, очень чувствительна к наличию периодичностей и трендов. Кроме того, по автокор-реляционной функции можно определить, насколько быстро затухает временна´ я зависимость в рядах4. 3Близки к этому, видимо, только темпы прироста курсов ценных бумаг. 4При интерпретации автокорреляционной функции возникают сложности, связанные с тем, что соседние значения автокорреляций коррелированы между собой.
366 Глава 11. Основные понятия в анализе временных рядов 2) Критерий можно применять к остаткам от модели, а не к самому исход-ному ряду. Пусть, например, была оценена модель вида "тренд плюс шум". После вычитания из ряда выявленного тренда получаются остатки, которые можно рас-сматривать как оценки случайной компоненты. Наличие в остатках каких-либо закономерностей свидетельствует о том, что модель неполна, либо в принципе некорректна. Поэтому критерии случайности можно использовать в качестве диа-гностических критериев при моделировании. Следует помнить, однако, что распределение статистики, рассчитанной по остаткам, и распределение статистики, рассчитанной по исходному случайному шуму, вообще говоря, не совпадают. В некоторых случаях при большом количе-стве наблюдений это различие несущественно, но часто в результате критерий становится несостоятельным и критические значения в исходном виде применять нельзя5. Существует большое количество различных критериев случайности. По-види-мому, наиболее популярными являются критерии, основанные на автокорреляци-онной функции. 11.6.1. Критерии, основанные на автокорреляционной функции Для того чтобы сконструировать критерии, следует рассмотреть, какими ста-тистическими свойствами характеризуется автокорреляционная функция стацио-нарного процесса. Известно, что выборочные автокорреляции имеют нормальное асимптотиче-ское распределение. При большом количестве наблюдений математическое ожи-дание rk приближенно равно ρk. Дисперсия автокорреляции приближенно равна var(rk) ≈ 1T +∞ i=−∞[ρ2i+ ρi−kρi+k − 4ρkρiρi+k + 2ρ2kρ2i]. (11.10) Для ковариации двух коэффициентов автокорреляции верно приближение cov(rk, rl) ≈ (11.11) ≈ 1T +∞ i=−∞[ρi+kρi+l + ρi−kρi+l − 2ρkρiρi+l − 2ρlρiρi+k + 2ρkρlρ2i] Эти аппроксимации были выведены Бартлеттом. 5Так, Q-статистика, о которой идет речь ниже, в случае остатков модели ARMA(p, q) будет распределена не как χ2m, а как χ2m−p−q . Применение распределения χ2m приводит к тому, что нулевая гипотеза о "случайности" принимается слишком часто.
11.6. Критерии, используемые в анализе временных рядов 367 В частности, для белого шума (учитывая, что ρk = 0 при k = 0) получаем согласно формуле (11.10) var(rk) ≈ 1T . (11.12) Это только грубое приближение для дисперсии. Для гауссовского белого шума известна точная формула для дисперсии коэффициента автокорреляции: var(rk) = T − k T(T + 2). (11.13) Кроме того, из приближенной формулы (11.11) следует, что автокорреляции rk и rl, соответствующие разным порядкам ( k = l), некоррелированы. Эти формулы позволяют проверять гипотезы относительно автокорреляцион-ных коэффициентов. Так, в предположении, что ряд представляет собой белый шум, можно использовать следующий доверительный интервал для отдельного ко-эффициента автокорреляции: 0rk −! T − k T(T + 2) ˆε1−θ, rk +! T − k T(T + 2) ˆε1−θ1 , где ˆε1−θ -квантиль нормального распределения. При больших T и малых k оправдано использование более простой формулы rk − εˆ√1−θ T , rk + εˆ√1−θ T , Вместо того чтобы проверять отсутствие автокорреляции для каждого отдель-ного коэффициента, имеет смысл использовать критерий случайности, основанный на нескольких ближних автокорреляциях. Рассмотрим m первых автокорреляций: r1, . . . , rm. В предположении, что ряд является белым шумом, при большом коли-честве наблюдений их совместное распределение приближенно равно N 0, 1T Im. На основе этого приближения Бокс иПирс предложили следующую статистику, на-зываемую Q-статистикой Бокса-Пирса: Q(r) = T mk=1 r2k. Она имеет асимптотическое распределение χ2m. При дальнейшем изучении выяснилось, что выборочные значения Q-статис-тики Бокса-Пирсамогут сильно отклонятся от распределения χ2m.Для улучшения
368 Глава 11. Основные понятия в анализе временных рядов аппроксимации Льюнг и Бокс предложили использовать точную формулу диспер-сии (11.13) вместо (11.12). Полученная ими статистика, Q-статистикаЛьюнга- Бокса: ˜Q(r) = T (T + 2) mk=1 r2k T − k , тоже имеет асимптотическое распределение χ2m, однако при малом количестве наблюдений демонстрирует гораздо лучшее соответствие этому асимптотическому распределению, чем статистика Бокса-Пирса. Было показано, что критерий не теряет своей состоятельности даже при невы-полнении гипотезы о нормальности процесса. Требуется лишь, чтобы дисперсия была конечной. Нулевая гипотеза в Q-критерии заключается в том, что рядпред ставляет собой белый шум, то есть является чисто случайным процессом. Используется стандарт-ная процедура проверки: если расчетное значение Q-статистики больше заданного квантиля распределения χ2m, то нулевая гипотеза отвергается и признается нали-чие автокорреляции до m-го порядка в исследуемом ряду. Кроме критериев случайности можно строить и другие критерии на основе ав-токорреляций. Пусть, например, ρi = 0 при i k, т.е. процесс автокоррелирован, но автокорреляция пропадает после порядка k 6. Тогда по формуле 11.10 получаем var(rk) ≈ 1 + 2k−1 i=1 ρ2i. Если в этой формуле заменить теоретические автокорреляции выборочными, то по-лучим следующее приближение: var(rk) ≈ 1 + 2k−1 i=1 r2i . На основе этого приближения (приближения Бартлетта) с учетом асимптотиче-ской нормальности можно стандартным образом построить доверительный интер-вал для rk: rk − ˆε1−θ'var(rk), rk + ˆε1−θ'var(rk). 6Это предположение выполнено для процессов скользящего среднего MA(q) при q < k (см. п. 14.4).
11.6. Критерии, используемые в анализе временных рядов 369 -0.2 0.0 0.2 0.4 0.6 0.8 0 10 20 30 40 95%-е доверительные интервалы Автокорреляции Рис. 11.2. Коррелограмма с доверительными интервалами, основанными на формуле Бартлетта. На рисунке 11.2 представлена коррелограмма некоторого ряда с доверитель-ными интервалами, основанными на формуле Бартлетта7. Для удобства довери-тельные интервалы построены вокруг нуля, а не вокруг rk. 11.6.2. Критерий Спирмена КритерийСпирмена принадлежит к числу непараметрических8 критериев про-верки случайности временного ряда и связан с использованием коэффициента ран-говой корреляцииСпирмена. Он позволяет уловить наличие или отсутствие тренда в последовательности наблюдений за исследуемой переменной. Идея критерия состоит в следующем. Допустим, что имеется временной ряд, представленный в хронологической последовательности. Если ряд случайный, то распределение отдельного наблюдения не зависит от того, в каком месте ря-да стоит это наблюдение, какой номер оно имеет. При расчете критерия Спирмена в соответствие исходному ряду ставится проранжированный ряд, т.е. полученный в результате сортировки изучаемой переменной по возрастанию или по убыванию. Новый порядок, или ранг θt, сравнивается с исходным номером t, соответству-7При использовании нескольких доверительных интервалов следует отдавать себе отчет, что они не являются совместными. В связи с этим при одновременном использовании интервалов вероят-ность ошибки первого рода будет выше θ 8Вотличие от параметрических, непараметрические критерии не имеют в своей основе априорных предположений о законах распределения временного ряда.
370 Глава 11. Основные понятия в анализе временных рядов ющим хронологической последовательности. Эти порядки будут независимы для чисто случайного процесса и коррелированы при наличии тенденции. В крайнем случае, если рядвсегд а возрастает, то полученная ранжировка совпадает с исходным порядком наблюдений, т.е. t = θt для всех наблюдений t = 1, . . . , T. В общем случае тесноту связи между двумя последовательностя-ми 1, . . . , T и θ1, . . . , θT можно измерить с помощью обычного коэффициента корреляции: η = T t=1 ˆxtˆyt 'T t=1 ˆx2tT t=1 ˆy2t , (11.14) заменяя xt на t и yt на θt. Такой показатель корреляции между рангами наблю-дений (когда xt и yt представляют собой перестановки первых T натуральных чисел) в статистике называется коэффициентом ранговой корреляции Спирмена: η = 1− 6 T(T2 − 1) T t=1(θt − t)2. (11.15) Для чисто случайных процессов η имеет нулевое математическое ожидание и дисперсию, равную 1 T − 1. В больших выборках величина η приближенно имеет нормальное распределение N(0, 1 T − 1). Для малых выборок предпочтительнее использовать в качестве статистики величину η! T − 2 1 − η2 , которая приближенно имеет распределение Стьюдента с T − 2 степенями свободы. Если искомая рас-четная величина по модулю меньше двусторонней критической границы распре-деления Стьюдента, то нулевая гипотеза о том, что процесс является случайным, принимается и утверждается, что тенденция отсутствует.Инаоборот, если искомая величина по модулю превосходит табличное значение, т.е. значение коэффициента η существенно отлично от нуля, то нулевая гипотеза о случайности ряда отверга-ется. Как правило, это можно интерпретировать как наличие тенденции. 11.6.3. Сравнение средних Кроме критериев случайности можно использовать различные способы про-верки неизменности во времени моментов первого и второго порядков. Из всего многообразия подобных критериев рассмотрим лишь некоторые. В статистике существует ряд критериев, оценивающих неоднородность выбор-ки путем ранжирования наблюдений с последующим разбиением их на группы
11.6. Критерии, используемые в анализе временных рядов 371 и сравнением межгрупповых показателей. Эти критерии применимы и к времен-ным рядам. При анализе временных рядов нет необходимости в ранжировании наблюдений и поиске адекватного способа сортировки-их порядок автоматиче-ски закреплен на временном интервале. Например, можно проверять, является ли математическое ожидание ("среднее") постоянным или же в начале ряда оно иное, чем в конце. Разобьем рядд линой T на две части примерно равной длины: x1, . . . , xT1 и xT1+1, . . . , xT. Пусть ¯x1 -среднее, s21-выборочная дисперсия (несмещен-ная оценка), T1 -количество наблюдений по первой части ряда, а ¯x2, s22и T2 = = T − T1 -те же величины по второй части. Статистика Стьюдента для проверки равенства средних в двух частях ряда равна9 t = (¯x1 − ¯x2)! T1 + T2 − 2 (1/T1 + 1/T2) 7(T1 − 1)s21+ (T2 − 1)s228. (11.16) В предположении, что ряд является гауссовским белым шумом, данная стати-стика имеет распределение Стьюдента с T1 + T2 − 2 степенями свободы. Если статистика t по модулю превосходит заданный двусторонний квантиль распреде-ления Стьюдента, то нулевая гипотеза отвергается. Данный критерий имеет хорошую мощность в случае, если альтернативой яв-ляется ряд со структурным сдвигом. С помощью данной статистики также мож-но обнаружить наличие тенденции в изучаемом ряде. Для того чтобы увеличить мощность критерия в этом случае, можно среднюю часть ряда (например, треть наблюдений) не учитывать. При этом T1 + T2 < T. Рассчитать статистику при T1 + T2 = T можно с помощью вспомогательной регрессии следующего вида: xt = αzt + β + ε, где zt -фиктивная переменная, принимающая значение 0 в первой части ряд а и 1 во второй части ряда. Статистика Стьюдента для переменной zt совпадает со статистикой (11.16). Критерий сравнения средних применим и в случае, когда ряд xt не являет-ся гауссовским, а имеет какое-либо другое распределение. Однако его использо-вание в случае автокоррелированного нестационарного ряда для проверки неиз-менности среднего неправомерно, поскольку критерий чувствителен не только 9Формула (11.16) намеренно записана без учета того, что T1 + T2 = T , чтобы она охватывала и вариант использования с T1 + T2 < T, о котором речь идет ниже.
372 Глава 11. Основные понятия в анализе временных рядов к структурным сдвигам, но и к автокоррелированности ряда. Поэтому в исход-ном виде критерий сравнения средних следует считать одним из критериев случай-ности. В какой-то степени проблему автокорреляции (а одновременно и гетероске-дастичности) можно решить за счет использования устойчивой к автокорреляции и гетероскедастичности оценки Ньюи-Уэста (см. п. 8.3). При использовании этой модификации критерий сравнения средних перестает быть критерием случайности и его можно использовать как критерий стационарности ряда. Легко распространить этот методна случай, когда ряд разбивается более чем на две части. В этом случае во вспомогательной регрессии будет более одной фик-тивной переменной и следует применять уже F-статистику, а не t-статистику. Так, разбиение на три части может помочь выявить U-образную динамику среднего (например, в первой и третьей части среднее велико, а во второй мало). Ясно, что с помощью подобных регрессий можно также проверять отсутствие неслучайной зависящей от времени t компоненты другого вида. Например, пере-менная zt может иметь видл инейного тренда zt = t. Можно также дополнительно включить в регрессию t2, t3 и т.д. и тем самым "уловить" нелинейную тенденцию. Однако в таком виде по указанным выше причинам следует проявлять осторож-ность при анализе сильно коррелированных рядов. 11.6.4. Постоянство дисперсии Сравнение дисперсий Так же как при сравнении средних, при сравнении дисперсий последователь-ность xt разбивается на две группы с числом наблюдений T1 и T2 = T − T1, д ля каждой из них вычисляется несмещенная дисперсия s2iи строится дисперсионное отношение: F = s22 s21. (11.17) Этот критерий представляет собой частный случай критерия Голдфельда- Квандта (см. п. 8.2). Если дисперсии однородны и выполнено предположение о нормальности рас-пределения исходного временного ряда (более точно - ряд представляет со-бой гауссовский белый шум), то F-статистика имеет распределение Фишера FT2−1, T1−1 (см. Приложение A.3.2). Смысл данной статистики состоит в том, что, когда дисперсии сильно отли-чаются, статистика будет либо существенно больше единицы, либо существенно
11.7. Лаговый оператор 373 меньше единицы. В данном случае естественно использовать двусторонний крите-рий (поскольку мы априорно не знаем, растет дисперсия или падает). Это, конечно, не совсем обычно для критериев, основанных на F-статистике. Для уровня θ можно взять в качестве критических границ такие величины, чтобы вероятность попадания и в левый, и в правый хвост была одной и той же- θ2. Нулевая гипотеза состоит в том, что дисперсия однородна. Если дисперсионное отношение попадает в один из двух хвостов, то нулевая гипотеза отклоняется. Мощность критерия можно увеличить, исключив часть центральных наблюде-ний. Этот подход оправдан в случае монотонного поведения дисперсии временного ряда, тогда дисперсионное отношение покажет больший разброс значений. Если же временной рядне монотонен, например имеет U-образную форму, то мощность теста в результате исключения центральных наблюдений существенно уменьшается. Как и в случае сравнения средних, критерий применим только в случае, когда проверяемый процесс является белым шумом. Если же, например, рядяв ляется стационарным, но автокоррелированным, то данный критерий применять не сле-дует. 11.7. Лаговый оператор Однимиз основных понятий, употребляемых примоделировании временных ря-дов, является понятие лага. В буквальном смысле в переводе с английского лаг- запаздывание. Под лагом некоторой переменной понимают ее значение в преды-дущие периоды времени. Например, для переменной xt лагом в k периодов бу-дет xt−k. При работе с временными рядами удобно использовать лаговый оператор L, т.е. оператор запаздывания, сдвига назадво времени. Хотя часто использование этого оператора сопряжено с некоторой потерейматематической строгости, однако это окупается значительным упрощением вычислений. Если к переменной применить лаговый оператор, то в результате получится лаг этой переменной: Lxt = xt−1. Использование лагового оператора L обеспечивает сжатую запись разностных уравнений и помогает изучать свойства целого ряда процессов. Удобство использования лагового оператора состоит в том, что с ним можно об-ращаться как с обычной переменной, т.е. операторы можно преобразовывать сами по себе, без учета тех временных рядов, к которым они применяются. Основное
374 Глава 11. Основные понятия в анализе временных рядов отличие лагового оператора от обычной переменной состоит в том, что оператор должен стоять перед тем рядом, к которому применяется, т.е. нельзя переставлять местами лаговый оператор и временной ряд. Как и для обычных переменных, существуют функции от лагового оператора, они, в свою очередь, тоже являются операторами. Простейшая функция-сте-пенная. По определению, для целых mLmxt = xt−m, т.е. Lm, действующий на xt, означает запаздывание этой переменной на m пери-одов. Продолжая ту же логику, можно определить многочлен от лагового оператора, или лаговый многочлен: α(L) = mi=0 αiLt−k = α0 + α1L + - - - + αmLm. Если применить лаговый многочлен к переменной xt, то получается α(L)xt = (α0 + α1L + - - - + αmLm)xt = α0xt + α1xt−1 + - - - + αmxt−m. Нетрудно проверить, что лаговые многочлены можно перемножать как обыч-ные многочлены. Например, (α0 + α1L)(β0 + β1L) = α0β0 + (α1β0 + α0β1)L + α1β1L2. При m→∞ получается бесконечный степенной рядот лагового оператора: ∞i=0 αiLixt = (α0 + α1L + α2L2 + - - - )xt = = α0xt + α1xt−1 + α2xt−2 + - - - = ∞i=0 αixt−i. Полезно помнить следующие свойства лаговых операторов: 1) Лаг константы есть константа: LC = C. 2) Дистрибутивность: (Li + Lj)xt = Lixt + Ljxt = xt−i + xt−j . 3) Ассоциативность: LiLjxt = Li(Ljxt) = Lixt−j = xt−i−j . Заметим, что: L0xt = xt, т.е. L0 = I.
11.8. Модели регрессии с распределенным лагом 375 4) L, возведенный в отрицательную степень,-опережающий оператор: L−ixt = xt+i. 5) При |α| < 1 бесконечная сумма (1 + αL + α2L2 + α3L3 + . . . )xt = (1 − αL)−1xt. Для доказательства умножим обе части уравнения на (1 − αL): (1 − αL)(1 + αL + α2L2 + α3L3 + . . . )xt = xt, поскольку при |α| < 1 выражение αnLnxt → 0 при n→∞. Кроме лагового оператора в теории временных рядов широко используют раз-ностный оператор Δ, который определяется следующим образом: Δ = 1− L, так что Δxt = (1 − L)xt = xt − xt−1. Разностный оператор превращает исходный ряд в ряд первых разностей. Ряд d-х разностей (разностей d-го порядка) получается как степень разност-ного оператора, то есть применением разностного оператора d раз. При d = 2 получается Δ2 = (1 − L)2 = 1 − 2L + L2, поэтому Δ2xt = = (1 − 2L + L2)xt = xt − 2xt−1 + xt−2. Для произвольного порядка d следует использовать формулу бинома Ньютона: Δd = (1 − L)d = d k=0(−1)kCkdLk , гд е Ckd = d! k!(d − k)! , так что Δdxt = (1 − L)dxt = dk=0(−1)kCkdxt−k . 11.8. Модели регрессии с распределенным лагом Часто при моделировании экономических процессов на изучаемуюпеременную xt влияют не только текущие значения объясняющего фактора zt, но и его ла-ги. Типичным примером являются капиталовложения: они всегда дают результат с некоторым лагом. Модель распределенного лага можно записать следующим образом: xt = μ + q j=0αjzt−j + εt = μ + α(L)zt + εt. (11.18)
376 Глава 11. Основные понятия в анализе временных рядов где q -величина наибольшего лага, α(B) = qj=0 αjjLj -лаговый многочлен, εt -случайное возмущение, ошибка. Коэффициенты αj задают структуру лага и называются весами. Конструкцию qj=0 αjzt−j часто называют "скользящим средним" переменной zt 10. Рассмотрим практические проблемы получения оценок коэффициентов αj в модели (11.18). Модель распределенного лага можно оценивать обычным ме-тодом наименьших квадратов, если выполнены стандартные предположения ре-грессионного анализа. В частности, количество лагов не должно быть слишком большим, чтобы количество регрессоров не превышало количество наблюдений, и все лаги переменной zt, т.е. zt−j (j = 0, . . . , q), не должны быть коррелированы с ошибкой εt. Одна из проблем, возникающих при оценивании модели распределенного лага, найти величину наибольшего лага q. При этом приходится начать с некоторого предположения, то есть взять за основу число Q, выше которого q быть не может. Выбор такого числа осуществляется на основе некоторой дополнительной инфор-мации, например, опыта человека, который оценивает модель. Можно предложить следующие способы практического определения величины q. 1) Для каждого конкретного q оценивается модель (11.18), и из нее берется t-статистика для последнего коэффициента, т.е. αq. Эти t-статистики рассматри-ваются в обратном порядке, начиная с q = Q (и заканчивая q = 0). Как только t-статистика оказывается значимой при некотором напередз аданном уровне, то следует остановиться и выбрать соответствующую величину q. 2) Следует оценить модель (11.18) при q = Q. Из этой регрессии берутся F-статистики для проверки нулевой гипотезы о том, что коэффициенты при по-следних Q − q + 1 лагах, т.е. αq, . . . , αQ, одновременно равны нулю: H0 : αj = 0, ∀j = q, . . . , Q. Соответствующие F-статистики рассчитываются по формулам: Fq = (RSSQ − RSSq−1)/(Q − q + 1) RSSQ/(T − Q − 2) , где RSSr-сумма квадратов остатков из модели распределенного лага при q = r, T -количество наблюдений. При этом при проведении расчетов для сопостави-мости во всех моделях надо использовать одни и те же наблюдения-те, которые использовались при q = Q (следовательно, при всех q используется одно и то же T ). Эти F-статистики рассматриваются в обратном порядке от q = Q до q = 0 (в последнем случае вмодели переменная z отсутствует). Как только F-статистика 10Другое часто используемое название-"линейный фильтр".
11.9. Условные распределения 377 оказывается значимой при некотором наперед заданном уровне, то следует оста-новиться и выбрать соответствующую величину q. 3) Для всех q от q = 0 до q = Q рассчитывается величина информационно-го критерия, а затем выбирается модель с наименьшим значением этого инфор-мационного критерия. Приведем наиболее часто используемые информационные критерии. Информационный критерий Акаике: AIC = ln(RSS T ) + 2(n + 1) T , где RSS сумма квадратов остатков в модели, T -фактически использовавшееся количество наблюдений, n - количество факторов в регрессии (не считая кон-станту). В рассматриваемом случае n = q+1, а T = T0 −q, гд е T0 -количество наблюдений при q = 0. Байесовский информационный критерий (информационный критерий Шварца): BIC = ln(RSS T ) + (n + 1) lnT T . Как видно из формул, критерий Акаике благоприятствует выбору более корот-кого лага, чем критерий Шварца. 11.9. Условные распределения Условные распределения играют важную роль в анализе временных рядов, осо-бенно при прогнозировании. Мы не будем вдаваться в теорию условных распре-делений, это предмет теории вероятностей (определения и свойства условных рас-пределений см. в Приложении A.3.1). Здесь мы рассмотрим лишь основные пра-вила, по которым можно проводить преобразования.При этом будем использовать следующее стандартное обозначение: если речь идет о распределении случайной величины X, условном по случайной величине Y (условном относительно Y ), то это записывается в виде X|Y . Основное правило работы с условными распределениями, которое следует за-помнить, состоит в том, что если рассматривается распределение, условное отно-сительно случайной величины Y , то с Y и ее функциями следует поступать так же, как с детерминированными величинами. Например, для условных математических ожиданий и дисперсий выполняется E(α(Y ) + β(Y )X|Y ) = α(Y ) + β(Y )E(X|Y ), var (α(Y ) + β(Y )X|Y ) = β2(Y )var(X|Y ).
378 Глава 11. Основные понятия в анализе временных рядов Как и обычное безусловное математическое ожидание, условное ожидание представляет собой линейный оператор. В частности, ожидание суммы есть сумма ожиданий: E(X1 + X2|Y ) = E(X1|Y ) + E(X2|Y ). Условное математическое ожидание E(X|Y ) в общем случае не является де-терминированной величиной, т.е. оно является случайной величиной, которая мо-жет иметь свое математическое ожидание, характеризоваться положительной дис-персией и т.п. Если от условного математического ожидания случайной величины X еще раз взять обычное (безусловное) математическое ожидание, то получится обычное (безусловное) математическое ожидание случайной величины X. Таким образом, действует следующее правило повторного взятия ожидания: E(E(X|Y )) = E(X). В более общей форме это правило имеет следующий вид: E(E(X|Y,Z)|Y ) = E(X|Y ), что позволяет применять его и тогда, когда второй раз ожидание берется не полно-стью, т.е. не безусловное, а лишь условное относительно информации, являющейся частью информации, относительно которой ожидание бралось первый раз. Если случайные величины X и Y статистически независимы, то распределе-ние X, условное по Y , совпадает с безусловным распределением X. След ова-тельно, для независимых случайных величин X и Y выполнено, в частности, E(X|Y ) = E(X), var(X|Y ) = var(X). 11.10. Оптимальное в среднеквадратическом смысле прогнозирование: общая теория 11.10.1. Условное математическое ожидание как оптимальный прогноз Докажем в абстрактном виде, безотносительно к моделям временных рядов, общее свойство условного математического ожидания, заключающееся в том, что оно минимизирует средний квадрат ошибки прогноза. Предположим, что строится прогноз некоторой случайной величины x на ос-нове другой случайной величины, z, и что точность прогноза при этом оценивается
11.10 Оптимальное в среднеквадратическом смысле прогнозирование 379 на основе среднего квадрата ошибки прогноза η = x − xp(z), гд е xp(z) -про-гнозная функция. Таким образом, требуется получить прогноз, который бы мини-мизировал Eη2= E(x − xp(z))2. Оказывается, что наилучший в указанном смысле прогноз дает математическое ожидание x, условное относительно z, т.е. E(x|z), которое мы будем обозначать ¯x(z). Докажем это. Возьмем произвольный прогноз xp(z) и представим ошибку прогноза в виде: x − xp(z) = η = (x − ¯x(z)) + (¯x(z) − xp(z)) . Найдем сначала математическое ожидание квадрата ошибки, условное относи-тельно z: Eη2|z= E(x − ¯x(z))2|z+ + 2E[(x − ¯x(z))(¯x(z) − xp(z))|z] + E(¯x(z) − xp(z)t)2|z. При взятии условного математического ожидания с функциями z можно обра-щаться как с константами. Поэтому E(¯x(z) − xp(z))2|z= (¯x(z) − xp(z))2 и E[(x − ¯x(z))(¯x(z) − xp(z))|z] = E(x − ¯x(z)|z) (¯x(z) − xp(z)) = = (¯x(z) − ¯x(z))(¯x(z) − xp(z)) = 0. Используя эти соотношения, получим Eη2|z= E(x − ¯x(z))2 |z+ (¯x(z) − xp(z))2 . Если теперь взять от обеих частей безусловное математическое ожидание, то (по правилу повторного взятия ожидания) получится Eη2= E(x − ¯x(z))2+ E(¯x(z) − xp(z))2. Поскольку второе слагаемое неотрицательно, то E(x − xp(z))2= Eη2≤ E(x − ¯x(z))2.
380 Глава 11. Основные понятия в анализе временных рядов Другими словами, средний квадрат ошибки прогноза достигает минимума при xp(z) = ¯x(z) = E(x|z). Оптимальный прогноз xp(z) = ¯x(z) = E(x|z) является несмещенным. Дей-ствительно, по правилу повторного взятия ожидания E(E(x|z)) = E(x) . Поэтому Eη = E(x − xp(z)) = E(x) − E(E(x|z)) = 0. 11.10.2. Оптимальное линейное прогнозирование Получим теперьформулу для оптимального (в смысле минимума среднего квад-рата ошибки) линейного прогноза. Пусть случайная переменная z, на основе кото-рой делается прогноз x, представляет собой n-мерный вектор: z = (z1, . . . , zn). Без потери общности можно предположить, что x и z имеют нулевое математи-ческое ожидание. Будем искать прогноз x в виде линейной комбинации zj : xp(z) = α1z1 + . . . + αnzn = zα, где α = (α1, . . . , αn)-вектор коэффициентов. Любой прогноз такого вида яв-ляется несмещенным, поскольку, как мы предположили, Ex = 0 и Ez = 0. Требуется решить задачу минимизации среднего квадрата ошибки (в данном случае это эквивалентно минимизации дисперсии ошибки): E(x − xp(z))2→ min α ! Средний квадрат ошибки можно представить в следующем виде: E(x − xp(z))2= Ex2 − 2αzx + αzzα= σ2x− 2αMzx + αMzzα, где σ2x= Ex2 -дисперсия x, Mzx = E[zx] -вектор, состоящий из ковариаций zj и x, а Mzz = E[zz] -ковариационная матрица z. (Напомним, что мы рас-сматриваем процессы с нулевым математическим ожиданием.) Дифференцируя по α, получим следующие нормальные уравнения: −2Mzx + 2Mzzα = 0, откуда α = M−1 zz Mzx.
11.10 Оптимальное в среднеквадратическом смысле прогнозирование 381 Очевидна аналогия этой формулы с оценками МНК, только матрицы вторых мо-ментов здесь не выборочные, а теоретические. Таким образом, оптимальный линейный прогноз имеет вид: xp(z) = zM−1 zz Mzx. (11.19) Ошибка оптимального линейного прогноза равна η = x − xp(z) = x − zM−1 zz Mzx. Эта ошибка некоррелирована с z, то есть с теми переменными, по которым делается прогноз. Действительно, умножая на z и беря математическое ожидание, получим E(zη) = Ezx − zzM−1 zz Mzx= Mzx −MzzM−1 zz Mzx, т.е. E(zη) = 0. Средний квадрат ошибки оптимального прогноза равен Eη2= E(x − xp(z))2= σ2x− 2MxzM−1 zz Mzx +MxzM−1 zz MzzM−1 zz Mzx. После преобразований получаем Eη2= σ2x−MxzM−1 zz Mzx. (11.20) Несложно увидеть аналогии между приведенными формулами и формулами МНК. Таким образом, данные рассуждения можно считать одним из возможных теоретических обоснований линейного МНК. Для того чтобы применить приведенные формулы, требуется, чтобы матрица Mzz была обратимой. Если она вырождена, то это означает наличие мультиколли-неарности между переменными z. Проблема вырожденности решается просто. Во-первых, можно часть "лиш-них" компонент z не использовать-оставить только такие, которые линейно независимы между собой. Во-вторых, в вырожденном случае прогноз можно по-лучить по той же формуле xp(z) = zα, взяв в качестве коэффициентов α любое решение системы линейных уравнений Mzzα = Mzx (таких решений будет беско-нечно много). Средний квадрат ошибки прогноза рассчитывается по формуле: Eη2= σ2x−Mxzα.
382 Глава 11. Основные понятия в анализе временных рядов В общем случае оптимальный линейный прогноз (11.19) не совпадает с услов-ным математическим ожиданием E(x|z). Другими словами, он не является опти-мальным среди всех возможных прогнозов. Пусть, например, z имеет стандартное нормальное распределение: z ∼ N(0, 1), а x связан с z формулой x = z2 − 1. Тогда, поскольку x и z некоррелированы, то α = 0, и оптимальный линейный прогноз имеет вид xp(z) = 0 при среднем квадрате ошибки прогноза равном E(z2 − 1)2= 2. В то же время прогноз по нелинейной формуле xp(z) = z2 − 1 будет безошибочным (средний квадрат ошибки прогноза равен 0). В частном случае, когда совместное распределение x и z является многомер-ным нормальным распределением: ⎛⎜⎝ xz ⎞⎟⎠∼ N ⎛⎜⎝ ⎛⎜⎝ 0 0n ⎞⎟⎠,⎛⎜⎝ σ2xMxz Mzx Mzz ⎞⎟⎠ ⎞⎟⎠, оптимальный линейный прогноз является просто оптимальным. Это связано с тем, что по свойствам многомерного нормального распределения (см. Приложение A.3.2) условное распределение x относительно z будет иметь следующий вид: x|z ∼ N zM−1 zz Mzx, σ2x−MxzM−1 zz Mzx. Таким образом, E(x|z) = zM−1 zz Mzx, что совпадает с формулой оптимального линейного прогноза (11.19). 11.10.3. Линейное прогнозирование стационарного временного ряда Пусть xt -слабо стационарный процесс с нулевым математическим ожида-нием. Рассмотрим проблему построения оптимального линейного прогноза этого процесса, если в момент t известны значения ряда, начиная с момента 1, т.е. толь-ко конечный ряд x = (x1, . . . , xt).Предположим, что делается прогноз на τ шагов вперед, т.е. прогноз величины xt+τ . Для получения оптимального линейного (по x) прогноза можно воспользоваться формулой (11.19). В случае стационарного временного ряда ее можно переписать в виде: xt(τ) = xΓ−1 t γt,τ , (11.21)
11.10 Оптимальное в среднеквадратическом смысле прогнозирование 383 где Γt = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ γ0 γ1 - - - γt−1 γ1 γ0 - - - γt−2 ... .... . . ... γt−1 γt−2 - - - γ0 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ -автоковариационная матрица ряда (x1, . . . , xt), а вектор γt,τ составлен из ковариаций xt+τ с (x1, . . . , xt), т.е. γt,τ = (γt+τ−1, . . . , γτ ). Можно заметить, что автоковариации здесь нужно знать только с точностью до множителя. Например, их можно заменить автокорреляциями. Рассмотрим теперь прогнозирование на один шаг вперед. Обозначим через γt вектор, составленный из ковариаций xt+1 с (x1, . . . , xt), т.е. γt = (γt, . . . , γ1)= = γt,1. Прогноз задается формулой: xt(1) = xΓ−1 t γt = xαt = t i=1 αtixt−i. Прогноз по этой формуле можно построить только если матрица Γt неособенная. Коэффициенты αti, минимизирующие средний квадрат ошибки прогноза, задаются нормальными уравнениями Γtα = γt или, в развернутом виде, t i=1 αtiγ|k−i| = γk, k = 1, . . . , t. Ошибка прогноза равна η = xt+1 − xt(1). Применив (11.20), получим, что средний квадрат этой ошибки равен Eη2= γ0 − γtΓ−1 t γt. Заметим, что γ0 − γtΓ−1 t γt = |Γt+1| /|Γt|, т.е предыдущую формулу можно пере-писать как Eη2= |Γt+1| /|Γt| . (11.22)
384 Глава 11. Основные понятия в анализе временных рядов Действительно, матрицу Γt+1 можно представить в следующей блочной форме: Γt+1 = ⎛⎜⎝ Γt γt γtγ0 ⎞⎟⎠. По правилу вычисления определителя блочной матрицы имеем: |Γt+1| = γ0 − γtΓ−1 t γt|Γt| . Если |Γt+1| = 0, т.е. если матрица Γt+1 вырождена, то средний квадрат ошибки прогноза окажется равным нулю, т.е. оптимальный линейный прогноз будет без-ошибочным. Процесс, для которого существует такой безошибочный линейный прогноз, называют линейно детерминированным. Укажем без доказательства следующее свойство автоковариационных матриц: если матрица Γt является вырожденной, то матрица Γt+1 также будет вырожден-ной.Отсюда следует, что на основе конечного отрезка стационарного ряда (x1, . . . , xt) можно сделать безошибочный линейный прогноз на один шаг впе-редв том и только в том случае, если автоковариационная матрица Γt+1 является вырожденной ( |Γt+1| = 0). Действительно, пусть существует безошибочный линейный прогноз. Возможны два случая: |Γt| = 0 и |Γt| = 0. Если |Γt| = 0, то средний квадрат ошибки прогноза равен |Γt+1| /|Γt|, откуд а |Γt+1| = 0, если же |Γt| = 0, то из этого также следует |Γt+1| = 0. Наоборот, если |Γt+1| = 0, тонайд ется s ( s t) такое, что |Γs+1| = 0, но |Γs| = 0. Тогда можно сделать безошибочный прогноз для xt+1 на основе (x1+t−s, . . . , xt), а, значит, и на основе (x1, . . . , xt). При использовании приведенных формул на практике возникает трудность, связанная с тем, что обычно теоретические автоковариации γk неизвестны. Тре-буется каким-то образом получить оценки автоковариаций. Обычные выборочные автоковариации ck здесь не подойдут, поскольку при больших k (сопоставимых с длиной ряда) они являются очень неточными оценками γk. Можно предложить следующий подход11: 1) Взять за основу некоторую параметрическуюмодель временного ряда.Пусть β -соответствующий вектор параметров. Рассчитать теоретические автоковари-ации для данной модели в зависимости от параметров: γk = γk(β). 11Этот подход, в частности, годится для стационарных процессов ARMA. В пункте 14.8 дается альтернативный способ прогнозирования в рамках модели ARMA.
11.10 Оптимальное в среднеквадратическом смысле прогнозирование 385 2) Оценить параметры на основе имеющихся данных. Пусть b -соответству-ющие оценки. 3) Получить оценки автоковариаций, подставив b в формулы теоретических автоковариаций: γk ≈ γk(b). 4) Использовать для прогнозирования формулу (11.21), заменяя теоретические автоковариации полученными оценками автоковариаций. 11.10.4. Прогнозирование по полной предыстории. Разложение Вольда Можно распространить представленную выше теорию на прогнозирование ря-д а вслучае, когд а вмомент t известна полная предыстория Ωt = (xt, xt−1, . . . ). Можно определить соответствующий прогноз как предел прогнозов, полученных на основе конечных рядов (xt, xt−1, . . . , xt−j ), j = t, t − 1, . . . ,−∞. Без доказа-тельства отметим, что этот прогноз будет оптимальным в среднеквадратическом смысле. Если рассматривается процесс, для которого |Γt| = 0∀t, то по аналогии с (11.22) средний квадрат ошибки такого прогноза равен Eη2= lim t→∞ |Γt+1| |Γt| . Заметим, что всегда выполнено 0 < |Γt+1| |Γt| ≤ |Γt| |Γt−1| , т.е. средний квадрат ошиб-ки не увеличивается с увеличением длины ряда, на основе которого делается про-гноз, и ограничен снизу нулем, поэтому указанный предел существует всегда. Существуют процессы, для которых |Γt| = 0 ∀t, т.е. для них нельзя сделать безошибочный прогноз, имея только конечный отрезок ряда, однако lim t→∞ |Γt+1| |Γt| = 0. Такой процесс по аналогии можно назвать линейно детерминированным. Его фак-тически можно безошибочно предсказать, если имеется полная предыстория про-цесса Ωt = (xt, xt−1, . . . ). Если же данный предел положителен, то линейный прогноз связан с ошибкой: Eη2> 0. Такой процесс можно назвать регулярным. Выполнены следующие свойства стационарных рядов. A. Пусть xt -слабо стационарный временной ряд, и пусть ηt -ошибки од -ношагового оптимального линейного прогноза по полной предыстории процесса (xt−1, xt−2, . . .). Тогд аошибки ηt являются белым шумом, т.е. имеют нулевое ма-
386 Глава 11. Основные понятия в анализе временных рядов тематическое ожидание, не автокоррелированы и имеют одинаковую дисперсию: E(ηt) = 0, ∀t, E(ηsηt) = 0, при s = t, Eη2= σ2η, ∀t. B. Пусть, кроме того, xt является регулярным, т.е. Eη2= σ2η > 0. Тогд аон представим в следующем виде:xt = ∞i=0 ψiηt−i + vt, (11.23) где ψ0 = 1, ∞i=0 ψ2i < ∞; процесс vt здесь является слабо стационарным, линейно детерминированным и не коррелирован с ошибками ηt: E(ηsvt) = 0 при ∀s, t. Такое представление единственно. Утверждения A и B составляют теорему Вольда. Эта теорема является одним из самых фундаментальных результатов в теории временных рядов. Утверждение B говорит о том, что любой стационарный процесс можно представить в виде так называемого линейного фильтра от белого шума12 плюс линейно детерминиро-ванная компонента. Это так называемое разложение Вольда. Доказательство теоремы Вольда достаточно громоздко.Мы не делаем попытки его излагать и даже обсуждать; отсылаем заинтересованных читателей к гл. 7 книги Т. Андерсона [2]. Заметим, что коэффициенты разложения ψi удовлетворяют соотношению ψi = E(ηt−ixt) Eη2t−i= E(ηt−ixt) σ2η . Для того чтобы это показать, достаточно умножить (11.23) на ηt−i и взять мате-матическое ожидание от обеих частей. Разложение Вольда имеет в своей основе прогнозирование на один шаг впе-ред. С другой стороны, если мы знаем разложение Вольда для процесса, то с по-мощью него можно делать прогнозы. Предположим, что в момент t делается прогноз на τ шагов вперед, т.е. прогноз величины xt+τ на основе предыстории Ωt = (xt, xt−1, . . . ). Сдвинем формулу разложения Вольда (11.23) на τ периодов вперед: xt+τ = ∞i=0 ψiηt+τ−i + τt+τ . 12Можно назвать первое слагаемое в 11.23 также процессом скользящего среднего бесконечного порядка MA(∞). Процессы скользящего среднего обсуждаются в пункте 14.4.
11.10 Оптимальное в среднеквадратическом смысле прогнозирование 387 Второе слагаемое, vt+τ , можно предсказать без ошибки, зная Ωt. Из первой суммы первые τ слагаемых не предсказуемы на основе Ωt. При прогнозирова-нии их можно заменить ожидаемыми значениями-нулями. Из этих рассуждений следует следующая формула прогноза: xt(τ) = ∞i=τ ψiηt+τ−i + τt+τ . Без доказательства отметим, что xt(τ ) является оптимальным линейным прогно-зом. Ошибка прогноза при этом будет равна τ−1 i=0 ψiηt+τ−i. Поскольку ηt -белыйшум с дисперсией σ2η, то средний квадрат ошибки про-гноза равен σ2η τ−1 i=0 ψ2i . Напоследок обсудим природу компоненты vt. Простейший пример линейно детерминированного ряда-это, говоря неформально, "случайная константа": vt = ξ, где ξ -напередзад анная случайная величина, Eξ = 0. Типичный случай линейно детерминированного ряда-это, говоря неформаль-но, "случайная синусоида": vt = ξ cos(ωt + ϕ), где ω -фиксированная частота, ξ и ϕ -независимые случайные величины, причем ϕ имеет равномерное распределение на отрезке [0; 2π]. Это два примера случайных слабо стационарных рядов, которые можно без-ошибочно предсказывать на основе предыстории. Первый процесс можно модели-ровать с помощью константы, а второй-с помощью линейной комбинации синуса и косинуса: α cos(ωt) + β sin(ωt). С точки зрения практики неформальный выводиз теоремы Вольда состоит в том, что любые стационарные временные ряды можно моделировать при помощи моделей линейного фильтра с добавлением констант и гармонических трендов.
388 Глава 11. Основные понятия в анализе временных рядов 11.11. Упражнения и задачи Упражнение 1 1.1. Дан временной ряд x = (5, 1, 1, −3, 2, 9, 6, 2, 5, 2). Вычислите среднее, дисперсию (смещенную), автоковариационную и авто-корреляционную матрицы. 1.2. Для временного ряда y = (6, 6, 1, 6, 0, 6, 6, 4, 3, 2)повторите упраж-нение 1.1. 1.3. Вычислите кросс-ковариации и кросс-корреляции для рядов x и y из преды-дущих упражнений для сдвигов −9, . . . , 0, . . . , 9. 1.4. Для временного ряда x = (7, −9, 10, −2, 21, 13, 40, 36, 67, 67) оцените параметры полиномиального тренда второго порядка. Постройте точечный и интервальный прогнозы по тренду на 2 шага вперед. 1.5. Сгенерируйте 20 рядов, задаваемых полиномиальным трендом третьего по-рядка τt = 5+4t−0.07t2+0.0005t3 длиной 100 наблюдений, с добавлением белого шума с нормальным распределением и дисперсией 20 . Допустим, истинные значения параметров тренда неизвестны. а) Для 5 рядов из 20 оцените полиномиальный трендпер вого, второго и третьего порядков и выберите модель, которая наиболее точно ап-проксимирует сгенерированные данные. б) Для 20 рядов оцените полиномиальный тренд третьего порядка по пер-вым 50 наблюдениям. Вычислите оценки параметров тренда и их ошиб-ки. Сравните оценки с истинными значениями параметров. в) Проведите теже вычисления, что и в пункте (б), для 20 рядов, используя 100 наблюдений. Результаты сравните. г) Используя предшествующие расчеты, найдите точечные и интерваль-ные прогнозы на три шага впередс уровнем доверия 95%. 1.6. Найдите данные о динамике денежного агрегатаM0 в России за 10 последо-вательных лет и оцените параметры экспоненциального тренда. 1.7. Ряд x = (0.02, 0.05, 0.06, 0.13, 0.15, 0.2, 0.31, 0.46, 0.58, 0.69, 0.78, 0.81, 0.95, 0.97, 0.98)характеризует долю семей, имеющих телевизор. Оцените параметры логиcтического тренда. 1.8. По ряду x из упражнения 3 рассчитайте ранговый коэффициент корреляции Спирмена и сделайте вывод о наличии тенденции.
11.11 Упражнения и задачи 389 Таблица 11.1 Расходы на рекламу 10 100 50 200 20 70 100 50 300 80 Объем продаж 1011 1030 1193 1149 1398 1148 1141 1223 1151 1576 1.9. Дан ряд: x = (10, 9, 12, 11, 14, 12, 17, 14, 19, 16, 18, 21, 20, 23, 22, 26, 23, 28, 25, 30). а) Оцените модель линейного тренда. Остатки, полученные после исклю-чения тренда, проверьте на стационарность с использованием рангового коэффициента корреляции Спирмена. б) Рассчитайте для остатков статистику Бартлетта, разбив ряд на 4 интер-вала по 5 наблюдений. Проверьте однородность выборки по дисперсии. в) Рассчитайте для остатков статистику Голдфельда-Квандта, исключив 6 наблюдений из середины ряда. Проверьте однородность выборки по дисперсии. Сравните с выводами, полученными на основе критерия Бартлетта. 1.10. По данным таблицы 11.1 оцените модель распределенного лага зависимости объема продаж от расходов на рекламу с лагом 2 . Определите величину максимального лага в модели распределенного лага, используя различные критерии ( t-статистики, F-статистики, информацион-ные критерии). Задачи 1. Перечислить статистики, использующиеся в расчете коэффициента автокор-реляции, и записать их формулы. 2. Чем различается расчет коэффициента автокорреляции для стационарных и нестационарных процессов? Записать формулы. 3. Вычислить значение коэффициента корреляции для двух рядов: x = (1, 2, 3, 4, 5, 6, 7, . . . ) и y = (2, 4, 6, 8, 10, 12, 14, . . . ). 4. Посчитать коэффициент автокорреляции первого порядка для ряда x = (2, 4, 6, 8). 5. Есть ли разница между автокорреляционной функцией и трендом автокорре-ляции?
390 Глава 11. Основные понятия в анализе временных рядов 6. Записать уравнения экспоненциального и полиномиального трендов и при-вести формулы для оценивания их параметров. 7. Записать формулу для оценки темпа прироста экспоненциального тренда. 8. Привести формулу логистической кривой и указать особенности оценивания ее параметров. 9. Оценить параметры линейного тренда для временного ряда x = (1, 2, 5, 6) и записать формулу доверительного интервала для прогноза на 1 шаг вперед. 10. Дан временной ряд: x = (1, 0.5, 2, 5, 1.5).Проверить его на наличие тренда среднего. 11. Пусть L -лаговый оператор. Представьте в виде степенного ряда следую-щие выражения: а) 2 1 − 0.8L; б) −1, 5 1 − 0.9L; в) 2.8 1 + 0.4L; г) −3 1 + 0.5L. Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т.2.-М.: "Юнити", 2001. 2. Андерсон Т. Статистический анализ временных рядов.-М.: "Мир", 1976. (Гл. 1, 3, 7). 3. Бокс Дж., Дженкинс Г. Анализ временных рядов. Прогноз и управление. Вып. 1.-М.: "Мир", 1974. (Гл. 1). 4. Кендалл М. Дж., Стьюарт А. Многомерный статистический анализ и вре-менные ряды.-М.: "Наука", 1976. (Гл. 45-47). Маленво Э.Статистические методы эконометрии. Вып. 2.-М.: "Статисти-ка", 1976. (Гл. 12). 5. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 12). 6. Enders Walter. Applied Econometric Time Series.-Iowa State University, 1995. (Ch. 1). 7. Mills Terence C. Time Series Techniques for Economists, Cambridge University Press, 1990 (Ch. 5). 8. Wooldridge JeffreyM. Introductory Econometrics:AModernApproach, 2nd ed., Thomson, 2003 (Ch. 10).
Глава 12 Сглаживание временного ряда 12.1. Метод скользящих средних Одним из альтернативных по отношению к функциональному описанию тренда вариантов сглаживания временного ряда является метод скользящих или, как еще говорят, подвижных средних. Суть метода заключается в замене исходного временного ряда последователь-ностью средних, вычисляемых на отрезке, который перемещается вдоль времен-ного ряда, как бы скользит по нему. Задается длина отрезка скольжения (2m + 1) по временной оси, т.е. берется нечетное число наблюдений. Подбирается полином τt = p k=0 aktk (12.1) к группе первых (2m + 1) членов ряда, и этот полином используется для опре-деления значения тренда в средней (m + 1)-й точке группы. Затем производится сдвиг на один уровень ряда вперед и подбирается полином того же порядка к группе точек, состоящей из 2-го, 3-го , . . . , (2m+2)-го наблюдения. Находится значение тренда в (m + 2)-й точке и т.д. тем же способом вдоль всего ряда до последней группы из (2m + 1) наблюдения. В действительности нет необходимости строить полином для каждого отрезка. Как будет показано, эта процедура эквивалентна нахождению линейной комбинации уровней временного ряда с коэффициентами,
392 Глава 12. Сглаживание временного ряда которые могут быть определены раз и навсегда и зависят только от длины отрезка скольжения и степени полинома. Для определения коэффициентов a0, a1, . . . , ap полинома (12.1) с помощью МНК по первым (2m + 1) точкам минимизируется функционал: ε = mt=−m(xt − a0 − a1t − . . . − aptp)2. (12.2) Заметим, что t принимает условные значения от −m до m. Это весьма удоб-ный прием, существенно упрощающийрасчеты. Дифференцирование функционала по a0, a1, . . . , ap дает систему из p + 1 уравнения типа: a0 mt=−mtj + a1 mt=−mtj+1 + a2 mt=−mtj+2 + - - - + ap mt=−mtj+p = mt=−mxttj , j = 0, 1, . . . , p. (12.3) Решение этой системы уравнений относительно неизвестных параметров a0, a1, . . . , ap (i = 0, 1, . . . , 2p) облегчается тем, что все суммы m t=−m ti при нечетных i равны нулю. Кроме того, т. к. полином, подобранный по 2m + 1 точ-кам, используется для определения значения тренда в средней точке, а в этой точке t = 0, то, положив в уравнении (12.1) t = 0, получаем значение тренда, равное a0. Стало быть, задача сглаживания временного ряда сводится к поиску a0. Система нормальных уравнений (12.3), которую нужно разрешить относитель-но a0, разбивается на две подсистемы: одну-содержащую коэффициенты с чет-ными индексами a0, a2, a4, . . ., другую-включающую коэффициенты с нечет-ными индексами a1, a3, a5, . . .. Решение системы относительно a0 зависит от чис-ленных значений m t=−m ti и линейных функций от x типа m t=−m xttj . В итоге, значением тренда в центральной точке отрезка будет средняя ариф-метическая, взвешенная из значений временного ряда от x−m до xm c весовыми коэффициентами βt, которые зависят от значений m и p: a0 = mt=−mβtxt. Указаннаяформула применяется для всех последующихотрезков скольжения, с вы-числением значений тренда в их средних точках. Продемонстрируем рассматриваемый метод на примере полинома второй степени и длины отрезка скольжения, равной пяти точкам. Здесь надо свести к минимуму сумму: ε = 2 t=−2(xt − a0 − a1t − a2t2)2.
12.1. Метод скользящих средних 393 Получается система уравнений: ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎩ 2 t=−2 a0 + a1 2 t=−2 t + a2 2 t=−2 t2 = 2 t=−2 xt, a0 2 t=−2 t + a1 2 t=−2 t2 + a2 2 t=−2 t3 = 2 t=−2 xtt, a0 2 t=−2 t2 + a1 2 t=−2 t3 + a2 2 t=−2 t4 = 2 t=−2 xtt2. Для конкретных значений сумм при ap система уравнений приобретает вид: ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎩ 5a0 + 10a2 = 2 t=−2 xt, 10a1 = 2 t=−2 xtt, 10a0 + 34a2 = 2 t=−2 xtt2. Решение этой системы относительно a0 дает следующий результат: a0 = 1 35 17 2 t=−2 xt − 5 2 t=−2 xtt2= 1 35 (−3x−2 + 12x−1 + 17x0 + 12x1 − 3x2) . Весовые коэффициенты для полиномов 2-5 степени и длины отрезка скольжения от 5 до 9 представлены в таблице 12.1. Таблица 12.1. Фрагмент таблицы Каудена для весов βt Длина отрезка скольже-ния Степени полинома 2m + 1 m p = 2, p = 3 p = 4, p = 5 5 2 1 35 (−3, 12, 17, 12,−3) 7 3 1 21 (−2, 3, 6, 7, 6, 3,−2) 1 231 (5,−30, 75, 131, 75,−30, 5) 9 4 1 231 (−21, 14, 39, 54, 59, 54, 39, 14,−21) 1 429 (15,−55, 30, 135, 179, 135, 30,−55, 15)
394 Глава 12. Сглаживание временного ряда Метод скользящих средних в матричной форме Введем следующие обозначения: 1. cj = 12 mt=−mxttj . Так как xt и tj известны, то cj также известно для каждого j = 0, 1, . . . , p. 2. ωi = 12 mt=−mti, i= 0, 1, . . . , 2p. Тогда ωi =⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎩ 0, если i-нечетно, 2m + 1 2 , если i = 0, 1i + 2i + . . . + mi, если i-четно. В таких обозначениях система (12.3) принимает вид: ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ ω0 ω1 - - - ωp ω1 ω2 - - - ωp+1 ... .... . . ... ωp ωp+1 - - - ω2p ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ a0 a1... ap ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ c0 c1... cp ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . В краткой записи эта система выглядит как Ma = c, где матрица M -известна, кроме того, ее элементы с нечетными индексами рав-ны нулю, вектор c также известен. Из полученной системы следуетa = M−1c. Теперь можно использовать формулу Крамера для нахождения ak: ak = detMk+1 detM ,
12.1. Метод скользящих средних 395 где матрица Mk+1 получается из матрицы M заменой (k + 1)-го столбца векто-ром c. Таким образом, a = detM1 detM , detM2 detM , . . . , detMp+1 detM . Рассмотрим частный случай, когда m = 2 и p = 2, т.е. временной рядаппр оксими-руется полиномом второй степени: τt = a0 + a1t + a2t2. Система уравнений, которую нужно решить относительно ak, имеет вид: a0 2 t=−2 tj + a1 2 t=−2 tj+1 + a2 2 t=−2 tj+2 = 2 t=−2 xttj , где x−2, x−1, x0, x1, x2 -известны, j = 0, . . . , p. Находим ωi : ωi =⎧⎪⎨ ⎪⎩ 0, если i-нечетно, 52, если i = 0, 1i + 2i, если i-четно. Тогда M = ⎛⎜⎜⎜⎜⎝ ω0 ω1 ω2 ω1 ω2 ω3 ω2 ω3 ω4 ⎞⎟⎟⎟⎟⎠ = ⎛⎜⎜⎜⎜⎝ 5/2 0 5 0 5 0 5 0 17⎞⎟⎟⎟⎟⎠ . Находим определители: detM = 25 - 7 2 detM1 = c0 0 5 c1 5 0 c2 0 17 = 5(17c0 − 5c2) = 52 17 2 t=−2 xt − 5 2 t=−2 xtt2, detM2 = 5/2 c0 5 0 c1 0 5 c2 17 = 35 2 c1 = 35 4 2 t=−2 xtt,
396 Глава 12. Сглаживание временного ряда detM3 = 5/2 0 c0 0 5 c1 5 0 c2 = 25c2 2 − c0= 25 2 12 2 t=−2 xtt2 − 2 t=−2 xt. Отсюда: a0 =detM1 detM = 1 35 17 2 t=−2 xt − 5 2 t=−2 xtt2, a1 =detM2 detM = 1 10 2 t=−2 xtt, a2 =detM3 detM = 1 14 2 t=−2 xtt2 − 17 2 t=−2 xt. Таким образом, a0 = − 3 35x−2 + 12 35x−1 + 17 35x0 + 12 35x1 − 3 35x2, a1 = −0, 2x−2 − 0, 1x−1 + 0, 1x1 + 0, 2x2, a2 = 17x−2 − 1 14x−1 − 17x0 − 1 14x1 + 17x2, и каждый из этих коэффициентов получается как взвешенная средняя из уровней временного ряда, входящих в отрезок. Оценки параметров a1, a2, . . . , ap необходимы для вычисления значений трен-д а в первых m и последних m точках временного ряда, поскольку рассмотренный способ сглаживания ряда через a0 сделать это не позволяет. Размерность матрицы M определяется степенью полинома: (p+1)×(p+1), пределы суммирования во всех формулах задаются длиной отрезка скольжения. Следовательно, для выбранных значений p и m можно получить общее решение в виде вектора (a0, a1, . . . , ap). Свойства скользящих средних 1. Сумма весов βt в формуле a0 = mt=−mβtxt равна единице. Действительно, пусть все значения временного ряда равны одной и той же константе c. Тогд аm t=−m βtxt = cm t=−m βt должна быть равна этой константе c, а это возможно только в том случае, если m t=−m βt = 1. 2. Веса симметричны относительно нулевого значения t, т.е. βt = β−t
12.1. Метод скользящих средних 397 Это следует из того, что весовые коэффициенты при каждом xt зависят от tj , а j принимает только четные значения. 3. Для полиномов четного порядка p = 2k формулы расчета a0 будут теми же самыми, что и для полиномов нечетного порядка p = 2k + 1. Пусть p = 2k +1, тогда матрица коэффициентов системы (12.3) при неизвест-ных параметрах a0, a1, . . . , ap будет выглядеть следующим образом: ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ m t=−m t0 m t=−m t - - - m t=−m t2k m t=−m t2k+1 m t=−m t m t=−m t2 - - - m t=−m t2k+1 m t=−m t2k+2 ... .... . . ... ... m t=−m t2k m t=−m t2k+1 - - - m t=−m t4k m t=−m t4k+1 m t=−m t2k+1 m t=−m t2k+2 - - - m t=−m t4k+1 m t=−m t4k+2 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Для нахождения a0 используются уравнения с четными степенями t при a0, следовательно, половина строк матрицы, включая последнюю, в расчетах участво-вать не будет. В этом блоке матрицы, содержащем коэффициенты при a0, a2, a4, . . . , по-следний столбец состоит из нулей, так как его элементы-суммы нечетных сте-пеней t. Таким образом, уравнения для нахождения a0 при нечетном значении p = 2k +1 в точности совпадают с уравнениями, которые надо решить для нахож-дения a0 при меньшем на единицу четном значении p = 2k: ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ m t=−m t0 m t=−m t2 - - - m t=−m t2k m t=−m t2 m t=−m t4 - - - m t=−m t2k+2 ... .... . . ... m t=−m t2k m t=−m t2k+2 - - - m t=−m t4k ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . 4.Оценки параметров a1, . . . , ap тоже выражены в виде линейной комбинации уровней временного ряда, входящих в отрезок, но весовые коэффициенты в этих формулах в сумме равны нулю и не симметричны. Естественным образом возникает вопрос, какой степени полином следует вы-бирать и какой должна быть длина отрезка скольжения. Закономерность такова: чем выше степень полинома и короче отрезок скольжения, тем ближе расчетные
398 Глава 12. Сглаживание временного ряда значения к первоначальным данным. При этом, помимо тенденции могут воспро-изводиться и случайные колебания, нарушающие ее смысл. И наоборот, чем ниже степень полинома и чем длиннее отрезок скольжения, тем более гладкой является сглаживающая кривая, тем в большей мере она отвечает свойствам тенденции, хотя ошибка аппроксимации будет при этом выше. В принципе, если ставится задача выявления тренда, то, с учетом особенностей покомпонентного разложения временного ряда, следует ориентироваться не на минимальнуюостаточнуюдисперсию, а на стационарность остатков,получающихся после исключения тренда. 12.2. Экспоненциальное сглаживание Кроме метода скользящей средней как способа фильтрации временного ряда известностью пользуется экспоненциальное сглаживание, в основе которого лежит расчет экспоненциальных средних. Экспоненциальная средняя рассчитывается по рекуррентной формуле: st = αxt + βst−1, (12.4) где st -значение экспоненциальной средней в момент t, α -параметр сглаживания (вес последнего наблюдения), 0 < α < 1, β = 1− α. Экспоненциальную среднюю, используя рекуррентность формулы (12.4), мож-но выразить через значения временного ряда: st = αxt + β(αxt−1 + βst−2) = αxt + αβxt−1 + β2st−2 = . . . = = αxt + αβxt−1 + αβ2xt−2 + . . . + αβjxt−j + . . . + αβt−1x1 + βts0 = = α t−1 j=0 βjxt−j + βts0, (12.5) t -количество уровней ряда, s0 -некоторая величина, характеризующая на-чальные условия для первого применения формулы (12.4) при t = 1. В качестве s0 можно использовать первое значение временного ряда, т.е. x1. Так как β < 1, то при t → ∞ величина βt → 0, а сумма коэффициентов α t−1 j=0 βj → 1. Действительно, α ∞j=0 βj = α 1 1 − β = (1 − β) 1 1 − β = 1.
12.2. Экспоненциальное сглаживание 399 Тогда последним слагаемым в формуле (12.5) можно пренебречь и st = α ∞j=0 βjxt−j = α ∞j=0(1 − α)jxt−j . Таким образом, величина st оказывается взвешенной суммой всех уровней ря-да, причем веса уменьшаются экспоненциально, по мере углубления в историю процесса, отсюда название-экспоненциальная средняя. Несложно показать, что экспоненциальная средняя имеет то же математиче-ское ожидание, что и исходный временной ряд, но меньшую дисперсию. Что касается параметра сглаживания α, то чем ближе α к единице, тем ме-нее ощутимо расхождение между сглаженным рядом и исходным. И наоборот, чем меньше α, тем в большей степени подавляются случайные колебания ряда и от-четливее вырисовывается его тенденция. Экспоненциальное сглаживание можно представить в виде фильтра, на вход которого поступают значения исходного вре-менного рядя, а на выходе формируется экспоненциальная средняя. Использование экспоненциальной средней в качестве инструмента выравнива-ния временного ряда оправдано в случае стационарных процессов с незначитель-ным сезонным эффектом. Однако многие процессы содержат тенденцию, сочета-ющуюся с ярко выраженными сезонными колебаниями. Довольно эффективный способ описания таких процессов-адаптивные се-зонные модели, основанные на экспоненциальном сглаживании. Особенность адаптивных сезонных моделей заключается в том, что по мере поступления новой информации происходит корректировка параметров модели, их приспособление, адаптация к изменяющимся во времени условиям развития процесса. Выделяют два вида моделей, которые можно изобразить схематично: 1. Модель с аддитивным сезонным эффектом, предложенная Тейлом и Вей-джем (Theil H.,Wage S.): xt = ft + gt + εt, (12.6) где ft отражает тенденцию развития процесса, gt, gt−1, . . . , gt−k+1 -аддитив-ные коэффициенты сезонности; k -количество опорных временных интервалов (фаз) в полном сезонном цикле; εt -белыйшум. 2. Модель с мультипликативным сезонным эффектом, разработанная Уин-терсом (Winters P.R.): xt = ft - mt - εt, (12.7) где mt, mt−1, . . . , mt−k+1 -мультипликативные коэффициенты сезонности.
400 Глава 12. Сглаживание временного ряда В принципе, эта модель после логарифмирования может быть преобразована в модель с аддитивным сезонным эффектом. Мультипликативные модели целесообразно использовать в тех ситуациях, ко-гда наряду, допустим, с повышением среднего уровня увеличивается амплитуда колебаний, обусловленная сезонным фактором. Если в аддитивных моделях индек-сы сезонности измеряются в абсолютных величинах, то в мультипликативных- в относительных. Ив том, и в другом случае обновление параметров модели производится по схе-ме экспоненциального сглаживания. Оба варианта допускают как наличие тенден-ции (линейной или экспоненциальной), так и ее отсутствие. Множество комбинаций различных типов тенденций с циклическими эффекта-ми аддитивного и мультипликативного характера можно представить в виде обоб-щенной формулы: ft = αfd1 + (1 − αf )d2, где ft -некоторый усредненный уровень временного ряда в момент t после устранения сезонного эффекта, αf -параметр сглаживания, 0 < αf < 1, d1 и d2 -характеристики модели. d1 =⎧⎪⎪⎪⎨ ⎪⎪⎪⎩ xt, -если сезонный эффект отсутствует, xt − gt−k, -в случае аддитивного сезонного эффекта, xt mt−k , -в случае мультипликативного сезонного эффекта. Таким образом, d1 представляет собой текущую оценку процесса xt, очищенную от сезонных колебаний с помощью коэффициентов сезонности gt−k или mt−k, рассчитанных для аналогичной фазы предшествующего цикла. d2 =⎧⎪⎨ ⎪⎩ ft−1, -при отсутствии тенденции, ft−1 + ct−1, -в случае аддитивного роста, ft−1 - rt−1, -в случае экспоненциального роста. В этой формуле ct−1 -абсолютный прирост, характеризующий изменение сред-него уровня процесса, или аддитивный коэффициент роста, rt−1 -коэффициент экспоненциального роста. Например, для модели с аддитивным ростом и мультипликативным сезонным эффектом подойдет график, изображенный на рисунке 12.1а, а для модели с экс-поненциальным ростом и аддитивным сезонным эффектом-график на рисунке 12.1б.
12.2. Экспоненциальное сглаживание 401 t a) б) xt Примеры графиков для некоторых типов адаптивных сезонных моделей Модель с аддитивным ростом и мультипликативным сезонным эффектом Модель с экспоненциальным ростом и аддитивным сезонным эффектом t xt Рис. 12.1. Графики некоторых типов временных рядов Адаптация всех перечисленных параметров осуществляется с помощью экспо-ненциального сглаживания: gt = αg(xt − ft) + (1 − αg)gt−k, mt = αmxt ft + (1 − αm)mt−k, ct = αc(ft − ft−1) + (1 − αc)ct−1, rt = αr ft ft−1 + (1 − αr)rt−1, где 0 < αg, αm, αc, αr < 1. Первые две формулы представляют собой линейную комбинацию текущей оценки коэффициента сезонности, полученной путем устранения из исходного уровня процесса значения тренда ( xt − ft и xt/ft), и оценки этого параметра на аналогичной фазе предшествующего цикла ( gt−k и mt−k). Аналогично, две последние формулы являются взвешенной суммой текущей оценки коэффициен-та роста (соответственно, аддитивного ft − ft−1 и экспоненциального ft/ft−1) и предыдущей его оценки ( ct−1 и rt−1). Очевидно, что в случае отсутствия тенденции и сезонного эффекта получается простая экспоненциальная средняя: ft = αfxt + (1 − αf )ft−1.
402 Глава 12. Сглаживание временного ряда Рассмотрим для иллюстрации модель Уинтерса с аддитивным ростом и мульти-пликативным сезонным эффектом: ft = αf xt mt−k + (1 − αf )(ft−1 + ct−1), mt = αmxt ft + (1 − αm)mt−k, (12.8) ct = αc(ft − ft−1) + (1 − αc)ct−1. Расчетные значения исследуемого показателя на каждом шаге, после обновле-ния параметров ft, mt и ct, получаются как произведение ft -mt. Прежде чем воспользоваться полной схемой экспоненциального сглаживания (12.8), а сделать это можно начиная с момента t = k + 1, необходимо получить начальные, отправные значения перечисленных параметров. Для этого с помощьюМНК можно оценить коэффициенты f1 и c1 регрессии: xt = f1 + c1t + εt, и на первом сезонном цикле (для t = 1, . . . , k) адаптацию параметров произвести по усеченному варианту:ft = αfxt + (1 − αf )ft−1, mt = xt ft, t= 1, . . . , k, ct = αc(ft − ft−1) + (1 − αc)ct−1, gt = xt − ft. Задача оптимизации модели сводится к поиску наилучших значений парамет-ров αf , αm, αc, выбор которых определяется целями исследования и характером моделируемого процесса. Уинтерс предлагает находить оптимальные уровни этих коэффициентов экспериментальным путем, с помощью сетки значений αf , αm, αc (например, (0, 1; 0, 1; 0, 1), (0, 1; 0, 1; 0, 2), . . . ). В качестве критерия сравне-ния вариантов рекомендуется стандартное отклонение ошибки. 12.3. Упражнения и задачи Упражнение 1 1.1. Сгенерируйте 20 рядов по 100 наблюдений на основе полиномиального тренда τt = 5+4t − 0, 07t2 + 0.0005t3 с добавлением белого шума с нор-мальным распределением и дисперсией 20 .
12.3. Упражнения и задачи 403 Таблица 12.2. Производство природного газа в СССР (миллиардов кубических футов) январь февраль март апрель май июнь июль август сентябрь октябрь ноябрь декабрь 1971 653.1 589.5 653.1 610.7 610.7 583.2 600.1 614.2 600.1 642.5 642.5 670.7 1972 670.8 649.5 695.4 664.5 638.9 621.3 620.7 619.4 624.8 653.1 663.6 706.0 1973 720.1 656.6 734.2 691.9 688.3 688.4 691.2 701.2 653.1 673.2 720.1 673.2 1974 720.1 709.5 776.6 737.7 741.3 723.7 724.6 758.9 760.0 808.4 811.2 882.5 1975 864.9 871.9 868.4 861.2 864.8 833.3 833.1 829.6 829.6 840.2 900.1 953.1 1976 953.1 914.3 967.2 921.3 917.8 916.8 924.9 924.9 917.8 988.4 974.3 1009.6 1977 1048.4 960.2 960.2 1048.4 998.9 956.6 984.9 995.5 999.0 1175.5 1180.0 1190.0 1978 1129.6 1129.4 1126.1 1076.7 1080.2 1034.3 1062.5 1064.7 1023.7 1147.2 1136.7 1196.8 1979 1230.0 1220.0 1220.0 1175.5 1182.5 1140.2 1157.8 1161.4 1164.9 1249.6 1250.6 1306.1 1980 1309.6 1232.0 1306.1 1246.1 1256.7 1200.2 1246.1 1260.2 1270.8 1270.0 1323.8 1376.7 1981 1419.1 1299.0 1420.0 1345.0 1313.0 1271.0 1270.0 1334.0 1334.0 1430.0 1430.0 1460.0 1982 1504.0 1380.0 1528.5 1436.7 1457.9 1412.0 1419.1 1436.7 1447.3 1546.1 1528.5 1623.8 1983 1627.4 1486.1 1652.0 1528.5 1570.8 1517.9 1514.4 1539.1 1482.6 1648.5 1648.5 1747.3 1984 1747.4 1648.5 1757.9 1680.3 1697.9 1623.8 1669.7 1697.9 1694.4 1821.5 1803.8 1870.9 1985 1930.9 1775.6 1941.5 1853.2 1892.1 1765.0 1825.0 1846.2 1870.9 1990.9 1962.7 2047.4 1986 2075.6 1895.6 2118.0 1983.9 2005.0 1906.2 1959.2 1969.7 1976.8 2103.9 2089.8 2188.6 1987 2221.3 2030.6 2210.7 2083.6 2118.9 2012.9 2048.3 2048.3 2083.6 2223.9 2259.2 2330.8 1988 2369.6 2221.3 2366.1 2224.8 2275.0 2146.6 2118.9 2189.5 2189.5 2357.0 2394.8 2447.7 1989 2510.0 2300.0 2391.0 2333.0 2336.0 2187.7 2208.0 2279.0 2200.0 2500.0 2484.0 2495.0 1990 2630.0 2400.0 2420.0 2391.0 2430.0 2250.0 2340.0 2340.0 2250.0 2500.0 2450.0 2460.0 
404 Глава 12. Сглаживание временного ряда а) Проведите сглаживание сгенерированных рядов с помощью полинома первой степени с длиной отрезка скольжения 5 и 9. б) Выполните то же задание, используя полином третьей степени. в) Найдите отклонения исходных рядов от сглаженных рядов, полученных в пунктах (а) и (б). По каждому ряду отклонений вычислите средне-квадратическую ошибку. Сделайте вывод о том, какой метод дает наи-меньшую среднеквадратическую ошибку. 1.2. Имеются данные о производстве природного газа в СССР (табл. 12.2). а) Постройте графики ряда и логарифмов этого ряда. Чем они различают-ся? Выделите основные компоненты временного ряда. Какой характер носит сезонность: аддитивный или мультипликативный?Сделайте вывод о целесообразности перехода к логарифмам. б) Примените к исходному ряду метод экспоненциального сглаживания, подобрав параметр сглаживания. в) Проведите сглаживание временного ряда с использованием адаптивной сезонной модели. Задачи 1. Сгладить временной ряд x = (3, 4, 5, 6, 7, 11), используя полином первого порядка с длиной отрезка скольжения, равной трем. 2. Записать формулу расчета вектора коэффициентов для полинома третьей степени с помощью метода скользящей средней в матричной форме с рас-шифровкой обозначений. 3. Вчем специфика аппроксимации первых m и последних m точек временного ряда при использовании метода скользящих средних? 4. Найти параметры адаптивной сезонной модели для временного ряда x = (1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, . . . ). 5. Изобразить график временного ряда с аддитивным ростом и мультиплика-тивным сезонным эффектом. 6. Изобразить график временного ряда с экспоненциальным ростом и аддитив-ным сезонным эффектом. 7. Записать модель с экспоненциальным ростом и мультипликативным сезон-ным эффектом, а также формулу прогноза на 5 шагов вперед.
12.3. Упражнения и задачи 405 Рекомендуемая литература 1. Андерсон Т. Статистический анализ временных рядов.-М.: "Мир", 1976. (Гл. 3). 2. Лукашин Ю.П. Адаптивные методы краткосрочного прогнозирования.- М.: "Статистика", 1979. (Гл. 1, 2). 3. Кендалл М. Дж., Стьюарт А. Многомерный статистический анализ и вре-менные ряды.-М.: "Наука", 1976. (Гл. 46). 4. Маленво Э.Статистические методы эконометрии. Вып. 2.-М.: "Статисти-ка", 1976. (Гл. 11, 12). 5. Mills Terence C. Time Series Techniques for Economists, Cambridge University Press, 1990 (Ch. 9).
Глава 13 Спектральный и гармонический анализ 13.1. Ортогональность тригонометрических функций и преобразование Фурье временного ряда Как известно, тригонометрические функции cos t и sin t являются периодиче-скими с периодом 2π:cos(t + 2π) = cos t, sin(t + 2π) = sint. Функции cos(λt − θ) и sin(λt − θ) периодичны с периодом 2π/λ. Действи-тельно, cos(λt − θ) = cos(λt +2π − θ) = cos (λ(t +2π/λ) − θ) , sin(λt − θ) = sin(λt + 2π − θ) = sin(λ(t + 2π/λ) − θ). Величина λ/2π, обратная периоду, называется линейной частотой, λ назы-вают угловой частотой. Линейная частота равна числу периодов (не обязательно целому), содержащемуся в единичном интервале, то есть именно такое число раз функция повторяет свои значения в промежутке [0, 1]. Рассмотрим функцию: Rcos(λt − θ) = R(cos λt cos θ + sinλt sin θ) = α cos(λt) + β sin(λt),
13.1 Ортогональность тригонометрических функций 407 где α = Rcos θ, β = Rsin θ или, что эквивалентно, R = :α2 + β2, tg θ = βα. Коэффициент R, являющийся максимумом функции Rcos(λt−θ) называется амплитудой этой функции, а угол θ называется фазой. Особенность тригонометрических функций заключается в том, что на опреде-ленном диапазоне частот они обладают свойством ортогональности. Две функции ϕ(t) и ψ(t), определенные на конечном множестве {1, . . . , T}, называются ортогональными, если их скалярное произведение, определенное как сумма произведений значений ϕ(t) и ψ(t) в этих точках, равно нулю: T t=1 ϕ(t) - ψ(t) = 0. Система T тригонометрических функций в точках t ∈ {1, . . . , T} ⎧⎪⎪⎨ ⎪⎪⎩ cjt = cos 2πj T t, j = 0, 1, . . . , T2 , sjt = sin 2πj T t, j = 1, . . . , T − 1 2 (13.1) ортогональна, т.е. скалярное произведение векторов (cj, ck) = T t=1 cjtckt = 0, j = k, 0 j, k T2 , (13.2) (sj, sk) = T t=1 sjtskt = 0, j = k, 0 < j, k T − 1 2 , (13.3) (cj, sk) = T t=1 cjtskt = 0, 0 j T2 , 0 < k T − 1 2 , (13.4) где операция [ . . . ]-это выделение целой части числа. Для доказательства этого утверждения полезны следующие равенства T t=1 cos 2πj T t = < 0, при j = 0, T, при j = 0, T , (13.5) T t=1 sin 2πj T t = 0, (13.6)
408 Глава 13. Спектральный и гармонический анализ истинность которых легко установить, выразив тригонометрические функции через показательные с использованием формул Эйлера: e±iγ = cosγ ± i sin γ, (13.7) cos γ = 12(eiγ + e−iγ), (13.8) sin γ = 1 2i (eiγ − e−iγ ). (13.9) Итак, при j = 0 T t=1 cos 2πj T t = 12 T t=1 ei 2πj T t + e−i 2πj T t= = 12 ei 2πj T 1 − ei2πj 1 − ei 2πj T + 12 e−i 2πj T 1 − e−i2πj 1 − e−i 2πj T = 0, где предпоследнее равенство получено из формулы суммы геометрической прогрес-сии, а последнее-из формулы (13.7), т.к. e±i2πj = cos(2πj) ± i sin(2πj) = 1. Очевидно, что при j = 0, T Tt=1 cos 2πj T t = T . Равенство (13.6) доказывается аналогично. При доказательстве соотношений (13.2-13.4) используются утверждения (13.5, 13.6). Таким образом, (cj, ck) = T t=1 cos 2πj T t-cos 2πk T t = 12 T t=1 cos 2π(j − k) T t+ 12 T t=1 cos 2π(j + k) T t = =⎧⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎩ 0, j= k, 0 j, k T2 , T2, j= k, 0 < j, k < T2 , T, j = k = 0, T2 (для четных T). (13.10) (sj, sk) = T t=1 sin 2πj T t - sin 2πk T t = 12 T t=1 cos 2π(j − k) T t− 12 T t=1 cos 2π(j + k) T t = =⎧⎪⎪⎪⎨ ⎪⎪⎪⎩ 0, j= k, 0 < j, k T − 1 2 , T2, j= k, 0 < j, k T − 1 2 . (13.11)
13.1 Ортогональность тригонометрических функций 409 (cj, sk) = T t=1 cos 2πj T t - sin 2πk T t = = 12 T t=1 sin 2π(j + k) T t + 12 T t=1 sin 2π(j − k) T t = 0. (13.12) Мы доказали выполнение (13.2-13.4) для указанного набора функций, полу-чив одновременно некоторые количественные их характеристики. Таким образом, функции cos 2πj T t и sin 2πj T t образуют ортогональный базис и всякую функцию, в том числе и временной ряд {xt}, определенный на множестве {1, . . . , T}, можно разложить по этому базису, т.е. представить в виде конечного ряда Фурье: xt = [T/2] j=0 αj cos 2πj T t + βj sin 2πj T t, (13.13) или, вспоминая (13.1), кратко xt = [T/2] j=0 (αjcjt + βjsjt) , где β0 и β[T/2] при четном T отсутствуют (т.к. sin 0 = 0, sin πt = 0). Величину 2πj/T = λj называют частотой Фурье, а набор скаляров αj и βj ( j = 0, 1, . . . , [T/2]) -коэффициентами Фурье. Если cjt и sjt -элементы векторов cj и sj , стоящие на t-ом месте, то, пе-реходя к векторным обозначениям, (13.13) можно переписать в матричном виде: x = C Sαβ, (13.14) где x = (x1, . . . , xT ), α = (α0, . . . , α[T/2]), β = (β1, . . . , β[(T−1)/2]), C = {cjt}, j= 0, 1, . . . , [T/2], t = 1, . . . , T, S = {sjt}, j= 1, . . . , [(T − 1)/2], t = 1, . . . , T.
410 Глава 13. Спектральный и гармонический анализ Перепишем в матричной форме свойства ортогональности тригонометрических функций, которые потребуются при вычислении коэффициентов Фурье: cjsk = 0, ∀j, k, ck1T = 0, ∀k = 0, sk1T = 0, ∀k, cjck = sjsk = 0, j= k, ckck = sksk = T/2, k= 0, T/2, c0c0 = T, cT/2cT/2 = T, для четных T, (13.15) где 1T = (1, . . . , 1)- T -компонентный вектор. Для нахождения коэффициентов Фурье скалярно умножим cj на вектор x и, воспользовавшись изложенными свойствами ортогональности (13.15), получим: cjx = cjC Sαβ= (cjc0, . . . , cjc[T/2], cjs1, . . . , cjs[(T−1)/2])αβ= = αjcjcj = T2 αj , для j = 0, T2 . Таким образом, αj = 2T cjx = 2T T t=1 xt cos 2πj T t, для j = 0, T2, α0 = 1T c0x = 1T T t=1 xt, (13.16) αT/2 = 1T cT/2x = 1T T t=1(−1)txt, для четных T. Аналогично находим коэффициенты βj : βj = 2T sjx = 2T T t=1 xt sin2πj T t. (13.17)
13.2. Теорема Парсеваля 411 13.2. Теорема Парсеваля Суть теоремыПарсеваля состоит в том, что дисперсия процесса xt разлагается по частотам соответствующих гармоник следующим образом: var(xt) = 12 T/2−1 j=1 R2j + R2T/2, для четных T, (13.18) var(xt) = 12 (T−1)/2 j=1 R2j , для нечетных T. (13.19) Покажем, что это действительно так. Из (13.14) мы имеем: xx = αβ⎛⎜⎝ CS⎞⎟⎠C S ⎛⎜⎝ αβ ⎞⎟⎠= = αβ⎛⎜⎝ CC CS SC SS ⎞⎟⎠ ⎛⎜⎝ αβ ⎞⎟⎠= = αβ⎛⎜⎝ ΛC 0 0 ΛS ⎞⎟⎠ ⎛⎜⎝ αβ ⎞⎟⎠= = αΛCα + βΛSβ = α201T 1T + [T/2] j=1 α2jcjcj + [(T−1)/2] j=1 β2j sjsj , где Λc и Λs -диагональные матрицы. Таким образом, если T -четно, то xx = α201T 1T + T/2 j=1 α2jcjcj + T/2−1 j=1 β2j sjsj = = α20T + T2 T/2−1 j=1 α2j+ T2 T/2−1 j=1 β2j +α2T/2T = α20T + T2 T/2−1 j=1 (α2j+β2j)+α2T/2T = = α20T + T2 T/2−1 j=1 R2j + α2T/2T = R20T + T2 T/2−1 j=1 R2j + R2T/2T. (13.20) Аналогично для нечетных T : xx = α201T 1T + (T−1)/2 j=1 α2jcjcj + (T−1)/2 j=1 β2j sjsj =
412 Глава 13. Спектральный и гармонический анализ = α20T + T2 (T−1)/2 j=1 α2j+ T2 (T−1)/2 j=1 β2j = = α20T + T2 (T−1)/2 j=1 (α2j+ β2j) = R20T + T2 (T−1)/2 j=1 R2j . (13.21) Разделим уравнения (13.20) и (13.21) на T и перенесем в левые части R20. С учетом того, что R20 = α20= ¯x2, получаем выражения для дисперсии процесса xt . var(xt) = xx T − R20 = 12 T/2−1 j=1 R2j + R2T/2, для четных T, (13.22) var(xt) = xx T − R20 = 12 (T−1)/2 j=1 R2j , для нечетных T . (13.23) Таким образом, вклад в дисперсию процесса для T/2-й гармоники равен R2T/2, а д ля k-й гармоники, k = T/2, равен 12R2k. Следовательно, наряду с определением коэффициентов Фурье для k-й гармо-ники, можно определить долю этой же гармоники в дисперсии процесса. 13.3. Спектральный анализ Введем понятия периодограммы и спектра. Периодограммой называют последовательность значений {Ij}: Ij = T2 (α2j+ β2j ), j= 0, 1, . . . , T2 , т.е. Ij равно квадрату амплитуды j-ой гармоники, умноженному на T2 , Ij = T2 R2j . Величина Ij называется интенсивностью на j-ой частоте. На практике естественнее при вычислении периодограммы использовать цен-трированный ряд ˆxt = xt − ¯x. При этом меняется только I0. Для центрированного ряда α0 = 0, поэтому I0 = α20= 0. Все остальные значения периодограммы не меняются, что следует из (13.5) и (13.6)-влияние константы на остальные значения обнуляется. В оставшейся части главы мы будем использовать только центрированный ряд. В определении периодограммы принципиальным является то, что гармониче-ские частоты fj = j/T (j = 0, 1, . . . , [T/2]) изменяются дискретно, причем наиболее высокая частота составляет 0, 5 цикла за временной интервал.
13.3. Спектральный анализ 413 Вводя понятие спектра, мы ослабляем это предположение и позволяем частоте изменяться непрерывно в диапазоне 0 − 0.5 Гц ( 0.5 цикла в единицу времени). Обозначим линейную частоту через f и введем следующие обозначения: αf = 2T T t=1 ˆxt cos(2πft), βf = 2T T t=1 ˆxt sin(2πft) и R2f = α2f+ β2f . Функция p∗(f) = T2 R2f = T2 α2f+ β2f= = 2T ⎛⎝T t=1 ˆxt cos(2πft)2 + T t=1 ˆxt sin(2πft)2⎞⎠, (13.24) где 0 f 12 , называется выборочным спектром. Очевидно, что значения пе-риодограммы совпадают со значениями выборочного спектра в точках fj, то есть p∗(fj) = Ij . Спектр показывает, как дисперсия стохастического процесса распределена в непрерывном диапазоне частот. Подобно периодограмме он может быть исполь-зован для обнаружения и оценки амплитуды гармонической компоненты неизвест-ной частоты f, скрытой в шуме. И периодограмму, и спектр представляют для наглядности в виде графика, на оси ординат которого-интенсивность Ij или p∗(f), на оси абсцисс-частота fj = jT или f , соответственно. График выборочного спектра часто называют спектрограммой. Спектрограмма нужна для более наглядного изображения распределения дис-персии между отдельными частотами. Если частоте f = kT соответствует пик на спектрограмме, то в исследуемом ряду есть существенная гармоническая состав-ляющая с периодом 1/f = Tk . Целью спектрального анализа является определение основных существенных гармонических составляющих случайного процесса путем разложения дисперсии процесса по различным частотам. Спектральный анализ позволяет исследовать смесь регулярных и нерегулярных спадов и подъемов, выделять существенные гармоники, получать оценку их периода и по значению спектра на соответствующих частотах судить о вкладе этих гармоник в дисперсию процесса.
414 Глава 13. Спектральный и гармонический анализ Исследования показывают, что наличие непериодического тренда (тренда с бес-конечным периодом) дает скачок на нулевой частоте, т.е. в начале координат спек-тральной функции. При наличии циклических составляющих в соответствующих частотах имеется всплеск; если рядсл ишком "зазубрен", мощность спектра пере-мещается в высокие частоты. Типичнымдля большинства экономических процессов является убывание спек-тральной плотности по мере того, как возрастает частота. Процесс выделения существенных гармоник - итеративный. При изучении периодограммы выделяется две-три гармоники с максимальной интенсивностью. Находятся оценки параметров этих наиболее существенных гармоник, и они удаля-ются из временного ряда с соответствующими весами. Затем остатки временного ряда, получающиеся после исключения значимых гармоник, снова изучаются в той же последовательности, т.е. строится периодограмма для этих остатков, и проявля-ются те гармоники, которые на начальном этапе были незаметны, и т.д. Количество итераций определяется задаваемой точностью аппроксимации модели процесса, которая представляется в виде линейной комбинации основных гармоник. Понятие спектра, являясь основополагающим в спектральном анализе, для экономистов играет важную роль еще и потому, что существует функциональная связь выборочного спектра и оценок автоковариационной функции. 13.4. Связь выборочного спектра с автоковариационнойфункцией Покажем, что выборочныйспектр представляет собой косинус-преобразование Фурье выборочной автоковариационной функции. Теорема Винера-Хинчина: p∗(f) = 2c0 + 2T−1 k=1 ck cos 2πfk= 2s21 + 2T−1 k=1 rk cos 2πfk, (13.25) где rk = ck/c0 = ck/s2 -выборочные автокорреляции. Доказательство. Объединим коэффициенты Фурье αf , βf в комплексное число df = αf − iβf , где i -мнимая единица. Тогда p∗(f) = T2 α2f+ β2f = T2 (αf − iβf) (αf + iβf) = T2 dfdf , (13.26) где df комплексно сопряжено с df .
13.4. Связь выборочного спектра с автоковариационной функцией 415 Используя формулы для αf и βf , получим: df = 2T T t=1 ˆxtcos 2πft − i sin 2πft= 2T T t=1 ˆxte−i2πft. Точно так же df = 2T T t=1 ˆxtei2πft. Подставляя полученные значения df и df в выражение (13.26), получаем: p∗(f) = T2 - 2T T t=1 ˆxte−i2πft- 2T T t=1 ˆxtei2πft= = 2T T t=1 T t=1 ˆxtˆxte−i2πf(t−t). (13.27) Произведем замену переменных: пусть k = t − t. Так как автоковариация равна ck = 1T T t=k+1 ˆxtˆxt−k, что тождественно ck = 1T T−k t=1 ˆxtˆxt+k, то выражение (13.27) преобразуется следующим образом: 2T T t=1 T t=1 ˆxtˆxte−i2πf(t−t) = 2T T−1 k=−(T−1) e−i2πfk T t=k+1 ˆxtˆxt−k = = 2 T−1 k=−(T−1) e−i2πfk - 1T T t=k+1 ˆxtˆxt−k = 2 T−1 k=−(T−1) e−i2πfkck. Тогда p∗(f) = 2 T−1 k=−(T−1) e−i2πfkck = 2 T−1 k=−(T−1) ckcos 2πfk − i sin 2πfk= = 2c0 + 2T−1 k=1 ck cos 2πfk, 0 f 12 .
416 Глава 13. Спектральный и гармонический анализ Теперь допустим, что выборочный спектр, характеризующийся эмпирическими значениями частоты, амплитуды и фазы, вычислен для ряда из T наблюдений и мы можем неоднократно повторить этот эксперимент, соответственно собрав множество значений αj , βj и p∗(f) по повторным реализациям. Тогда среднее значение p∗(f) будет равно: E(p∗(f)) = 2E(c0) + 2T−1 k=1 E(ck) cos 2πfk, (13.28) где c0 и ck -эмпирические значения автоковариации. C учетом того, что E(ck) при больших T стремится к теоретической автоковариации γk, получим, переходя к пределу, так называемую теоретическую спектральную плотность, или спектр мощности: p(f) = lim T→∞E(p∗(f)), 0 f 12 или p(f) = 2γ0 + 2 ∞k=1 γk cos(2πfk)= = 2σ21 + 2 ∞k=1 ρk cos(2πfk), (13.29) где ρk = γk/γ0 = γk/σ2 -теоретические автокорреляции. Итак, это соотношение связывает функцию спектральной плотности с теоре-тическими автоковариациями. Иногда более удобно использовать автокорреляции: разделим обе части p(f) на дисперсию процесса, σ2, и получим нормированный спектр g(f): g(f) = 21 +2 ∞k=1 ρk cos 2πfk, 0 f 12. Если процесс представляет собой белый шум, то, согласно приведенным фор-мулам, p(f) = 2σ2, т.е. спектральная плотность белого шума постоянна. Этим объясняется термин "белый шум". Подобно тому, как в белом цвете смешаны в одинаковых объемах все цвета, так и белый шум содержит все частоты, и ни одна из них не выделяется.
13.5. Оценка функции спектральной плотности 417 13.5. Оценка функции спектральной плотности На первый взгляд, выборочный спектр, определенный как p∗(f) = 2 T−1 k=−(T−1) cke−i2πfk = (13.30) = 2c0 +2T−1 k=1 ck cos 2πfk, 0 f 12, (13.31) является естественной и правильной оценкой функции спектральной плотности: p(f) = 2 +∞ k=−∞γke−i2πfk = 2γ0 + 2 ∞k=1 γk cos 2πfk, 0 f 12. (13.32) Известно, что выборочная автоковариация ck - это асимптотически несме-щенная оценка параметра γk, так как lim T→∞E(ck) = γk, и поэтому выборочный спектр есть также асимптотически несмещенная оценка функции спектральной плотности. Однако дисперсия оценки (выборочного спектра) не уменьшается по мере роста размера выборки. Это означает, что рассматриваемая оценка несостоятельна. В то время как график функции теоретической спектральной плотности стационарного стохастического процесса "гладкий",-график выборочного спектра, построен-ный на основе эмпирических данных, "неровный". Использование данной оценки в качестве оценки функции спектральной плотности может привести к ложным выводам. Поведение выборочного спектра иллюстрируют спектрограммы на рис. 13.1 а), б), в). Гладкая жирная линия соответствует теоретической спектральной плотно-сти случайного процесса, а неровная тонкая линия-оценке по формуле (13.24). Видно, что с увеличением длины ряда оценка не становится более точной, а только увеличивает частоту флуктуаций. Существует два подхода к решению проблемы несостоятельности выборочного спектра как оценки теоретического спектра. Оба заключаются в том, что выбороч-ный спектр сглаживается, так что за счет некоторого увеличения смещения этой оценки достигается существенное снижение дисперсии (см. рис. 13.1 г) ). Один подход сглаживает оценку спектра в "частотной области", видоизменяя формулу (13.24), другой же подход видоизменяет формулу (13.25).
418 Глава 13. Спектральный и гармонический анализ T = 100 T = 300 T = 1000 T = 1000, сглаженная оценка а) в) б)г) Рис. 13.1 1) Взвешивание ординат периодограммы. Первый способ сглаживания выборочного спектра применяет методск ользя-щего среднего, рассмотренный в предыдущей главе, к значениям периодограммы. Сглаживающая оценка определяется в форме ps jT = Mk=−M μk p∗ j − k T = Mk=−M μkIj−k. (13.33) Здесь {μ−M, . . . , μ−1, μ0, μ1, . . . , μM} - 2M+1 коэффициентов скользящего среднего, которые в сумме должны давать единицу, а также должны быть симмет-ричными, в смысле μ−k = μk. Как правило, коэффициент μ0 максимальный, а остальные коэффициенты снижаются в обе стороны. Таким образом, наиболь-ший вес в этой оценке имеют значения выборочного спектра, ближайшие к данной частоте jT , в связи с чем данный набор коэффициентов называют спектральным окном. Через это окно мы как бы "смотрим" на функцию спектральной плотности. Формула сглаженной спектральной оценки определяется только для значений j = M, . . . , [T/2] −M. Для гармонических частот с номерами j = 0, . . . , M − 1 и j = [T/2] − M + 1, . . . , [T/2] оценка не определена, поскольку при дан-ных значениях j величина (j − k) может принимать значения −M, . . . , −1; [T/2] + 1, . . . , [T/2] + M. Однако проблема краевых эффектов легко решается,
13.5. Оценка функции спектральной плотности 419 если доопределить функцию выборочного спектра (и, соответственно, периодо-грамму), сделав ее периодической. Формула (13.24) дает такую возможность, по-скольку основана на синусоидах и косинусоидах. Таким образом, будем считать, что выборочный спектр определен формулой (13.24) для всех частот f ∈ (−∞, +∞). При этом ясно, что выборочный спектр будет периодической функцией с перио-дом 1, зеркально-симметричной относительно нуля (и относительно любой часто-ты вида i/2, гд е i -целое число): p∗(f − i) = p∗(f), i = . . . , −1, 0, 1, . . . и p∗(−f) = p∗(f). Формулу (13.33) несложно обобщить так, чтобыможно было производить сгла-живание не только в гармонических частотах. Кроме того, в качестве расстояния между "усредняемыми" частотами можно брать не 1/T , а произвольное Δ > 0. Таким образом приходим к следующей более общей формуле: ps(f) = Mk=−M μk p∗(f − kΔ). (13.34) Просматривая поочередно значения выборочного спектра и придавая наиболь-ший вес текущему значению, можно сгладить спектр в каждой интересующей нас точке. 2) Взвешивание автоковариационных функций. Известно, что при увеличении лага k выборочные автоковариации ck явля-ются все более неточными оценками. Это связано с тем, что k-я автоковариация вычисляется как среднее по T −k наблюдениям. Отсюда возникает идея в формуле (13.25) ослабить влияние дальних автоковариаций за счет применения понижаю-щих весов так, чтобы с ростом k происходило уменьшение весового коэффициента при ck. Если рядв есов, связанных с автоковариациями c0, c1, . . . , cT−1, обозначить как m0,m1, . . . , mT−1, оценка спектра будет иметь вид: pc(f) = 2m0c0 + 2T−1 k=1 mkck cos 2πfk, где 0 f 12. (13.35) Набор весов mk, используемый для получения такой оценки, получил название корреляционное, или лаговое окно.
420 Глава 13. Спектральный и гармонический анализ При использовании корреляционного окна для уменьшения объемавычислений удобно брать такую систему весов, что mk = 0 при k K, гд еK < T. Тогд а формула (13.35) приобретает вид pc(f) = 2m0c0 + 2K−1 k=1 mkck cos 2πfk. (13.36) Корреляционное окно удобно задавать с помощью функции m(-), заданной на интервале [0; 1], такой, что m(0) = 1, m(1) = 0. Обычно функцию выбирают так, чтобы между нулем и единицей эта функция плавно убывала. Тогда понижающие веса mk при k = 0, . . . ,K вычисляются по формуле mk = m(k/K). Ясно, что при этом m0 = 1 (это величина, с помощью которой мы взвешиваем дисперсию в формуле (13.35)) и mK = 0. Наиболее распространенные корреляционные окна, удовлетворяющие пере-численным свойствам-это окна Парзена и Тьюки-Хэннинга (см. рис. 13.2). Окно Тьюки-Хэннинга:m(z) = 12 (1 + cos(πz)) . Окно Парзена:m(z) =⎧⎪⎨ ⎪⎩ 1 − 6z2 + 6z3, если z ∈ [0; 12 ]; 2(1 − z)3, если z ∈ [ 12; 1]. Можно доказать, что сглаживание в частотной области эквивалентно пони-жающему взвешиванию автоковариаций. Чтобы это сделать, подставим в (13.34) выборочный спектр, выраженный через комплексные экспоненты (13.30): ps(f) = 2 Mj=−M μj T−1 k=−(T−1) cke−i2π(f−jΔ)k. Меняя здесь порядок суммирования, получим ps(f) = 2 T−1 k=−(T−1) cke−i2πfk Mj=−M μjei2πjkΔ.
13.5. Оценка функции спектральной плотности 421 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 окно Парзена m(z) z окно Тьюки-Хэннинга 0 Рис. 13.2. Наиболее популярные корреляционные окна Введя обозначение mk = Mj=−M μjei2πjkΔ = μ0 + Mj=1 μj ei2πjkΔ + e−i2πjkΔ= μ0 +2 Mj=1 μj cos(2πjkΔ), придем к формуле ps(f) = 2 T−1 k=−(T−1) mkcke−i2πfk = = 2m0c0 + 2T−1 k=1 mkck cos 2πfk, которая, очевидно, совпадает с (13.35). Кроме того, без доказательства отметим, что подбором шага Δ и весов μj мы всегда можем сымитировать применение корреляционного окна (13.35) с помощью (13.34). Существует бесконечно много способов это сделать. В частности, как несложно проверить, частотное окно Тьюки-Хэннинга полу-чим, если возьмем M = 1, μ0 = 12 , μ1 = μ−1 = 14 и Δ = 1 2K.
422 Глава 13. Спектральный и гармонический анализ Особого внимания требует вопрос о том, насколько сильно нужно сглаживать спектральную плотность. Для корреляционных окон степень гладкости зависит от того, насколько быстро убывают понижающие веса. При фиксированной функ-ции m(-) все будет зависеть от параметра K -чемменьше K, тем более гладкой будет оценка. Для спектральных окон степень гладкости зависит от того, насколько близко "масса" коэффициентов μk лежит к той частоте, для которойвычисляется ошибка. При этом принято говорить о ширине окна, или ширине полосы. Если ширина окна слишкомбольшая, то произойдет"пересглаживание", оцен-ка будет сильно смещенной, и пики спектральной плотности станут незаметными. (В предельном случае оценка будет ровной, похожей на спектр белого шума.) Если ширина окна слишком малая, то произойдет "недосглаживание", и оценка будет похожа на исходную несглаженную оценку и иметь слишком большую дисперсию. Таким образом,ширина окна выбирается на основе компромисса между смещением и дисперсией. 13.6. Упражнения и задачи Упражнение 1 Сгенерируйте ряд длиной 200 по модели: xt = 10+0.1t + 4sin(πt/2) − 3 cos(πt/2) + εt, где εt -нормально распределенный белый шум с дисперсией 3. Предположим, что параметры модели неизвестны, а имеется только сгенерированный ряд xt. 1.1. Оцените модель линейного тренда и найдите остатки. Постройте график ря-да остатков, а также графики автокорреляционной функции и выборочного спектра. 1.2. Выделите тренд и гармоническую составляющую, сравните их параметры с истинными значениями. Упражнение 2 Для временного ряда, представленного в таблице 12.2 (с. 403), выполните следующие задания. 2.1. Исключите из временного ряда тренд.
13.6. Упражнения и задачи 423 2.2. Остатки ряда, получившиеся после исключения тренда, разложите в ряд Фу-рье. 2.3. Найдите коэффициенты αj и βj разложения этого ряда по гармоникам. 2.4. Постройте периодограмму ряда остатков и выделите наиболее существенные гармоники. 2.5. Постройте модель исходного временного ряда как линейную комбинацию модели тренда и совокупности наиболее значимых гармоник. 2.6. Вычислите выборочные коэффициенты автоковариации и автокорреляции для ряда остатков после исключения тренда. 2.7. Найдите значение периодограммы для частоты 0.5 разными способами (в том числе через автоковариационную функцию). Упражнение 3 Используя данные таблицы 12.2, выполните следующие задания. 3.1. С помощью периодограммы вычислите оценку спектра на частоте fj = j 2K, K = 4. Получите сглаженную оценку спектра с помощью спектрального окна Тьюки-Хэннинга. 3.2. Рассчитайте автокорреляционную функцию rj , j = 1, . . . , 4. Оцените спектр с помощью корреляционного окна Тьюки-Хэннинга при K = 4. Сравните с предыдущим результатом. 3.3. Оцените спектр с помощью корреляционного окна Парзена при K = 4. 3.4. Постройте график оценки спектра для корреляционного окна Тьюки- Хэннинга в точках fj = j 40 , j = 0, . . . , 20. Задачи 1. Записать гармонику для ряда x = (1,−1, 1,−1, 1,−1, . . . ). 2. Пусть временной ряд xt имеет гармонический тренд: τt = 3cos(πt) + + 4sin(πt). Найти значения амплитуды, фазы и периода. 3. Записать ортогональный базис, по которому разлагается исследуемый про-цесс xt в рядФурье для T = 6, T = 7.
424 Глава 13. Спектральный и гармонический анализ 4. Записать ковариационную матрицу для гармонических переменных, состав-ляющих ортогональный базис, если T = 6, T = 7. 5. Привести формулу расчета коэффициентов гармонических составляющих временного ряда. 6. Что описывает формула: I(f) = T2 (α2f+ β2f ), гд е 0 f 12? Почему f меняется в указанном диапазоне значений? 7. Как соотносятся понятия интенсивности и амплитуды,периодограммыи спек-тра? 8. Вывести формулу для определения периодограммы на нулевой частоте. 9. Как связаны выборочный спектр и автокорреляционная функция для чисто случайного процесса? Записать формулу с расшифровкой обозначений. 10. Пусть для ряда из 4-х наблюдений выборочная автокорреляционная функция равна: r1 = 1√2, r2 = 12, r3 = 1√2, дисперсия равна 1. Вычислить значение выборочного спектра на частоте 14. 11. Как соотносится выборочный спектр с автоковариационной функцией, спек-тральными и корреляционными окнами? 12. Пусть для ряда из 4-х наблюдений выборочная автокорреляционная функция равна: r1 = 12, r2 = 14, r3 = −14, дисперсия равна 1. Вычислить значение сглаженной оценкивыборочного спектра на частоте 14 спомощью окна Парзена с весами mk, k = 1, 2. 13. По некоторому временному ряду рассчитана периодограмма: I(0) = 2, I 16= 6, I 13= 1, I 12= 4. Найти оценки спектральной плотности для тех же частот с использованием окна Тьюки-Хэннинга. 14. Записать уравнение процесса с одной периодической составляющей для ча-стоты 0.33, амплитуд ы 2 и фазы 0. 15. Изобразить графики спектра для стационарных и нестационарных процессов.
13.6. Упражнения и задачи 425 Рекомендуемая литература 1. Андерсон Т. Статистический анализ временных рядов.-М.: "Мир", 1976. (Гл. 4, 9). 2. Бокс Дж., Дженкинс Г. Анализ временных рядов. Прогноз и управление. Вып. 1.-М.: "Мир", 1974. (Гл. 2). 3. Гренджер К., Хатанака М. Спектральный анализ временных рядов в эконо-мике.-М.: "Статистика", 1972. 4. Дженкинс Г., Ваттс Д. Спектральный анализ и его приложения. - М.: "Мир", 1971. 5. Кендалл М. Дж., Стьюарт А. Многомерный статистический анализ и вре-менные ряды.-М.: "Наука", 1976. (Гл. 49). 6. Маленво Э. Статистические методы эконометрии. Вып. 2. - М.: "Стати-стика", 1976. (Гл. 11, 12). 7. Бриллинджер Д. Временные ряды. Обработка данных и теория.-М.:Мир, 1980. (Гл. 5). 8. Hamilton JamesD., Time Series Analysis.-PrincetonUniversity Press, 1994. (Ch. 6). 9. JudgeG.G., GriffithsW.E., Hill R.C., Luthepohl H., Lee T. Theory and Practice of Econometrics.-New York: John Wiley & Sons, 1985. (Ch. 7).
Глава 14 Линейные стохастические модели ARIMA 14.1. Модель линейного фильтра Стационарный стохастический процесс {xt} с нулевым математическим ожи-данием иногда полезно представлять в виде линейной комбинации последователь-ности возмущений εt, εt−1, εt−2, . . ., т.е. xt = εt + ψ1εt−1 + ψ2εt−2 + . . . = ∞i=0 ψiεt−i, (14.1) или с использованием лагового оператора: xt = (1+ψ1L + ψ2L2 + - - - )εt, где ψ0 = 1 и выполняется ∞i=0 |ψi| < ∞, (14.2) т.е. рядаб солютных значений коэффициентов сходится. Уравнение (14.1) называется моделью линейного фильтра, а линейный опе-ратор: ψ(L) = 1+ψ1L + ψ2L2 + - - - = ∞i=0 ψiLi,
14.1. Модель линейного фильтра 427 преобразующий εt в xt,-оператором линейного фильтра. Компактная запись модели линейного фильтра выглядит следующим образом: xt = ψ(L)εt. Предполагается, что последовательность { εt } представляет собой чисто слу-чайный процесс или, другими словами, белый шум (см. стр. 353). Напомним, что автоковариационная и автокорреляционная функции белого шума имеют очень простую форму: γεk =⎧⎪⎨ ⎪⎩ σ2ε, k= 0, 0, k= 0, ρεk =⎧⎪⎨ ⎪⎩ 1, k= 0, 0, k= 0, а его спектральная плотность имеет вид pε(f) = 2γε0 = 2σ2ε = const. Таким образом, белыйшум легко идентифицируется с помощью графиков автокор-реляционной функции и спектра. Часто предполагается, что последовательность {εt} состоит из независимых одинаково распределенных величин. Упростить ана-лиз помогает дополнительное предположение о том, что {εt} имеет нормальное распределение, т.е. представляет собой гауссовский белый шум. Данная модель не является произвольной.Фактически, согласно теореме Воль-да, любой слабо стационарный ряд допускает представление в виде модели линей-ного фильтра, а именно: разложение Вольда ряда xt1. Следует помнить, однако, что разложение Вольда единственно, в то время как представление (14.1), вообще говоря, неоднозначно2. Таким образом, разложение Вольда представляет процесс в виде модели линейного фильтра, в то время как модель линейного фильтра не обя-зательно задает разложение Вольда. Как мы увидим в дальнейшем, модель линейного фильтра (14.1) применима не только к стационарным процессам, таким что выполняется (14.2),-с соответ-ствующими оговорками она упрощает анализ и многих нестационарных процессов. Если процесс {xt} подчинен модели (14.1), то при выполнении условия (14.2) он имеет математическое ожидание, равное нулю: E(xt) = ∞i=0 ψiE(εt−i) = 0. 1В разложении Вольда произвольного стационарного процесса может присутствовать также пол-ностью предсказуемая (линейно детерминированная) компонента. Однако такая компонента, если ее свойства известны, не создает больших дополнительных сложностей для анализа. 2См. ниже в этой главе анализ обратимости процесса скользящего среднего.
428 Глава 14. Линейные стохастические модели ARIMA Если требуется, чтобы математическое ожидание xt не было равно нулю, то урав-нение модели линейного фильтра должно включать константу: xt = μ + ∞i=0 ψiεt−i = μ + ψ(L)εt. Выведем формулы для автоковариаций рассматриваемой модели: γk = E(xtxt+k) = E⎛⎝ ∞i=0 ψiεt−i ∞j=0ψjεt+k−j⎞⎠ = = ∞i=0 ∞j=0ψiψjE(εt−iεt+k−j) = σ2ε ∞i=0 ψiψi+k. (14.3) Здесь учитывается, что для белого шума E(εt−iεt+k−j) =⎧⎪⎨ ⎪⎩ σ2ε, j = i + k, 0, j = i + k. Заметим, что из (14.2) следует сходимость возникающих здесь рядов. Это го-ворит о том, что данное условие подразумевает стационарность. Действительно, пусть (14.2) выполнено. Тогда существует индекс I, такой что |ψi| 1 при i > I (иначе бы рядне сошелся). Тогда ∞i=0 |ψiψi+k| ∞i=0 |ψi||ψi+k| I i=0 |ψi||ψi+k| + ∞i=I+1 |ψi+k|. Поскольку оба слагаемых здесь конечны, то ∞i=0 |ψiψi+k| < ∞. Ясно, что модель линейного фильтра (14.1) в общем виде представляет в основ-ном теоретический интерес, поскольку содержит бесконечное число параметров. Для прикладного моделирования желательно использовать уравнения с конечным числом параметров. В основе таких моделей может лежать так называемая рацио-нальная аппроксимация для ψ(L), т.е. приближение в виде частного двух лаговых многочленов: ψ(L) ≈ θ(L) ϕ(L) ,
14.1 Влияние линейной фильтрации . . . 429 где лаговые многочлены θ(L) и ϕ(L) имеют уже конечное число параметров. Как показывает практика, многие ряды можно достаточно хорошо аппроксимировать этим методом. Частными случаями применения рациональной аппроксимации являются мо-дели авторегрессии AR(p) и скользящего среднего MA(q). В общем случае полу-чаем смешанные процессы авторегрессии - скользящего среднего ARMA(p, q). Прежде чем перейти к рассмотрению этих широко используемых линейных моде-лей временных рядов, рассмотрим общий вопрос о том, как изменяет применение линейного фильтра характеристики случайного процесса. 14.2. Влияние линейной фильтрации на автоковариации и спектральную плотность Пусть два стационарных процесса c нулевым математическим ожиданием {zt} и {yt} связаны между собой соотношением: zt =j αjyt−j , т.е. zt получается применением к yt линейного фильтра α(L) =j αjLj . Пределы суммирования не указываем, поскольку они могут быть произвольными3. Пусть γyk -автоковариации процесса yt, а py(f) -его спектральная плот-ность. Найдем те же величины для zt . Автоковариации zt равны γzk = E(ztzt−k) = E⎛⎝j αjyt−js αsyt−k−s⎞⎠ = = j s αjαsE(yt−jyt−k−s) =j s αjαsγyk+s−j . Спектральную плотность zt можно записать в виде pz(f) = 2 ∞k=−∞γzkei2πfk. 3В том числе, при соответствующих предположениях, пределы суммирования могут быть беско-нечными. Кроме того, zt здесь может зависеть от опережающих значений yt .
430 Глава 14. Линейные стохастические модели ARIMA Подставим сюда формулы для ковариаций: pz(f) = 2 ∞k=−∞j s αjαsγyk+s−jei2πfk = = 2j s ⎛⎝αjαs ∞k=−∞γyk+s−jei2πfk⎞⎠. Произведем здесь замену k= k + s − j: pz(f) = 2j s ⎛⎝αjαs ∞k=−∞γykei2πf(k+j−s)⎞⎠ = = 2 ∞k=−∞γykei2πfk-j s αjei2πfjαse−i2πfs = = 2 ∞k=−∞γkei2πfk-j αjei2πfj -s αse−i2πfs. Первый множитель-это спектральная плотность yt. Два последних множи-теля представляют собой сопряженные комплексные числа, поэтому их произве-дение равно квадрату их модуля. Окончательно получим pz(f) = py(f)j αje−i2πfj2 (14.4) или pz(f) = py(f)⎛⎜⎝ ⎛⎝j αj cos 2πfj⎞⎠2 + ⎛⎝j αj sin 2πfj⎞⎠2⎞⎟⎠. (14.5) Данную теорию несложно применить к модели линейного фильтра (14.1). Для этого заменяем zt на xt, а yt на εt . Автоковариации xt равны γk = ∞i=0 ∞j=0ψiψjγεk+j−i. Поскольку γε0 = σ2ε, и γεk = 0 при k = 0, то γk = σ2ε ∞i=0 ψiψi+k,
14.3. Процессы авторегрессии 431 что совпадает с полученной ранее формулой (14.3). Для спектральной плотности из (14.4) получаем p(f) = 2σ2ε∞j=0ψje−i2πfj2. (14.6) Поскольку |e−i2πfj | = 1, то ряд здесь сходится и | p(f)| < ∞. 14.3. Процессы авторегрессии В модели авторегрессии текущее значение процесса xt представляется в виде линейной комбинации конечного числа предыдущих значений процесса и белого шума εt : xt = ϕ1xt−1 + ϕ2xt−2 + - - - + ϕpxt−p + εt, (14.7) при этом предполагается, что текущее значение εt не коррелировано с лагами xt. Такая модель называется авторегрессией p-го порядка и обозначается AR(p) (от английского autoregression). Используя лаговый оператор L, представим уравнение авторегрессии в виде: (1 − ϕ1L − ϕ2L2 − . . . − ϕpLp)xt = εt, или кратко, через лаговый многочлен ϕ(L) = 1 − ϕ1L − ϕ2L2 − . . . − ϕpLp: ϕ(L)xt = εt. Нетрудно показать, что модель авторегрессии является частным случаем моде-ли линейного фильтра: xt = ψ(L)εt, где ψ(L) = ϕ−1(L), т.е. ψ(L) -оператор, обратный оператору ϕ(L). Удобным и полезным инструментом для изучения процессов авторегрессии яв-ляется характеристический многочлен (характеристический полином) ϕ(z) = 1 − ϕ1z − ϕ2z2 − . . . − ϕpzp = 1− p j=1ϕjzj и связанное с ним характеристическое уравнение 1 − ϕ1z − ϕ2z2 − . . . − ϕpzp = 0.
432 Глава 14. Линейные стохастические модели ARIMA Как мы увидим в дальнейшем, от того, какие корни имеет характеристическое уравнение, зависят свойства процесса авторегрессии, в частности, является ли процесс стационарным или нет. Рассмотрим наиболее часто использующиеся частные случаи авторегрессион-ных процессов. ПроцессМаркова ПроцессомМаркова (марковским процессом) называется авторегрессионный процесс первого порядка, AR(1):xt = ϕxt−1 + εt, (14.8) где εt представляет собой белый шум, который не коррелирует с xt−1. Зд есь мы упростили обозначения, обозначив ϕ = ϕ1. Найдем необходимые условия стационарности марковского процесса. Предпо-ложим, что процесс {xt} слабо стационарен. Тогда его первые и вторые момен-ты неизменны. Находя дисперсии от обеих частей (14.8), получим, учитывая, что cov(xt−1, εt) = 0: var(xt) = ϕ2var(xt−1) + var(εt) или σ2x= ϕ2σ2x+ σ2ε . Ясно, что при |ϕ| 1, с учетом σ2ε > 0, правая часть этого равенства должна быть больше левой, что невозможно. Получаем, что у стационарного марковского процесса |ϕ| < 1. Пусть, с другой стороны, |ϕ| < 1. Представим xt через белый шум { εt }. Это можно осуществить с помощью последовательных подстановок по формуле (14.8): xt = ϕxt−1 + εt = ϕ(ϕxt−2 + εt−1) + εt = ϕ2xt−2 + ϕεt−1 + εt, потом xt = ϕ2(ϕxt−3 + εt−3) + ϕεt−1 + εt = ϕ3xt−3 + ϕ2εt−3 + ϕεt−1 + εt, и т.д . В пределе, поскольку множитель при лаге xt стремится к нулю, получим сле-дующее представление xt в виде модели линейного фильтра: xt = εt + ϕεt−1 + ϕ2εt−2 + . . . .
14.3. Процессы авторегрессии 433 Этоже представление можно получить с использованием оператора лага. Урав-нение (14.8) запишется в виде (1 − ϕL)xt = εt. Применив к обеим частям уравнения (1 − ϕL)−1, получим xt = (1 − ϕL)−1εt = (1+ϕL + ϕ2L2 + . . . )εt = εt + ϕεt−1 + ϕ2εt−2 + . . . . В терминах модели линейного фильтра (14.1) для марковского процесса ψi = ϕi. Поэтому ∞i=0 |ψi| = ∞i=0 |ϕ|i = 1 1 − |ϕ| < ∞, (14.9) т.е. условие стационарности модели линейного фильтра (14.2) выполняется при |ϕ| < 1. Можно сделать вывод, что условие стационарности процесса Маркова имеет следующий вид: |ϕ| < 1. Свойства стационарного процесса AR(1): 1) Если процесс xt слабо стационарен, то его математическое ожидание неиз-менно, поэтому, беря математическое ожидание от обеих частей (14.8), получим E(xt) = ϕE(xt), откуда E(xt) = 0. Если добавить в уравнение (14.8) константу: xt = μ + ϕxt−1 + εt, то E(xt) = μ + ϕE(xt) и E(xt) = μ 1 − ϕ. 2) Найдем дисперсию процесса Маркова, используя полученное выше уравне-ние σ2x= ϕ2σ2x+ σ2ε : var(xt) = γ0 = σ2x= σ2ε 1 − ϕ2 . (14.10)
434 Глава 14. Линейные стохастические модели ARIMA Можно также применить общую формулу для автоковариаций в модели линей-ного фильтра (14.3) с k = 0: γ0 = σ2x= σ2ε ∞i=0 ψ2i = σ2ε (1 + ϕ2 + ϕ4 + . . .) = σ2ε 1 − ϕ2 . При |ϕ| 1 дисперсия процесса {xt}, вычисляемая по этой формуле, неограни-ченно растет. 3) Коэффициент автоковариации k-го порядка по формуле (14.3) равен γk = σ2ε ∞i=0 ψiψi+k = σ2ε ∞i=0 ϕiϕi+k = = σ2ε ∞i=0 ϕ2i+k = σ2εϕk ∞i=0 ϕ2i = σ2ε 1 − ϕ2ϕk. Можно вывести ту же формулу иным способом. Ошибка εt некоррелирована не только с xt−1, но и с xt−2, xt−3 и т.д. Поэтому, умножая уравнение процесса (14.8) на xt−k при k > 0 и беря математическое ожидание от обеих частей, получим E(xtxt−k) = ϕE(xt−1xt−k) или γk = ϕγk−1, k>0. Таким образом, учитывая (14.10), получим γk = γ0ϕk = σ2ε 1 − ϕ2ϕk. (14.11) 4) Коэффициент автокорреляции, исходя из (14.11), равен: ρk = γk γ0 = ϕk. При 0 < ϕ < 1 автокорреляционная функция имеет форму затухающей экс-поненты (рис. 14.1а), при −1 < ϕ < 0 - форму затухающей знакопеременной экспоненты (рис. 14.1б). Если ϕ > 1, процессМаркова превращается во "взрывной" процесс. В случае ϕ = 1 имеет место так называемый процесс случайного блуждания, который относится к разряду нестационарных.
14.3. Процессы авторегрессии 435 0 < ϕ1 < 1 ρk k а) б) 1 ρk k 1 -1 −1 < ϕ1 < 0 Рис. 14.1 ПроцессЮла ПроцессомЮла называют авторегрессию второго порядка AR(2): xt = ϕ1xt−1 + ϕ2xt−2 + εt, (14.12) или, через лаговый оператор: (1 − ϕ1L − ϕ2L2)xt = εt. Для стационарности процесса авторегрессии AR(2) необходимо, чтобы корни λ1, λ2 характеристического уравнения 1 − ϕ1z − ϕ2z2 = 0, которые, вообще говоря, могут быть комплексными, находились вне единичного круга на комплексной плоскости, т.е. |λ1| > 1, |λ2| > 1. Неформально обоснуем условия стационарности AR(2), разложив характеристиче-ский полином ϕ(z) на множители (по теореме Виета λ1λ2 = −1ϕ2 ): ϕ(z) = −ϕ2(λ1 − z)(λ2 − z) = 1 − z λ1 1 − z λ2 = (1 − G1z)(1 − G2z), где мы ввели обозначения G1 = 1λ1 и G2 = 1λ2 . Такое разложение позволяет представить уравнение AR(2) в вид е: (1 − G1L)(1 − G2L)xt = εt. (14.13)
436 Глава 14. Линейные стохастические модели ARIMA Введем новую переменную: vt = (1 − G2L)xt, (14.14) тогда уравнение (14.13) примет вид: (1 − G1L)vt = εt. Видим, что ряд vt является процессомМаркова: vt = G1vt−1 + εt. Для того чтобы процесс vt был стационарным, коэффициент G1 по модулю должен быть меньше единицы, т.е. |λ1| > 1. С другой стороны, из (14.14) следует, что xt имеет видпр оцесса Маркова с ошиб-кой vt : xt = G2xt−1 + vt. Условие стационарности такого процесса имеет аналогичный вид: |λ2| > 1. Приведенные рассуждения не вполне корректны, поскольку vt не является белым шумом. (Укажем без доказательства, что из стационарности vt следует стационар-ность xt .) Кроме того, корни λ1, λ2 могут быть, вообще говоря, комплексными, что требует изучения свойств комплексного марковского процесса. Более коррект-ное обоснование приведенного условия стационарности будет получено ниже для общего случая AR(p). Условия стационарности процесса AR(2), |λ1| > 1, |λ2| > 1,можно переписать в эквивалентном виде как ограничения на параметры уравнения авторегрессии: ⎧⎪⎪⎨ ⎪⎪⎩ ϕ2 + ϕ1 < 1, ϕ2 − ϕ1 < 1, − 1 < ϕ2 < 1. (14.15) Проверим эти условия. Для этого рассмотрим два случая. 1) Пусть корни характеристического уравнения вещественные, то есть ϕ21+ 4ϕ2 0. Тогда для выполнения условий |λ1| > 1, |λ2| > 1 необходимым требованием является |λ1λ2| > 1 или − 1 ϕ2> 1. В таком случае один из корней обязательно лежит вне отрезка [−1, 1]. Для того чтобы и второй корень не попал в этот отрезок, необходимо и достаточно, чтобы значения характеристического по-линома ϕ(L) = 1 − ϕ1L − ϕ2L2 в точках −1 и 1 были одного знака. Это условие можно описать неравенством: ϕ(−1) - ϕ(1) > 0, или (1 + ϕ1 − ϕ2)(1 − ϕ1 − ϕ2) > 0.
14.3. Процессы авторегрессии 437 1 1 -1 -1 ϕ2 ϕ1 1 1 -1 -1 ϕ2 ϕ1 a) б) Рис. 14.2 Таким образом, случай вещественных корней описывается системой: ⎧⎪⎪⎨ ⎪⎪⎩ ϕ21+ 4ϕ2 0, |ϕ2| < 1, (1 + ϕ1 − ϕ2)(1 − ϕ1 − ϕ2) > 0. Если связать ϕ1 и ϕ2 с координатными осями, то область, соответствующуюданной системе, можно изобразить на рисунке (см. рис. 14.2а). 2) Если корни комплексные, то они имеют одинаковую абсолютную величину: |λ1| = |λ2| =  − 1 ϕ2 . И тогда вместе с отрицательностью дискриминанта достаточно условия: |ϕ2| < 1 (область решений см. на рис. 14.2б). Если объединить случаи 1) и 2), то общее решение как раз описывается системой неравенств (14.15) и соответствующая область на координатной плоскости пред-ставляет собой треугольник, ограниченный прямыми: ϕ2 = ϕ1 + 1, ϕ2 = −ϕ1 + 1, ϕ2 = −1. Автокорреляционная и автоковариационная функция AR(p) Для стационарного процесса авторегрессии: xt = ϕ1xt−1 + ϕ2xt−2 + - - - + ϕpxt−p + εt (14.16)
438 Глава 14. Линейные стохастические модели ARIMA можно вывести формулу автокорреляционной функции. Умножив обе части урав-нения на xt−k: xt−kxt = ϕ1xt−kxt−1 + ϕ2xt−kxt−2 + . . . + ϕpxt−kxt−p + xt−kεt, и перейдя к математическим ожиданиям, получим уравнение, связывающее коэф-фициенты автоковариации различного порядка: γk = ϕ1γk−1 + ϕ2γk−2 + . . . + ϕpγk−p, k>0. (14.17) Это выражение является следствием того, что соответствующие кросс-ковариации между процессом и ошибкой равны нулю: E(xt−kεt) = 0 при k > 0, т.к. xt−k может включать лишь ошибки εj для j t − k. Делением уравнения (14.17) на γ0 получаем важное рекуррентное соотноше-ние для автокорреляционной функции: ρk = ϕ1ρk−1 + ϕ2ρk−2 + . . . + ϕpρk−p, k>0. (14.18) Подставляя в выражение (14.18) k = 1, . . . , p, получаем, с учетом симметрич-ности автокорреляционной функции, так называемые уравнения Юла-Уокера (Yule-Walker) для AR(p): ⎧⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩ ρ1 = ϕ1 + ϕ2ρ1 + . . . + ϕpρp−1, ρ2 = ϕ1ρ1 + ϕ2 + . . . + ϕpρp−2, - - - ρp = ϕ1ρp−1 + ϕ2ρp−2 + . . . + ϕp. (14.19) Мы имеем здесь p линейных уравнений, связывающих p автокорреляций, ρ1, . . . , ρp.Из этой системы при данных параметрах можно найти автокорреляции. Сдругой стороны, при данных автокорреляциях из уравненийЮла-Уокера можно найти параметры ϕ1, . . . , ϕp. Замена теоретических автокорреляций выборочны-ми дает метод оценивания параметров процесса AR(p). В частности, для процесса Юла получим из (14.19) ρ1 = ϕ1 1 − ϕ2, ρ2 = ϕ21 1 − ϕ2 + ϕ2. Зная ρ1, . . . , ρp, все последующие автокорреляции ρk (k > p)можем найти по рекуррентной формуле (14.18). Для нахождения автоковариационной функции требуется знать γ0, дисперсию процесса xt. Если умножить обе части (14.16) на εt и взять математические ожи-дания, получим, что E(εtxt) = E(ε2t) = σ2ε . Далее, умножая обе части (14.16) на xt и беря математические ожидания, получим γ0 = ϕ1γ1 + ϕ2γ2 + . . . + ϕpγp + σ2ε .
14.3. Процессы авторегрессии 439 Значит, если известны автокорреляции, то дисперсию xt можно вычислять по формуле: γ0 = σ2x= σ2ε 1 − ϕ1ρ1 − ϕ2ρ2 − . . . − ϕpρp . (14.20) Автоковариации затем можно вычислить как γj = ρjσ2x. С учетом полученного дополнительного уравнения можно записать вариант уравнений Юла-Уокера для автоковариаций: ⎧⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎩ γ0 = ϕ1γ1 + ϕ2γ2 + . . . + ϕpγp + σ2ε , γ1 = ϕ1γ0 + ϕ2γ1 + . . . + ϕpγp−1, γ2 = ϕ1γ1 + ϕ2γ0 + . . . + ϕpγp−2, - - - γp = ϕ1γp−1 + ϕ2γp−2 + . . . + ϕpγ0. (14.21) В этой системе имеется p+1 уравнений, связывающих p+1 автоковариацию, что позволяет непосредственно вычислять автоковариации при данных параметрах. Заметим, что 14.17 и 14.18 имеют вид линейных однородных конечно-разностных уравнений, а для подобных уравнений существует общий метод на-хождения решения. Решив уравнение 14.18, можно получить общий видавток ор-реляционной функции процесса авторегрессии. Проведем это рассуждение более подробно для процесса Юла, а затем рассмотрим общий случай AR(p). Вывод формулы автокорреляционной функции процесса Юла Из соотношения (14.18) для p = 2 получаем: ρk = ϕ1ρk−1 + ϕ2ρk−2, k>0. (14.22) или, используя лаговый оператор, который в данном случае действует на k, ϕ(L)ρk = 0, гд еϕ(z) = 1 − ϕ1z − ϕ2z2 - характеристический полином про-цесса Юла. Как мы видели, этот характеристический полином можно представить в вид е: ϕ(z) = (1 − G1z)(1 − G2z), где G1 = 1λ1 и G2 = 1λ2, а λ1, λ2 -корни характеристического уравнения. Таким образом, ρk удовлетворяет уравнению: (1 − G1L)(1 − G2L)ρk = 0. (14.23)
440 Глава 14. Линейные стохастические модели ARIMA Найдем общее решение этого уравнения. Введем обозначение: ωk = (1 − G2L)ρk. (14.24) Для ωk имеем (1 − G1L)ωk = 0 или ωk = G1ωk−1. Поэтому ωk = G1ωk−1 = - - - = Gk−1 1 ω1. В свою очередь, из (14.24), с учетом того, что ρ0 = 1 и ρ1 = ϕ1(1 − ϕ2), следует, что ω1 = ρ1 − G2ρ0 = ϕ1 1 − ϕ2 − G2. Поскольку, по теореме Виета, G1 + G2 = ϕ1, G1G2 = −ϕ2, получаем выра-жение для ω1 через корни характеристического уравнения: ω1 = G1 + G2 1 + G1G2 − G2 = G1(1 − G22) 1 +G1G2 . (14.25) Возвращаясь к формуле (14.24), имеем, исходя из рекуррентности соотноше-ния: ρk = G2ρk−1 + ωk = G2(G2ρk−2 + ωk−1) + ωk = . . . = = Gk2+ Gk−1 2 ω1 + Gk−2 2 ω2 + . . . + G2ωk−1 + ωk = Gk2+ k−1 s=0Gk−1−s 2 ωs+1. Подставляя сюда ωk = Gk−1 1 ω1, получим ρk = Gk2+ ω1Gk−1 2 k−1 s=0 G1 G2s = Gk2+ ω1Gk−1 2 (G1/G2)k − 1 (G1/G2) − 1 = = Gk2+ ω1Gk1− Gk2 G1 − G2 = ω1 G1 − G2Gk1+ 1 − ω1 G1 − G2Gk2. Таким образом, общее решение уравнения (14.22) имеет вид: ρk = A1Gk1+ A2Gk2, (14.26) где коэффициенты A1 и A2 вычисляются по формулам: A1 = G1(1 − G22) (G1 − G2)(1 + G1G2), A2 = − G2(1 − G21) (G1 − G2)(1 + G1G2) , (14.27) причем A1 + A2 = 1.
14.3. Процессы авторегрессии 441 1 2 1 3 4 1 ϕ2 ϕ1 1 ρ 1 k ρk k 1 ρk kk Рис. 14.3 В стационарных процессах корни характеристического уравнения лежат вне единичного круга. Следовательно, |G1| < 1 и |G2| < 1, и автокорреляцион-ная функция состоит из совокупности затухающих экспонент, что на рисунке 14.3 соответствует областям 1, 2, 3 и 4, лежащим выше параболической границы ϕ21+ 4ϕ2 0. При этом, если оба корня положительны или доминирует по модулю от-рицательный корень (соответственно, положительное G), автокорреляционная функция затухает, асимптотически приближаясь к экспоненте (области 1 и 4 на рис. 14.3). Когда же оба корня отрицательны или доминирует по модулю поло-жительный корень (или отрицательное G), автокорреляционная функция затухает по экспоненте знакопеременно (области 2 и 3 на рис. 14.3). Если корни разного знака и совпадают по модулю, то затухание ρk происходит в области положительных значений, но имеет колебательный характер: G1 = −G2, следовательно A1 = A2 = 0.5 и ρk = ⎧⎪⎨ ⎪⎩ 0, если k-нечетное, Gk1, если k-четное. Рассмотрим случай, когда корни λ1 = G−1 1 и λ2 = G−1 2 -комплексные. Тогда автокорреляционная функция процесса авторегрессии второго порядка будет представлять собой затухающую синусоиду: ρk = dk- sin(kα + β) sin β ,
442 Глава 14. Линейные стохастические модели ARIMA где d = √−ϕ2, α= arccos ϕ1 2√−ϕ2, β= arctg1 + d2 1 − d2 tg α. Подтвердим справедливость формулы. Поскольку коэффициенты характеристи-ческого многочлена - действительные числа, то λ1 и λ2 - комплексно-сопряженные, поэтому G1 и G2 тожеявляются комплексно-сопряженными.Далее, G1, как и любое комплексное число, можно представить в виде: G1 = deiα = d cos α + i dsin α, где d = |G1| ( = |G2|)- мод уль, а α -аргумент комплексного числа G1. Соот-ветственно, для G2 как для сопряженного числа верно представление: G2 = de−iα = d cos α − i dsin α. Для нахождения d воспользуемся тем, что −ϕ2 = G1-G2 = deiα- de−iα = d2. Значит, d = √−ϕ2. Кроме того, cos α = eiα + e−iα 2 = G1 + G2 2d , поэтому, учиты-вая, что G1 + G2 = ϕ1, получаем: cos α = ϕ1 2√−ϕ2 . Подставим теперь G1 = deiα и G2 = de−iα в выражения (14.27) для A1 и A2: A1 = deiα(1 − d2e−2iα) (deiα − de−iα)(1 + d2eiαe−iα) = eiα − d2e−iα (eiα − e−iα)(1 + d2) = = (1 − d2) cos α + i(1 + d2) sinα 2i(1 + d2) sinα = 1 + i1+ d2 1 − d2 tg α 2i . Пусть β -такой угол, что tg β = 1 + d2 1 − d2 tg α. Тогда A1 = 1 + i tg β 2i = cos β + i sin β 2i sin β = eiβ eiβ − e−iβ . Отсюда найдем A2: A2 = 1− A1 = − e−iβ eiβ − e−iβ .
14.3. Процессы авторегрессии 443 Несложно увидеть, что A1 и A2 являются комплексно-сопряженными. Подставим найденные A1 и A2 в (14.26): ρk = A1Gk1+ A2Gk2= eiβ eiβ − e−iβ - dkeikα − e−iβ eiβ − e−iβ - dke−ikα = = dk ei(kα+β) − e−i(kα+β) eiβ − e−iβ . Таким образом, подтверждается, что автокорреляционную функцию можно записать в форме ρk = dk sin(kα + β) sin β . Нахождение автокорреляционной функции AR(p) с помощью решения конечно-разностного уравнения Аналогичным образом можно изучать автокорреляционную функцию процес-са AR(p) при произвольном p. Запишем уравнение (14.18) с помощью лагового оператора, действующего на k: ϕ(L)ρk = 0, k>0, (14.28) где ϕ(L) = 1 − ϕ1L − ϕ2L2 − . . . − ϕpLp. Рассмотрим характеристическое уравнение: ϕ(z) = 1 − ϕ1z − ϕ2z2 − . . . − ϕpzp = 0. Пусть λi (i = 1, . . . , p) -корни этого уравнения.Мыбудем предполагать, что все они различны. Характеристический многочлен ϕ(z) можно разложить следующим образом: ϕ(z) = −ϕp p i=1(λi − z). Обозначим через Gi значения, обратные корням характеристического урав-нения: Gi = 1λi . Тогда, учитывая, что λ1- λ2- . . . - λp = −1ϕp и соответственно G1-G2- . . . -Gp = −ϕp, имеем: ϕ(z) = −ϕp p i=1 1 Gi − z= −ϕp p i=1(1 − Giz) p i=1Gi = p i=1(1 − Giz).
444 Глава 14. Линейные стохастические модели ARIMA Исходя из этого, перепишем уравнение (14.28) в виде: (1 − G1L)(1 − G2L)- . . . - (1 − GpL)ρk = 0. (14.29) Из теории конечно-разностных уравнений известно, что если все корни λi различны, то общее решение уравнения (14.18) имеет вид: ρk = A1Gk1+ A2Gk2+ . . . + ApGkp, k>− p, (14.30) где Ai -некоторые константы, в общем случае комплексные. (Обсуждение ре-шений линейных конечно-разностных уравнений см. в Приложении A.4.) Проверим, что это действительно решение. Подставим ρk в (14.29): p i=1(1 − GiL)ρk = p i=1(1 − GiL)(A1Gk1+ A2Gk2+ . . . + ApGkp) = = p i=1(1 − GiL)A1Gk1+ p i=1(1 − GiL)A2Gk2+ . . . + p i=1(1 − GiL)ApGkp= 0. Для доказательства использовался тот факт, что LGkj= Gk−1 j и поэтому p i=1(1 − GiL)Gkj= i=j(1 − GiL)(1 − GjL)Gkj= 0. Формулы для коэффициентов A1, . . . , Ap можно получить из условий: ρ0 = 1, ρk = ρ−k, откуда p i=1 Ai = 1 и p i=1 AiGki= p i=1 Ai Gki, k= 1, . . . , p − 1. Другой способ состоит в том, чтобы вычислить ρ1, . . . , ρp−1 из уравнений Юла- Уокера (14.19), а затем составить на основе (14.30) при k = 0, . . . , p − 1 систему линейных уравнений, откуда и найти A1, . . . , Ap. Если все корни характеристического уравнения удовлетворяют условию |λi| > 1, то |Gi| < 1 ∀i и все слагаемые в (14.30) затухают с ростом k. Если же для какого-то корня выполнено |λi| < 1, то (при условии, что Ai = 0) соответ-ствующее слагаемое "уходит на бесконечность". Если |λi| = 1, то соответствую-щее слагаемое не затухает. Из этих рассуждений следует условие стационарности AR(p)-все корни соответствующего характеристического уравнения по модулю должны быть больше единицы.
14.3. Процессы авторегрессии 445 Если корень λi = G−1 i действителен, элемент AiGkiв (14.30) убывает с ро-стом k экспоненциально, коль скоро |λi| > 1. Если же есть пара комплексно-сопряженных корней λi = G−1 i , λj = G−1 j , то соответствующие коэффициен-ты Ai,Aj также будут сопряженными и в составе автокорреляционной функции появится экспоненциально затухающая синусоида (см. вывод автокорреляционной функции процесса Юла). Таким образом, из соотношения (14.30) следует, что в общем случае автокорре-ляционная функция стационарного процесса авторегрессии является комбинацией затухающих экспонент и затухающих синусоид. Итак, мы вывели общий видавток орреляционной функции стационарного процесса авторегрессии. Теоретически выборочная автокорреляционная функция может служить инструментом для распознавания авторегрессионого процесса. На практике же для коротких рядов различительная сила автокорреляционной функции не очень высока. Однако часто изучение автокорреляционной функции является хорошим заделом исследования системы. Кроме автокорреляционной функции важныминструментом для распознавания типа процесса является его спектр. Спектр стационарного процесса авторегрессии В главе 13 мы определили спектральную плотность стационарного процесса как косинус-преобразование Фурье автоковариационной функции (13.29): p(f) = 2γ0 + 2 ∞k=1 γk cos 2πfk. (14.31) Из этой общей формулы найдем спектральную плотность для стационарного процесса Маркова ( |ϕ1| < 1). Автоковариационнаяфункция этого процесса имеет вид: γk = σ2εϕk1 1 − ϕ21. Подставляя эти автоковариации в формулу (14.31), получим p(f) = 2σ2ε 1 − ϕ21 1 + 2 ∞k=1ϕk1cos 2πfk. Воспользовавшись представлением косинуса через комплексную экспоненту (формулами Эйлера)- 2 cos 2πfk = ei2πfk +e−i2πfk, после несложных преобра-
446 Глава 14. Линейные стохастические модели ARIMA зований получим: p(f) = 2σ2ε 1 − ϕ21 ∞k=0 ϕ1ei2πfk + ∞k=0 ϕ1e−i2πfk − 1, т.е. p(f) = 2σ2ε 1 − ϕ21 ∞k=0(ϕ1z)k + ∞k=0(ϕ1z−1)k − 1, где мы ввели обозначение z = ei2πf . По формуле бесконечной геометрической прогрессии p(f) = 2σ2ε 1 − ϕ21 1 1 − ϕ1z + 1 1 − ϕ1z−1 − 1. Произведение знаменателей двух дробей можно записать в разных формах: (1 − ϕ1z)(1 − ϕ1z−1) = 1 − ϕ1(z + z−1) + ϕ21= = 1− 2ϕ1 cos 2πf + ϕ21=1 − ϕ1e−i2πf2. Приведя к общему знаменателю, получим p(f) = 2σ2ε 1 − ϕ21 1 − ϕ1z−1 + 1 − ϕ1z − 1 +ϕ1(z + z−1) − ϕ21 (1 − ϕ1z)(1 − ϕ1z−1) = = 2σ2ε 1 − ϕ21 1 − ϕ21 (1 − ϕ1z)(1 − ϕ1z−1) . Таким образом, спектральная плотность марковского процесса равна p(f) = 2σ2ε 1 − 2ϕ1 cos 2πf + ϕ21, или, в другой форме, p(f) = 2σ2ε 1 − ϕ1e−i2πf2 . Ошибку εt авторегрессии произвольногопорядкаAR(p)можно выразить в виде линейного фильтра от yt: εt = xt − p j=1ϕjxt−j ,
14.3. Процессы авторегрессии 447 поэтому для вычисления спектра авторегрессионного процесса можно воспользо-ваться общей формулой (14.4), характеризующей изменение спектра при приме-нении линейного фильтра. Спектральная плотность белого шума εt равна 2σ2ε . Применение формулы (14.4) дает 2σ2ε = p(f)1 − p j=1ϕje−i2πfj2, откуда p(f) = 2σ2ε 1 − ϕ1e−i2πf − ϕ2e−i4πf −- - -−ϕpe−i2pπf2 . В частном случае процесса Юла формула спектральной плотности имеет вид: p(f) = 2σ2ε 1 − ϕ1e−i2πf − ϕ2e−i4πf2 = = 2σ2ε 1 + ϕ21+ ϕ22− 2ϕ1(1 − ϕ2) cos 2πf − 2ϕ2 cos 4πf . Разложение Вольда и условия стационарности процессов авторегрессии Как уже говорилось, модель AR(p) можно записать в виде модели линейного фильтра: xt = ∞i=0 ψiεt−i = ψ(L)εt, где ψ(L) = ϕ−1(L). Если процесс авторегрессии стационарен, то это разложение Вольда такого процесса. Найдем коэффициенты модели линейного фильтра ψi процесса авторегрессии. Для этого в уравнении ψ(L)ϕ(L) = ∞i=0 ψiLi1 − p j=1ϕjLj= 1
448 Глава 14. Линейные стохастические модели ARIMA приравняем коэффициенты при одинаковых степенях L.Получимследующие урав-нения: ψ0 = 1, ψ1 − ψ0ϕ1 = 0, ψ2 − ψ1ϕ1 − ψ0ϕ2 = 0, . . . ψp − ψp−1ϕ1 − . . . − ψ1ϕp−1 − ψ0ϕp = 0, ψp+1 − ψpϕ1 − . . . − ψ2ϕp−1 − ψ1ϕp = 0, . . . Общая рекуррентная формула имеет следующий вид: ψi = p j=1ϕjψi−j, i>0, (14.32) где ψ0 = 1 и ψi = 0 при i < 0. Это разностное уравнение, которое фактически совпадает с уравнением для автокорреляций (14.18). Соответственно, если все корни характеристического уравнения различны, то общее решение такого уравнения такое же, как указа-но в (14.30), т.е. ψi = B1Gi1 + B2Gi2 + . . . + BpGip, i>− p, где Bi - некоторые константы. Коэффициенты Bi можно вычислить, исходя из известных значений ψi при i 0. Очевидно, что если |Gj | < 1 ∀j, то все слагаемые здесь экспоненциально затухают, и поэтому ряд, составленный из коэффициентов ψi сходится абсолютно: ∞i=0 |ψi| < ∞. Таким образом, указанное условие гарантирует, что процесс авторегрессии явля-ется стационарным. Это дополняет вывод, полученный при анализе автокорреля-ционной функции. Итак, условием стационарности процесса AR(p) является то, что корни λi характеристического уравнения лежат вне единичного круга на комплексной плос-кости. Для процесса AR(2) имеем два уравнения для коэффициентов B1,B2: B1 G1 + B2 G2 = ψ−1 = 0, B1 + B2 = ψ0 = 1,
14.3. Процессы авторегрессии 449 откуда B1 = G1 G1−G2 и B2 = − G2 G1−G2 . Таким образом, в случае процесса Юла имеем ψi = Gi+1 1 − Gi+1 2 G1 − G2 , i>− 1, и xt = 1 G1 − G2 ∞i=0(Gi+1 1 − Gi+1 2 )εt−i. Оценивание авторегрессий Термин авторегрессия для обозначения модели (14.7) используется потому, что она фактически представляет собой модель регрессии, в которой регрессорами служат лаги изучаемого ряда xt. По определению авторегрессии ошибки εt явля-ются белым шумом и некоррелированы с лагами xt. Таким образом, выполнены все основные предположения регрессионного анализа: ошибки имеют нулевое ма-тематическое ожидание, некоррелированы с регрессорами, не автокоррелированы и гомоскедастичны. Следовательно, модель (14.7) можно оценивать с помощью обычного метода наименьших квадратов. Отметим, что при таком оценивании p начальных наблюдений теряются.Пусть имеется ряд x1, . . . , xT . Тогда регрессия в матричной записи будет иметь следую-щий вид:⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ xp+1 xp+2 ... xT ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ xp - - - x1 xp+1 - - - x2 ... ... xT−1 - - - xT−p ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ ⎛⎜⎜⎜⎜⎜⎝ ϕ1... ϕp ⎞⎟⎟⎟⎟⎟⎠ + ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ εp+1 εp+2 ... εT ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Как видим, здесь используется T − p наблюдений. Оценки МНК параметров авторегрессии равны ϕ = M−1m, гд еϕ = = (ϕ1, ϕ2, . . . , ϕp), аматрицы M и m, как нетрудно увидеть, фактически состоят из выборочных автоковариаций ряда xt. Отличие от стандартных выборочных ав-токовариаций состоит в том, что используются не все наблюдения. Можно рассматривать данную регрессию как решение уравнений Юла- Уокера для автоковариаций (14.21) (или, что эквивалентно, уравнений для автокор-реляций (14.19)), где теоретические автоковариации заменяются выборочными.
450 Глава 14. Линейные стохастические модели ARIMA Действительно, уравненияЮла-Уокера (14.21) без первой строки записываются в вид е γ = Γϕ, где γ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ γ1 γ2... γp ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , Γ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 γ1 γ2 - - - γp−1 γ1 1 γ1 - - - γp−2 ... ... .... . . ... γp−1 γp−2 γp−3 - - - 1 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , откуда ϕ = Γ−1γ. Замена теоретических значений автоковариаций γk выборочными автоковари-ациями ck позволяет найти параметры процесса авторегрессии. Ясно, что при этом можно использовать и стандартные формулы выборочных автоковариаций. Тем са-мым, мы получаем еще один из возможных методов оценивания авторегрессий- методмоментов. Частная автокорреляционная функция Как мы видели, автокорреляционная функция процесса авторегрессии состоит из экспоненциально затухающих компонент. Такая характеристика не очень на-глядна, поскольку соседние автокорреляции сильно связаны друг с другом, и, кроме того, для полного описания свойств ряда используется бесконечная последователь-ность автокорреляций. Более наглядными характеристиками авторегрессии являются частные ав-токорреляции. Частная автокорреляция измеряет "чистую" корреляцию между уровнями временного ряда xt и xt−k при исключении опосредованного влияния промежуточных уровней ряда. Такой показатель корреляции между элементами ряда более информативен. Пусть {xt} -произвольный стационарный ряд(не обязательно авторегрес-сия) и ρj - его автокорреляции. Применим к нему уравнения Юла-Уокера (14.19), как если бы процесс представлял собой авторегрессию k-го порядка, и найдем по автокорреляциям коэффициенты. Если обозначить j-й коэффици-ент уравнения авторегрессии порядка k через ϕkj , то уравнения Юла-Уокера
14.3. Процессы авторегрессии 451 принимают вид: ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 ρ1 ρ2 . . . ρk−1 ρ1 1 ρ1 . . . ρk−2 ρ2 ρ1 1 . . . ρk−3 ... ... .... . . ... ρk−1 ρk−2 ρk−3 . . . 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ ϕk1 ϕk2 ϕk3 ... ϕkk ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ ρ1 ρ2 ρ3... ρk ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Частная автокорреляция k-го порядка определяется как величина ϕkk, полученная из этих уравнений. Решение этих уравнений соответственно для k = 1, 2, 3 дает следующие результаты (здесь используется правило Крамера): ϕ11 = ρ1, ϕ22 = 1 ρ1 ρ1 ρ2 1 ρ1 ρ1 1 = ρ2 − ρ21 1 − ρ21, ϕ33 = 1 ρ1 ρ1 ρ1 1 ρ2 ρ2 ρ1 ρ3 1 ρ1 ρ2 ρ1 1 ρ1 ρ2 ρ1 1 . Частная автокорреляционная функция рассматривается как функция частной автокорреляции от задержки k, гд е k = 1, 2, . . .. Для процесса авторегрессии порядка p частная автокорреляционная функция {ϕkk} будет ненулевой для k p и равна нулю для k > p,то есть обрывается на задержке p. Значение выборочного частного коэффициента автокорреляции ϕkk вычисля-ется какМНК-оценка последнего коэффициента в уравнении авторегрессии AR(k). Частная автокорреляционная функция может оказаться полезной в решении задачи идентификации модели временного ряда: если она быстро затухает, то это авторегрессия, причем ее порядок следует выбрать по последнему большому зна-чению частной автокорреляционной функции.
452 Глава 14. Линейные стохастические модели ARIMA 14.4. Процессы скользящего среднего Другой частный случай модели линейного фильтра, широко распространенный в анализе временных рядов,-модель скользящего среднего, когд а xt линейно зависит от конечного числа q предыдущих значений ε: xt = εt − θ1εt−1 − θ2εt−2 − . . . − θqεt−q. (14.33) Модель скользящего среднего q-го порядка обозначают MA(q) (от английского moving average). Данную модель можно записать и более сжато: xt = θ(L)εt, через оператор скользящего среднего: θ(L) = 1 − θ1L − θ2L2 − . . . − θqLq. (14.34) Легко видеть, что процесс MA(q) является стационарным без каких-либо огра-ничений на параметры θj . Действительно, математическое ожидание процесса E(xt) = 0, а дисперсия γ0 = (1+θ21 + θ22 + . . . + θ2q )σ2ε , т.е. равна дисперсии белого шума, умноженной на конечную величину (1 + θ21 + + θ22 + . . . + θ2q ). Остальные моменты второго порядка (γk, ρk) также от времени не зависят. Автоковариационная функция и спектр процесса MA(q) Автоковариационная функция MA(q) γk =⎧⎪⎨ ⎪⎩ (−θk + θ1θk+1 + . . . + θq−kθq)σ2ε, k= 1, 2, . . . , q, 0, k> q. (14.35) В частном случае для MA(1) имеем: γ0 = (1+θ21)σ2ε , γ1 = −θ1σ2ε , γk = 0, k>1,
14.4. Процессы скользящего среднего 453 и автоковариационная матрица, соответствующая последовательности x1, x2, . . . , xT , будет иметь следующий трехдиагональный вид: Γ = σ2ε ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 + θ21 −θ1 0 - - - 0 −θ1 1 + θ21 −θ1 - - - 0 0 −θ1 1 + θ21 - - - 0 ... ... .... . . ... 0 0 0 - - - 1 + θ21⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . В общем случае автоковариационная матрица процесса скользящего среднего порядка q имеет q ненулевых поддиагоналей и q ненулевых наддиагоналей, все же остальные элементы матрицы равны нулю. Автокорреляционная функция имеет вид: ρk =⎧⎪⎪⎨ ⎪⎪⎩ −θk + θ1θk+1 + . . . + θq−kθq 1 + θ21 + . . . + θ2q , k= 1, 2, . . . , q, 0, k> q. (14.36) Таким образом, автокорреляционная функция процесса MA(q) обрывается на задержке q, и в этом отличительная особенность процессов скользящего среднего. С другой стороны, частная автокорреляционная функция, в отличие от авторе-грессий, не обрывается и затухает экспоненциально. Например, для MA(1) частная автокорреляционная функция имеет вид −θp1 1 − θ21 1 − θ2p+2 1 . Ясно, что модель скользящего среднего является частным случаем модели ли-нейного фильтра (14.1), где ψj = −θj при j = 1, . . . , q и ψj = 0 при j > q. Фактически модель линейного фильтра является моделью MA(∞). Формула спектра для процесса скользящего среднего следует из общей форму-лы для модели линейного фильтра (14.6): p(f) = 2σ2ε1 − θ1e−i2πf − θ2e−i4πf − . . . − θqe−i2qπf2.
454 Глава 14. Линейные стохастические модели ARIMA Соответственно, для MA(1): p(f) = 2σ2ε1 − θ1e−i2πf2 = 2σ2ε (1 + θ21 − 2θ1 cos 2πf); для MA(2):p(f) = 2σ2ε1 − θ1e−i2πf − θ2e−i4πf2 = = 2σ2ε1 + θ21 + θ22 − 2θ1(1 − θ2) cos 2πf − 2θ2 cos 4πf. Обратимость процесса MA(q) Авторегрессию, как мы видели выше, можно представить как MA(∞). С другой стороны, процесс скользящего среднего можно представить в виде AR(∞). Рассмотрим, например, MA(1) (будем для упрощения писать θ вместо θ1): xt = εt − θεt−1, (14.37) Сдвигом на один период назад получим εt−1 = xt−1 +θεt−2 и подставим в (14.37): xt = εt − θxt−1 − θ2εt−2. Далее, εt−2 = xt−2 + θεt−3, поэтому xt = εt − θxt−1 − θ2xt−2 − θ3εt−3. Продолжая, получим на k-м шаге xt = εt − θxt−1 − θ2xt−2 −- - -−θkxt−k − θk+1εt−k−1. Если |θ| < 1, то последнее слагаемое стремится к нулю при k → ∞. Переходя к пределу, получаем представление AR(∞) для MA(1): xt = − ∞j=1 θjxt−j + εt. (14.38) С помощью лагового оператора можем записать это как π(L)xt = εt, где π(L) = (1 − θL)−1 = ∞j=0 θjLj .
14.4. Процессы скользящего среднего 455 В то время как процесс (14.37) стационарен при любом θ, процесс (14.38) стационарен только при |θ| < 1. При |θ| 1 веса −θj в разложении (14.38) растут (при |θ| = 1 не меняются) по абсолютной величине по мере увеличения j. Тем самым, нарушается разумная связь текущих событий с событиями в прошлом. Говорят, что при |θ| < 1 процесс MA(1) является обратимым, а при |θ| 1 - необратимым. В общем случае уравнение процесса MA(q) в обращенной форме можно запи-сать как εt = θ−1(L)xt = π(L)xt = ∞j=0 πjxt−j . Процесс MA(q) называется обратимым, если абсолютные значения весов πj в об-ращенном разложении образуют сходящийся ряд. Стационарным процесс MA(q) является всегда, но для того, чтобы он обладал свойством обратимости, параметры процесса должны удовлетворять определенным ограничениям. Выведем условия, которым должны удовлетворять параметры θ1, θ2, . . . , θq процесса MA(q), чтобы этот процесс был обратимым. Пусть H−1 i , i = 1, . . . , q -корни характеристического уравнения θ(L) = 0 (будем предполагать, что они различны). Оператор скользящего среднего θ(L) через обратные корни характеристического уравнения можно разложить на мно-жители: θ(L) = q i=1(1 − HiL). Тогда обратный к θ(L) оператор π(L) можно представить в следующем виде: π(L) = θ−1(L) = 1 q i=1(1 − HiL) = q i=1 Mi 1 − HiL. (14.39) Каждое слагаемое (14.39) можно, по аналогии с MA(1), представить в виде беско-нечного ряда: Mi 1 − HiL = Mi ∞j=0Hji Lj, i = 1, . . . , q, который сходится, если |Hi| < 1.
456 Глава 14. Линейные стохастические модели ARIMA Тогда процесс MA(q) в обращенном представлении выглядит как εt = q i=1Mi ∞j=0Hji Ljxt, и он стационарен, если корни характеристического уравнения θ(L) = 0 лежат вне единичного круга. Иными словами, MA(q) обладает свойством обратимости, если для всех корней выполнено |H−1 i | > 1, т.е. |Hi| < 1 ∀i. Если же для одного из корней |Hi| 1, то ряд не будет сходиться, и процесс MA(q) будет необратимым. Для каждого необратимого процесса MA(q), у которого корни характеристи-ческого уравнения не равны по модулю единице, существует неотличимый от него обратимый процесс того же порядка. Например, процесс MA(1) (14.37) с |θ| >1 можно записать в виде xt = ξt − 1θ ξt−1, где ξt = 1 − θL 1 − 1/θ - Lεt является белым шумом. Мы не будем доказывать, что ξt является белымшумом, поскольку это технически сложно. Вместо этогомыукажем на простой факт: пусть ξt -некоторый белыйшум. Тогда процесс ξt−1θ ξt−1 имеет такую же автоковариационную функцию, как и процесс xt, заданный уравнением (14.37), если дисперсии связаны соотношением σ2ξ = θ2σ2ε . Для того чтобы в этом убедиться, достаточно проверить совпадение дисперсий и автоковариаций первого порядка (остальные автоковариации равны нулю). В общем случае процесса MA(q), чтобы сделать его обратимым, требуется обратить все корни характеристического уравнения, которые по модулю меньше единицы. А именно, пусть θ(L) = q i=1(1−HiL), -характеристический многочлен где |Hi| < 1 при i = 1, . . . , m и |Hi| > 1 при i = m + 1, . . . , q. Тогд а ˜θ(L) = mi=1(1 − HiL) q i=m+1(1 − H−1 i L) (14.40) -характеристический многочлен эквивалентного обратимого процесса. Заметим, что хотя уравнение (14.33) по форме напоминает разложение Вольда процесса xt, оно будет таким, только если все корни характеристического урав-нения по модулю будут не меньше единицы. Для получения разложения Вольда произвольного процесса MA(q) требуется проделать описанную операцию обра-щения корней, которые по модулю меньше единицы.
14.5. Смешанные процессы авторегрессии-скользящего среднего 457 14.5. Смешанные процессы авторегрессии- скользящего среднего ARMA (модель Бокса-Дженкинса) На практике иногда бывает целесообразно ввести в модель как элементы ав-торегрессии, так и элементы скользящего среднего. Это делается для того, чтобы с использованием как можно меньшего числа параметров уловить характеристики исследуемого эмпирического ряда. Такой процесс называется смешанным процес-сом авторегрессии-скользящего среднего и обозначается ARMA(p, q): xt = ϕ1xt−1 + . . . + ϕpxt−p + εt − θ1εt−1 − . . . − θqεt−q, (14.41) или, с использованием оператора лага, (1 − ϕ1L − ϕ2L2 − . . . − ϕpLp)xt = (1 − θ1L − θ2L2 − . . . − θqLq)εt. В операторной форме смешанная модель выглядит так: ϕ(L)xt = θ(L)εt, где ϕ(L) -оператор авторегрессии, θ(L) -оператор скользящего среднего. Модель (14.41) получила название модели Бокса-Дженкинса, поскольку бы-ла популяризирована Дж. Боксом и Г. Дженкинсом в их известной книге "Анализ временных рядов" [3]. Методология моделирования с помощью (14.41) получила название методологии Бокса-Дженкинса. Автокорреляционная функция и спектр процесса ARMA(p, q) Рассмотрим, как можно получить автоковариационную и автокорреляционную функции стационарного процесса ARMA(p, q), зная параметры этого процесса. Для этого умножим обе части уравнения (14.41) на xt−k, гд е k 0, и перейд ем к математическим ожиданиям: E(xt−kxt) = ϕ1E(xt−kxt−1) + ϕ2E(xt−kxt−2) + . . . + ϕpE(xt−kxt−p) + + E(xt−kεt) − θ1E(xt−kεt−1) − θ2E(xt−kεt−2) − . . . − θqE(xt−kεt−q). Обозначим через δs кросс-ковариацию изучаемого ряда xt и ошибки εt с за-держкой s, т.е. δs = E(xtεt−s).
458 Глава 14. Линейные стохастические модели ARIMA Поскольку процесс стационарен, то эта кросс-ковариационная функция не зависит от момента времени t. В этих обозначениях E(xt−kεt−j) = δj−k. Получаем выражение для автоковариационной функции: γk = ϕ1γk−1 + . . . + ϕpγk−p + δ−k − θ1δ1−k − . . . − θqδq−k. (14.42) Так как xt−k зависит только от импульсов, которые произошли до момента t − k, то δj−k = E(xt−kεt−j) = 0 приj < k. Для того чтобы найти остальные нужные нам кросс-ковариации, δ0, . . . , δq , необходимо поочередно умножить все члены выражения (14.41) на εt, εt−1, . . . , εt−q и перейти к математическим ожиданиям. В итоге получится следующая систе-ма уравнений: δ0 = σ2ε , δ1 = ϕ1δ0 − θ1σ2ε , δ2 = ϕ1δ1 + ϕ2δ0 − θ2σ2ε , . . . Общая формула для всех 1 s p имеет вид: δs = ϕ1δs−1 + - - - + ϕsδ0 − θsσ2ε . При s > p (такой случай может встретиться, если p < q) δs = ϕ1δs−1 + - - - + ϕpδs−p − θsσ2ε . Отсюда рекуррентно, предполагая σ2ε и параметры ϕ и θ известными, найдем δs. Далее, зная δs, по аналогии с уравнениями Юла-Уокера (14.21) по формуле (14.42) при k = 0, . . . , p с учетом того, что γ−k = γk найдем автоковариа-ции γ0, . . . , γp.Остальные автоковариации вычисляются рекуррентно по формуле (14.42). Автокорреляции рассчитываются как ρk = γk/γ0. Заметим, что если требуется найти только автокорреляции, то без потери общности можно взять ошибку εt с единичной дисперсией: σ2ε = 1. Если в уравнении (14.42) k > q, то все кросс-корреляции равны нулю, поэтому γk = ϕ1γk−1 + . . . + ϕpγk−p, k>q. (14.43)
14.5. Смешанные процессы авторегрессии-скользящего среднего 459 Поделив это выражение на γ0, выводим уравнение автокорреляционной функции для k > q: ρk = ϕ1ρk−1 + . . . + ϕpρk−p, (14.44) или ϕ(L)ρk = 0, k>q. Таким образом, начиная с некоторой величины задержки, а точнее, когда q < p, поведение автокорреляционной функции стационарного процесса ARMA(p, q) определяется, как и в случае чистой авторегрессии AR(p), однородным конеч-но-разностным уравнением (14.44). В свою очередь, решение этого конечно-разностного уравнения определяется корнями характеристического уравнения ϕ(z) = 0. То есть при q < p автокорреляционная функция будет состоять из ком-бинации затухающих экспонент и экспоненциально затухающих синусоид4. По аналогии с AR(p) условия стационарности ARMA(p, q) определяются кор-нями характеристического уравнения ϕ(L) = 0: если эти корни лежат вне единич-ного круга, то процесс стационарен. В качестве примера рассмотрим процесс ARMA(1, 1): xt = ϕxt−1 + εt − θεt−1, (14.45) или через лаговый оператор:(1 − ϕL)xt = (1 − θL)εt. Процесс стационарен, если −1 < ϕ < 1, и обратим, если −1 < θ < 1. Для вывода формулы автокорреляционной функции умножим (14.45) на xt−k и перейдем к математическим ожиданиям: E(xt−kxt) = ϕE(xt−kxt−1) + E(xt−kεt) − θE(xt−kεt−1), или γk = ϕγk−1 + δ−k − θδ1−k. (14.46) Исследуем поведение автоковариационной функции при различных значениях па-раметра k. 4Заметим, что граничные условия у AR(p) другие, поэтому автокорреляционные функции не будут совпадать.
460 Глава 14. Линейные стохастические модели ARIMA При k = 0 γ0 = ϕγ−1 + δ0 − θδ1. (14.47) Чтобы найти второе слагаемое, умножим уравнение процесса (14.45) на εt и возьмем математическое ожидание: δ0 = E(xtεt) = ϕE(xt−1εt) + E(εtεt) − θE(εtεt−1) = σ2ε . (14.48) Аналогичным способом распишем E(xtεt−1): δ1 = E(xtεt−1) = ϕE(xt−1εt−1) + E(εt−1εt) − θE(εt−1εt−1) = (ϕ − θ)σ2ε . Равенство E(xt−1εt−1) = σ2ε подтверждается так же, как (14.48). Итак, принимая во внимание, что γk = γ−k, выражение для дисперсии запи-сывается как γ0 = ϕγ1 + σ2ε − θ(ϕ − θ)σ2ε . (14.49) При k = 1 равенство (14.46) преобразуется в γ1 = ϕγ0 + δ−1 − θδ0. Используя ранее приведенные доводы относительно математических ожиданий, стоящих в этом уравнении, имеем: γ1 = ϕγ0 − θσ2ε . (14.50) При k 2 γk = ϕγk−1. Выразим автоковариации в (14.49) и (14.50) через параметры модели ϕ и θ. Получим систему уравнений < γ0 = ϕγ1 + σ2ε − θ(ϕ − θ)σ2ε ; γ1 = ϕγ0 − θσ2ε ; и решим ее относительно γ0 и γ1. Решение имеет вид: γ0 = 1 − 2ϕθ + θ2 1 − ϕ2 σ2ε , γ1 = (ϕ − θ)(1 − ϕθ) 1 − ϕ2 σ2ε .
14.5. Смешанные процессы авторегрессии-скользящего среднего 461 ρk θ1 ϕ1 1 1 −1 −1 k 1 -1 k 1 -1 k 1 -1 ρk ρk ρk ρk k k 1 1 ρk k 1 Рис. 14.4. График автокорреляционной функции процесса ARMA(1, 1) С учетом того, что ρk = γk/γ0, получаем выражения для автокорреляционной функции процесса ARMA(1, 1): ⎧⎪⎨ ⎪⎩ ρ1 = (ϕ − θ)(1 − ϕθ) 1 − 2ϕθ + θ2 , ρk = ϕk−1ρ1, k2. На рисунке 14.4 изображены графики автокорреляционной функции процесса ARMA(1, 1) при различных сочетаниях значений параметров ϕ и θ. По аналогии с процессами AR(p) и MA(q) выводится формула спектра процесса ARMA(p, q). Пусть {ηt} -такой процесс, что ηt = εt − θ1εt−1 − . . . − θqεt−q. Тогда xt, описываемый уравнением (14.41), можно записать в виде xt = ϕ1xt−1 + . . . + ϕpxt−p + ηt.
462 Глава 14. Линейные стохастические модели ARIMA По формуле (14.4), с одной стороны, p(f) = px(f) = pη(f)1 − ϕ1e−i2πf − ϕ2e−i4πf − . . . − ϕpe−i2pπf2, а с другой стороны, для процесса скользящего среднего {ηt} выполняется pη(f) = 2σ2ε 1 − θ1e−i2πf − θ2e−i4πf − . . . − θqe−i2qπf2 . Таким образом, получаем p(f) = 2σ2ε 1 − θ1e−i2πf − θ2e−i4πf − . . . − θqe−i2qπf2 1 − ϕ1e−i2πf − ϕ2e−i4πf − . . . − ϕpe−i2pπf2 . (14.51) Представление процесса ARMA в виде MA(∞) и функция реакции на импульсы Так же, как и в случае авторегрессии, стационарный процесс ARMA можно записать в виде модели линейного фильтра, или, другими словами, скользящего среднего бесконечного порядка MA(∞): xt = θ(L) ϕ(L) εt = εt + ψ1εt−1 + ψ2εt−2 + . . . = ∞i=0 ψiεt−i = ψ(L)εt, (14.52) где ψ0 = 1. Коэффициенты ψi представляют собой функцию реакции на импульсы для процесса ARMA, т.е. ψi является количественным измерителем того, как неболь-шое изменение ("импульс") в εt влияет на x через i периодов, т.е. на xt+i, что можно символически записать как ψi = dxt+i dεt . Один из способов вычисления функции реакции на импульсы сводится к ис-пользованию уравнения ϕ(z)ψ(z) = θ(z), или (1 − ϕ1z − ϕ2z2 − . . . − ϕpzp)(1 + ψ1z + ψ2z2 + . . .) = (1 − θ1z − . . . − θqzq),
14.6. Модель ARIMA 463 из которого, приравнивая коэффициенты в левой и правой частях при одинаковых степенях z, можно получить выражения для ψi. Более простой способ состоит в том, чтобы продифференцировать по εt урав-нение ARMA-процесса, сдвинутое на i периодов вперед, xt+i = p j=1ϕjxt+i−j + εt+i − q j=1 θjεt+i−j , dxt+i dεt = p j=1ϕj dxt+i−j dεt − θi, где θ0 = −1 и θj = 0 при j > q.Таким образом, получим рекуррентную формулу для ψi = dxt+i/dεt : ψi = p j=1ϕjψi−j − θi. При расчетах по этой формуле следует положить ψ0 = 1 и ψi = 0 при i < 0. Если процессARMAявляется обратимым5, то полученное представление в виде MA(∞) является разложением Вольда этого процесса. 14.6. Модель авторегрессии- проинтегрированного скользящего среднего ARIMA Характерной особенностью стационарных процессов типа ARMA(p, q) являет-ся то, что корни λi характеристического уравнения ϕ(L) = 0 находятся вне еди-ничного круга. Если один или несколько корней лежат на единичной окружности или внутри нее, то процесс нестационарен. Теоретически можно предложить много различных типов нестационарных мо-делей ARMA(p, q), однако, как показывает практика, наиболее распространен-нымтипом нестационарных стохастических процессов являются интегрированные процессы или, как их еще называют, процессы с единичным корнем. Единичным называют корень характеристического уравнения, равный действительной едини-це: λi = 1. 5Разложение Вольда необратимого процесса, у которого некоторые корни характеристического уравнения помодулюбольше единицы, такоеже, как у эквивалентного обратимого процесса.Ошибки однопериодных прогнозов, лежащие в основе разложения Вольда, при этом не будут совпадать с ошибками мод ели εt .
464 Глава 14. Линейные стохастические модели ARIMA -1 действительная часть -11 мнимая часть 1 Рис. 14.5. Корни характеристического уравнения процесса xt = 2.8xt−1 − 3.1xt−2 +1.7xt−3 − 0.4xt−4 + εt +0.5εt−1 − 0.4εt−2 на комплексной плоскости Рассмотрим в качестве примера следующий процесс ARMA(4, 2): xt = 2.8xt−1 − 3.1xt−2 + 1.7xt−3 − 0.4xt−4 + εt + 0.5εt−1 − 0.4εt−2. (14.53) Характеристическое уравнение этого процесса имеет следующие корни: λ1 = 1+ + i, λ2 = 1 − i, λ3 = 1.25, λ4 = 1. Все корни лежат за пределами единичного круга, кроме последнего, который является единичным. Эти корни изображены на рисунке 14.5. Оператор авторегрессии этого процесса можно представить в следующем виде: 1 − 2.8L + 3.1L2 − 1.7L3 + 0.4L4 = (1 − 1.8L + 1.3L2 − 0.4L3)(1 − L) = = (1 − 1.8L + 1.3L2 − 0.4L3)Δ, где Δ = 1− L -оператор первой разности. Введем обозначение wt = Δxt = xt − xt−1. Полученный процесс {wt} явля-ется стационарным процессом ARMA(3, 2), задаваемым уравнением: wt = 1.8wt−1 − 1.3wt−2 + 0.4wt−3 + εt + 0.5εt−1 − 0.4εt−2. (14.54) В общем случае, если характеристическое уравнение процесса ARMA(p+d, q) содержит d единичных корней, а все остальные корни по модулю больше единицы, то d-я разность этого временного ряда wt = Δdxt = ϕ(L)(1 − L)dxt может быть представлена как стационарный процесс ARMA(p, q): ϕ(L)Δdxt = θ(L)εt или ϕ(L)wt = θ(L)εt. (14.55)
14.6. Модель ARIMA 465 В развернутой форме модель 14.55 выглядит как wt = ϕ1wt−1 + ϕ2wt−2 + . . . + ϕpwt−p + (14.56) + εt − θ1εt−1 − θ2εt−2 − . . . − θqεt−q. Из-за практического значения такую разновидность моделей ARMA выделяют в отдельный класс моделей авторегрессии-проинтегрированного скользяще-го среднего и обозначают ARIMA(p, d, q). При d = 0 модель описывает стацио-нарный процесс. Как и исходную модель ARMA, мод ель ARIMA также называют моделью Бокса-Дженкинса. Обозначив f (L) = θ(L)(1 − L)d, представим процесс ARMA(p + d, q) в вид е: f (L)xt = θ(L)εt. f (L) называют обобщенным (нестационарным) оператором авторегрессии, таким, что d корней характеристического уравнения f (z) = 0 равны единице, а остальные по модулю больше единицы. Такой процесс можно записать в виде модели ARIMA ϕ(L)(1 − L)dxt = θ(L)εt. (14.57) Ряд {xt} называют интегрированным, поскольку он является результатом при-менения к стационарному ряду {wt} операции кумулятивной (накопленной) суммы d раз. Так, если d = 1, то д ля t > 0 xt = t i=1 wi + x0. Этим объясняется название процесса авторегрессии - проинтегрированного скользящего среднего ARIMA(p, d, q). Этот факт можно символически записать как xt = Sdwt, где S = Δ−1 = (1 − L)−1 - оператор суммирования, обратный к оператору разности. Следует понимать, однако, что оператор S не определен однозначно, поскольку включает некоторую константу суммирования. Простейшим процессом с единичным корнем является случайное блуждание: xt = Sεt, где εt -белыйшум.
466 Глава 14. Линейные стохастические модели ARIMA 14.7. Оценивание, распознавание и диагностика модели Бокса-Дженкинса Для практического моделирования с использованием модели Бокса-Джен-кинса требуется выбрать порядок модели (значения p, q и d), оценить ее пара-метры, а затем убедиться, правильно ли была выбрана модель и не нарушаются ли какие-либо предположения, лежащие в ее основе. Заметим, что один и тот же процесс может быть описан разными моделя-ми ARMA (14.41). Во-первых, неоднозначна компонента скользящего среднего θ(L)εt, о чем говорилось выше. Из разных возможных представлений MA здесь следует предпочесть обратимое. Во-вторых, характеристические многочлены ав-торегрессии и скользящего среднего могут содержать общие корни. Пусть ϕ(z) и θ(z) содержат общий корень λ. Тогда характеристические многочлены можно представить в виде ϕ(z) = (1 − z/λ)ϕ∗(z) и θ(z) = (1 − z/λ)θ∗(z). Соответ-ственно, один и тот же процесс можно записать как ϕ(L)xt = θ(L)εt или как ϕ∗(L)xt = θ∗(L)εt. Ясно, что вторая запись предпочтительнее, поскольку содержит меньше парамет-ров. Указанные неоднозначности могут создавать проблемы при оценивании. Прежде, чем рассмотреть оценивание, укажем, что уравнение (14.41) задает модель в довольно ограничительной форме. А именно, стационарный процесс, за-данный уравнением (14.41), должен иметь нулевое математическое ожидание. Для того чтобы сделать математическое ожидание ненулевым, можно ввести в модель константу:xt = μ + ϕ1xt−1 + . . . + ϕpxt−p + εt − θ1εt−1 − . . . − θqεt−q. Если процесс {xt} стационарен, то E(xt) = μ 1 − ϕ1 −- - -−ϕp . Альтернативно можно задать xt как xt = β + wt, (14.58) где ошибка {wt} является стационарным процессом ARMA: wt = ϕ1wt−1 + . . . + ϕpwt−p + εt − θ1εt−1 − . . . − θqεt−q. (14.59)
14.7 Оценивание, распознавание и диагностика модели ARIMA 467 При этом E(xt) = β. Ясно, что для стационарных процессов два подхода являются эквивалентными. Последнюю модель можно развить, рассматривая регрессию xt = Ztα + wt, (14.60) с ошибкой wt в виде процесса (14.59). В этой регрессии Zt не должны быть коррелированы с процессом wt и его лагами. Составляющая Zt может включать детерминированные тренды, сезонные переменные, фиктивные переменные для выбросов и т.п. Метод моментов для оценивания параметров модели Бокса-Дженкинса Опишем в общих чертах процедуру оценивания ARIMA(p, d, q).Предположим, что имеется ряд x1, . . . , xT , по которому требуется оценить параметры процес-са. Оценке подлежат три типа параметров: параметры детерминированной части модели (такие как β,α, о которых речь шла выше), авторегрессионные парамет-ры ϕ и параметры скользящего среднего θ. При оценивании предполагается, что порядок разности d, порядок авторегрессии p и порядок скользящего среднего q заданы. Если ряд {xt} описывается моделью (14.58) (которая предполагает d = 0), то параметр β этой модели можно оценить с помощью среднего ¯x, а далее дей-ствовать так, как если бы процесс сразу задавался моделью (14.59). В качестве wt рассматриваются центрированные значения, полученные как отклонения исходных уровней временного ряда от их среднего значения: wt = xt − ¯x. Если ряд {xt} описывается более общей моделью (14.60), которая тоже пред-полагает d = 0, то можно оценить параметры α с помощью обычного МНК, который дает здесь состоятельные, но не эффективные оценки a. Далее можно взять wt = xt − Zta и действовать так, как если бы процесс задавался моделью (14.59). При d > 0 от ряда xt следует взять d-е разности: wt = Δdxt. Мы не бу-дем рассматривать оценивание детерминированной составляющей в случае d > 0. Заметим только, что исходный ряд не нужно центрировать, поскольку уже первые разности исходных уровней ряда совпадают с первыми разностями центрирован-ного ряда. Имеет смысл центрировать d-е разности Δdxt. Проведя предварительное преобразование ряда, мы сведем задачу к оценива-нию стационарной модели ARMA (14.59), где моделируемая переменная wt имеет нулевое математическое ожидание.Получив ряд w1, . . . , wT (при d > 0 рядбуд ет
468 Глава 14. Линейные стохастические модели ARIMA на d элементов короче), можно приступить к оцениванию параметров авторегрес-сии и скользящего среднего. Выше мы рассмотрели, как можно оценивать авторегрессии на основе урав-нений Юла-Уокера (14.21). Прямое использование этого метода для модели ARMA(p, q) при q > 0 невозможно, поскольку в соответствующиеуравнения будут входить кросс-ковариации между изучаемым процессом и ошибкой (см. 14.42).Од-нако можно избавиться от влияния элементов скользящего среднего, если сдвинуть уравнения на q значений вперед. Тогда уравнения для автокорреляций будут иметь вид(14. 43). При k = q + 1, . . . , q + p получим следующую систему (т.е. исполь-зуем здесь тот же подход, что и раньше: умножаем (14.59) на wt−q−1, . . . , wt−q−p и переходим к математическому ожиданию): ⎧⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎩ γq+1 = ϕ1γq + ϕ2γq−1 + . . . + ϕpγq−p+1, γq+2 = ϕ1γq+1 + ϕ2γq + . . . + ϕpγq−p+2, - - - γq+p = ϕ1γq+p−1 + ϕ2γq+p−2 + . . . + ϕpγq. (14.61) В итоге имеем систему, состоящую из p уравнений относительно p неизвестных параметров ϕj . Решение этих уравнений, в которых вместо γk берутся эмпириче-ские значения автоковариаций ck для последовательности значений {wt}, т.е. ck = 1T T t=k+1wtwt−k, дает нам оценки параметров ϕ1, . . . , ϕp 6. С помощью оценок авторегрессионных параметров можно, с учетом (14.59) построить новый временной ряд ηp+1, . . . , ηT : ηt = wt − ϕ1wt−1 − . . . − ϕpwt−p, и для него рассчитать первые q выборочных автокорреляций rη1, . . . , rηq . Полу-ченные автокорреляции используются при расчете начальных оценок параметров скользящего среднего θ1, . . . , θq . 6На данный метод получения оценок параметров авторегрессии можно смотреть как на примене-ние метода инструментальных переменных к уравнению регрессии: wt = ϕ1wt−1 + . . . + ϕpwt−p + ηt, где ошибка ηt является MA(q) и поэтому коррелирована с лагами wt только вплоть до q-го. В качестве инструментов здесь используются лаги wt−q−1, . . . , wt−q−p .
14.7 Оценивание, распознавание и диагностика модели ARIMA 469 Действительно, {ηt} фактически представляет собой процесс скользящего среднего: ηt = εt − θ1εt−1 −- - -−θqεt−q, (14.62) для которого, как мы знаем, первые q автокорреляций могут быть выражены через параметры модели (см. (14.36)): ρηk = −θk + θ1θk+1 + θ2θk+2 + . . . + θq−kθq 1 + θ21 + θ22 + . . . + θ2q , k= 1, . . . , q. Заменив в этих выражениях ρηk на rηk , решаем полученную систему q нелинейных уравнений относительно q неизвестных параметров θ и получаем их оценки. Поскольку система уравнений нелинейная, то могут возникнуть некоторые про-блемы с ее решением. Во-первых, система может не иметь решений. Во-вторых, решение может быть не единственным. Рассмотрим в качестве примера случай q = 1. При этом имеем одно уравнение с одним неизвестным: rη1 = −θ1 1 + θ21 . Максимальное по модулю значение правой части 12 достигается при θ1 = ±1.Ес-ли |rη1 | > 12, то уравнение не имеет действительного решения7. Если |rη1| < 12, то оценку θ1 получим, решая квадратное уравнение. А оно будет иметь два корня: θ1 = −1 ±'1 − 4(rη1 )2 2rη1 . Один из корней по модулю больше единицы, а другой меньше, т.е. один соответ-ствует обратимому процессу, а другой-необратимому. Таким образом, из нескольких решений данных уравнений следует выбирать та-кие, которые соответствуют обратимому процессу скользящего среднего. Для это-го, если некоторые из корней характеристического уравнения скользящего средне-го по модулю окажутся больше единицы, то их следует обратить и получить коэф-фициенты, которые уже будут соответствовать обратимому процессу (см. 14.40). Для q > 1 следует применить какую-либо итеративную процедуру решения нелинейных уравнений8. 7Если решения не существует, то это может быть признаком того, что порядок разности d выбран неверно или порядок авторегрессии p выбран слишком низким. 8Например, методН ьютона, состоящий в линеаризации нелинейных уравнений в точке текущих приближенных параметров (т.е. разложение в рядТейлора до линейных членов).
470 Глава 14. Линейные стохастические модели ARIMA Описанный здесь метод моментов дает состоятельные, но не эффективные (не самые точные) оценки параметров. Существует рядметод ов, позволяющих повысить эффективность оценок. Методы уточнения оценок Система (14.61) при q > 1 основана на уравнениях для автоковариаций, ко-торые сдвинуты на q. Поскольку более дальние выборочные автоковариации вы-числяются не очень точно, то это приводит к не очень точным оценкам параметров авторегрессии. Чтобы повысить точность, можно предложить следующий метод. С помощью вычисленных оценок θ1, . . . , θq, на основе соотношения (14.62), находим последовательность значений {εt} по рекуррентной формуле: εt = ηt + θ1εt−1 + . . . + θqεt−q. В качестве εt−j при t j берем математическое ожидание ряда E(εt) = 0. Получив с помощью предварительных оценок ϕ и θ последовательность зна-чений {εt} и имея в наличии ряд {wt}, методом наименьших квадратов находим уточненные оценки параметров модели (14.59), рассматривая εt в этом уравнении как ошибку. Можно также получить уточненные оценки параметров детерминированной компоненты α в модели (14.60). Для этого можно использовать обобщенный ме-тоднаименьш их квадратов (см. гл. 8), основанный на оценке ковариационной мат-рицы ошибок wt, которуюможно получить, имея некоторые состоятельные оценки параметров процесса ARMA. Автоковариационную матрицу процесса ARMA можно представить в виде Γ = σ2εΩ. Оценку матрицы Ω можно получить, имея оценки параметров авто-регрессии и скользящего среднего (см. выше выводавток овариационной функции процесса ARMA). Имея оценку Ω, воспользуемся обобщенным МНК для оцени-вания параметров регрессии: aОМНК = (ZΩ−1Z)−1ZΩ−1X. Можно использовать также автокорреляционную матрицу R: aОМНК = (ZR−1Z)−1ZR−1X. Вкачестве примера приведем регрессию с процессом AR(1) в ошибке. Матрица автокорреляций для стационарного процесса AR(1), соответствующего последова-
14.7 Оценивание, распознавание и диагностика модели ARIMA 471 тельности значений w1, . . . , wT , имеет вид: R = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 ϕ ϕ2 - - - ϕT−1 ϕ 1 ϕ - - - ϕT−2 ϕ2 ϕ 1 - - - ϕT−3 ... ... .... . . ... ϕT−1 ϕT−2 ϕT−3 - - - 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Несложно убедиться, что обратная к R матрица имеет вид: R−1 = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 −ϕ 0 - - - 0 0 −ϕ (1 + ϕ2) −ϕ - - - 0 0 0 −ϕ (1 + ϕ2) - - - 0 0 ... ... .... . . ... ... 0 0 0 - - - (1 + ϕ2) −ϕ 0 0 0 - - - −ϕ 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Матрицу R−1 легко представить в виде произведения: R−1 = DD, где D = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ :1 − ϕ2 0 0 - - - 0 −ϕ 1 0 - - - 0 0 −ϕ 1 - - - 0 ... ... .... . . ... 0 0 0 - - - 1 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Далее можем использовать полученную матрицу D для преобразования в про-странстве наблюдений: Z∗ = DZ, X∗ = DX, (14.63) тогда полученные в преобразованной регрессии с помощью обычногоМНК оценки будут оценками обобщенного МНК для исходной регрессии: aОМНК = (Z∗Z∗)−1Z∗X∗.
472 Глава 14. Линейные стохастические модели ARIMA 50 100 150 200 50 100 150 00.2 0.4 0.6 0.8 0 0.8 0.6 0.4 0.2 0 0 200 Рис. 14.6. Автокорреляционная функция интегрированного процесса (слева) и стационарного процесса (справа) Они будут обладать не только свойством состоятельности, но и свойством эффек-тивности. К примеру, для парной регрессии xt = αzt + wt, где w = ϕwt−1 + εt, преоб-разование (14.63) приводит к уравнениям '1 − ϕ2x1 = α'1 − ϕ2z1 + ε∗1 и xt − ϕxt−1 = α(zt − ϕzt−1) + ε∗t, t>1, для оценивания которых при данном ϕ применим обычныйМНК. После получения эффективных оценок параметров регрессии можно пере-смотреть оценки параметров процесса ARMA. Можно продолжать такие итера-ции и далее до тех пор, пока не будет достигнута требуемая сходимость (см. метод Кочрена-Оркатта, описанный в п. 8.3). Распознавание порядка модели Сначала вычисляются разности исходного ряда до тех пор, пока они не окажутся стационарными относительно математического ожидания и дисперсии, и отсюда получают оценку d. Если процесс является интегрированным, то его выборочная автокорреляци-онная функция затухает медленно, причем убывание почти линейное. Если же автокорреляционная функция затухает быстро, то это является признаком стаци-онарности. В качестве примера на рисуке 14.6 слева изображена коррелограмма
14.7 Оценивание, распознавание и диагностика модели ARIMA 473 ряда длиной в 1000 наблюдений, полученного по модели ARIMA(3, 1, 2), заданной уравнением (14.53). Справа на том же рисунке изображена коррелограмма первых разностей того же ряда, которые подчиняются стационарной модели ARMA(3, 2), заданной уравнением (14.54). Формальные критерии выбора d рассматриваются в пункте 17.4. Следует понимать, что достаточно сложно отличить процесс, который имеет единичный корень, от стационарного процесса, в котором есть корень близкий к единице. Если d окажется меньше, чем требуется, то дальнейшее оценивание будет применяться к нестационарному процессу, что должно проявиться в оценках па-раметров авторегрессии - в сумме они будут близки к единице. Если d ока-жется больше, чем требуется, то возникнет эффект избыточного взятия разности (overdifferencing), который проявляется в том, что в характеристическом уравнении скользящего среднего появляется единичный корень. Это может создать трудности при оценивании скользящего среднего. Для выбора порядка авторегрессии можно использовать выборочную частную автокорреляционнуюфункцию. Как известно, теоретическая частная автокорреля-ционная функция процесса AR(p) обрывается на лаге p. Таким образом, p следует выбрать равным порядку, при котором наблюдается последнее достаточно боль-шое (по модулю) значение выборочной частной автокорреляционной функции. До-верительные интервалы можно основывать на стандартной ошибке выборочного частного коэффициента автокорреляции, которая равна примерно 1√T для по-рядков выше p (для которых теоретическая автокорреляционная функция равна нулю). Аналогично, для выбора порядка скользящего среднего можно использовать выборочную автокорреляционную функцию, поскольку теоретическая автокорре-ляционнаяфункция процессаMA(q) обрывается на лаге q. Такимобразом, q следу-ет выбрать равным порядку, при котором наблюдается последнее достаточно боль-шое (по модулю) значение выборочной автокорреляционной функции.Стандартная ошибка выборочного коэффициента автокорреляции тоже примерно равна 1√T . Более точная формула стандартнойошибки для автокорреляции порядка k (k > q) имеет вид ! T − k T(T + 2) (см. стр. 367). Если же нет уверенности, что процесс является чистой авторегрессией или чистымпроцессом скользящегосреднего, то эти методы не подходят.Но, по крайней мере, по автокорреляционной функции и частной автокорреляционной функции можно проследить, насколько быстро угасает зависимость в ряде.
474 Глава 14. Линейные стохастические модели ARIMA Порядок модели ARMA(p, q) можно выбирать на основе информационных кри-териев: информационный критерий Акаике: AIC = ln(s2e) + 2(p + q + n+ 1) T ; байесовский информационный критерийШварца: BIC = ln(s2e) + (p + q + n+ 1) lnT T . Здесь s2e-остаточная дисперсия, рассчитанная по модели, n + 1 относится к дополнительным оцениваемым параметрам-константе и коэффициентам при факторах регрессии (14.60). Порядок (p, q) выбирается посредством перебора из некоторого множества моделей так, чтобы информационный критерий достигал минимума. Критерий Акаике нацелен на повышение точности прогнозирования, а байесовский критерий- на максимизацию вероятности выбора истинного по-рядка модели. Можно также выбирать порядок по тому принципу, что остатки должны быть похожи на белый шум, для чего использовать проверку остатков на автокорреля-цию. Если остатки автокоррелированы, то следует увеличить p или q. Диагностика В основе модели ARIMA лежит предположение, что ошибки εt являются бе-лым шумом. Это предполагает отсутствие автокорреляции и гомоскедастичность ошибок. Для проверки ошибок на гомоскедастичность могут использоваться те же критерии, которые были рассмотрены ранее в других главах. Здесь мы рассмотрим диагностику автокорреляции ошибок. Простейший способ диагностики-графический, состоящий в изучении кор-релограммы и спектрограммы остатков. Кореллограмма должна показывать толь-ко малые, статистически незначимые значения автокорреляций. Спектрограмма должна быть достаточно "плоской", не иметь наклона и не содержать сильно вы-деляющихся пиков. Для формальной проверки отсутствия автокорреляции ошибок можно исполь-зовать Q-статистику Бокса-Пирса и ее модификацию - статистику Льюнга- Бокса, которые основаны на квадратах нескольких (m) первых выборочных коэф-фициентов автокорреляции9 (см. стр. 368). 9При выборе числа m следует помнить, что прималом m, если имеется автокорреляция высокого порядка, критерий может не показать автокорреляцию. При большом же m присутствие значитель-
14.8. Прогнозирование по модели Бокса-Дженкинса 475 Статистика Бокса-Пирса: ˜Q= n mk=1 r2k. Статистика Льюнга-Бокса: Q = n(n + 2) mk=1 r2k n − k . Здесь в качестве rk следует использовать выборочные коэффициенты авто-корреляции, рассчитанные на основе остатков et модели ARIMA: rk = nt=k+1 etet−k nt=1 e2t . Поскольку для вычисления rk используется не белый шум, а остатки, то асимпто-тическое распределение этих Q-статистик отличается от того, которое имеет место для истинного белого шума, на количество параметров авторегрессии и скользя-щего среднего, оцененных по модели, т.е. на величину (p + q). Обе статистики асимптотически распределены как χ2m−p−q. Как показали Льюнг и Бокс, предло-женная ими модифицированная Q-статистика, которая придает меньший вес даль-ним автокорреляциям, имеет распределение, которое ближе аппроксимирует свой асимптотический аналог, поэтому более предпочтительно использовать именно ее. Нулевая гипотеза состоит в том, что ошибка представляет собой белый шум (автокорреляция отсутствует). Если Q-статистика превышает заданный квантиль распределения хи-квадрат, то нулевая гипотеза отвергается и делается вывод о том, что модель некорректна. Возможная причина некорректности-неудачный выбор порядка модели (слишком малые значения p и q). 14.8. Прогнозирование по модели Бокса-Дженкинса Прогнозирование стационарного процесса ARMA Пусть для стационарного обратимого процессаARMAвмомент t делается про-гноз процесса x на τ шагов вперед, т.е. прогноз величины xt+τ . Для упрощения ных автокорреляций может быть не замечено при наличии большого числа незначительных. То есть мощность критерия зависит от правильного выбора m.
476 Глава 14. Линейные стохастические модели ARIMA рассуждений предположим, что при прогнозировании доступна вся информация о процессе x до момента t включительно, т.е. информация, на основе которой строится прогноз, совпадает с полной предысторией процесса Ωt = (xt, xt−1, . . . ) . Заметим, что на основе (xt, xt−1, . . . ) можно однозначно определить ошиб-ки (εt, εt−1, . . . ) и наоборот, поэтому при сделанных предположениях ошибки (εt, εt−1, . . . ) фактически входят в информационное множество. Кроме того, имея полную предысторию, можно точно вычислить параметры процесса, поэтому будем далее исходить из того, что параметры процесса нам известны. Из теории прогнозирования известно, что прогнозом, минимизирующим сред-ний квадрат ошибки, будет математическое ожидание xt+τ , условное относительно Ωt, т.е. E(xt+τ |Ωt). Убедимся в этом, воспользовавшись представлением модели ARMA в виде модели линейного фильтра (разложением Вольда) (14.52) xt+τ = εt+τ + ψ1εt+τ−1 + . . . + ψτ−1εt+1 + ψτ εt + ψτ+1εt−1 + . . . , где во вторую строчку вынесены слагаемые, относящиеся к предыстории; получим таким образом следующее представление условного математического ожидания: E(xt+τ |Ωt) = E(εt+τ |Ωt) + ψ1E(εt+τ−1|Ωt) + . . . + + ψτ−1E(εt+1|Ωt) + ψτ εt + ψτ+1εt−1 + . . . . Вторую строчку формулы пишем без оператора условного математического ожи-дания, поскольку соответствующие слагаемые входят в предысторию Ωt. Будем предполагать, что условное относительно предыстории математическое ожидание будущих ошибок равно нулю, т.е. E(εt+k|Ωt) = 0 при k > 0. Это буд ет выполнено, например, если все ошибки εt независимы между собой. (Отсутствия автокорреляции тут недостаточно. В приложении приводится пример белогошума, для которого это неверно.) Тогда рассматриваемое выражение упрощается: E(xt+τ |Ωt) = ψτ εt + ψτ+1εt−1 + . . . , что дает нам линейную по ошибкам формулу для оптимального прогноза: xt (τ) = ψτ εt + ψτ+1εt−1 + . . . = ∞i=0 ψτ+iεt−i, (14.64) где мы обозначили через xt (τ ) прогноз на τ периодов, сделанный в момент t.
14.8. Прогнозирование по модели Бокса-Дженкинса 477 Проверим, что эта прогнознаяфункция будет оптимальной (в смысле минимума среднего квадрата ошибки) среди линейных прогнозных функций, т.е. среди про-гнозных функций, представимых в виде линейной комбинации случайных ошибок, входящих в предысторию: xt(τ) = ψ∗τ εt + ψ∗τ+1εt−1 + ψ∗τ+2εt−2 + . . . = ∞i=0 ψ∗τ+iεt−i. Для этого найдем веса ψ∗τ, ψ∗τ+1, ψ∗τ+2, . . ., которые обеспечивают минимум сред-него квадрата ошибки. С учетом того, что xt+τ = ∞i=0ψiεt+τ−i, ошибка такого прогноза ηt(τ ) равна ηt(τ) = xt+τ − xt(τ) = ∞i=0 ψiεt+τ−i − ∞i=0 ψ∗τ+iεt−i = = τ−1 i=0 ψiεt+τ−i + ∞i=0(ψτ+i − ψ∗τ+i)εt−i, а средний квадрат ошибки прогноза (с учетом некоррелированности ошибок) равен E[ηt(τ )2] = E⎡⎣τ−1 i=0 ψiεt+τ−i + ∞i=0(ψτ+i − ψ∗τ+i)εt−i2⎤⎦ = = E0τ−1 i=0 ψ2i ε2 t+τ−i + ∞i=0(ψτ+i − ψ∗τ+i)2ε2t−i1 = = σ2ε (1 + ψ21 + ψ22 + . . . + ψ2τ−1) + σ2ε ∞i=0(ψτ+i − ψ∗τ+i)2. Очевидно, что средний квадрат ошибки достигает минимума при ψ∗τ+i = ψτ+i и равен E[ηt(τ )2] = σ2ε 01 + τ−1 i=1 ψ2i 1 . Ошибка такого прогноза рассчитывается по формуле ηt(τ) = τ−1 i=0 ψiεt+τ−i. Из формулы видно, что эта ошибка проистекает из будущих ошибок εt+k, которые в момент t еще неизвестны. Беря математическое ожидание от обеих частей,
478 Глава 14. Линейные стохастические модели ARIMA видим, что математическое ожиданиеошибки прогноза равно нулю. Таким образом, прогноз, полученный по формуле (14.64), будет несмещенным. Из несмещенности прогноза следует, что дисперсия ошибки прогноза равна среднему квадрату ошибки прогноза, т.е. σ2p = E[ηt(τ )2] = σ2ε 01 + τ−1 i=1 ψ2i 1 . или σ2p = σ2ε τ−1 i=0 ψ2i , где ψ0 = 1. Хотя представление в виде бесконечного скользящего среднего удобно для ана-лиза прогнозирования, однако для вычисления прогноза предпочтительнее вер-нуться к исходному представлению модели ARMA в виде разностного уравнения (со сдвигом на τ периодов вперед): xt+τ = ϕ1xt+τ−1 + . . . + ϕpxt+τ−p + +εt+τ − θ1εt+τ−1 − . . . − θqεt+τ−q. Возьмем от обеих частей уравнения условное относительно предыстории мате-матическое ожидание: xt(τ) = E[xt+τ |Ωt] = ϕ1E[xt+τ−1|Ωt] + . . . + ϕpE[xt+τ−p|Ωt] + + E[εt+τ |Ωt] − θ1E[εt+τ−1|Ωt] − . . . − θqE[εt+τ−q|Ωt]. Введем более компактные обозначения: E[xt+i|Ωt] = ¯xt+i, E[xt+τ |Ωi] = ¯εt+i. В этих обозначениях xt(τ) = ¯xt+τ = ϕ1¯xt+τ−1 + . . . + ϕp¯xt+τ−p + + ¯εt+τ − θ1¯εt+τ−1 − . . . − θq ¯εt+τ−q. (14.65)
14.8. Прогнозирование по модели Бокса-Дженкинса 479 При вычислении входящих в эту формулу условных математических ожиданий используют следующие правила: ¯xt+i = E[xt+i|Ωt] =⎧⎪⎨ ⎪⎩ xt+i, i0, xt(i), i>0, ¯εt+i = E[εt+i|Ωt] =⎧⎪⎨ ⎪⎩ εt+i, i0, 0, i>0, дающие удобную рекуррентную формулу для вычисления прогнозов. Для вычисления показателя точности прогноза (дисперсии ошибки прогноза или, что в данном случае то же самое, поскольку прогноз несмещенный, среднего квадрата ошибки прогноза), удобно опять вернутся к представлению модели в виде бесконечного скользящего среднего. Как мы видели, дисперсия ошибки прогно-за равна σ2p = σ2ε(1 + τ−1 i=1 ψ2i ). Формулы для вычисления коэффициентов ψi скользящего среднего приведены на стр. 462. Мывывелиформулыдля расчета точечногопрогнозапомоделиARMAи диспер-сии этого прогноза. Если дополнительно предположить, что ошибки εt подчиняют-ся нормальному закону (т.е. представляют собой гауссовский процесс), то можно получить также интервальный прогноз. При этом предположении при известных значениях процесса до момента t распределение будущего значения процесса xt+τ (т.е. условное распределение xt+τ |Ωt) также будет нормальным со средним значе-нием xt(τ ) и дисперсией σ2p:xt+τ |Ωt ∼ N xt(τ ), σ2p. Учитывая это, получаем доверительный интервал для xt+τ , т.е. интервальный про-гноз: [xt(τ ) − ξ1−ασp, xt(τ) + ξ1−ασp] , или 0xt(τ ) − ξ1−ασε τ−1 i=0 ψ2i, xt(τ) + ξ1−ασε τ−1 i=0 ψ2i 1 , (14.66) где ξ1−α -двусторонний (1 − α)-квантиль стандартного нормального распреде-ления. Это (1 − α) - 100-процентный доверительный интервал.
480 Глава 14. Линейные стохастические модели ARIMA Прогнозирование процесса ARIMA Для прогнозирования процесса ARIMA(p, d, q) при d > 0 можно воспользо-ваться представлением его в виде ARMA(p + d, q): f (L)xt = θ(L)εt, где f (L) = 1 − f1L − f2L2 − . . . − fp+dLp+d = ϕ(L)(1 − L)d, а коэффициенты f1, f2, . . . , fp+d выражаются через ϕ1, ϕ2, . . . , ϕp. В разверну-той записи xt = p+d j=1 fjxt−j + εt − q j=1 θjεt−j . (14.67) Например, модель ARIMA(1, 1, 1), (1 − ϕ1L)(1 − L)xt = (1 − θ1L)εt, можно записать в виде: xt = (1+ϕ1)xt−1 − ϕ1xt−2 + εt − θ1εt−1 = f1xt−1 + f2xt−2 + εt − θ1εt−1, где f1 = 1+ϕ1, f2 = −ϕ1. При расчетах можно использовать те же приемы, что и выше для ARMA. Некоторую сложность представляет интерпретация ARIMA(p, d, q) в вид емод е-ли линейного фильтра, поскольку рядмод улей коэффициентов такого разложения является расходящимся. Однако это только технические сложности обоснования формул (в которые мы не будем вдаваться), а сами формулы фактически не меня-ются. Таким образом, отвлекаясь от технических тонкостей, можем записать ARIMA(p, d, q) в вид еMA(∞): xt = θ(L) f (L) εt = εt + ψ1εt−1 + ψ2εt−2 + . . . = ψ(L)εt. (14.68) Функцию реакции на импульсы можно рассчитать по рекуррентной формуле ψi = p+d j=1 fjψi−j − θi, (14.69)
14.8. Прогнозирование по модели Бокса-Дженкинса 481 где ψ0 = 1, ψi = 0 при i < 0 и θi = 0 при i > q. Кроме того, прогнозы xt(τ ) можно вычислять по рекуррентной формуле, ко-торая получается из (14.67) по аналогии с (14.65): xt(τ) = ¯xt+τ = p+d j=1 fj ¯xt+τ−j + ¯εt+τ − q j=1 θj ¯εt+τ−j , (14.70) где условные математические ожидания ¯xt+i = E[xt+i|Ωt] и ¯εt+i = E[εt+i|Ωt] рассчитываются по тому же принципу, что и в (14.65). Таким образом, для прогнозирования в модели ARIMA можно использовать формулы (14.70), (14.8) и (14.66), где коэффициенты ψi рассчитываются в соот-ветствии с (14.69). Альтернативный подход к прогнозированию в модели ARIMA(p, d, q) состоит в том, чтобы сначала провести необходимые вычисления для wt = (1 − L)dxt, т.е. процесса ARMA(p, q), который лежит в основе прогнозируемого процесса ARIMA(p, d, q), а потом на их основе получить соответствующиепоказатели для xt. Так, (14.70) можно записать в виде E[f (L)xt+τ |Ωt] = E[ϕ(L)(1 − L)dxt+τ |Ωt] = E[θ(L)εt+τ |Ωt], т.е. E[ϕ(L)wt+τ |Ωt] = E[θ(L)εt+τ |Ωt] или ϕ(L) ¯ wt+τ = θ(L)¯εt+τ . Здесь по аналогии ¯ wt+i = E[wt+i|Ωt], причем ¯ wt+i = wt(i) (равно прогнозу) при i > 0 и ¯ wt+i = wt+i (равно значению самого ряда) при i 0. Отсюда видно, что можно получить сначала прогнозы для процесса wt по фор-мулам (14.65), заменив xt на wt, а затем применить к полученным прогнозам оператор Sd = (1 − L)−d, т.е. попросту говоря, просуммировать такой ряд d раз, добавляя каждый раз нужную константу суммирования. В частности, при d = 1 получаем xt(i) = xt + i j=0wt(j). Далее, ψ(L) можно записать в виде ψ(L) = θ(L) f (L) = (1 − L)−d θ(L) ϕ(L) = (1 − L)−dψw(L) = Sdψw(L).
482 Глава 14. Линейные стохастические модели ARIMA Здесь ψw(L) = θ(L)/ϕ(L) - оператор представления MA(∞) д ля wt. Таким образом, коэффициенты ψi можно рассчитать из коэффициентов ψwi . Например, при d = 1 получаем ψi = i j=0ψwj . Получение общих формул для прогнозирования в модели ARIMA с помощью решения разностных уравнений Формула (14.70) представляет собой разностное уравнение для ¯xt+τ, решив которое получаем в явном виде общую формулу прогноза. Проиллюстрируем этот прием на примере процесса ARIMA(1, 1, 1), для которого f1 = 1+ϕ1, f2 = −ϕ1: xt+τ = (1+ϕ1)xt+τ−1 − ϕ1xt+τ−2 + εt+τ − θ1εt+τ−1. (14.71) Берем условное математическое ожидание от обеих частей равенства (14.71), получаем точечные прогнозы на 1, 2, . . . , τ шагов вперед. xt(1) = (1 + ϕ1)xt − ϕ1xt−1 − θ1εt, xt(2) = (1 + ϕ1)xt(1) − ϕ1xt, xt(3) = (1 + ϕ1)xt(2) − ϕ1xt(1), ... xt(τ) = (1+ϕ1)xt(τ − 1) − ϕ1xt(τ − 2), τ >2. Мы видим, что начиная с τ > q = 1 природу прогнозирующей функции опре-деляет только оператор авторегрессии: ¯xt+τ = (1+ϕ1)¯xt+τ−1 − ϕ1¯xt+τ−2, τ >1. Общее решение этого разностного уравнения имеет следующий вид: ¯xt+τ = A0 + A1ϕτ1. Чтобы вычислить неизвестные коэффициенты, необходимо учесть, что ¯xt = xt и ¯xt+1 = xt(1) = (1 + ϕ1)xt − ϕ1xt−1 − θ1εt. Получается система уравнений: A0 + A1 = xt, A0 + A1ϕ1 = (1+ϕ1)xt − ϕ1xt−1 − θ1εt, из которой находятся A0 и A1.
14.8. Прогнозирование по модели Бокса-Дженкинса 483 Точно так же можно рассматривать формулу (14.69) как разностное уравнение, решая которое относительно ψi,получимв явномвиде общуюформулу дляфункции реакции на импульсы. По формуле (14.69) получаем ψ0 = 1, ψ1 = (1+ϕ1)ψ0 − θ1 = 1+ϕ1 − θ1, ψ2 = (1+ϕ1)ψ1 − ϕ1ψ0, ... ψi = (1+ϕ1)ψi−1 − ϕ1ψi−2, i>1. Легко показать, что решение этого разностного уравнения имеет следующий общий вид: ψi = B0 + B1ϕi1, где B0 = 1 − θ1 1 − ϕ1 , B1 = 1− B0 = θ1 − ϕ1 1 − ϕ1 . С учетом этого модель ARIMA(1, 1, 1) представляется в виде: xt = ∞i=0 B0 + B1ϕi1εt−i. Используя полученную формулу для коэффициентов ψi, найдем также диспер-сию прогноза: σ2p = σ2ε τ−1 i=0 ψ2i = σ2ε (1 − ϕ1)2 τ−1 i=0 1 − θ1 + (θ1 − ϕ1) ϕi12 . Отметим, что в пределе слагаемые стремятся к положительному числу 1 − θ1. Это означает, что с ростом горизонта прогноза τ дисперсия (а, следовательно, ширина прогнозного интервала) неограниченно возрастает. Такое поведение дис-персии связано с тем, что рассматриваемый процесс является нестационарным. Прогнозирование по модели Бокса-Дженкинса в конечных выборках Выше мы предполагали, что в момент t известна полная предыстория Ωt = (xt, xt−1, . . . ). Фактически, однако, человеку, производящему прогноз, из-вестен только некоторый конечный ряд (x1, . . . , xt). В связи с этим для практиче-ского использования приведенных формул, требуется внести в них определенные поправки.
484 Глава 14. Линейные стохастические модели ARIMA В частности, параметры модели на практике не известны, и их требуется оце-нить. Это вносит дополнительную ошибку в прогноз. Кроме того, ошибки εt, вообще говоря, неизвестны, и вместо них в выражении (14.65) следует использовать остатки et, полученные в результате оценивания модели. При наличии в модели скользящего среднего (т.е. при q > 0) ошибки не выражаются однозначно через наблюдаемый ряд {xt} и требуется использовать какое-то приближение. Наиболее простой методсостоит в том, чтобы положить остатки et при t 0 равными нулю, а остальные остатки вычислять рекуррентно, пользуясь формулой εt = xt − p+d j=1 fjxt−j + q j=1 θjεt−j , где вместо ошибок εt используются остатки et, а вместо неизвестных истинных параметров fj и θj -их оценки. Из-за того, что параметры не известны, а оцениваются, дисперсия ошибки про-гноза будет выше, чем следует из (14.8). Имея некоторую оценку ковариационной матрицы оценок параметров, можно было бы внести приблизительную поправку, но эти расчеты являются достаточно громоздкими. Далее, расчеты дисперсии прогноза с использованием (14.8) сами по себе явля-ются приближенными, поскольку встречающиеся там величины приходится оцени-вать.Это относится и к функции реакции на импульсы ψi, и к дисперсииошибки σ2ε . Приложение. Неоптимальность линейных прогнозов в модели ARMA То, что ошибка εt представляет собой белый шум (т.е. ошибки некоррелиро-ваны, имеют нулевое математическое ожидание и одинаковую дисперсию), не под-разумевает, что E(εt+k|Ωt) = 0 при k > 0. Поэтому в общем случае прогнозная функция, полученная по формуле (14.64) (или, эквивалентно, (14.65)) не будет оптимальной в среднеквадратическом смысле. Однако она, как было показано, является оптимальной среди линейных прогнозных функций. Кроме того, если ошибки независимы, то требуемое свойство выполнено. В частности, оно верно для гауссовского белого шума, т.к. для гауссовских процессов некоррелирован-ность эквивалентна независимости. Рассмотрим в качестве примера неоптимальности прогноза (14.64) следующий процесс {εt}. Значения, соответствующие четным t независимы и распределены как N(0; 1). При нечетных же t значения определяются по формуле εt = ε2t−1 − 1 √2 .
14.9. Модели, содержащие стохастический тренд 485 Таким образом, при нечетных t значения ряда полностью предопределены предыс-торией. Несложно проверить, что данный процесс представляет собой белый шум. Он имеет нулевое математическое ожидание, единичную дисперсию и не автокор-релирован. Если же построить на основе такого белого шума марковский процесс xt = ϕ1xt−1 + εt, то при нечетных t оптимальным прогнозом на один шаг вперед будет не ϕ1xt−1, а ϕ1xt−1 +ε2t−1 − 1/√2, причем прогноз будет безошибочным. При четных же t стандартная формула будет оптимальной. Приведенный пример наводит на мысль о том, что во многих случаях мож-но подобрать нелинейную прогнозную функцию, которая позволяет сделать более точный прогноз, чем полученная нами оптимальная линейная прогнозная функция. Вкачествеменее экзотическогопримераможнопривестимодели с авторегрессион-ной условной гетероскедастичностью, о которых речь идет в одной из последующих глав. 14.9. Модели, содержащие стохастический тренд Модели со стохастическим трендом можно отнести к классу линейных нестаци-онарных моделей ARIMA(p, d, q), но они имеют свои особенности. Рассмотрим эти модели. 1. Модель случайного блуждания (The Random Walk Model). Эта модель является частным случаем модели AR(1) с единичным корнем: xt = xt−1 + εt. (14.72) Еслиначальное условие x0 известно,общеерешениеможетбытьпредставлено в вид е xt = x0 + t i=1 εi. Безусловное математическое ожидание: E(xt) = E(xt+k) = x0. Условное математическое ожидание: E(xt+k|Ωt) = E((xt + k i=1 εt+i)|Ωt) = xt = x0 + t i=1 εi. Таким образом, условное математическое ожидание E(xt+k|Ωt) обязательно включает в себя случайную компоненту, равную t i=1 εi, которую называют сто-хастическим трендом.
486 Глава 14. Линейные стохастические модели ARIMA Для любых значений k влияние каждой ошибки на последовательность {xt} со временем не исчезает. Безусловная дисперсия: var(xt) = tσ2ε , var(xt+k) = (t + k)σ2ε . Условная дисперсия: var(xt+k|Ωt) = var((xt + k i=1 εt+i)|Ωt) = kσ2ε . Таким образом, и безусловная, и условная дисперсии зависят от времени, что свидетельствует о нестационарности процесса случайного блуждания. Этот вывод подтверждается расчетом коэффициентов автоковариации и авто-корреляции, которые также зависят от времени: γk = cov(xt, xt+k) = E((xt − x0)(xt+k − x0)) = = E((ε1 + . . . + εt)(ε1 + . . . + εt+k)) = E(ε21+ . . . + ε2t) = t - σ2ε . Тогда ρk = tσ2ε :tσ2ε (t + k)σ2ε = ! t t + k . В практических ситуациях нередко модель случайного блуждания используется для описания динамики темпов роста. В модели случайного блуждания первая разность Δxt = εt -чисто слу-чайный процесс, следовательно, эта модель может быть интерпретирована как ARIMA(0, 1, 0). 2. Модель случайного блуждания с дрейфом (The Random Walk plus Drift Model). Эта модель получается из модели случайного блуждания добавлением констан-ты a0: xt = xt−1 + a0 + εt. (14.73) Общее решение для xt при известном x0: xt = x0 + a0t + t i=1 εi. Здесь поведение xt определяется двумя нестационарными компонентами: линей-ным детерминированным трендом a0t и стохастическим трендом ti=1 εi. Ясно, что динамику ряда определяет детерминированный тренд. Однако не сле-дует думать, что всегда легко различить процесс случайного блуждания и процесс случайного блуждания с дрейфом.
14.9. Модели, содержащие стохастический тренд 487 На практике многие ряды, включая предложение денег и реальный ВНП, ведут себя как процесс случайного блуждания с дрейфом. Заметим, что первая разность ряда стационарна, т.е. переход к первой разности создает стационарную последовательность: {Δxt} = {a0 + εt} с математическим ожиданием, равным a0, дисперсией σ2ε и γk = 0 для всех t, следовательно, это тоже ARIMA(0, 1, 0). 3. Модель случайного блуждания с шумом (The Random Walk plus Noise Model). Эта модель представляет собой совокупность стохастического тренда и компо-ненты белого шума. Формально модель описывается двумя уравнениями: < xt = μt + ηt, μt = μt−1 + εt, (14.74) где {ηt} -белый шум с распределением N(0, σ2η), εt и ηt независимо распре-делены для всех t и k: E(εt, ηt−k) = 0. Общее решение системы (14.74) имеет вид: xt = μ0 + t i=1 εi + ηt. Легко убедиться в том, что все моменты второго порядка зависят от времени: var(xt) = tσ2ε + σ2η, γk = cov(xt, xt+k) = E((ε1 + . . . + εt + ηt)(ε1 + . . . + εt−k + ηt)) = tσ2ε , ρk = t σ2ε '(tσ2ε + σ2η) ((t + k) σ2ε + σ2η) . Следовательно, процесс 14.74 нестационарен. Но первая разность этого процесса Δxt = εt +Δηt стационарна с параметрами: E(Δxt) = E(εt +Δηt) = 0, var(Δxt) = E((Δxt)2) = E((εt +Δηt)2) = = σ2ε + 2E(εtΔηt) + E(η2t − 2ηtηt−1 + η2t−1) = σ2ε + 2σ2η, γ1 = cov(Δxt,Δxt−1) = E((εt + ηt − ηt−1)(εt−1 + ηt−1 − ηt−2)) = −σ2η, γk = cov(Δxt,Δxt−k) = E((εt + ηt − ηt−1)(εt−k + ηt−k − ηt−k−1)) = 0, k>1. Таким образом, первые разности ведут себя как MA(1)-процесс, а модель случай-ного блуждания с шумом можно квалифицировать как ARIMA(0, 1, 1).
488 Глава 14. Линейные стохастические модели ARIMA 4. Модель общего тренда с нерегулярностью (The General Trend plus Irregular Model). Эта модель содержит детерменированный и стохастический тренды, а также MA(q)-ошибку. Частный ее вариант: < xt = μt + ηt, μt = μt−1 + a0 + εt. (14.75) Решением (14.75) является модель общего тренда a0t + t i=1 εi с шумом: xt = μ0 + a0t + t i=1 εi + ηt. Первая разность этой модели отличается от предыдущего варианта на константу a0: Δxt = a0 + εt +Δηt. Поэтому E(Δxt) = a0, var(Δxt) = σ2ε + 2σ2η, γ1 = −σ2η, γk = 0, k>1. Следовательно, модель общего тренда с шумом-это также ARIMA(0, 1, 1). В более общей постановке эта модель формулируется при помощи операто-ра θ(L): xt = μ0 + a0t + t i=1 εi + θ(L)ηt. 5. Модель локального линейного тренда (The Local Linear Trend Model). Пусть {εt}, {ηt} и {δt} -три взаимно некоррелированных процесса белого шума. Тогда модель представляется следующими уравнениями: ⎧⎪⎪⎨ ⎪⎪⎩ xt = μt + ηt, μt = μt−1 + at + εt, at = at−1 + δt. (14.76) Легко показать, что рассмотренные ранее модели являются частными случаями данной модели.
14.9. Модели, содержащие стохастический тренд 489 Для нахождения решения выражаем at из последнего уравнения системы (14.76): at = a0 + t i=1 δi. Этот результат используется для преобразования μt: μt = μt−1 + a0 + t i=1 δi + εt. Далее, μt = μ0 + t i=1 εi + a0t + t−1 j=0(t − j)δj+1. Наконец, находим решение для xt: xt = μ0 + t i=1 εi + a0t + t−1 j=0(t − j)δj+1 + ηt. Каждый элемент в последовательности {xt} содержит детерминированный тренд, причем весьма специфического вида, стохастический тренд и шум ηt. Модель локального линейного тренда ведет себя как ARIMA(0, 2, 2). Действи-тельно, первые разности процесса Δxt = at + εt +Δηt нестационарны, посколь-ку at -процесс случайного блуждания. Однако, вторая разность Δ2xt = δt +Δεt +Δ2ηt уже стационарна и имеет с параметры: E(Δ2xt) = 0, var(Δ2xt) = σ2δ + σ2ε + 6σ2η, γ1 = −σ2ε − 4σ2η, γ2 = σ2η. Все остальные коэффициенты автоковариации γk для k > 2 равны нулю.
490 Глава 14. Линейные стохастические модели ARIMA 14.10. Упражнения и задачи Упражнение 1 Сгенерируйте ряд длиной 4000 наблюдений по модели AR(1) с параметром ϕ = 0.5 и нормально распределенной неавтокоррелированной ошибкой с единич-ной дисперсией, предполагая что значение ряда в момент t = 0 равно нулю. В дей-ствительности вид модели неизвестен, а задан только ряд. 1.1. Разбейте рядна 200 непересекающихся интервалов по 20 наблюдений. По каждому из них с помощью МНК оцените модель AR(1). Проанализи-руйте распределение полученных оценок авторегрессионного параметра. На-сколько велика дисперсия оценок и есть ли значимое смещение по сравнению с истинным параметром? 1.2. Рассмотрите построение прогноза на 1 шаг впередс помощью трех моде-лей: AR(1), AR(2) и модели линейного тренда. Для этого ряд следует раз-бить на 200 непересекающихся интервалов по 20 наблюдений. По каждому из этих интервалов с помощью МНК необходимо оценить каждую из трех моделей, построить прогноз и найти ошибку прогноза. Сравните среднеквад-ратические ошибки прогноза по трем моделям и сделайте выводы. Упражнение 2 Сгенерируйте 200 рядов длиной 20 наблюдений по модели AR(1) с авторе-грессионным параметром ϕ1 = (k −1)/200, k = 1, . . . , 200, с нормально распре-деленной неавтокоррелированной ошибкой и единичной дисперсией. По каждому ряду с помощьюМНКоцените модель AR(1).Постройте график отклонения оценки от истинного значения параметра в зависимости от истинного значения параметра ϕ1. Что можно сказать по этому графику о поведении смещения оценок в зависи-мости от ϕ1? Подтверждается ли, что оценки смещены в сторону нуля и смещение тем больше, чем ϕ1 ближе к единице? Упражнение 3 Сгенерируйте 200 рядов длиной 20 наблюдений по модели MA(2) с парамет-рами θ1 = 0.5, θ2 = 0.3 и нормально распределенной неавтокоррелированной ошибкой с единичной дисперсией. 3.1. Для каждого из рядов постройте выборочную автокорреляционную функцию для лагов 1, 2, 3. Рассмотрите распределение коэффициентов автокорреля-ции и сравните их с теоретическими значениями.
14.10 Упражнения и задачи 491 3.2. По каждому ряду на основе выборочных коэффициентов автокорреляции оцените модель MA(2), выбирая то решение квадратного уравнения, которое соответствует условиям обратимости процесса. Рассмотрите распределение оценок и сравните их с истинными значениями. Упражнение 4 Имеется информация о реальных доходностях ценных бумаг для трех фирм Blaster, Mitre, и Celgene (дневные доходности, приведенные к годовым). (См. табл. 14.1 􀀀􀀀􀀀.) 4.1. Изобразите график ряда для каждой из фирм и кратко охарактеризуйте свой-ства ряда. 4.2. Для каждой из фирм посчитайте среднее по двум разным непересекающимся подпериодам.Проверьте гипотезу равенства двух средних, используя простой t-критерий. 4.3. Для каждой из фирм посчитайте дисперсию по тем же двум подпериодам. Проверьте гипотезу равенства дисперсий, используя F-критерий. Какой вы-вод можно сделать относительно стационарности рядов? 4.4. Для каждой из фирм рассчитайте выборочные автоковариации, автокорре-ляции и выборочные частные автокорреляции для лагов 0, . . . , 6. Постройте соответствующие графики. Оцените значения параметров p и q модели ARMA(p, q). 4.5. Оцените параметры ϕ и θ модели ARMA(p, q) для каждой фирмы. 4.6. Постройте прогнозы доходности ценных бумаг на основе полученных моделей на 6 дней вперед. Упражнение 5 Для данных по производству природного газа в СССР (табл. 12.2, с. 403) по-стройте модель ARIMA(p, d, q) и оцените доверительный интервал для прогноза на 5 шагов вперед. Задачи 1. Записать с использованием лагового оператора случайный процесс: а) xt = μ + ϕ1xt−1 + ϕ2xt−2 + . . . + ϕpxt−p + εt;
492 Глава 14. Линейные стохастические модели ARIMA Таблица 14.1 t Blaster Mitre Celgene 1 0.41634 0.12014 0.37495 2 -0.23801 -0.05406 -0.18396 3 0.29481 -0.32615 0.15691 4 -0.61411 -0.29108 -0.40341 5 0.27774 -0.02705 0.05401 6 -0.40301 -0.78572 -0.11704 7 -0.45766 -1.29175 -0.75328 8 -0.89423 -2.42880 -0.73129 9 -1.88832 -1.67366 -1.96585 10 -0.28478 2.05054 -0.33561 11 1.58336 0.13731 1.85731 12 -1.55354 -1.62557 -1.79765 13 0.47464 0.40286 -0.12857 14 -0.35276 -0.60609 0.58827 15 -0.54645 0.32509 -1.19108 16 1.00044 -0.02063 1.21914 17 -1.26072 0.59998 -1.11213 18 2.05555 1.71220 1.51049 19 -0.37029 1.48972 0.48379 20 2.20692 2.90700 1.21797 21 1.26647 1.98764 2.19226 22 1.25773 1.16628 0.60026 23 0.88074 -1.06153 1.25668 24 -1.52961 -1.80661 -1.52435 25 -0.04273 1.19855 -0.15779 26 0.72479 0.76704 1.33383 27 -0.18366 0.18001 -0.56059 28 0.79879 0.50357 0.61117 29 -0.21699 1.20339 0.12349 30 1.55485 1.29821 1.23577 31 -0.01895 0.57772 0.41794 32 0.99008 0.30803 0.46562 33 -0.32137 -0.55553 0.21106 34 -0.12852 -1.63234 -0.49739 35 -1.43837 -0.79997 -1.10015 36 0.28452 0.83711 0.05172 37 0.14292 1.01700 0.54283 38 0.78978 0.93925 0.33560 39 0.48850 -0.73179 0.65711 40 -0.96077 0.31793 -1.09358 41 1.45039 -1.34070 1.42896 42 -2.99638 -0.75896 -2.47727 t Blaster Mitre Celgene 43 2.26050 0.04130 1.26855 44 -2.67700 1.04649 -1.18533 45 3.59910 1.50975 1.83224 46 -1.91408 -0.78975 -0.08880 47 1.41616 0.89511 -0.57678 48 -0.13194 0.49100 1.61395 49 0.30387 -0.62728 -0.74057 50 -0.40587 -0.53830 0.09482 51 -0.31838 1.17944 -0.52098 52 1.42911 0.42852 1.59066 53 -1.06378 -0.62532 -0.95433 54 0.83204 -1.31001 0.25028 55 -2.19379 -1.15293 -1.40754 56 0.77155 -0.56983 -0.01986 57 -1.72097 0.12615 -0.77365 58 1.49679 -0.37488 0.48386 59 -1.90651 0.63942 -0.99107 60 2.46331 0.47653 1.36935 61 -1.94601 0.52649 -0.69769 62 2.61428 2.10024 1.16649 63 -0.29189 0.75862 1.24826 64 1.28001 0.44524 -0.08931 65 -0.12871 -1.52804 0.85085 66 -1.39518 -1.76352 -1.92776 67 -0.25838 -0.16521 -0.01712 68 -0.58997 0.26832 -0.30668 69 0.57022 -0.32595 0.19657 70 -0.90955 0.05597 -0.70997 71 0.97552 0.40011 0.58198 72 -0.64304 -0.27051 -0.11007 73 0.42530 0.30712 -0.17654 74 -0.02964 -0.43812 0.48921 75 -0.50316 -0.19260 -0.82995 76 0.41743 0.35469 0.50887 77 -0.26740 -0.43595 -0.10447 78 -0.10744 -0.69389 -0.39020 79 -0.56917 0.77134 -0.37322 80 1.14361 0.19675 1.03231 81 -0.99591 0.28117 -0.74445 82 1.49172 0.62375 0.92118 83 -0.83109 -0.96831 -0.06863 84 -0.00917 1.77019 -0.81888 t Blaster Mitre Celgene 85 1.86053 1.29274 2.52889 86 -0.71307 0.01583 -0.79911 87 1.43504 2.43129 0.86014 88 1.13397 0.92309 1.99122 89 0.06334 -1.09645 -0.48509 90 -0.42419 0.20130 -0.32999 91 0.56689 -1.83314 0.76937 92 -2.57400 -1.88158 -2.43071 93 0.59467 1.17998 0.05017 94 -0.09736 -0.17633 0.85066 95 -0.32876 -0.36017 -1.22773 96 0.31690 -0.82044 0.56133 97 -1.38083 -1.70314 -1.28727 98 -0.36896 1.23095 -0.68313 99 1.17509 0.94729 1.62377 100 -0.47758 1.29398 -0.64384 101 2.24964 0.14180 1.81862 102 -1.87715 -0.80889 -1.17413 103 1.42413 0.46915 0.43877 104 -1.04233 1.22177 0.25152 105 2.09249 1.09135 0.99786 106 -0.65041 0.10892 0.29152 107 1.02974 -1.06781 0.05320 108 -1.75288 -0.89211 -0.86576 109 0.65799 0.73063 -0.05176 110 -0.17125 -0.07063 0.67682 111 -0.08333 0.45717 -0.79937 112 0.81080 0.32701 1.08553 113 -0.58788 -0.50112 -0.54259 114 0.33261 1.75660 0.02756 115 1.37591 0.46400 1.79959 116 -0.84495 -2.36130 -0.98938 117 -0.99043 -0.78306 -1.35391 118 -0.04455 -0.13031 0.46131 119 -0.70314 -1.05764 -0.71196 120 -0.43426 -0.38007 -0.74377 121 -0.13906 0.42198 0.04100 122 0.27676 -1.59661 0.20112 123 -1.86932 -0.04115 -1.89508 124 1.74776 2.69062 1.46584 125 0.52476 1.04090 1.30779 126 0.86322 0.23965 -0.09381
14.10 Упражнения и задачи 493 Таблица 14.1. (продолжение) t Blaster Mitre Celgene 127 0.12222 1.31876 0.60471 128 1.30642 0.45303 1.17204 129 -0.57192 0.41682 -0.26719 130 1.30327 0.32936 0.87774 131 -0.84416 0.61935 -0.20724 132 1.58641 0.83562 0.93209 133 -0.58774 0.34732 0.17484 134 1.12133 -0.03793 0.34328 135 -0.88277 -0.54302 -0.15044 136 0.33158 -1.70839 -0.28336 137 -2.03067 -1.22414 -1.43274 138 0.43818 -0.54955 -0.13700 139 -1.44659 -0.53705 -0.71974 140 0.56263 1.64035 -0.26807 141 0.96118 1.70204 1.61264 142 0.71138 0.30461 0.25218 143 0.25957 -0.42989 0.26638 144 -0.39911 1.64975 -0.31866 145 2.09642 2.59804 2.14297 146 0.55396 2.44417 0.91428 147 2.59219 0.87112 1.93566 148 -0.92353 -1.59130 -0.23407 149 -0.06896 -2.00737 -0.80695 150 -1.90109 -0.21639 -0.97295 151 1.04222 0.08106 0.52037 152 -1.32252 -1.41365 -0.69199 153 -0.15447 0.10208 -1.09443 154 0.16722 0.11579 0.87557 155 -0.45657 -0.63291 -0.78207 156 0.00584 -0.89310 -0.08703 157 -1.05101 0.07757 -0.86295 158 0.93651 -0.18388 0.70155 159 -1.32255 -1.33106 -0.91757 160 0.01609 0.35961 -0.66348 161 0.15497 3.33824 0.82107 162 2.87330 0.51674 2.49646 163 -1.82484 0.42367 -1.53503 164 3.02300 0.22201 2.08618 165 -2.73530 0.34709 -1.13867 166 3.23531 0.78318 1.51341 167 -2.32656 0.76416 -0.36499 168 3.15157 0.54297 1.16592 t Blaster Mitre Celgene 169 -2.20714 -0.41948 -0.27731 170 1.83600 -1.04210 -0.00317 171 -2.62281 0.90594 -0.89549 172 3.12742 -0.71167 1.64780 173 -3.79724 -0.02254 -2.18906 174 3.80089 2.35129 1.77382 175 -1.54768 -0.64010 0.83427 176 1.05327 -1.19450 -1.29993 177 -1.57945 0.55914 0.11559 178 1.54446 0.18847 0.55672 179 -1.28825 0.66119 -0.37065 180 1.94968 0.46034 0.92503 181 -1.29186 -0.82241 -0.26353 182 0.58699 -1.93992 -0.48556 183 -2.43099 -0.43524 -1.43990 184 1.49486 -0.15310 0.67209 185 -2.02672 -0.86983 -1.00252 186 1.01176 0.74648 -0.28958 187 -0.32973 0.50541 0.83129 188 0.60712 -1.01920 -0.30054 189 -1.24396 -0.47570 -0.74958 190 0.61988 0.73345 0.21236 191 -0.09182 1.10966 0.49188 192 1.14494 -0.35472 0.60138 193 -1.15962 -0.25419 -0.80978 194 1.05295 0.23071 0.57739 195 -0.92113 -0.21711 -0.17697 196 0.66513 0.87219 -0.07976 197 0.28363 1.51583 0.92934 198 1.19450 0.91161 0.75373 199 0.17008 0.90071 0.41868 200 1.08470 -0.51061 0.82681 201 -1.27643 -0.82689 -0.90190 202 0.57728 0.16717 0.14775 203 -0.61943 0.62127 0.07797 204 1.06910 1.37518 0.45461 205 0.47147 -0.10956 0.94669 206 -0.36583 1.29469 -0.80073 207 2.02680 1.27776 2.17034 208 -0.75375 -0.42283 -0.35325 209 0.86800 -0.58282 0.10153 210 -1.27776 0.43351 -0.41637 t Blaster Mitre Celgene 211 1.51485 -0.97684 0.86781 212 -2.50036 -1.13180 -1.74068 213 1.32618 1.48218 0.27384 214 -0.16750 0.39183 1.11966 215 0.42028 -0.52533 -0.69835 216 -0.42438 -1.01260 0.10685 217 -0.74802 -0.83307 -0.98493 218 -0.18508 -0.07511 -0.02618 219 -0.27858 0.72752 -0.21877 220 0.87498 -1.35822 0.70596 221 -2.18781 -1.93248 -2.04429 222 0.29517 -0.01706 -0.23267 223 -0.87564 -2.02272 0.01847 224 -1.55568 0.90288 -2.35370 225 2.33976 1.99827 2.58159 226 -0.86069 -0.33414 -0.42717 227 1.13389 -0.46117 -0.00198 228 -1.28321 1.37416 -0.22666 229 2.46195 0.98181 1.71017 230 -1.33630 0.18633 -0.40385 231 1.88226 0.65077 0.67102 232 -0.96236 0.01818 0.31244 233 0.99833 0.56680 -0.07017 234 -0.15850 0.72097 0.75775 235 0.79860 0.01263 0.17952 236 -0.44636 0.10951 -0.02671 237 0.60990 -0.43066 0.26746 238 -0.97024 -0.33060 -0.56602 239 0.58530 1.47322 0.16698 240 0.78037 -0.52826 1.28774 241 -1.25522 -0.88925 -1.62637 242 0.72438 -1.90852 0.57099 243 -2.97849 -1.06136 -2.35532 244 1.61219 1.26234 0.73721 245 -0.92336 -0.22703 0.30842 246 0.60912 1.03409 -0.76861 247 0.78275 -0.54114 1.64521 248 -1.45383 -0.91160 -1.83561 249 0.90928 2.84407 0.73683 250 1.53899 1.62233 2.23506 
494 Глава 14. Линейные стохастические модели ARIMA б) xt = μ + εt + θ1εt−1 + θ2εt−2 + . . . + θqεt−q ; в) xt = μ+ϕ1xt−1+ϕ2xt−2+. . .+ϕpxt−p+εt+θ1εt−1+θ2εt−2+. . .+θqεt−q . 2. Вывести формулы для вычисления математического ожидания, дисперсии и ковариаций случайного процесса xt = μ + εt + ∞i=1ϕiεt−i при условии его слабой стационарности, если εt -белый шум с дисперсией σ2 и матема-тического ожидания E(εtxt−k) = 0, ∀ |k| 1. 3. Вывести формулы для вычисления математического ожидания, дисперсии и ковариаций случайного процесса xt = μ + ϕ1xt−1 + εt при условии его слабой стационарности, если εt -белый шум с дисперсией σ2. 4. Обосновать утверждение о том, что модель авторегрессии является частным случаем модели линейного фильтра. 5. Записать случайный процесс xt = 0.2+0.6xt−1 +εt с использованием лаго-вого оператора и виде процесса скользящего среднего бесконечного порядка. 6. При каких условиях процесс AR(1) стационарен и обратим? 7. Задана модель: xt = 0.25xt−1+εt, гд е εt -белыйшум. Дисперсия процесса xt равна единице. Вычислить дисперсию белого шума. 8. Чему равна дисперсия Марковского процесса xt = 0.5xt−1+εt, если диспер-сия белого шума равна 1? Изобразить график автокорреляционной функции данного процесса. 9. Для процесса xt = −0.7 − 0.7xt−1 + εt, гд е εt -белый шум, рассчитать частную автокорреляционную функцию, вычислить первые 6 значений ав-токорреляционной функции и начертить ее график. 10. Для модели AR(1): xt = μ + 0.5xt−1 + εt показать, что частная автокорре-ляционная функция ϕ1,1 = 0.5, ϕk,k = 0 при k 2. 11. Даны два марковских процесса: xt = 0.5xt−1 + εt; yt = 0.2yt−1 + εt. Дисперсия какого процесса больше и во сколько раз? 12. Найти математическое ожидание, дисперсию и ковариации случайного про-цесса xt, если εt -белый шум с единичной дисперсией. а) xt = 0.1 + 0.9xt−1 + εt; б)xt = −0.2xt−1 + εt. 13. Найти спектр процесса xt = εt + 0.1εt−1 + 0.01εt−2 + . . . .
14.10 Упражнения и задачи 495 14. Корень характеристического уравнения, соответствующего процессу Мар-кова, равен 2, остаточная дисперсия равна 1. Найти значение спектра на частоте 0.5. 15. Имеется ли разница в графиках спектра для процессов xt = 0.9xt−1 + εt и xt = 0.2xt−1 + εt? Если да, то в чем она выражается? 16. Корни характеристических уравнений, соответствующих двум марковским процессам, равны +1.25 и −1.25. В чем отличие процессов и как это разли-чие отражается на графиках спектра и автокорреляционной функции? 17. Пусть εt - белый шум с единичной дисперсией. Найти математическое ожидание, дисперсию и ковариации случайного процесса: а) xt = 1+0.5xt−1 + εt; б)xt = 0.5xt−1 + εt. 18. Проверить на стационарность следующие процессы: а) xt − 0.4xt−1 − 0.4xt−2 = εt; б)xt + 0.4xt−1 − 0.4xt−2 = εt; в) xt − 0.4xt−1 + 0.4xt−2 = εt . Изобразить схематически графики автокорреляционной функции этих про-цессов. Проверить правильность выводов с помощью точного вычисления автокорреляционной функции для каждого из процессов. 19. Корни характеристического уравнения для процесса Юла равны, соответ-ственно, 5 и −5. Изобразить график автокорреляционной функции. Дать обоснование. 20. Корни характеристического уравнения, соответствующего процессу Юла, равны 1.9 и −1.3. Изобразить график автокорреляционной функции это-го процесса. Ответ обосновать. 21. Коэффициенты автокорреляции первого и второго порядка в процессе Юла равны, соответственно, 0.5 и 0.4. Оценить параметры процесса. Найти дис-персию белого шума, если дисперсия процесса равна 1. 22. При каких значениях ϕ2 следующие случайные процессы являются стацио-нарными в широком смысле? а) xt = xt−1 + ϕ2xt−2 + εt; б)xt = −xt−1 + ϕ2xt−2 + εt. Вывести автокорреляционные функции данных случайных процессов при ϕ2 = −0.5. 23. Параметры ϕ1 и ϕ2 процесса AR(2) равны, соответственно, 0.6 и −0.2. Каковы первые три значения автокорреляционной функции?
496 Глава 14. Линейные стохастические модели ARIMA 24. Для процесса xt = 0.5xt−1 + 0.25xt−2 + εt коэффициенты автокорреля-ции первого и второго порядка равны, соответственно, 23 и 712. Найти коэффициент автокорреляции четвертого порядка. 25. Пусть процесс AR(2) xt = ϕ1xt−1 + ϕ2xt−2 + εt является стационарным в широком смысле, и εt - белый шум с дисперсией σ2. Показать, что частная автокорреляционаня функция ϕ1, 1 = ϕ1 1 − ϕ2, ϕ2, 2 = ϕ21ϕ2 + ϕ2(1 − ϕ2)2 (1 − ϕ1 − ϕ2)(1 + ϕ1 − ϕ2) , ϕk, k = 0, при k 3. 26. По известным значениям частной автокорреляционной функции ϕ1, 1 = 0.5 и ϕ2, 2 = 23 случайного процесса найти значения коэффициентов автокор-реляции первого и второго порядка. 27. Пусть процесс AR(p) является стационарным в широком смысле. Показать, что частная автокорреляционаня функция ϕp+1,p+1 = 0. 28. В каком случае процесс, описываемый моделью MA(q), стационарен и об-ратим? 29. Коэффициент автокорреляции первого порядка для обратимого процесса скользящего среднего первого порядка равен −0.4. Записать уравнение про-цесса и изобразить график его автокорреляционной функции. 30. Показать, что обратимый процесс MA(1) можно представить в виде процесса авторегрессии. 31. Показать, что процесс xt = μ + εt + θ1εt−1 эквивалентен процессу AR(∞), если |θ1| < 1 и εt -белыйшум. 32. Найти автокорреляционную функцию процесса: xt = εt − 0.5xt−1 − 0.25xt−2 − 0.125xt−3 − 0.0625xt−4 + . . . . 33. Вывести формулы для вычисления математического ожидания, дисперсии и ковариаций случайного процесса xt = μ + εt + θ1εt−1 при условии его слабой стационарности, если εt -белый шум с дисперсией σ2. 34. Пусть εt -белый шум с единичной дисперсией. Чему равна дисперсия про-цесса xt = εt + 0.2εt−1? Изобразить график автокорреляционной функции. 35. Идентифицировать процесс, автокорреляционная функция которого имеет следующий вид: а) ρ1 = 0.25, ρk = 0, ∀k 2; б)ρ1 = −0.4, ρk = 0, ∀k 2.
14.10 Упражнения и задачи 497 36. Является ли случайный процесс, автокорреляционнаяфункция которого име-ет следующий вид: ρ1 = 0.5, ρk = 0, ∀k 2, обратимым? 37. Для каждого из случайных процессов: а) xt = εt + 0.5εt−1; б)xt = εt − 0.5εt−1; в)xt = −1 + εt + 0.8εt−1; рассчитать частную автокорреляционнуюфункцию, вычислитьпервые 6 зна-чений автокорреляционной функции и построить ее график. 38. Показать, что частные автокорреляционные функции следующих слабо ста-ционарных случайных процессов совпадают: а) xt = μ + εt + θ1εt−1 и zt = υt + θ1υt−1, б) xt = μ + εt + θ1εt−1 + θ2εt−2 + . . . + θqεt−q и zt = υt + θ1υt−1 + θ2υt−2 + . . . + θqυt−q , где εt и υt-процессы белогошумас дисперсиями σ2ε и σ2v , соответственно. 39. Имеется следующий обратимый процесс : xt = εt+θ1εt−1+θ2εt−2, гд е εt - белый шум с дисперсией σ2. Рассчитать коэффициенты автоковариации. Записать автокорреляционную функцию для этого процесса. 40. Построить график автокорреляционной функции процесса: а) xt = εt + 0.5εt−1 − 0.3εt−2; б)xt = 1+εt − 0.4εt−1 + 0.4εt−2. 41. Переписать случайный процесс xt = 0.5xt−1 + 0.5xt−2 + εt − εt−1 + 3εt−2 с использованием лагового оператора, где εt - белый шум. Проверить процесс на стационарность и обратимость. 42. Найти математическое ожидание, дисперсию и ковариации случайного про-цесса xt = 0.5xt−1 +εt −0.7εt−1, если εt -белый шум. Построить график автокорреляционной функции. 43. Напримере процессаARMA(1, 1) продемонстрировать алгоритм оценивания его параметров методом моментов. 44. Найти параметры модели ARMA(1, 1), если ρ1 = 31 41 , ρ2 = 93 205 . 45. Проверить на стационарность и обратимость процесс xt = 0.6 + 0.3xt−1 + 0.4xt−2 + εt − 0.7εt−1, где εt -белый шум с дисперсией σ2. Представить процесс в виде AR(∞), если это возможно.
498 Глава 14. Линейные стохастические модели ARIMA 46. Определить порядок интегрирования процесса xt = 1.5xt−1 +0.5xt−2 +εt − − 0.5εt−1. Ответ обосновать. 47. Для модели (1 − L)(1 + 0.4L)xt = (1 − 0.5L)εt определить параметры p, d , q. Является ли процесс стационарным? 48. Записать формулу расчета коэффициента автоковариации первого порядка для процесса ARIMA(2, 2, 2). 49. Какую роль выполняет оператор скользящего среднего в прогнозировании процессов ARMA(p, q)? Ответ обосновать. 50. Построить точечный прогноз на один шаг вперед, если известно, что процесс xt = 0.1xt−1 + εt + 0.2εt−1, xT = 10, εT = 0.1. 51. Построить доверительный интервал для прогноза на два шага вперед для случайного процесса xt = 0.5xt−1+εt, если известно, что xT = −1.6 и εt - белый шум с единичной дисперсией. 52. Построить интервальный прогноз на 2 шага впередд ля случайного процесса: а) xt = 1+εt + 0.7εt−1, если εt -белый шум с единичной дисперсией и εT = −6.7; б) xt = 1+1.3xt−1 + εt, если εt -белый шум с единичной дисперсией и xT = 7.1, xT−1 = 6.7, εt = 0.5. 53. Записать в компактной и развернутой формах уравнение процесса ARIMA(1, 2, 2), привести формулу доверительного интервала для прогноза на 4 шага вперед с выводом формул для параметров ψj и дисперсии белого шума. 54. Записать формулу доверительного интервала для прогноза по модели ARIMA(1, 1, 1), с выводом формул для ψj и дисперсии белого шума. Рекомендуемая литература 1. Айвазян С.А. Основы эконометрики. Т. 2.-М.: "Юнити", 2001. (Гл. 3). 2. Андерсон Т. Статистический анализ временных рядов.-М.: "Мир", 1976. (Гл. 5). 3. Бокс Дж., Дженкинс Г. Анализ временных рядов. Прогноз и управление. Вып. 1.-М.: "Мир", 1974. (Гл. 3-6).
14.10 Упражнения и задачи 499 4. Кендалл М. Дж., Стьюарт А. Многомерный статистический анализ и вре-менные ряды.-М.: "Наука", 1976. (Гл. 47). 5. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 12). 6. Справочник по прикладной статистике. В 2-х т. Т. 2. //Под ред. Э. Ллойда, У. Ледермана.-М.: "Финансы и статистика", 1990. (Гл. 18). 7. Chatfield Chris. The Analysis of Time Series: An Introduction... 5th ed.- Chapman & Hall/CRC, 1996. (Ch. 3-5). 8. Enders Walter. Applied Econometric Time Series. - Iowa State University, 1995. (Ch. 5). 9. JudgeG.G., GriffithsW.E., Hill R.C., Luthepohl H., Lee T. Theory and Practice of Econometrics.-New York: John Wiley & Sons, 1985. (Ch. 7). 10. Hamilton JamesD., Time Series Analysis.-PrincetonUniversity Press, 1994. (Ch. 3, 4). 11. Mills Terence C. Time Series Techniques for Economists.-Cambridge University Press, 1990. (Ch. 5-8). 12. Pollock D.S.G. A handbook of time-series analysis, signal processing and dynamics.-"Academic Press", 1999. (Ch. 16-19).
Глава 15 Динамические модели регрессии При моделировании экономических процессов с помощью регрессионного ана-лиза часто приходится наряду с некоторым временным рядом вводить в модель также лаг этого ряда.Вэкономике практически нет примеровмгновенного реагиро-вания на какое-либо экономическое воздействие-существуют задержки прояв-ления эффектов от капиталовложений, внесения удобрений и т.д., иными словами, при моделировании необходимо учитывать воздействие факторов в предыдущие моменты времени. Выше были введены некоторые из таких моделей: регрессия с распределенным лагом и модели ARIMA. В этой главе рассматриваются различ-ные аспекты подобного рода моделей. 15.1. Модель распределенного лага: общие характеристики и специальные формы структур лага Напомним, что простейшаямодель распределенного лага-этомодель регрес-сии, в которой на динамику исследуемой переменной xt влияет не только какой-то объясняющий фактор zt, но и его лаги. Модель имеет следующий вид: xt = μ + q j=0 αjzt−j + εt = μ + α(L)zt + εt, (15.1)
15.1 Модель распределенного лага 501 где α(L) = q j=0 αjLj, a q -величина максимального лага. Данную модель можно охарактеризовать следующими показателями. Функция реакции на импульс (impulse response function, IRF) показывает, насколько изменится xt при изменении zt−j на единицу для лагов j = 0, 1, 2, .... Таким образом, можно считать, что речь идет о производной dxt dzt−j как функции запаздывания j. Ясно, что для модели распределенного лага этот показатель сов-падает с коэффициентом αj при j q и равен нулю при j > q. При j < 0 (влияние будущих значений переменной z на переменную x) реакцию на импульс можно положить равной нулю. Накопленная реакция на импульс для лага k -это просуммированные зна-чения простой функции реакции на импульс от j = 0 до j = k. Для мод ели распределенного лага это сумма коэффициентов: min{k, q} j=0 αj . Долгосрочный мультипликатор является измерителем общего влияния пере-менной z на переменную x. Он равен αΣ = q j=0 αj = α(1). Это предельное значение накопленной реакции на импульс. Если x и z - логарифмы исходных переменных, то αΣ -долгосрочная эластичность. Средняя длина лага показывает, на сколько периодов в среднем запаздывает влияние переменной z на переменную x. Она вычисляется по формуле ¯j= q j=0 jαj q j=0 αj = q j=0 jαj αΣ . Заметим, что среднюю длину лага можно записать через производную логариф-ма многочлена α(L) в точке 1. Действительно, α(v) = ⎛⎝ q j=0 αjvj⎞⎠= q j=0 jαjvj−1
502 Глава 15. Динамические модели регрессии и α(1) = q j=0 jαj . Поэтому ¯j= α(1) α(1) = (lnα(v))v=1. Наряду со средней длиной лага можно рассматривать также медианную длину лага, то есть такую величину лага, при которой накопленная функция реакции на импульс равна половине долгосрочного мультипликатора. Ясно, что для большин-ства возможных структур лага такое равенство может выполняться только при-ближенно. Поэтому невозможно дать однозначное определение медианной длины лага. Оценивание модели распределенного лага может быть затруднено проблемой мультиколлинеарности, если величина фактора zt мало меняется со временем. Ес-ли zt -случайный процесс, то такая ситуация возникает, когда данный процесс сильно положительно автокоррелирован. Например, это может быть авторегрес-сия первого порядка с коэффициентом авторегрессии, близким к единице. Если бы фактор zt был линейным трендом, например, zt = t, то модель невозможно было бы оценить. Действительно, несложно увидеть, что тогда zt, zt−1 = t − 1 и константа связаны между собой линейной зависимостью. Если zt -линейный трендс добавлением небольшой стационарной случайной составляющей, то, хо-тя строгой линейной зависимости уже не будет, проблема мультиколлинеарности останется. Если возникает подобная проблема мультиколлинеарности, то нельзя точно оценить структуру лага, хотя можно оценить сумму весов αi -т.е. долгосрочный мультипликатор αΣ. Эта сумма вычленяется из модели следующим образом: xt = μ + αΣzt + q j=1 αj (zt−j − zt) + εt. Вслучае мультиколлинеарности лаговых переменных обычно на лаговуюструк-туру накладывают какое-нибудь ограничение, чтобы уменьшить количество оце-ниваемых коэффициентов. Ниже рассматриваются две наиболее важные модели этого типа. Полиномиальный лаг Одна из возможных структур лага-полиномиальный лаг1, веса которого задаются многочленом от величины лага j: αj = p s=0 γsjs, j= 0, . . . , q, (15.2) 1Эту модель предложила С. Алмон, поэтому часто используют термин "лаг Алмон" (Almon lag).
15.1 Модель распределенного лага 503 αj 0 1 2 q j . . . . . Рис. 15.1 где p -степень многочлена, p < q.Вводя такую зависимость, мы накладываем q − p линейных ограничений на структуру лага. Простейший полиномиальный лаг-линейный. Для него αj = γ0 + γ1j. Как правило, здесь γ1 < 0. Его структура изображена на диаграмме (рис. 15.1). Поскольку исходная модель регрессии линейна и ограничения, которые поли-номиальный лаг накладывает на ее коэффициенты, являются линейными, то полу-ченная модель останется линейной. Рассмотрим, каким образом ее можно оценить. C учетом выражений для αj , проведем преобразование исходной модели: q j=0 αjzt−j = q j=0p s=0 γsjs" α#$j %zt−j = p s=0 γs q j=0 jszt−j = p s=0 γsyts. Получим новую модель линейной регрессии: xt = μ + p s=0 γsyts + εt с преобразованными факторами yts = q j=0 jszt−j . Оценив γs, можно вычислить веса αj , воспользовавшись формулой (15.2). При оценивании модели с ограничениями на структуру лага нужно проверить, правильно ли наложены ограничения. Спомощью соответствующей F-статистики можно сравнить ее с исходной, неограниченной моделью, поскольку она является ее частным случаем. Модель xt = μ + q s=0 γsyts + εt
504 Глава 15. Динамические модели регрессии αj 0 1 2 j 3 . . . . Рис. 15.2 эквивалентна исходной модели с точностью до линейных преобразований, поэто-му достаточно проверить гипотезу о том, что последние q − p коэффициентов (γp+1, . . . , γq) равны нулю. Часто принимают, что веса на концах полиномиальной лаговой структуры(15.2) равны нулю. Это требование накладывает на коэффициенты модели дополнитель-ные ограничения. Можно, например, потребовать, чтобы αq = 0, то есть p s=0 γsqs = 0. Учесть такие ограничения несколько сложнее, но в целом не требуется выходить за рамки обычной линейной регрессии. Геометрический лаг Еще один популярный вид структуры лага-геометрический лаг. Его веса αj задаются следующими соотношениями: αj = α0δj, j= 0, . . . , ∞, где 0 < δ < 1. Веса геометрического лагаубывают экспоненциально с увеличением лага (рис. 15.2). Модель распределенного лага с этими весами, модель Койка, имеет следующий вид: xt = μ + α0 ∞j=0 δjzt−j + εt. (15.3) Используя формулу суммы бесконечной геометрической прогрессии, получим α(v) = ∞j=0αjvj = α0 ∞j=0 (δv)j = α0 1 − vδ .
15.1 Модель распределенного лага 505 Сумма весов в этой модели (долгосрочный мультипликатор) равна αΣ = ∞j=0 αj = α(1) = α0 1 − δ . Кроме того, ln α(v) = lnα0 − ln(1 − vα) и (ln α(v))= δ 1 − vδ , поэтому средняя длина геометрического лага равна ¯j= (lnα(v))v=1 = δ 1 − δ . Чтобы избавиться от бесконечного ряда, к модели с геометрическим лагом при-меняют преобразование Койка (Koyck transformation). Сдвинем исходное урав-нение на один период назад: xt−1 = μ + ∞j=0 α0δjzt−j−1 + εt−1, затем умножим это выражение на δ и вычтем из исходного уравнения (15.3): xt − δxt−1 = (1 − δ)μ + α0zt + εt − δεt−1. (15.4) Такой же результат можно получить, используя лаговые операторы: xt = μ + α0 ∞j=0 δjzt−j + εt = μ + α0∞j=0 (δL)jzt + εt. Выражение в скобках упрощается с использованием формулы суммы беско-нечной геометрической прогрессии: xt = μ + α0 1 1 − δLzt + εt. Умножим это уравнение на оператор (1 − δL): (1 − δL) xt = (1 − δL) μ + α0zt + (1 − δL) εt или учитывая, что оператор сдвига, стоящий перед константой, ее сохраняет, по-лучаем формулу (15.4). В результате имеем следующую модель: xt = μ+ δxt−1 + α0zt + εt,
506 Глава 15. Динамические модели регрессии где μ= (1−δ)μ и εt = εt−δεt−1. Это частный случай авторегрессионной модели с распределенным лагом, рассматриваемой в следующем пункте. Заметим, что в полученной здесь модели ошибка εt не является белымшумом, а представляет собой процесс скользящего среднего первого порядка. Модель яв-ляется линейной регрессией, однако для нее не выполнено требование о некорре-лированности регрессоров и ошибки. Действительно, εt−1 входит как в xt−1, так и в εt. Следовательно, оценки метода наименьших квадратов не являются состоя-тельными и следует пользоваться другими методами. Можно оценивать модель Койка в исходном виде (15.3). Сумму в этом урав-нении можно разделить на две части: соответствующую имеющимся наблюдени-ям для переменной zt и относящуюся к прошлым ненаблюдаемым значениям, т.е. z0, z−1 и т.д.: xt = μ + α0t−1 j=0 δjzt−j + α0 ∞j=t δjzt−j + εt. Далее, во второй сумме сделаем замену j = s + t: xt = μ + α0t−1 j=0 δjzt−j + α0δt ∞s=0 δsz−s + εt. Обозначив θ = α0 ∞s=0 δsz−s, получим модель нелинейной регрессии с четырьмя неизвестными параметрами: xt = μ + α0t−1 j=0 δjzt−j + θδt + εt. В такой модели ошибка и регрессоры некоррелированы, поэтому нелинейный МНК дает состоятельные оценки. 15.2. Авторегрессионная модель с распределенным лагом Авторегрессионная модель с распределенным лагом является примером ди-намической регрессии, в которой, помимо объясняющих переменных и их лагов, в качестве регрессоров используются лаги зависимой переменной.
15.2. Авторегрессионная модель с распределенным лагом 507 Авторегрессионную модель с распределенным лагом, которая включает одну независимую переменную, можно представить в следующем виде: xt = μ + p j=1ϕjxt−j + q j=0αjzt−j + εt, (15.5) где первая сумма представляет собой авторегрессионнуюкомпоненту-распреде-ленный лаг изучаемой переменной, вторая сумма-распределенный лаг незави-симого фактора. Обычно предполагается, что в этой модели ошибки εt являются белым шумом и не коррелированны с фактором zt, его лагами и с лагами изуча-емой переменой xt. При этих предположениях МНК дает состоятельные оценки параметров модели. Сокращенно эту модель обозначают ADL(p, q) (от английского autoregressive distributed lag), также часто используется аббревиатура ARDL, гд е p -поря-док авторегрессии, q -порядок распределенного лага. Более компактно можно записать модель в операторной форме: ϕ(L) xt = μ + α (L) zt + εt, где ϕ(L) = 1 − p j=1ϕjLj и α(L) = q j=0 αjLj -лаговые многочлены. Модель ADL(1, 1) имеет следующий вид: xt = μ + ϕ1xt−1 + α0zt + α1zt−1 + εt. Некоторые частные случаи модели ADL уже были рассмотрены ранее. Модель ADL(0, q) - это модель распределенного лага, рассмотренная в предыдущем пункте (в правой части нет лагов зависимой переменной). Модель геометрического распределенного лага после преобразования Койка можно интерпретировать как ADL(1, 0) с процессом MA(1) в ошибке и ограниче-нием на коэффициент при xt−1, который равен параметру MA-процесса (δ): xt = μ+ δxt−1 + α0zt + (εt − δεt−1). Авторегрессионную модель AR(p) можно считать ADL(p, −1). В этой модели переменная в левой части зависит только от своих собственных лагов: xt = μ + p j=1ϕjxt−j + εt. Как и в случае модели распределенного лага, можно ввести ряд показателей, характеризующихмодельADL.Если обратить лаговый многочлен ϕ(L) и умножить
508 Глава 15. Динамические модели регрессии на него исходное уравнение модели, то получим xt = ϕ−1(L)ϕ(L)xt = μ ϕ(L) + α(L) ϕ(L) zt + εt ϕ(L) или xt = μ∗ + ∞i=0 πizt−i + ε∗t , где μ∗ = μ ϕ(1), ε∗t = εt ϕ(L) и α(L) ϕ(L) = π(L) = ∞i=0 πiLi. Как и в модели ARMA, такое преобразование корректно, если все корни мно-гочлена ϕ(-) лежат за пределами единичной окружности. Коэффициенты πi показывают влияние лагов переменной z на переменную x, то есть они представляют собой функцию реакции на импульс. Символически эти коэффициенты можно записать в виде: πi = dxt dzt−i . Рекуррентная формула для расчета коэффициентов πi получается дифферен-циацией по zt−i исходного уравнения модели (15.5): πi = d(μ + p j=1ϕjxt−j + q j=0αjzt−j + εt) dzt−i = p j=1ϕjπi−j + αi. Здесь принимается во внимание, что dxt−j dzt−i = πi−j , dzt−j dzt−i =⎧⎪⎨ ⎪⎩ 0, j = i, 1, j = i, и dεt dzt−i = 0. При использовании этой рекуррентной формулы следует взять πi = 0 для i < 0. В частном случае модели распределенного лага (когда p = 0) эта формула дает πi = αi, то есть влияние zt−i на πi количественно выражается коэффициентом при zt−i (весом лага).
15.2 Некоторые прикладные динамические модели 509 Сумма коэффициентов πi показывает долгосрочное влияние z на x (долго-срочный мультипликатор). Она равна πΣ = ∞i=0 πi = π(1) = α(1) ϕ(1) = q j=0 αj 1 − p j=1ϕj . (15.6) По аналогии смоделью распределенного лагаможно ввести показатель средней длины лага влияния z на x. Он равен ∞i=0 iπi ∞i=0 πi = (lnπ(v))v=1 = (lnα(v) − ln ϕ(v))v=1 = q j=0 jαj q j=0 αj + p j=1 jϕj 1 − p j=1ϕj . 15.3. Модели частичного приспособления, адаптивных ожиданий и исправления ошибок Рассмотрим некоторые прикладные динамические модели, сводящиеся к моде-ли авторегрессионного распределенного лага. Модель частичного приспособления В экономике субъекты не сразу могут приспособиться к меняющимся усло-виям - это происходит постепенно. Нужно время на изменение запасов, обу-чение, переходна новые технологии, изменение условий долгосрочных контрак-тов и т.д. Эти процессы можно моделировать с помощью модели частичного приспособления. Для иллюстрации приведем следующий пример: инфляция зависит от денежной массы, меняя денежную массу, мы можем получить какой-то желаемый уровень инфляции. Но реальность несколько запаздывает. Пусть xDt-желаемый уровень величины xt, zt -независимый фактор, определяющий xDt. Тогдамодель частичного приспособления задается следующими двумя уравнениями: xDt= β + αzt + ξt, xt − xt−1 = γ(xDt− xt−1) + εt. (15.7)
510 Глава 15. Динамические модели регрессии Здесь γ ∈ [0; 1] -скорость приспособления. Если γ = 0, то xt = xt−1, тоесть xt не меняется, если же γ = 1, то приспособление происходит мгновенно, и в этом случае сразу xt = xDt. Предположим, что переменная xDtненаблюдаема. Исключим из этих двух вы-ражений ненаблюдаемую переменную: xt = γβ + (1 − γ)xt−1 + γαzt + εt + γξt. Ясно, что это модель ADL(1, 0), гд е γβ = μ, 1−γ = ϕ1 и γα = α0. Оценивпа-раметры μ, ϕ1 и α0, мы можем с помощью обратного преобразования вычислить оценки параметров исходной модели. Модель адаптивных ожиданий Очень часто экономические решения, принимаемые людьми, зависят от про-гнозов того, что будет в будущем.При этом уровень экономических величин, на ко-торые воздействуют такие решения, зависит не от текущего значения показателя, а от ожидаемого значения (например, если ожидается высокий уровень инфляции, то следует скупать доллары, курс доллара в результате вырастет). В теории рас-сматриваются 2 вида ожиданий-рациональные и адаптивные. В соответствии с одним из определений,ожидания называют рациональными, еслиматематическое ожидание прогноза равно фактическому значению, которое будет в будущем. Мо-дели рациональных ожиданий часто оказываются довольно сложными. Адаптивные ожидания-это ожидания, которые зависят только от предыдущих значений ве-личины. По мере того, как наблюдаются процессы движения реальной величины, мы адаптируем наши ожидания к тому, что наблюдаем на самом деле. Чтобы ввести в экономические модели ожидания экономических субъектов, в простейшем случае используют модель адаптивных ожиданий. Адаптивные ожи-дания некоторой величины формируются только на основе прошлых значений этой величины. Например, пусть xt зависит от ожиданий ( zEt ) величины zt, zt -ве-личина, от прогноза которой должен зависеть xt (например, инфляция), zEt - ожидание (прогноз) этой величины в момент времени t. xt = β + αzEt + εt. В целом xt выгодно выбирать в зависимости от того, какой величина zt будет в будущем: zt+1, zt+2, . . ., од нако в момент выбора t известны только текущее и прошлые значения ( . . ., zt−1, zt). Ошибка в ожиданиях zEt приводит к их корректировке. Модель адаптации ожиданий к фактическому значению zt записывается так: zEt − zEt−1 = θ(zt − zEt−1),
15.3 Некоторые прикладные динамические модели 511 где θ -скорость приспособления ожиданий. Если θ = 0, то ожидания никак не адаптируются к действительности и прогнозы не сбываются (скорость адаптации нулевая); если θ = 1, скорость адаптации мгновенная, наши ожидания сбываются (полностью адаптировались): zEt = zt. Обычно 0 < θ < 1. Легко видеть, что модель адаптации ожиданий основывается на формуле экс-поненциальной средней: zEt = θzt + (1 − θ)zEt−1. Для оценки параметров модели надо исключить ненаблюдаемые ожидания zEt . Используя лаговый оператор, получаем: zEt − (1 − θ)zEt−1 = (1 − (1 − θ)L)zEt = θzt, откуда zEt = θzt 1 − (1 − θ)L = θ ∞i=0 (1 − θ)izt−i. Таким образом, ожидания в рассматриваемой модели описываются бесконечным геометрическим распределенным лагом с параметром затухания δ = 1− θ. Если в уравнение для xt вместо zEt подставить данный бесконечный ряд, то по-лучится модель регрессии с геометрическим распределенным лагом: xt = β + αθzt 1 − (1 − θ)L + εt. (15.8) Как было показано ранее, модель геометрического лага с помощью преоб-разования Койка приводится к модели ADL. Умножим обе части уравнения 15.8 на 1 − (1 − θ)L и получим: (1 − (1 − θ)L)xt = (1 − (1 − θ)L)β + αθzt + (1 − (1 − θ)L)εt. После соответствующего переобозначения параметров модель адаптивных ожиданий приобретает новую форму-ADL(1, 0) с MA(1)-ошибкой: xt = θβ + (1 − θ)xt−1 + αθzt + εt − (1 − θ)εt−1. Оцениватьмодель адаптивныхожиданийможнотемижеметодами, что и модель Койка.
512 Глава 15. Динамические модели регрессии Модель исправления ошибок В динамических регрессионных моделях важно различие между долгосрочной и краткосрочной динамикой. Это различие можно анализировать в рамках модели исправления ошибок. Рассмотрим в долгосрочном аспекте модель ADL(1, 1): xt = μ + ϕ1xt−1 + α0zt + α1zt−1 + εt. Предположим, что фактор zt и ошибка εt являются стационарными процес-сами. Тогда при |ϕ1| < 1 изучаемая переменная xt также стационарна. Возьмем математические ожидания от обеих частей уравнения модели: ¯x = μ + ϕ1¯x + α0¯z + α1¯z. В этой формуле ¯x = E(xt), ¯z = E(zt) (стационарные уровни x и z) и учиты-вается, что E(εt) = 0. Получаем уравнение ¯x = μ 1 − ϕ1 + α0 + α1 1 − ϕ1 ¯z = μ+ λ¯z, которое описывает долгосрочное стационарное состояние экономического про-цесса. Коэффициент λ = α0 + α1 1 − ϕ1 (15.9) отражает долгосрочное влияние z на x. Он совпадает с долгосрочным мультипли-катором (15.6). Модель ADL(1, 1) можно привести к виду, который описывает краткосрочную динамику экономической системы. В этом виде модель называется моделью ис-правления ошибок, сокращенно ECM (error-correction model): Δxt = μ − (1 − ϕ1)xt−1 + α0Δzt + (α0 + α1)zt−1 + εt или Δxt = α0Δzt − θ xt−1 − (μ+ λzt−1)+ εt, (15.10) где θ = 1− ϕ1, Δxt = xt − xt−1, Δzt = zt − zt−1. Предполагается, что если в предыдущий период переменная x отклонилась от своего "долгосрочного значения" μ+λz, тоэлемент xt−1−(μ+λzt−1) корректи-рует динамику в нужном направлении. Для того чтобыэто происходило, необходимо выполнение условия |ϕ1| < 1.
15.4. Упражнения и задачи 513 Иногда из теории, описывающей явление, следует, что λ = 1, тогда ϕ1 + α0 + + α1 = 1. Часто именно такую модель называют ECM. Модели частичного приспособления и адаптивных ожиданий являются частны-ми случаями модели исправления ошибок-не только формально математически, но и по экономическому содержанию. Например, модель частичного приспособле-ния (15.7) в форме ECM выглядит как Δxt = αγΔzt − γ(xt−1 − β − αzt−1) + εt + γξt. Рассмотрим теперь авторегрессионную модель с распределенным лагом обще-го вида (15.5) и покажем, что ее можно представить в виде модели исправления ошибок.При предположениях о стационарности xt и εt математические ожидания от обеих частей уравнения (15.5) приводят к выражению: ¯x = μ + p j=1ϕj ¯x + q j=0 αj ¯z или ¯x = μ 1 −pj=1 ϕj + qj=0 αj 1 −pj=1 ϕj ¯z = μ+ λ¯z, где коэффициент долгосрочного влияния z на x: λ = qj=0 αj 1 −pj=1 ϕj , как и в случае ADL(1, 1), совпадает с долгосрочным мультипликатором πΣ. В этих обозначениях можно представить модель ADL(p, q) в виде модели ис-правления ошибок: Δxt = −θ xt−1 − (μ+ λzt−1)+ p−1 j=1 γjΔxt−j + q−1 j=0 βjΔzt−j + εt, (15.11) где θ = 1 − p j=1ϕj, γj = − p i=j+1ϕi, βj = − q i=j+1αi приj > 0, и β0 = α0. 15.4. Упражнения и задачи Упражнение 1 Сгенерируйте нормально распределенный некоррелированный ряд xt длиной 24 со средним 2000 и дисперсией 900.
514 Глава 15. Динамические модели регрессии 1.1. На основе этого ряда по модели yt = 5+5xt + 8xt−1 + 9xt−2 + 8xt−3 + +5xt−4 + εt, гд е εt -нормально распределенный белый шум с дисперсией 100, сгенерируйте 100 рядов yt, t = 1, . . . , 20. 1.2. Используя 100 сгенерированных наборов данных, оцените модель распреде-ленного лага с максимальной длиной лага q = 4. Найдите среднее и диспер-сию оценок коэффициентов. Сравните с истинными значениями. 1.3. Для первых 5 наборов данных выберите наиболее подходящую длину лага на основе информационных критериев Акаике (AIC) и Шварца (BIC), оценив модель для q = 2, . . . , 6. 1.4. Используя все 100 наборов, оцените модель полиномиального лага с мак-симальной длиной лага q = 4 и степенью полинома p = 2. Рассчитайте средние и дисперсии оценок весов распределенного лага. Сравните с истин-ными значениями. Сравните с результатами из упражнения 1.2 и сделайте выводо том, какие оценки точнее. 1.5. Повторите упражнение 1.4 для а) q = 4, p = 3; б) q = 6, p = 2; в) q = 3, p = 2. Упражнение 2 Втаблице 15.1 приведены данные из известной статьиС. Алмон по промышлен-ным предприятиям США (EXPEND-capital expenditures, капитальные расходы, APPROP-appropriations). 2.1. Постройте графики двух рядов. Что можно сказать по ним о рядах? Видна ли зависимость между рядами (в тот же период или с запаздыванием)? 2.2. Постройте кросс-корреляционную функцию для сдвигов −12, . . . , 0, . . . , 12. Сделайте выводы. 2.3. Используя данные, оцените неограниченную модель распределенного лага с максимальной длиной лага q = 6, . . . , 12 для зависимости EXPEND от APPROP. Используя известные вам методы, выберите длину лага. 2.4. Оцените модель полиномиального лага с максимальным лагом 8 и степенью многочлена p = 2, . . . , 6. С помощью статистики Стьюдента проверьте ги-потезы p = 5 против p = 6, . . . , p = 3 против p = 2 и выберите наиболее подходящую степень p.
15.4. Упражнения и задачи 515 Таблица 15.1. (Источник: Almon Shirley. "The Distributed Lag between Capital Appropriations and Expenditures", Econometrica 33, January1965, pp. 178-196) Квартал EXPEND APPROP Квартал EXPEND APPROP 1953.1 2072.0 1660.0 1960.3 2721.0 2131.0 1953.2 2077.0 1926.0 1960.4 2640.0 2552.0 1953.3 2078.0 2181.0 1961.1 2513.0 2234.0 1953.4 2043.0 1897.0 1961.2 2448.0 2282.0 1954.1 2062.0 1695.0 1961.3 2429.0 2533.0 1954.2 2067.0 1705.0 1961.4 2516.0 2517.0 1954.3 1964.0 1731.0 1962.1 2534.0 2772.0 1954.4 1981.0 2151.0 1962.2 2494.0 2380.0 1955.1 1914.0 2556.0 1962.3 2596.0 2568.0 1955.2 1991.0 3152.0 1962.4 2572.0 2944.0 1955.3 2129.0 3763.0 1963.1 2601.0 2629.0 1955.4 2309.0 3903.0 1963.2 2648.0 3133.0 1956.1 2614.0 3912.0 1963.3 2840.0 3449.0 1956.2 2896.0 3571.0 1963.4 2937.0 3764.0 1956.3 3058.0 3199.0 1964.1 3136.0 3983.0 1956.4 3309.0 3262.0 1964.2 3299.0 4381.0 1957.1 3446.0 3476.0 1964.3 3514.0 4786.0 1957.2 3466.0 2993.0 1964.4 3815.0 4094.0 1957.3 3435.0 2262.0 1965.1 4093.0 4870.0 1957.4 3183.0 2011.0 1965.2 4262.0 5344.0 1958.1 2697.0 1511.0 1965.3 4531.0 5433.0 1958.2 2338.0 1631.0 1965.4 4825.0 5911.0 1958.3 2140.0 1990.0 1966.1 5160.0 6109.0 1958.4 2012.0 1993.0 1966.2 5319.0 6542.0 1959.1 2071.0 2520.0 1966.3 5574.0 5785.0 1959.2 2192.0 2804.0 1966.4 5749.0 5707.0 1959.3 2240.0 2919.0 1967.1 5715.0 5412.0 1959.4 2421.0 3024.0 1967.2 5637.0 5465.0 1960.1 2639.0 2725.0 1967.3 5383.0 5550.0 1960.2 2733.0 2321.0 1967.4 5467.0 5465.0 
516 Глава 15. Динамические модели регрессии Упражнение 3 В таблице 15.2 имеются следующие данные: gas-логарифм среднедушевых реальных расходов на бензин и нефть, price-логарифм реальной цены на бензин и нефть, income-логарифм среднедушевого реального располагаемого дохода, miles-логарифм расхода топлива (в галлонах на милю). 3.1. Оцените авторегрессионную модель с распределенным лагом. В качестве зависимой переменной возьмите gas, а в качестве факторов-income и price. Лаг для всех переменных равен 5. 3.2. Найдите коэффициенты долгосрочного влияния дохода на потребление топ-лива и цены на потребление топлива (долгосрочные эластичности): d(gas) d(income) -долгосрочная эластичность по доходу, d(gas) d(price) -долгосрочная эластичность по цене. 3.3. Вычислите функцию реакции на импульсы (для сдвигов 0, 1, . . . , 40) д ля влияния дохода на потребление топлива и цены на потребление топлива. Найдите среднюю длину лага для этих двух зависимостей. 3.4. Оцените ту же модель в виде модели исправления ошибок. Упражнение 4 В таблице 15.3 приводятся данные о потреблении и располагаемом доходе в США за 1953-1984 гг. 4.1. Оцените следующую модель адаптивных ожиданий. Потребление Ct зави-сит от перманентного дохода Y E t : Ct = β + αY E t + εt, гд е Y E t задается уравнением Y E t − Y E t−1 = θ Yt − Y E t−1. Найдите долгосрочную предельную склонность к потреблению.
15.4. Упражнения и задачи 517 Таблица 15.2. (Источник: Johnston and DiNardo's EconometricMethods (1997, 4th ed)) квартал gas price income miles 1959.1 -8.015248 4.67575 -4.50524 2.647592 1959.2 -8.01106 4.691292 -4.492739 2.647592 1959.3 -8.019878 4.689134 -4.498873 2.647592 1959.4 -8.012581 4.722338 -4.491904 2.647592 1960.1 -8.016769 4.70747 -4.490103 2.647415 1960.2 -7.976376 4.699136 -4.489107 2.647238 1960.3 -7.997135 4.72129 -4.492301 2.647061 1960.4 -8.005725 4.722736 -4.496271 2.646884 1961.1 -8.009368 4.706207 -4.489013 2.648654 1961.2 -7.989948 4.675196 -4.477735 2.650421 1961.3 -8.003017 4.694643 -4.469735 2.652185 1961.4 -7.999592 4.68604 -4.453429 2.653946 1962.1 -7.974048 4.671727 -4.446543 2.65377 1962.2 -7.972878 4.679437 -4.441757 2.653594 1962.3 -7.970209 4.668647 -4.439475 2.653418 1962.4 -7.963876 4.688853 -4.439333 2.653242 1963.1 -7.959317 4.675881 -4.437187 2.65148 1963.2 -7.951941 4.652961 -4.434306 2.649715 1963.3 -7.965396 4.658816 -4.427777 2.647946 1963.4 -7.960272 4.653714 -4.414002 2.646175 1964.1 -7.936954 4.645538 -4.39974 2.645998 1964.2 -7.92301 4.635629 -4.379863 2.64582 1964.3 -7.911883 4.631469 -4.371019 2.645643 1964.4 -7.918471 4.637514 -4.362371 2.645465 1965.1 -7.916095 4.652727 -4.360062 2.64582 1965.2 -7.894338 4.658141 -4.349969 2.646175 1965.3 -7.889203 4.656464 -4.327738 2.646529 1965.4 -7.871711 4.655432 -4.313128 2.646884 1966.1 -7.860066 4.644139 -4.309959 2.64511 1966.2 -7.840754 4.644212 -4.308563 2.643334 1966.3 -7.837658 4.647137 -4.29928 2.641554 1966.4 -7.838668 4.653788 -4.289879 2.639771 1967.1 -7.836081 4.659936 -4.278718 2.639057 1967.2 -7.830063 4.661554 -4.274659 2.638343 1967.3 -7.82532 4.654018 -4.270247 2.637628 1967.4 -7.812695 4.644671 -4.266353 2.636912 1968.1 -7.792308 4.641605 -4.256872 2.633506 1968.2 -7.781042 4.625596 -4.24436 2.630089 1968.3 -7.763533 4.627113 -4.24817 2.626659 1968.4 -7.766507 4.621535 -4.243286 2.623218 1969.1 -7.748152 4.620802 -4.247602 2.618672 1969.2 -7.728209 4.634679 -4.24156 2.614106 1969.3 -7.724049 4.61451 -4.225354 2.609518 
518 Глава 15. Динамические модели регрессии Таблица 15.2. (продолжение) квартал gas price income miles 1969.4 -7.707266 4.603419 -4.221179 2.604909 1970.1 -7.691294 4.589637 -4.223849 2.60343 1970.2 -7.696625 4.593972 -4.214468 2.601949 1970.3 -7.683176 4.57555 -4.207761 2.600465 1970.4 -7.68114 4.576285 -4.213643 2.598979 1971.1 -7.671161 4.563018 -4.20528 2.599722 1971.2 -7.660023 4.528258 -4.199421 2.600465 1971.3 -7.659501 4.5376 -4.20086 2.601207 1971.4 -7.659155 4.54487 -4.19967 2.601949 1972.1 -7.655547 4.51949 -4.206896 2.599351 1972.2 -7.657851 4.500299 -4.203082 2.596746 1972.3 -7.651443 4.515703 -4.187756 2.594135 1972.4 -7.634623 4.531717 -4.157555 2.591516 1973.1 -7.606151 4.531126 -4.150146 2.589642 1973.2 -7.625179 4.54356 -4.149023 2.587764 1973.3 -7.620612 4.540858 -4.144428 2.585882 1973.4 -7.628435 4.601014 -4.130499 2.583997 1974.1 -7.737227 4.73435 -4.155976 2.586259 1974.2 -7.703093 4.796311 -4.174081 2.588516 1974.3 -7.68182 4.773173 -4.174542 2.590767 1974.4 -7.647267 4.730467 -4.180104 2.593013 1975.1 -7.67028 4.721725 -4.198524 2.594882 1975.2 -7.673598 4.726068 -4.157104 2.596746 1975.3 -7.694903 4.766653 -4.175806 2.598607 1975.4 -7.689864 4.767389 -4.168096 2.600465 1976.1 -7.673608 4.74403 -4.158282 2.600836 1976.2 -7.66296 4.723825 -4.158494 2.601207 1976.3 -7.660979 4.723013 -4.159304 2.601578 1976.4 -7.651936 4.728776 -4.157702 2.601949 1977.1 -7.657642 4.731314 -4.160647 2.60694 1977.2 -7.64901 4.725569 -4.154308 2.611906 1977.3 -7.646001 4.705353 -4.139049 2.616848 1977.4 -7.649135 4.709094 -4.137531 2.621766 1978.1 -7.657363 4.699367 -4.129727 2.626298 1978.2 -7.642133 4.673999 -4.11695 2.630809 1978.3 -7.637718 4.678699 -4.113466 2.6353 1978.4 -7.644944 4.711411 -4.107745 2.639771 1979.1 -7.634155 4.737268 -4.10542 2.646352 1979.2 -7.684886 4.848168 -4.110224 2.65289 1979.3 -7.690778 4.965428 -4.108253 2.659385 1979.4 -7.699923 5.018858 -4.108714 2.665838 1980.1 -7.727037 5.130968 -4.107552 2.683928 1980.2 -7.747409 5.149823 -4.130193 2.701697 
15.4. Упражнения и задачи 519 Таблица 15.2. (продолжение) квартал gas price income miles 1980.3 -7.768291 5.121161 -4.123072 2.719155 1980.4 -7.770349 5.108043 -4.106715 2.736314 1981.1 -7.746104 5.176053 -4.107763 2.743739 1981.2 -7.749401 5.162098 -4.113578 2.75111 1981.3 -7.752783 5.128757 -4.10358 2.758426 1981.4 -7.758903 5.126579 -4.10885 2.76569 1982.1 -7.748258 5.089216 -4.116735 2.776643 1982.2 -7.740548 5.016708 -4.109173 2.787477 1982.3 -7.760993 5.046284 -4.1106 2.798196 1982.4 -7.765389 5.018583 -4.112362 2.8088 1983.1 -7.734547 4.951649 -4.111137 2.816157 1983.2 -7.761149 4.973684 -4.105544 2.82346 1983.3 -7.738656 4.978372 -4.09589 2.83071 1983.4 -7.72975 4.947877 -4.079108 2.837908 1984.1 -7.743051 4.937271 -4.059039 2.847957 1984.2 -7.723525 4.926296 -4.050855 2.857906 1984.3 -7.719794 4.885598 -4.04159 2.867757 1984.4 -7.715204 4.886159 -4.039545 2.877512 1985.1 -7.721207 4.876889 -4.040108 2.882564 1985.2 -7.717261 4.896123 -4.021586 2.88759 1985.3 -7.71862 4.877967 -4.034639 2.892591 1985.4 -7.71905 4.865803 -4.03058 2.897568 1986.1 -7.702826 4.79969 -4.020278 2.898395 1986.2 -7.689694 4.598733 -4.006467 2.899221 1986.3 -7.685297 4.52283 -4.012002 2.900047 1986.4 -7.670334 4.488154 -4.016886 2.900872 1987.1 -7.685728 4.581026 -4.011549 2.913573 1987.2 -7.664967 4.594879 -4.030798 2.926114 1987.3 -7.682947 4.624788 -4.019994 2.9385 1987.4 -7.672442 4.617012 -4.00732 2.950735 1988.1 -7.678686 4.583605 -3.996853 2.959328 1988.2 -7.669295 4.572124 -3.997311 2.967847 1988.3 -7.675229 4.578095 -3.993513 2.976295 1988.4 -7.657843 4.557322 -3.985354 2.984671 1989.1 -7.670982 4.561117 -3.979249 2.990091 1989.2 -7.713211 4.683528 -3.988068 2.995482 1989.3 -7.675435 4.627944 -3.985719 3.000844 1989.4 -7.636929 4.584531 -3.980442 3.006177 1990.1 -7.672857 4.628345 -3.971611 3.013695 1990.2 -7.706511 4.617825 -3.969884 3.021156 1990.3 -7.710044 4.693574 -3.971754 3.028562 1990.4 -7.717076 4.829451 -3.978981 3.035914 
520 Глава 15. Динамические модели регрессии Таблица 15.3. (Источник: Greene W., Econometric Analysis, 3rd. edition,Macmillan, 1997) Год, квартал C Y 1953.1 362.8 395.5 1953.2 364.6 401.0 1953.3 363.6 399.7 1953.4 362.6 400.2 1954.1 363.5 399.7 1954.2 366.2 397.3 1954.3 371.8 403.8 1954.4 378.6 411.8 1955.1 385.2 414.7 1955.2 392.2 423.8 1955.3 396.4 430.8 1955.4 402.6 437.6 1956.1 403.2 441.2 1956.2 403.9 444.7 1956.3 405.1 446.6 1956.4 409.3 452.7 1957.1 411.7 452.6 1957.2 412.4 455.4 1957.3 415.2 457.9 1957.4 416.0 456.0 1958.1 411.0 452.1 1958.2 414.7 455.1 1958.3 420.9 464.6 1958.4 425.2 471.3 1959.1 424.1 474.5 1959.2 439.7 482.2 Год, квартал C Y 1959.3 443.3 479.0 1959.4 444.6 483.1 1960.1 448.1 487.8 1960.2 454.1 490.7 1960.3 452.7 491.0 1960.4 453.2 488.8 1961.1 454.0 493.4 1961.2 459.9 500.7 1961.3 461.4 505.5 1961.4 470.3 514.8 1962.1 474.5 519.5 1962.2 479.8 523.9 1962.3 483.7 526.7 1962.4 490.0 529.0 1963.1 493.1 533.3 1963.2 497.4 538.9 1963.3 503.9 544.4 1963.4 507.5 552.5 1964.1 516.6 563.6 1964.2 525.6 579.4 1964.3 534.3 586.4 1964.4 535.3 593.0 1965.1 546.0 599.7 1965.2 550.7 607.8 1965.3 559.2 623.6 1965.4 573.9 634.6 Год, квартал C Y 1966.1 581.2 639.7 1966.2 582.3 642.0 1966.3 588.6 649.2 1966.4 590.5 700.7 1967.1 594.8 665.0 1967.2 602.4 671.3 1967.3 605.2 676.5 1967.4 608.2 682.0 1968.1 620.7 690.4 1968.2 629.9 701.9 1968.3 642.3 703.6 1968.4 644.7 708.7 1969.1 651.9 710.4 1969.2 656.2 717.0 1969.3 659.6 730.1 1969.4 663.9 733.2 1970.1 667.4 737.1 1970.2 670.5 752.6 1970.3 676.5 759.7 1970.4 673.9 756.1 1971.1 687.0 771.3 1971.2 693.3 779.7 1971.3 698.2 781.0 1971.4 708.6 785.5 1972.1 718.6 791.7 1972.2 731.1 798.5 Год, квартал C Y 1972.3 741.3 842.2 1972.4 757.1 838.1 1973.1 768.8 855.0 1973.2 766.3 862.1 1973.3 769.7 868.0 1973.4 766.7 873.4 1974.1 761.2 859.9 1974.2 764.1 859.7 1974.3 769.4 859.7 1974.4 756.5 851.1 1975.1 763.3 845.1 1975.2 775.6 891.3 1975.3 785.4 878.4 1975.4 793.3 884.9 1976.1 809.9 899.3 1976.2 817.1 904.1 1976.3 826.5 908.8 1976.4 838.9 914.9 1977.1 851.7 919.6 1977.2 858.0 934.1 1977.3 867.3 951.9 1977.4 880.4 965.9 1978.1 883.8 973.5 1978.2 901.1 982.6 1978.3 908.6 994.2 1978.4 919.2 1005.0 Год, квартал C Y 1979.1 921.2 1011.1 1979.2 919.5 1011.8 1979.3 930.9 1019.7 1979.4 938.6 1020.2 1980.1 938.3 1025.9 1980.2 919.6 1011.8 1980.3 929.4 1019.3 1980.4 940.0 1030.2 1981.1 950.2 1044.0 1981.2 949.1 1041.0 1981.3 955.7 1058.4 1981.4 946.8 1056.0 1982.1 953.7 1052.8 1982.2 958.9 1054.7 1982.3 964.2 1057.7 1982.4 976.3 1067.5 1983.1 982.5 1073.3 1983.2 1006.2 1082.2 1983.3 1015.6 1102.1 1983.4 1032.4 1124.4 1984.1 1044.1 1147.8 1984.2 1064.2 1165.3 1984.3 1065.9 1176.7 1984.4 1075.4 1186.9 
15.4. Упражнения и задачи 521 4.2. Оцените по темже данныммодель частичного приспособления, преобразовав ее в ADL(1, 0).Желаемый уровень потребления CDt зависит от текущего до-хода Yt: CDt = β+αYt+ξt, гд еCDt -ненаблюдаемая переменная, задавае-мая уравнением частичного приспособления Ct−Ct−1 = γ CDt − Ct−1+εt. Найдите долгосрочную предельную склонность к потреблению. Задачи 1. С помощью какого метода можно оценить параметры модели распределен-ного лага (конечного)? 2. В чем основная причина перехода от обычной модели распределенного лага к модели полиномиального лага? 3. В чем основная причина перехода от обычной модели распределенного лага к модели с экспоненциальным лагом? 4. Пусть веса в модели распределенного лага экспоненциально убывают и сам лаг бесконечный. Каким преобразованием можно перейти к авторегресси-онной модели с распределенным лагом? В чем особенность ошибок в этой модели? 5. Процесс с геометрическим лагом задан формулой xt = 1+ 12zt + 14zt−1 + 18zt−2 + . . . + ε. Примените к нему преобразование Койка. 6. Примените обратное преобразование Койка к модели ADL(1, 0). 7. Представьте ожидания переменной X в модели адаптивных ожиданий в виде распределенного лага дляфактически наблюдаемых значенийпеременной X. 8. Для процесса xt = 0.1+0.5xt−1+0.1zt+0.2zt−1+εt запишите долгосрочную зависимость между x и z. 9. Запишите следующие процессы в виде модели исправления ошибок: а) xt = 0.5x−1 + 0.7zt−1 + εt; б) xt = 2+0.4xt−1 + 2zt + 3zt−1 − 3zt−2 + εt .
522 Глава 15. Динамические модели регрессии Рекомендуемая литература 1. Доугерти К. Введение в эконометрику.-М.: "Инфра-М", 1997. (Гл. 10). 2. Драймз Ф. Распределенные лаги. Проблемы выбора и оценивания моде-ли.-М.: "Финансы и статистика", 1982. 3. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2004. (Гл. 12). 4. Маленво Э.Статистические методы эконометрии. Вып. 2.-М.: "Статисти-ка", 1976. (Гл. 15). 5. Песаран М., Слейтер Л. Динамическая регрессия: теория и алгоритмы.- М: "Финансы и статистика", 1984. (Гл. 5, стр. 67-91). 6. Baltagi, Badi H. Econometrics, 2nd edition.-Springer, 1999. (Ch. 6). 7. EndersW. Applied Econometric Time Series.-New York: John Wiley & Sons, 1992. 8. GreeneW.H. Econometric Analysis.-Prentice-Hall, 2000. (гл.17) 9. JudgeG.G., GriffithsW.E., Hill R.C., Luthepohl H., Lee T. Theory and Practice of Econometrics.-New York: John Wiley & Sons, 1985. (Ch. 9, 10).
Глава 16 Модели с авторегрессионной условной гетероскедастичностью Традиционные модели временных рядов, такие как модель ARMA, не могут адекватно учесть все характеристики, которыми обладают финансовые временные ряды, и требуют расширения.Одна из характерных особенностей финансовых рын-ков состоит в том, что присущая рынку неопределенность изменяется во времени. Как следствие, наблюдается"кластеризация волатильности". Имеется в виду чере-дование периодов, когда финансовый показатель ведет себя непостоянно и относи-тельно спокойно. На рисунке 16.1 для иллюстрации этого явления показаны темпы прироста индекса РТС1 за несколько лет. На графике период 1-сравнительно спокойный, период 2-более бурный, период 3-опять спокойный. Термин во-латильность (volatility-англ. изменчивость, непостоянство) используется, как правило, для неформального обозначения степени вариабельности, разброса пере-менной.Формальной мерой волатильности служит дисперсия (или среднеквадрати-ческое отклонение). Эффект кластеризации волатильности отмечен в таких рядах, как изменение цен акций, валютных курсов, доходности спекулятивных активов. 1Фондовый индекс Российской Торговой Системы. См. http://www.rts.ru.
524 Модели с авторегрессионной условной . . . 1996-08-26 1997-07-22 1998-06-18 1999-05-14 -15 -10 -505 10 15 1 2 3 1995-09-21 2000-04-07 Рис. 16.1. Темпы прироста индекса РТС с 21 сентября 1995 г. по 7 апреля 2000 г., в процентах. 16.1. Модель ARCH Модель ARCH, т.е. модель с авторегрессионной условной гетероскедастично-стью (autoregressive conditional heteroskedasticity), предложена Р. Энглом в 1982 г. для моделирования кластеризации волатильности. Процесс ARCH q-го порядка, {εt}+∞ t=−∞, задается следующими соотношениями: εt|Ωt−1 ∼ N(0, σ2t ), σ2t = ω + γ1ε2t−1 + . . . + γqε2t−q. (16.1) Здесь Ωt−1 = (εt−1, εt−2, . . . ) -предыстория процесса {εt}, а σ2t -услов-ная по предыстории дисперсия εt, т.е. σ2t = var(εt|Ωt−1) = E(ε2t|Ωt−1). Условную дисперсию часто называют волатильностью процесса. Для того чтобы условная дисперсия оставалась положительной, требуется выполнение соотношений ω > 0 и γ1, . . . , γq 0. Данный процесс можно записать несколько иначе: ξt ∼ NID(0, 1), εt = ξtσt, σ2t = ω + γ1ε2t−1 + . . . + γqε2t−q. Аббревиатура NID означает, что ξt нормально распределены и независимы. Такая запись удобна тем, что нормированный случайный процесс ξt не зависит от предыс-тории. Смысл модели ARCH состоит в том, что если абсолютная величина εt оказы-вается большой, то это приводит к повышению условной дисперсии в последующие
16.1. Модель ARCH 525 периоды. В свою очередь, при высокой условной дисперсии более вероятно появ-ление больших (по абсолютной величине) значений εt . Наоборот, если значения εt в течение нескольких периодов близки к 0, то это приводит к понижению услов-ной дисперсии в последующие периоды практически до уровня ω. В свою очередь, при низкой условной дисперсии более вероятно появление малых (по абсолютной величине) значений εt . Таким образом, ARCH-процесс характеризуется инерци-онностью условной дисперсии (кластеризацией волатильности). Несложно показать, что процесс ARCH не автокоррелирован: E(εtεt−j) = E(E(εtεt−j |Ωt−1)) = E(εt−jE(εt|Ωt−1)) = 0. Поскольку процесс имеет постоянное (нулевое) математическое ожидание и не автокоррелирован, то он является слабо стационарным в случае, если у него есть дисперсия. Если обозначить разницу между величиной ε2tи ее условным математическим ожиданием, σ2t , через ηt, то получится следующая эквивалентная запись процесса ARCH: ε2t= ω + γ1ε2t−1 + . . . + γqε2t−q + ηt. (16.2) Поскольку условное математическое ожидание ηt равно 0, то безусловное ма-тематическоеожидание также равно0.Крометого, какможно показать, {ηt} не ав-токоррелирован.Следовательно, квадраты процессаARCH(q) следуют авторегрес-сионному процессу q-го порядка. Если все корни характеристического уравнения 1 − γ1z − . . . − γqzq = 0 лежат за пределами единичного круга, то у процессаARCH(q) существует безуслов-ная дисперсия, и он является слабо стационарным. Поскольку коэффициенты γj неотрицательны, то это условие эквивалентно условию qj=1 γj < 1. Действительно, вычислим безусловную дисперсию стационарного ARCH-процесса, которую мы обозначим через σ2. Для этого возьмем математическое ожидание от обеих частей уравнения условной дисперсии (16.1): E(σ2t) = ω + γ1E(ε2t−1) + . . . + γqE(ε2t−q). Заметим, что Eσ2t = EE(ε2t|Ωt−1)= Eε2t= var(ε2t) = σ2, т.е. мате-матическое ожидание условной дисперсии равно безусловной дисперсии. Следова-тельно, σ2 = ω + γ1σ2 + . . . + γqσ2,
526 Модели с авторегрессионной условной . . . 0 0.05 0.1 0.15 0.2 0.25 0.3K8 K6 K4 K2 0 2 4 6 8 ARCH(1), γ1 = 0.7, ω = 1 N(0; ω /(1Kγ1)) Рис. 16.2. Плотность ARCH(1) и плотность нормального распределения с той же дисперсией или σ2 = ω 1 − γ1 − . . . − γq . Таким образом, дл я всех εt безусловная дисперсия одинакова, т.е. имеет место гомоскедастичность. Однако условная дисперсия меняется, поэтому одновременно имеет место условная гетероскедастичность2. Если не все корни приведенного выше характеристического уравнения лежат за пределами единичного круга, т.е. если q j=1 γj 1, то безусловная дисперсия не существует, и поэтому ARCH-процесс не будет слабо стационарным3. Еще одно свойство ARCH-процессов состоит в том, что безусловное распре-деление εt имеет более высокий куртозис (т.е. более толстые хвосты и острую вершину), чем нормальное распределение (определение куртозиса и эксцесса см. в Приложении A.3.1). У ARCH(1) эксцесс равен E(ε4t) σ4 −3 = 6γ21 1 − 3γ21 , 2Она называется авторегрессионной, поскольку динамика квадратов ARCH-процесса описыва-ется авторегрессией. 3При этом у ARCH-процессов есть интересная особенность: они могут быть строго стационарны, не будучи слабо стационарны. Дело в том, что определение слабой стационарности требует суще-ствования конечных первых и вторых моментов ряда. Строгая же стационарность этого не требует, поэтому даже если условная дисперсия бесконечна (и, следовательно, ряд не является слабо стаци-онарным), ряд все же может быть строго стационарным.
16.2. Модель GARCH 527 причем при 3γ21 1 четвертый момент распределения не существует (эксцесс равен бесконечности). Это свойство ARCH-процессов хорошо соответствует фи-нансовым временным рядам, которые обычно характеризуются толстыми хвоста-ми. На рисунке 16.2 изображен график плотности безусловного распределения ARCH(1). Для сравнения на графике приведена плотность нормального распреде-ления с той же дисперсией. Получить состоятельные оценки коэффициентов ARCH-процесса можно, используя вышеприведенное представление его квадратов в виде авторегрес-сии (16.2). Более эффективные оценки получаются при использовании метода максимального правдоподобия. При применении ARCH-моделей к реальным данным было замечено, что мо-дель ARCH(1) не дает достаточно длительных кластеров волатильности, а только порождает большое число выбросов (выделяющихся наблюдений).Для корректно-го описания данных требуется довольно большая длина лага q, что создает трудно-сти при оценивании.Вчастности, зачастуюнарушается условие неотрицательности оценок коэффициентов γj . Поэтому Энгл предложил использовать модель со сле-дующими ограничениями на коэффициенты лага: они задаются с помощью весов вида: wj = q + 1 − j 0.5q(q + 1), сумма которых равна 1 и которые линейно убывают до нуля. Сами коэффициенты берутся равными γj = γwj . Получается модель с двумя параметрами, ω и γ: σ2t = ω + γ w1ε2t−1 + . . . + wqε2t−q. 16.2. Модель GARCH Модель GARCH (generalized ARCH-обобщенная модель ARCH), предло-женная Т. Боллерслевом, является альтернативной модификацией модели ARCH (16.2), позволяющей получить более длинные кластеры при малом числе пара-метров. Модель ARMA зачастую позволяет получить более сжатое описание вре-менных зависимостей для условного математического ожидания, чем модель AR. Подобным же образом модель GARCH дает возможность обойтись меньшим ко-личеством параметров по сравнению с моделью ARCH, если речь идет об условной дисперсии. В дальнейшем мы проведем прямую аналогию между моделями GARCH и ARMA.
528 Модели с авторегрессионной условной . . . Для того чтобы вывести модель GARCH, используем в модели ARCH бесконеч-ный геометрический лаг: σ2t = ω + γ ∞j=1 δj−1ε2t−j = ω + γ 1 − δLε2t−1. Применяя преобразование Койка, получим σ2t = (1 − δ)ω + δσ2t−1 + γε2t−1. Поменяв очевидным образом обозначения, получим модель GARCH(1, 1): σ2t = ω + δσ2t−1 + γε2t−1. Модель GARCH(p, q) обобщает эту формулу: σ2t = ω + δ1σ2t−1 + . . . + δpσ2t−p + γ1ε2t−1 + . . . + γqε2t−q = = ω + p j=1 δjσ2t−j + q j=1 γjε2t−j . (16.3) При этом предполагается, что ω > 0, δ1, . . . , δp 0 и γ1, . . . , γq 0. На практике, как правило, достаточно взять p =1 и q =1. Изредка используют GARCH(1, 2) или GARCH(2, 1). Как и в модели ARCH, σ2t служит условной дисперсией процесса: εt|Ωt ∼ N(0, σ2t ). Рассчитаем безусловную дисперсию GARCH-процесса, предполагая, что он ста-ционарен. Для этого возьмем математические ожидания от обеих частей уравнения 16.3 для условной дисперсии: E(σ2t) = p j=1 δjE(σ2t−j) + q j=1 γjE(ε2t−j), откуда σ2 = p j=1 δjσ2 + q j=1 γjσ2 и σ2 = 1 1 − p j=1 δj − q j=1 γj .
16.2. Модель GARCH 529 Таким образом, с точки зрения безусловной дисперсии GARCH-процесс гомос-кедастичен. Для того чтобы дисперсия была конечной, необходимо выполнение условия p j=1 δj + q j=1 γj < 1. В частности, для модели GARCH(1, 1) требуется δ1 + γ1 < 1. Процесс GARCH можно записать в эквивалентной форме, если, как и выше, в уравнении 16.2 для модели ARCH, обозначить ηt = ε2t− σ2t : ε2t= ω + mj=1 (δj + γj)ε2t−j + ηt − p j=1 δjηt−j , где m = max(p, q). (В этой записи подразумевается δj = 0 при j > p и γj = 0 при j > q.) Такая форма записи позволяет увидеть, что квадраты GARCH-процесса подчиняются модели ARMA(m, p). Этот факт дает возможность получить автокорреляционнуюфункциюквадратов GARCH-процесса. В частности, для GARCH(1, 1) автокорреляционная функция квадратов имеет вид ρ1 = γ1(1 − δ21 − δ1γ1) 1 − δ21 − 2δ1γ1 , ρτ = (δ1 + γ1)τ−1ρτ−1, τ>1. Условие существования безусловного четвертого момента у отдельного наблю-дения процесса GARCH(1, 1) состоит в том, что 3γ21 + 2γ1δ1 + δ21 < 1. Если это условие выполняется, то эксцесс равен E(ε4t) σ4 −3 = 6γ21 1 − δ21 − 2δ1γ1 − 3γ21 и является положительным. То есть GARCH-процесс (как и его частный слу-чай-ARCH-процесс) имеет более высокий куртозис, чем нормальное распреде-ление.Втоже время, безусловное распределение отдельного наблюденияGARCH-процесса является симметричным, поэтому все нечетные моменты, начиная с тре-тьего, равны нулю. Стандартным методом оценивания для моделей GARCH является методмак -симального правдоподобия. Условно по предыстории Ωt−1 отдельное наблюдение GARCH-процесса распределено нормально: εt|Ωt−1 ∼ N(0, σ2t ). Функция прав-доподобия для ряда ε1, . . . , εT , подчиняющегося GARCH-процессу, вычисляется как произведение плотностей этих условных нормальных распределений: L = T t=1 1 '2πσ2t exp− ε2t 2σ2t .
530 Модели с авторегрессионной условной . . . Максимизируя эту функцию правдоподобия по неизвестным параметрам, по-лучим оценки максимального правдоподобия для GARCH-процесса. При оценивании условную дисперсию σ2t следует считать функцией парамет-ров модели и вычислять по рекуррентной формуле 16.3. Для этих вычислений требуются "довыборочные" значения самого процесса и его условной дисперсии, а они неизвестны. Для решения этой проблемы можно использовать различные приемы. Самый простой, по-видимому, состоит в том, чтобы заменить условные дисперсии в начале ряда ( t = 1, . . . , m) оценкой безусловной дисперсии, т.е. ве-личиной s2 = 1T T t=1 ε2t. Оценки максимального правдоподобия являются состоятельными и асимпто-тически эффективными. На практике модель GARCH дополняют какой-либо моделью, описывающей поведение условного или безусловного математического ожидания наблюдаемо-го ряда. Например, можно предположить, что наблюдается не εt, а εt плюс константа, т.е. что наблюдаемый ряд {xt} имеет постоянное безусловное ма-тематическое ожидание β, к которому добавляется ошибка εt в виде процесса GARCH: xt = β + εt. Можно моделировать безусловное математическое ожидание с помощью ли-нейной регрессии, т.е. xt = Ztα + εt. Это позволяет учитывать линейный тренд, детерминированные сезонные пе-ременные и т.п. При оценивании в функции правдоподобия вместо εt используют xt − Ztα. С точки зрения прогнозирования перспективной является модель, сочетающая ARIMA с GARCH. Модель ARIMA в этом случае используется для моделирования поведения условного математического ожидания ряда, а GARCH-для мод елиро-вания условной дисперсии. Важнейший вывод, который следует из анализа модели ARCH, состоит в том, что наблюдаемые изменения в дисперсии (волатильности) временного ряда могут иметь эндогенный характер, то есть порождаться определенной нелинейной моде-лью, а не какими-то внешними структурными сдвигами.
16.3. Прогнозы и доверительные интервалы для модели GARCH 531 16.3. Прогнозы и доверительные интервалы для модели GARCH Одна из важнейших целей эконометрических моделей временных рядов- построение прогнозов. Какие преимущества дают модели с авторегрессионной условной гетероскедастичностью с точки зрения прогнозирования временных рядов по сравнению с моделями линейной регрессии или авторегрессии-скользящего среднего? Оказывается, что прямых преимуществ нет, но есть ряд опосредованных преимуществ, которые в отдельных случаях могут иметь большое значение. Рассмотрим модель линейной регрессии xt = Ztα + εt, t= 1, . . . , T, в которой ошибка представляет собой GARCH-процесс. Поскольку ошибки не ав-токоррелированы и гомоскедастичны, то, как известно, оценки наименьших квад-ратов являются наилучшими в классе линейных по x несмещенных оценок. Од-нако наличие условной гетероскедастичности позволяет найти более эффективные (т.е. более точные) оценки среди нелинейных и смещенных оценок. Действительно, метод максимального правдоподобия дает асимптотически эффективные оценки, более точные, чем оценкиМНК. В ошибку прогноза вносит свой вклад, во-первых, ошибка εT+1, а во-вторых, разница между оценками параметров и истинными зна-чениями параметров. Использование более точных оценок позволяет уменьшить в некоторой степени вторую составляющую ошибки прогноза. В обычных моделях временного ряда с неизменными условными дисперсия-ми (например, ARMA) неопределенность ошибки прогноза-это возрастающая функция горизонта прогноза, которая (если не учитывать разницу между оценка-ми параметров и истинными значениями параметров, отмеченную в предыдущем абзаце) не зависит от момента прогноза. Однако в присутствии ARCH-ошибок точ-ность прогноза будет нетривиально зависеть от текущей информации и, следова-тельно, от момента прогноза. Поэтому для корректного построения интервальных прогнозов требуется иметь оценки будущих условных дисперсий ошибки. Кроме того, в некоторых случаях полезно иметь прогнозы не только (условного) математического ожидания изучаемой переменной, но и ее (условной) дисперсии. Это важно, например, при принятии решений об инвестициях в финансовые ак-тивы. В этом случае дисперсию (волатильность) доходности естественно рассмат-ривать как меру рискованности финансового актива. Таким образом, сами по себе прогнозы условной дисперсии могут иметь практическое применение. Покажем, что доверительный интервал прогноза зависит от предыстории ΩT = (xT, xT−1, . . . , x1, . . . ).
532 Модели с авторегрессионной условной . . . Реально прогноз делается на основе имеющегося ряда (x1, . . . , xT ), а не всей предыстории, однако различие это не столь существенно. При этом мы будем исхо-дить из того, что нам известны истинные параметры процесса. Прогноз на τ пери-одов-это математическое ожидание прогнозируемой величины xT+τ , условное относительно имеющейся на момент t информации ΩT. Он равен xT (τ) = E(xT+τ |ΩT) = E(ZT+τα + εT+τ |ΩT) = ZT+τ α. Здесь учитывается, что поскольку информация ΩT содержится в информации ΩT+τ−1 при τ 1, то по правилу повторного взятия математического ожида-ния выполнено E(εT+τ |ΩT) = E(E(εT+τ |ΩT+τ−1) |ΩT) = E(0|ΩT) = 0. Таким образом, если известны истинные параметры, присутствие GARCH-ошибок не отражается на том, как строится точечный прогноз,-он оказывается таким же, как для обычной линейной регрессии. Ошибка предсказания в момент времени T на τ шагов вперед dT (τ) = xT+τ − xT (τ) = εT+τ . Условная дисперсия ошибки предсказания равна σ2p = Ed2T (τ )ΩT = Eε2T+τΩT . Из этого следует, что она зависит как от горизонта прогноза τ , так и от предысто-рии ΩT . Заметим, что при t > T выполнено Eε2t|ΩT = Eσ2t |ΩT , поскольку Eε2t− σ2tΩT = EEε2t− σ2tΩt−1ΩT = E(0|ΩT) = 0. Учитывая, что Eε2t− σ2t |Ωt−1= 0 и информация ΩT содержится в информа-ции Ωt−1 при t > T, применяем правило повторного взятия математического ожидания: σ2p = Eε2T+τ |ΩT = Eσ2T+τ |ΩT . Таким образом, фактически дисперсия прогноза xT+τ -это прогноз вола-тильности на τ шагов вперед. Возьмем от обеих частей рекуррентного уравнения (16.3) дляGARCH-процесса математическое ожидание, условное относительно ΩT . Получим Eσ2t |ΩT = ω + p j=1 δjEσ2t−j |ΩT + q j=1 γjEε2t−j |ΩT . (16.4)
16.3. Прогнозы и доверительные интервалы для модели GARCH 533 Можно использовать эту рекуррентную формулу для расчета Eσ2t |ΩT при t > T. При этом следует учесть, что Eε2t|ΩT = ε2tпри t T , поскольку информация об εt содержится в информационном множестве ΩT , и по той же причине Eσ2t |ΩT = σ2t при t T +1. Кроме того, как только что было доказано, Eε2t|ΩT = Eσ2t |ΩT при t > T. Таким образом, имеются все данные для того, чтобы с помощью формулы (16.4) рассчитать дисперсию ошибки прогноза для xT+τ в мод ели GARCH. При τ = 1 можно сразу без применения (16.4) записать σ2dT (1) = Eσ2T+1|ΩT = σ2T+1, где σ2T+1 рассчитывается по обычному правилу. В модели GARCH(1, 1) при τ > 1 по формуле (16.4)Eσ2T+τ |ΩT = ω + (δ1 + γ1)Eσ2T+τ−1|ΩT , т.е. σ2dT (τ) = ω + (δ1 + γ1)σ2dT (τ−1). Отсюда следует, что общее выражение дляGARCH(1, 1), не подходящее только для случая δ1 + γ1 = 1, имеет вид σ2dT (τ) = ω1 − (δ1 + γ1)τ−1 1 − δ1 − γ1 + (δ1 + γ1)τ−1 σ2T+1. Впределе при условии стационарности δ1+γ1 < 1 условная дисперсия ошибки прогноза сходится к безусловной дисперсии процесса GARCH(1, 1): lim τ→∞σ2dT (τ) = ω 1 − δ1 − γ1 . Хотя получено общее выражение для дисперсии ошибки прогноза, этого, вообще говоря, недостаточно для корректного построения доверительных интервалов, по-скольку условное относительно ΩT распределение εT+τ , а следовательно, и рас-пределение ошибки прогноза dT (τ ), имеет более толстые хвосты, чем нормальное распределение. Чтобы обойти эту проблему, можно использовать, например, про-гнозные интервалы в виде плюс/минус двух среднеквадратических ошибок про-гноза без выяснения того, какой именно доверительной вероятности это соот-ветствует4, т.е. xT (τ ) ± 2σdT (τ). 4Ясно, что для нормального распределения это примерно 95%-й двусторонний квантиль.
534 Модели с авторегрессионной условной . . . 20 40 60 80 100 Рис. 16.3. Иллюстрация интервальных прогнозов для процесса GARCH Чтобы проиллюстрировать зависимость доверительных интервалов прогнозов от предыстории,мы сгенерировали ряд GARCH(1, 1) длиной 100 наблюд енийспа-раметрами δ1 = 0.3 и γ1 = 0.3 и построили теоретические доверительные интер-валы при T = 20 и T = 40. Прогноз везде равен нулю. Рисунок 16.3 показывает условные доверительные интервалыпрогнозов для процессаGARCH(1, 1), а также сам ряд. Интервал для T = 20 постепенно сужается, а для T = 40 расширяется до уровня, соответствующего безусловной дисперсии. Такое поведение объясняет-ся тем, что при T = 21 волатильность (условная дисперсия) была относительно высокой, а при T = 41 -относительно низкой. Очевидна способность услов-ных прогнозных интервалов приспосабливаться к изменениям в волатильности. Примечательно то, что интервалы прогнозов могут сужаться с ростом горизонта прогнозов, если прогноз делается в момент, соответствующий высокому уровню волатильности. Это объясняется тем, что в будущем следует ожидать снижения (ожидаемого) уровня волатильности. На практике следует внести изменения в приведенные выше формулы, которые выведены в предположении, что истинные параметры процесса известны. Если па-раметры неизвестны, они заменяются соответствующими оценками.Можно также добавить к дисперсии прогноза поправку, связанную с тем, что при прогнозирова-нии используются оценки a, а не истинные коэффициенты регрессии α, которая примерно равна ZT+kvar(a)−1ZT+k. Вместо неизвестной ковариационной матрицы оценок коэффициентов var(a) сле-дует взять ее оценку, получаемую в методе максимального правдоподобия.
16.4. Разновидности моделей ARCH 535 16.4. Разновидности моделей ARCH Существует огромное количество модификаций классической модели GARCH. Дадим обзор только важнейших направлений, в которых возможна модификация модели. Все эти модели включают в себя какие-либо авторегрессионно услов-но гетероскедастичные процессы. Формально процесс {εt} с нулевым условным математическим ожиданием E(εt|Ωt) = 0 является авторегрессионно условно гетероскедастичным, если его условная относительно предыстории дисперсия σ2t = E(ε2t|Ωt) = var(εt|Ωt) нетривиальным образом зависит от предыстории Ωt. 16.4.1. Функциональная форма динамики условной дисперсии Модели авторегрессионно условно гетероскедастичных процессов могут раз-личаться тем, какой именно функцией задается зависимость условной дисперсии от своих лагов и лагов εt. Например, в логарифмической GARCH-модели условная дисперсия задается уравнением ln σ2t = ω + p j=1 δj ln σ2t−j + q j=1 γj ln ε2t−j . В такой модели условная дисперсия всегда положительна вне зависимости от зна-чений коэффициентов. Следующая нелинейная GARCH-модель включает в себя как частный случай обычную GARCH-модель: σλt = ω + p j=1 δjσλt−j + q j=1 γj |εt−j |λ. Кроме того, логарифмическая GARCH-модель является предельным частным случаем этой модели (после небольших изменений) при λ → 0. В приведенных моделях условная дисперсия не зависит от знаков лагов εt, а зависит только от их абсолютной величины. Это может быть серьезным огра-ничением, поскольку в реальных финансовых данных часто наблюдается эффект левереджа. Снижение рыночной стоимости акционерного капитала увеличивает отношение заемных средств к собственным и, следовательно, повышает риско-ванность вложений в фирму. Последнее проявляется в увеличении волатильности.
536 Модели с авторегрессионной условной . . . В результате, будущие значения волатильности отрицательно коррелируют с теку-щей доходностью. Это дало толчок к развитию разного рода асимметричных по εt моделей.Самой известной является экспоненциальная модельGARCH(EGARCH), предложенная Д. Нельсоном. Она имеет следующий вид: ξt ∼ NID(0, 1), εt = ξtσt, ln σ2t = ω + p j=1 δj ln σ2t−j + q j=1 αjg(ξt−j), g(ξt) = θξt + γ(|ξt| − E|ξt|). В мод ели EGARCH логарифм условной дисперсии представляет собой процесс ARMA.Первая сумма в уравнении соответствует авторегрессии, а вторая-сколь-зящему среднему. Функция g(-) построена так, что E(g(ξt)) = 0. Таким образом, в EGARCH σ2t зависит и от величины, и от знака лагов εt и ξt. Логарифм услов-ной дисперсии ln σ2t описывается процессом ARMA(p, q) с обычными для ARMA условиями стационарности. Эффект левереджа можно также учесть в нелинейной GARCH-модели, введя дополнительный параметр сдвига κ: σλt = ω + p j=1 δjσλt−j + q j=1 γj |εt−j − κ|λ. 16.4.2. Отказ от нормальности Как уже говорилось, финансовые ряды обычно характеризуются большой ве-личиной куртозиса. Модель GARCH частично учитывает это, поскольку в ней без-условное распределениеGARCH-процесса имеет толстые хвосты.Это является ре-зультатом стохастического характера условной дисперсии. Однако, как показывает опыт, этот эффект не полностью улавливается моделью GARCH. Это проявляется в том, что нормированные остатки модели, соответствующие ξt = εt/σt, все еще характеризуются большой величиной куртозиса. Таким образом, не выполняется одно из предположений модели GARCH-о том, что εt условно по предыстории имеет нормальное распределение. Это создает трудности при использовании метода максимального правдопо-добия для оценивания модели. Допустим, на самом деле ошибки распределены не нормально, но мы максимизируем функцию правдоподобия, основывающуюся на нормальности, т.е. используемтак называемыйметодк вазимаксимального прав-доподобия. Что при этом произойдет? Во-первых, при нарушении предположения
16.4. Разновидности моделей ARCH 537 о нормальности оценки хотя и будут состоятельными, но не будут асимптотически эффективными (т.е. наиболее точными в пределе). Во-вторых, стандартные мето-ды оценивания ковариационной матрицы оценок максимального правдоподобия не годятся,-требуется скорректированная оценка ковариационной матрицы. Альтернативой методу квазимаксимального правдоподобия служат модели, в которых в явном виде делается предположение о том, что ξt = εt/σt име-ет распределение, отличающееся от нормального. Наиболее часто используется t-распределение Стьюдента, поскольку это распределение при малых степенях свободы имеет большой куртозис (см. Приложение A.3.2). При этом количество степеней свободы рассматривается как неизвестный параметр, причем непрерыв-ный (формула плотности t-распределения подходит и в случае, когда берется неце-лое количество степеней свободы). Можно использовать и другие распределения, например, так называемое обобщенное распределение ошибки (GED). Часто распределение ξt является скошенным вправо. Для учета этой ситуации следует использовать асимметричные распределения с толстыми хвостами. Напри-мер, можно использовать нецентральное t-распределение, известное из статисти-ки. Другой вариант, более простой в использовании,-так называемое скошенное t-распределение, которое "склеивается" из двух половинок t-распределений, по-разному масштабированных. 16.4.3. GARCH-M В мод ели GARCH-M непосредственно в уравнение регрессии добавляется условная дисперсия: xt = Ztα + πg(σ2t) + εt, где g(-) -некоторая возрастающая функция. Эта новая компонента вводится для отражения влияния волатильности временного ряда на зависимую переменную, поскольку из многих финансовых моделей следует, что доходность актива должна быть положительно связана с рискованностью этого актива. В качестве g(-) обычно используют g(σ2t) = σ2t , g(σ2t) = 'σ2t = σt или g(σ2t) = lnσ2t . 16.4.4. Стохастическая волатильность В рассмотренных моделях с авторегрессионной гетероскедастичностью услов-ная дисперсия однозначно определяется предысторией. Это не оставляет места для случайных влияний на волатильность, помимо влияний лагов самого процесса. Од-нако авторегрессионная гетероскедастичность может возникнуть по-другому.При-
538 Модели с авторегрессионной условной . . . мером является модель авторегрессионной стохастической волатильности, в кото-рой логарифм условной дисперсии описывается авторегрессионным процессом. Модель авторегрессионной стохастической волатильности первого порядка имеет следующий вид: ξt ∼ NID(0, 1), ηt ∼ NID(0, σ2η), εt = ξtσt, ln σ2t = ω + δ ln σ2t−1 + ηt. Эта модель по структуре проще, чем модель GARCH, и лучше обоснована тео-ретически, с точки зрения финансовых моделей, однако ее широкому использова-нию мешает сложность эффективного оценивания. Проблема состоит в том, что для нее, в отличие от моделей типа GARCH, невозможно в явном виде выписать функцию правдоподобия. Таким образом, в случае применения модели стохасти-ческой волатильности возникает дилемма: либо использовать алгоритмы, которые дают состоятельные, но неэффективные оценки, например, методмоментов, ли-бо применять алгоритмы, требующие сложных расчетов, например, алгоритмы, использующие методМонте-Кар ло для интегрирования многомерной плотности. Несложно придумать модели, которые бы объединяли черты моделей типа GARCH и моделей стохастической волатильности. Однако подобные модели на-следуют описанные выше проблемы оценивания. 16.4.5. ARCH-процессы с долгосрочной памятью Для многих финансовых данных оценка p j=1 δj+ q j=1 γj , оказывается очень близ-кой к единице. Это дает эмпирическое обоснование для так называемой интегриро-ванной модели GARCH, сокращенно IGARCH. Это обычные модели GARCH, в ко-торых характеристическое уравнение для условной дисперсии имеет корень равный единице, и, следовательно, p j=1 δj + q j=1 γj = 1. В частности, процесс IGARCH(1, 1) можно записать следующим образом: σ2t = ω + (1 − γ)σ2t−1 + γε2t−1. IGARCH-процессы могут быть строго стационарны, однако не имеют ограни-ченной безусловной дисперсии и поэтому не являются слабо стационарными. Вмод ели IGARCH(1, 1) прогноз волатильности на τ шагов вперед(или, что то же самое, дисперсия прогноза самого процесса на τ шагов вперед) равен E(σ2T+τ |ΩT) = σd2T (τ) = ω(k − 1) + σ2T+1.
16.4. Разновидности моделей ARCH 539 Следовательно, шок условной дисперсии инерционен в том смысле, что он влияет на будущие прогнозы всех горизонтов. В последние годы получило распространение понятие так называемой дробной интегрированности. Дробно-интегрированный процесс (ARFIMA) с парамет-ром интегрированности d ∈ (0, 1) занимает промежуточное положение между стационарными процессами ARMA ( d = 0) и интегрированными ( d = 1). Такие процессы имеют автокорреляционную функцию, которая затухает гиперболиче-ски, в то время как автокорреляционная функция стационарного процесса ARMA затухает экспоненциально, т.е. более быстро. В связи с этим принято говорить, что дробно-интегрированные процессы характеризуются долгосрочной памятью. Это явление было обнаружено как в уровнях, так и в дисперсиях многих финансовых ря-дов. В связи с этим появились модели дробно-интегрированных ARCH-процессов, такие как FIGARCH, HYGARCH. 16.4.6. Многомерные модели волатильности Часто из экономической теории следует, что финансовые временные ряды должны быть взаимосвязаны, в том числе и через волатильность: краткосроч-ные и долгосрочные процентные ставки; валютные курсы двух валют, выражен-ные в одной и той же третьей валюте; курсы акций фирм, зависящих от одного и того же рынка, и т.п. Кроме того, условные взаимные ковариации таких финан-совых показателей могут меняться со временем. Ковариация между финансовыми активами играет существенную роль в моделях поиска оптимального инвестици-онного портфеля. С этой точки зрения многомерные модели авторегрессионной условной гетероскедастичности являются естественным расширением одномерных моделей. Общее определение многомерного ARCH-процесса не представляет никакой теоретической сложности: рассматривается m-мерный наблюдаемый случайный вектор xt, m-мерный вектор его условного математического ожидания, услов-ная ковариационная матрица размерностью m × m. В современной литературе предложено множество подобных моделей разной степени сложности. Оценива-ние многомерной ARCH-модели, однако, сопряжено со значительными трудностя-ми. В частности, эти трудности связаны с необходимостью максимизации по боль-шому количеству неизвестных параметров. Поэтому в прикладных исследованиях отдается предпочтение таким многомерным моделям волатильности, в которых количество параметров мало. В то же время для таких компактных моделей (на-пример, для факторных моделей волатильности) может не существовать явной формулы для функции правдоподобия, что создает дополнительные трудности при оценивании.
540 Модели с авторегрессионной условной . . . 16.5. Упражнения и задачи Упражнение 1 Сгенерируйте ряд длиной 1000 наблюдений в соответствии с моделью ARCH(4) по уравнению: σ2t = 1+0.3ε2t−1+0.25ε2t−2+0.15ε2t−2+0.1ε2t−2. (Вместо начальных значений квадратов ошибок возьмите безусловную дисперсию.) В действительности мы имеем только ряд наблюдений, а вид и параметры мо-дели неизвестны. 1.1. Оцените модель ARCH(4).Сравните оценки с истинными параметрами моде-ли. Сравните динамику оценки условной дисперсии и ее истинных значений. 1.2. Проделайте то же самое для модели GARCH(1, 1). 1.3. Рассчитайте описательные статистики ряда: среднее, дисперсию, автокорре-ляцию первого порядка, асимметрию и эксцесс. Соответствуют ли получен-ные статистики теории? Упражнение 2 В Таблице 16.1 дана цена pt акций IBMза периодc 25 июня 2002 г. по 9 апреля 2003 г. 2.1. Получите ряд логарифмических доходностей акций по формуле yt = 100 ln(pt/pt−1). 2.2. Постройте график ряда yt, график автокорреляционной функции yt, график автокорреляционной функции квадратов доходностей y2t . Сделайте выводы об автокоррелированности самих доходностей и их квадратов. Наблюдается ли авторегрессионная условная гетероскедастичность? 2.3. Оцените для доходностей модельGARCH(1, 1) на основе первых 180 наблю-дений. Значимы ли параметры модели? Постройте график условной диспер-сии и укажите периоды высокой и низкой волатильности. 2.4. Найдите эксцесс изучаемого ряда доходностей yt и эксцесс нормированных остатков из модели GARCH. Сравните и сделайте выводы. 2.5. Постройте прогноз условной дисперсии для 19 оставшихся наблюдений. По-стройте интервальный прогноз изучаемого ряда для тех же 19 наблюдений. Какая доля наблюдений попадает в прогнозные интервалы?
16.5 Упражнения и задачи 541 Таблица 16.1 25.06.02 68.19 26.06.02 69.63 27.06.02 71.47 28.06.02 71.57 1.07.02 67.20 2.07.02 68.17 3.07.02 70.09 5.07.02 73.06 8.07.02 70.87 9.07.02 69.25 10.07.02 68.35 11.07.02 69.00 12.07.02 68.80 15.07.02 70.58 16.07.02 68.60 17.07.02 70.27 18.07.02 71.62 19.07.02 71.57 22.07.02 68.09 23.07.02 66.65 24.07.02 69.12 25.07.02 68.94 26.07.02 66.00 29.07.02 70.75 30.07.02 71.36 31.07.02 69.98 1.08.02 67.84 2.08.02 67.47 5.08.02 65.60 6.08.02 67.49 7.08.02 68.91 8.08.02 71.34 9.08.02 71.56 12.08.02 71.50 13.08.02 71.63 14.08.02 74.64 15.08.02 76.21 16.08.02 79.05 19.08.02 82.18 20.08.02 80.96 21.08.02 80.69 22.08.02 81.68 23.08.02 80.10 26.08.02 79.12 27.08.02 77.67 28.08.02 75.77 29.08.02 76.33 30.08.02 75.10 3.09.02 72.08 4.09.02 73.45 5.09.02 71.91 6.09.02 72.92 9.09.02 74.22 10.09.02 75.31 11.09.02 73.92 12.09.02 71.60 13.09.02 72.23 16.09.02 72.05 17.09.02 71.48 18.09.02 69.29 19.09.02 64.56 20.09.02 63.68 23.09.02 63.13 24.09.02 59.52 25.09.02 62.77 26.09.02 61.79 27.09.02 60.13 30.09.02 58.09 1.10.02 60.94 2.10.02 59.40 3.10.02 59.77 4.10.02 56.39 7.10.02 56.65 8.10.02 56.83 9.10.02 54.86 10.10.02 57.36 11.10.02 63.68 14.10.02 63.18 15.10.02 68.22 16.10.02 64.65 17.10.02 71.93 18.10.02 73.97 21.10.02 75.26 22.10.02 74.21 23.10.02 74.32 24.10.02 71.83 25.10.02 74.28 28.10.02 76.27 29.10.02 76.45 30.10.02 78.37 31.10.02 78.64 1.11.02 80.10 4.11.02 82.19 5.11.02 81.37 6.11.02 81.38 7.11.02 78.80 8.11.02 77.44 11.11.02 77.14 12.11.02 79.00 13.11.02 79.20 14.11.02 80.56 15.11.02 79.85 18.11.02 79.03 19.11.02 78.22 20.11.02 81.45 21.11.02 84.74 22.11.02 84.27 25.11.02 86.03 26.11.02 84.89 27.11.02 87.53 29.11.02 86.75 2.12.02 87.13 3.12.02 85.04 4.12.02 83.53 5.12.02 82.90 6.12.02 82.16 9.12.02 79.44 10.12.02 80.64 11.12.02 81.28 12.12.02 80.01 13.12.02 79.84 16.12.02 81.46 17.12.02 80.15 18.12.02 78.98 19.12.02 78.51 20.12.02 79.64 23.12.02 80.10 24.12.02 79.61 26.12.02 78.35 27.12.02 77.21 30.12.02 76.10 31.12.02 77.35 2.01.03 80.41 3.01.03 81.49 6.01.03 83.43 7.01.03 85.83 8.01.03 84.03 9.01.03 86.83 10.01.03 87.51 13.01.03 87.34 14.01.03 88.41 15.01.03 87.42 16.01.03 85.88 17.01.03 81.14 21.01.03 80.38 22.01.03 79.55 23.01.03 80.89 24.01.03 78.84 27.01.03 78.27 28.01.03 79.95 29.01.03 80.16 30.01.03 78.15 31.01.03 78.05 3.02.03 78.03 4.02.03 76.94 5.02.03 77.11 6.02.03 77.51 7.02.03 77.10 10.02.03 77.91 11.02.03 77.39 12.02.03 76.50 13.02.03 75.86 14.02.03 77.45 18.02.03 79.33 19.02.03 79.51 20.02.03 79.15 21.02.03 79.95 24.02.03 78.56 25.02.03 79.07 26.02.03 77.40 27.02.03 77.28 28.02.03 77.95 3.03.03 77.33 4.03.03 76.70 5.03.03 77.73 6.03.03 77.07 7.03.03 77.90 10.03.03 75.70 11.03.03 75.35 12.03.03 75.18 13.03.03 78.45 14.03.03 79.00 17.03.03 82.46 18.03.03 82.47 19.03.03 82.00 20.03.03 82.20 21.03.03 84.90 24.03.03 82.25 25.03.03 83.45 26.03.03 81.55 27.03.03 81.45 28.03.03 80.85 31.03.03 78.43 1.04.03 78.73 2.04.03 81.46 3.04.03 81.91 4.04.03 80.79 7.04.03 80.47 8.04.03 80.07 9.04.03 78.71
542 Модели с авторегрессионной условной . . . Задачи 1. Объясните значение термина "волатильность". 2. Рассмотрите следующие два утверждения: а) "GARCH является слабо стационарным процессом"; б) "GARCH является процессом с изменяющейся во времени дис-персией". Поясните смысл каждого из них. Объясните, почему между ними нет проти-воречия. 3. Почему процесс GARCH представляет особый интерес для финансовой эко-нометрии? 4. В каком отношении находятся между собой модели ARCH и GARCH? 5. Коэффициент процесса ARCH(1) равен γ = 0.8, безусловная дисперсия равна 2. Запишите уравнение процесса. 6. Приведите конкретный пример процесса ARCH(1), который имел бы конеч-ную безусловную дисперсию. Ответ обоснуйте. 7. Приведите конкретный пример процесса ARCH(1), куртозис которого не определен ("бесконечен"). Ответ обоснуйте. 8. Вычислите безусловную дисперсию следующего процесса GARCH: σ2t = 0.4 + 0.1σ2t−1 + 0.7ε2t. Докажите формально, что ваш ответ верен. 9. Докажите, что следующий процесс GARCH не может иметь конечную без-условную дисперсию: σ2t = 1+0.6σ2t−1 + 0.6ε2t−1. (Подсказка: можно использовать доказательство от противного.) 10. Процесс GARCH(1, 1) задан уравнением σ2t = 1+0.8σ2t−1+0.1ε2t−1. Извест-но, что σ2T= 9, εT = −2. Чему равен прогноз на следующий период? Чему равна дисперсия этого прогноза? 11. Покажите, что модель GARCH(1, 1) можно записать в виде модели ARCH бесконечного порядка.
16.5 Упражнения и задачи 543 12. Утверждается, что "модель GARCH более компактна, чем модель ARCH". Что при этом имеется в виду? Почему важна "компактность" модели? 13. Докажите, что процесс GARCH(p, q) не автокоррелирован. 14. Докажите, что процессGARCH(p, q) является белымшумом, если существу-ет его безусловная дисперсия. 15. Пользуясь представлением квадратов процесса GARCH(1, 1) в вид е ARMA(1, 1) выведите их автокорреляционную функцию. 16. Запишите автокорреляционную функцию квадратов процесса GARCH(1, 1). Покажите, что при значении суммы коэффициентов δ1 + γ1, приближа-ющемся к 1 (но меньшем 1) автокорреляционная функция затухает мед-ленно. Покажите, что при фиксированном значении суммы коэффициен-тов δ1 + γ1 = ϕ (0 < ϕ < 1), автокорреляция тем слабее, чем меньше γ1, и стремится к нулю при γ1 → 0. 17. Рассмотрите следующую модель с авторегрессионной условной гетероскеда-стичностью: σ2t = ω + δσ2t−1 + γ[f(εt−1)]2, где f(z) = |z| − ϕz. а) Укажите значения параметров, при которых этамодель сводится к обыч-ной модели GARCH. б) Утверждается, что в этой модели имеет место асимметричность влияния "шоков" εt на условную дисперсию. Что при этом имеется в виду? При каких значениях параметров влияние будет симметричным? 18. Запишите модель GARCH(1, 1)-M с квадратным корнем условной дисперсии в уравнении регрессии (с расшифровкой обозначений). 19. Рассмотрите модель AR(1) с независимыми одинаково распределенными ошибками и модель AR(1) с ошибками, подчиняющимися процессу GARCH. а) Объясните, почему точечные прогнозы по этим двум моделям не будут отличаться. б) Как будут отличаться интервальные прогнозы? 20. Пусть имеется некоторый процесс с авторегрессионной условной гетероске-дастичностью εt, задаваемый моделью σ2t = var(εt|Ωt−1) = h(σ2t−1, . . . , σ2t−p, ε2t−1, . . . , ε2t−q),
544 Модели с авторегрессионной условной . . . где Ωt−1 -предыстория процесса, h(-) -некоторая функция и εt = ξtσt. Предполагается, что инновации ξt имеют стандартное нормальное распреде-ление и не зависят от предыстории Ωt−1. Найдите куртозис εt, если известно, что E(σ2t) = 5, E(σ4t ) = 100. О чем говорит величина куртозиса? 21. Докажите, что эксцесс распределения отдельного наблюдения εt процесса ARCH(1) εt = ξtσt, σ2t = ω + γε2t−1, ξt ∼ NID(0, 1) равен 6γ2 1 − 3γ2 . 22. Какие сложности возникают при построении прогнозных интервалов процес-са GARCH с заданным уровнем доверия (например, 95%)? Каким способом можно обойти эту проблему? 23. Почему модель GARCH не подходит для прогнозирования автокоррелиро-ванных временных рядов? Рекомендуемая литература 1. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 12). 2. Предтеченский А.Г. Построение моделей авторегрессионной условной гете-роскедастичности (ARCH) некоторых индикаторов российского финансового рынка (дипломная работа), ЭФ НГУ, 2000. (􀀀􀀀􀀀􀀀). 3. Шепард Н. Статистические аспекты моделей типа ARCH и стохастическая волатильность. Обозрение прикладной и промышленной математики. Т. 3, вып. 6.-1996. 4. Baillie Richard T. and Tim Bollerslev. Prediction in Dynamic Models with Time Dependent Conditional Variances //Journal of Econometrics, No. 52, 1992. 5. Bera A.K. and Higgins M.L. ARCH Models: Properties, Estimation and Testing //Journal of Economic Surveys, No. 7, 1993. 6. Bollerslev T., Engle R.F. and Nelson D.B. ARCH Models //Handbook of Econometrics. Vol. IV. Ch. 49.-Elsevier Science, 1994.
16.5 Упражнения и задачи 545 7. Bollerslev Tim. Generalized Autoregressive Conditional Heteroskedasticity //Journal of Econometrics, No. 31, 1993. 8. Bollerslev Tim, Ray Y. Chou and Kenneth F. Kroner. ARCH Modeling in Finance: A Review of the Theory and Empirical Evidence //Journal of Econometrics, No. 52, 1992. 9. Campbell John Y., Lo Andrew W., MacKinlay A. Craig. The Econometrics of Financial Markets.-Princeton University Press, 1997. (Ch. 12). 10. Diebold, Francis X. and Jose A. Lopez Modeling Volatility Dynamics, Macroeconometrics: Developments, Tensions and Prospects.-Kluwer Academic Press, 1995. 11. Engle, Robert F. Autoregressive Conditional Heteroskedasticity with Estimates of the Variance of U.K. Inflation //Econometrica, No. 50, 1982. 12. GreeneW.H. Econometric Analysis.-Prentice-Hall, 2000. (Ch. 18). 13. Hamilton JamesD. TimeSeriesAnalysis.Ch. 21.-PrincetonUniversity Press, 1994. 14. Mills Terence C. The Econometric Financial Modelling Time Series.-Cambridge University Press, 1999. (Ch. 4).
Глава 17 Интегрированные процессы, ложная регрессия и коинтеграция 17.1. Стационарность и интегрированные процессы Для иллюстрации различия между стационарными и нестационарными случай-ными процессами рассмотрим марковский процесс, т.е. авторегрессию первого порядка: xt = μ + ϕxt−1 + εt, или (1 − ϕL)xt = μ + εt. В данной модели xt -не центрированы. Будем предполагать, что ошибки εt -независимые одинаково распределен-ные случайные величины с нулевым математическим ожиданием и дисперсией σ2ε . Как известно, при |ϕ| < 1 процесс авторегрессии первого порядка слабо стацио-нарен и его можно представить в виде бесконечного скользящего среднего: xt = μ + εt 1 − ϕL = μ 1 − ϕ + ∞i=0 ϕiεt−i.
17.1. Стационарность и интегрированные процессы 547 Условие |ϕ| < 1 гарантирует, что коэффициенты ряда затухают. Математиче-ское ожидание переменной xt постоянно: E(xt) = μ 1 − ϕ. Дисперсия равна var(xt) = ∞i=0 ϕ2ivar(εt−i) = σ2ε 1 − ϕ2 . Найдем также автоковариации процесса: γk = cov(xt, xt−k) = ∞i=0 ϕi+kϕiσ2ε = ϕk ∞i=0 ϕ2iσ2ε = ϕk 1 − ϕ2 σ2ε . Таким образом, рассматриваемый процесс слабо стационарен, поскольку сла-бое определение стационарности требует, чтобы математическое ожидание xt бы-ло постоянным, а ковариации не зависели от времени, но только от лага. На самом деле, поскольку ошибки εt одинаково распределены, то он стационарен и в строгом смысле. При |ϕ| > 1 это будет взрывной процесс. Такие процессы рассматриваться не будут. Как известно (см. гл. 14), авторегрессионный процесс первого порядка при ϕ = 1 называют случайным блужданием. Если μ = 0, то это просто случайное блуждание, а при μ = 0 это случайное блуждание с дрейфом. У процесса случайного блуждания, начавшегося бесконечно давно, не суще-ствует безусловногоматематическогоожидания и дисперсии.Забесконечное время процесс "уходит в бесконечность", его дисперсия становится бесконечной. В свя-зи с этим будем рассматривать все моменты процесса случайного блуждания как условные, т.е. будем действовать так, как если бы x0 была детерминированной величиной. Выразим xt через x0: xt = x0 + μt + t i=1 εi. Таким образом, константа (дрейф) в авторегрессионной записи процесса при-водит к появлению линейного тренда в xt. Мы получили разложение процесса xt на две составляющие: детерминированный линейный тренд μt и случайное блуж-дание ε∗t = x0 + t i=1 εi, такое что ошибка εt представляет собой его приросты: εt = Δε∗t . Вторую составляющую, как мы помним, называют стохастическим трен-дом, поскольку влияние каждой ошибки не исчезает со временем.
548 Интегрированные процессы... K2 0 2 4 20 40 60 80 100 ϕ = 0.1 K5 0 5 10 15 20 40 60 80 100 ϕ = 0.9 0 10 20 30 40 50 20 40 60 80 100 ϕ = 1 0 40 80 120 160 20 40 60 80 100 ϕ = 1.02 Рис. 17.1. Поведение процесса AR(1) в зависимости от значения ϕ Используя данное представление, найдем математическое ожидание и диспер-сию: E(xt|x0) = x0 + μt. var(xt|x0) = vart i=1 εi= t i=1 var (εi) = tσ2ε . (17.1) Дисперсия со временем растет линейно до бесконечности. Случайное блуждание является примером авторегрессионого процесса с еди-ничным корнем. Это название следует из того, что при ϕ = 1 корень характери-стического многочлена 1−ϕL, соответствующего процессу AR(1), равен единице. Рисунок 17.1 иллюстрирует поведение марковских процессов при различных коэффициентах авторегрессии. На каждом из графиков изображены 20 рядов дли-ной T = 100, случайно сгенерированных по формуле xt = 0.3 +ϕxt−1 + εt с раз-ными значениями ϕ: 1) ϕ = 0.1; 2) ϕ = 0.9; 3) ϕ = 1; 4) ϕ = 1.02. Во всех случаях использовалось стандартное нормальное распределение для εt и x0 = 0. Добавим к стационарному процессу AR(1) детерминированный тренд μ1t: xt = μ0 + μ1t + ϕxt−1 + εt.
17.1. Стационарность и интегрированные процессы 549 Тогда xt = μ0 + μ1t + εt 1 − ϕL = μ0 1 − ϕ + μ1 ∞i=0 ϕi(t − i) + ∞i=0 ϕiεt−i = = μ0 1 − ϕ − μ1 ∞i=0 iϕi + μ1 1 − ϕt + ∞i=0 ϕiεt−i. Ряд ∞i=0 iϕi сходится, поскольку i возрастает линейно, а ϕi убывает экспо-ненциально при |ϕ| < 1, т.е. значительно быстрее. Его сумма равна ϕ (1 − ϕ)2 . Используя это, получаем xt = μ0 1 − ϕ − ϕμ1 (1 − ϕ)2 + μ1 1 − ϕt + ∞i=0 ϕiεt−i = γ0 + γ1t + ∞i=0 ϕiεt−i, (17.2) где γ0 = μ0 1 − ϕ − ϕμ1 (1 − ϕ)2 и γ1 = μ1 1 − ϕ. Можно также записать уравнение процесса в виде: (xt − γ0 − γ1t) = ϕ(xt−1 − γ0 − γ1(t − 1)) + εt. Ясно, что если вычесть из xt тренд γ1t, то получится стационарный процесс. Подобного рода процессы называют стационарными относительно тренда. Рассмотрим теперь процесс ARMA(p, q): xt = p i=1 ϕixt−i + εt − q i=1 θiεt−i. Если все корни характеристического многочлена ϕ(z) = 1 − p i=1 ϕizi по абсолютной величине больше 1, т.е. лежат за пределами единичного круга на комплексной плоскости, то процесс стационарен. Если один из корней лежит в пределах единичного круга, то процесс взрывной. Если же d корней равны еди-нице, а остальные лежат за пределами единичной окружности, то процесс неста-ционарный, но не взрывной и о нем говорят, что он имеет d единичных корней.
550 Интегрированные процессы... Нестационарный процесс, первые разности которого стационарны, называют интегрированнымпервогопорядкаи обозначаютI(1).Стационарный процесс обо-значают I(0). Если d-e разности случайного процесса стационарны, то его называют интегрированным d-го порядка и обозначают I(d). Рассмотрим, например, процесс yt = t i=1 xi, где xt = xt−1 + εt. Он будет I(2), то есть его вторые разности, Δ2yt, стационарны. Для процессов ARIMA можно дать более удачное определение интегрирован-ности. Процессом I(0) называется стационарный процесс с обратимым скользя-щим средним.Процесс I(d)-такой процесс, d-e разности которого являются I(0). Соответственно, процесс, являющийся d-ой разностью процесса I(0), буд ет I(−d). Такое уточнение нужно для того, чтобы необратимые процессы, такие как εt−εt−1, где εt -белый шум, по определению были I(−1), ноне I(0).По этому уточненному определению процесс I(d) при d > 0 будет иметь в точности d единичных корней. 17.2. Разложение Бевериджа-Нельсона для процесса I(1) Рассмотрим ARIMA-процесс I(1), интегрированный первого порядка.Пусть его исходная форма, записанная через лаговый оператор, имеет вид ϕ(L)xt = μ + θ(L)εt. Поскольку это процесс I(1), томногочлен ϕ(L) имеет единичный корень и урав-нение процесса можно представить в виде (1 − L)ϕ∗(L)xt = ϕ∗(L)Δxt = μ + θ(L)εt, где у многочлена ϕ∗(L) все корни находятся за пределами единичного круга. От-сюда следует разложение Вольда для приростов Δxt, которые являются стацио-нарными: Δxt = μ + θ(L) ϕ∗(L) εt = μ ϕ∗(L) + ∞i=0 ciεt−i = μ 1 − p j=1ϕ∗j + c(L)εt = γ + c(L)εt. Ряд c(z) можно представить следующим образом: c(z) = c(1) + c∗(z)(1 − z),
17.3. Ложная регрессия 551 где c∗(z) = ∞i=0 c∗i zi, с коэффициентами c∗i = − ∞j=i+1 cj . Действительно, c(1) + c∗(z)(1 − z) = ∞i=0 ci + ∞i=0 c∗i zi − ∞i=0 c∗i zi+1 = = ∞i=0 ci + c∗0 + ∞i=1 (c∗i − c∗i−1)zi = c0 + ∞i=1 cizi = ∞i=0 cizi. Таким образом, можно представить Δxt в вид е Δxt = γ + (c(1) + c∗(L)(1 − L)) εt = γ + c(1)εt + c∗(L)Δεt. Суммируя Δxt, получимxt = γt + c(1)ε∗t + c∗(L)εt, где ε∗t -случайное блуждание, такое что Δε∗t = εt . Без доказательства отметим, что ряд c∗(L) сходится абсолютно1: ∞i=0 |c∗i | < ∞.Следовательно, он соответствует разложению Вольда стационарного процесса. Мы получили так называемое разложение Бевериджа-Нельсона. Процесс xt вида I(1) мы представили как комбинацию детерминированного тренда γt, стоха-стического тренда c(1)ε∗t и стационарного процесса c∗(L)εt, который здесь обычно интерпретируется как циклическая компонента. 17.3. Ложная регрессия Одним из важнейших условий получения корректных оценок в регрессионных моделях является требование стационарности переменных. В экономике довольно часто встречаются стационарные ряды, например, уровень безработицы. Одна-ко, как правило, экономические процессы описываются нестационарными рядами: объем производства, уровень цен и т.д. 1Это можно понять из того, что ∞i=0 |c∗i | = ∞i=0∞j=i+1 ci∞i=0 ∞j=i+1 |ci| = ∞i=0 i|ci|. Поскольку ко-эффициенты ci у стационарного процесса ARMA сходятся экспоненциально, то ряд должен сойтись (экспоненциальное убывание превосходит рост i).
552 Интегрированные процессы... Очень важным условием корректного оценивания регрессионных моделей яв-ляется условие стационарности регрессоров. Если зависимая переменная является I(1), и, кроме того, модель неверно специфицирована, т.е. некоторые из факторов, введенные ошибочно, являются I(1), то полученные оценки будут очень "плохими". Они не будут обладать свойством состоятельности, т.е. не будут сходиться к истин-ным значениям параметров по мере увеличения размеров выборки.Привычные по-казатели, такие как коэффициент детерминации R2, t-статистики, F-статистики, будут указывать на наличие связи там, где на самом деле ее нет. Такой эффект называют эффектом ложной регрессии. Показать эффект ложной регрессии для переменных I(1) можно с помощью метода Монте-Карло. Сгенерируем достаточно большое число пар независимых процессов случайного блуждания с нормально распределенными ошибками: xt = xt−1 + εt и zt = zt−1 + ξt, где εt ∼ N(0, 1) и ξt ∼ N(0, 1). Оценив для каждой пары рядов xt и zt достаточно много раз регрессию вида xt = azt + b + ut, получим экспериментальные распределения стандартных статистик. Проведенные экспериментальные расчеты для рядов длиной 50 наблюдений показывают, что t-статистика для a при номинальной вероятности 0.05 (т.е. 5%) в действительности отвергает вернуюгипотезу об отсутствии связипримерно в 75% случаев. Для того чтобы нулевая гипотеза об отсутствии связи отклонялась с веро-ятностью 5%, вместо обычного 5%-го квантиля распределения Стьюдента, рав-ного примерно 2, нужно использовать критическую границу t0.05 = 11.2. Из экспериментов также следует, что регрессии с независимыми процесса-ми случайного блуждания с большой вероятностью имеют высокий коэффициент детерминации R2 из-за нестационарности. Более чем в половине случаев коэф-фициент детерминации превышает 20%, и несколько менее чем в 5%случаев пре-вышает 70%. Для сравнения можно построить аналогичные регрессии для двух независимых нормально распределенных процессов типа белый шум. Оказывает-ся, что в таких регрессиях R2 чрезвычайно редко превышает 20% (вероятность этого порядка 0.1%)2. Тоже самое, хотя и вменьшей степени,можно наблюдать и в случае двух стацио-нарных AR(1)-процессов с коэффициентом автокорреляции ϕ , близким к единице. Отличие заключается в том, что здесь ложная связь асимптотически (при стрем-лении длины рядов к бесконечности) исчезает, а в случае I(1)-процессов - нет. 2Для двух независимых I(2)-процессов, построенных как проинтегрированные процессы случай-ного блуждания, примерно в половине случаев коэффициент детерминации превышает 80%!
17.4. Проверка на наличие единичных корней 553 Всеже проблема остается серьезной, поскольку на практике экономист имеет дело с конечными и часто довольно короткими рядами. Таким образом, наличие в двух независимых процессах стохастических трен-дов может с высокой вероятностью привести к получению ложного вывода об их взаимосвязанности, если пользоваться стандартными методами. Стандартные методы проверки гипотез, применяемые в регрессионном анали-зе, в данном случае не работают. Это происходит по причине нарушения некоторых предположений, лежащих в основе модели регрессии. Какие же предположения нарушаются? Приведем одну из возможных точек зрения. Предположим, как и выше, что xt и zt -два независимых процесса случай-ного блуждания, и оценивается регрессия xt = azt + b + ut. Поскольку в этой регрессии истинное значение параметра a равно нулю, то ut = xt − b, т.е. ошибка в регрессии является процессом случайного блуждания. Выше получено выражение (17.1) для дисперсии процесса случайного блуждания (условной по начальному наблюдению): var(ut) = tσ2ε , где σ2ε -дисперсия εt (приростов xt). Таким образом, здесь наблюдается силь-нейшая гетероскедастичность. С ростом номера наблюдения дисперсия ошибки растет до бесконечности. Вследствие этого t-статистика регрессии имеет нестан-дартное распределение, и обычные таблицы t-распределения использовать нельзя. Отметим, что наличие в переменных регрессии обычного детерминированного тренда также может приводить к появлению ложной регрессии. Пусть, например, xt и zt заданы формулами xt = μ0 + μ1t + εt и zt = ν0 + ν1t + ξt, где εt и ξt -два независимых процесса типа белый шум. Регрессия xt по кон-станте и zt может иметь высокий коэффициент детерминации, и этот эффект только усиливается с ростом размера выборки. С "детерминированным" вари-антом ложной регрессии достаточно легко бороться. В рассматриваемом случае достаточно добавить в уравнение в качестве регрессора тренд, т.е. оценить регрес-сию xt = azt + b + ct + ut, и эффект ложной регрессии исчезает. 17.4. Проверка на наличие единичных корней С осознанием опасности применения ОМНК к нестационарным рядам появи-лась необходимость в критериях, которые позволили бы отличить стационарный процесс от нестационарного.
554 Интегрированные процессы... К неформальным методам проверки стационарности можно отнести визуаль-ный анализ графиков спектральной плотности и автокорреляционной функции. В настоящее время самым популярным из формальных критериев является критерий, разработанный Дики и Фуллером (DF-тест). Предположим, нужно выяснить, какой из двух процессов лучше подходит для описания временного ряда: xt = μ0 + μ1t + εt или xt = μ0 + xt−1 + εt, где εt -стационарный ARMA-процесс. Первый из процессов является стацио-нарным относительно тренда, а второй содержит единичный корень и дрейф. Каж-дый из вариантов может рассматриваться как правдоподобная модель экономиче-ского процесса. Внешне два указанных процесса сильно различаются, однако можно показать, что оба являются частными случаями одного и того же процесса: xt = γ0 + γ1t + vt, vt = ϕvt−1 + εt, что можно переписать также в виде (xt − γ0 − γ1t) = ϕ(xt−1 − γ0 − γ1(t − 1)) + εt. (17.3) Как было показано ранее (17.2) для марковского процесса, при |ϕ| < 1 данный процесс эквивалентен процессу xt = μ0 + μ1t + εt, где коэффициенты связаны соотношениями: γ0 = μ0 1 − ϕ − ϕμ1 (1 − ϕ)2 и γ1 = μ1 1 − ϕ. При ϕ = 1 получаем xt − γ0 − γ1t = xt−1 − γ0 − γ1t + γ1 + εt, т.е. xt = γ1 + xt−1 + εt. Таким образом, как и утверждалось, обе модели являются частными случаями одной и той же модели (17.3) и соответствуют случаям |ϕ| < 1 и ϕ = 1. Модель (17.3) можно записать следующим образом: xt = γ0 + γ1t + ϕ(xt−1 − γ0 − γ1(t − 1)) + εt.
17.4. Проверка на наличие единичных корней 555 Это модель регрессии, нелинейная по параметрам. Заменой переменных мы можем свести ее к линейной модели: xt = μ0 + μ1t + ϕxt−1 + εt, где μ0 = (1 − ϕ)γ0 + ϕγ1, μ1 = (1 − ϕ)γ1. Эта новая модель фактически эквивалентна (17.3), и метод наименьших квад-ратов даст ту же самую оценку параметра ϕ. Следует, однако, иметь в виду, что линейная модель скрывает тот факт, что при ϕ = 1 будет выполнено μ1 = 0. Базовая модель, которую использовали Дики и Фуллер,-авторегрессионный процесс первого порядка: xt = ϕxt−1 + εt. (17.4) При ϕ = 1 это случайное блуждание. Конечно, вряд ли экономическая пере-менная может быть описана процессом (17.4). Более реалистично было бы пред-положить наличие в этом процессе константы и тренда (линейного или квадратич-ного): xt = μ0 + ϕxt−1 + εt, (17.5) xt = μ0 + μ1t + ϕxt−1 + εt, (17.6) xt = μ0 + μ1t + μ2t2 + ϕxt−1 + εt. (17.7) Нулевая гипотеза в критерии Дики-Фуллера состоит в том, что рядн естацио-нарен и имеет один единичный корень ϕ = 1, при этом μi = 0, альтернативная- в том, что рядстац ионарен |ϕ| < 1: H0 : ϕ = 1, μi = 0, HA : |ϕ| < 1. Здесь i = 0, если оценивается (17.5), i = 1, если оценивается (17.6), и i = 2, если оценивается (17.7). Предполагается, что ошибки εt некоррелированы. Это предположение очень важно, без него критерий работать не будет. Для получения статистики, с помощью которой можно было бы проверить ну-левую гипотезу, Дики и Фуллер предложили оценить данную авторегрессионную мод ель и взять из нее обычную t-статистику для гипотезы о том, что ϕ = 1. Эту статистику называют статистикой Дики-Фуллера и обозначают DF. При этом критерий является односторонним, поскольку альтернатива ϕ > 1, соответству-ющая взрывному процессу, не рассматривается. DF заключается в том, что с помощью одной t-статистики как бы проверяется гипотеза сразу о двух коэффициентах. На самом деле, фактически подразумевается модель вида (17.3), в которой проверяется гипотеза об одном коэффициенте ϕ.
556 Интегрированные процессы... Если в регрессии (17.6) нулевая гипотеза отвергается, то принимается альтер-нативная гипотеза-о том, что процесс описывается уравнением (17.6) с ϕ < 1, то есть это стационарный относительно линейного тренда процесс. В против-ном случае имеем нестационарный процесс, описываемый уравнением (17.5), где ϕ = 1, то есть случайное блуждание с дрейфом, но без временного тренда в урав-нении авторегрессии. Часто встречается несколько иная интерпретация этой особенности данного критерия: проверяется гипотеза H0 : ϕ = 1 против гипотезы HA : ϕ < 1, и оцениваемая регрессия не совпадает с порождающим данные процессом, каким он предполагается согласно альтернативной гипотезе, а именно, в оцени-ваемой регрессии имеется "лишний" детерминированный регрессор. Так, чтобы проверить нулевую гипотезу для процесса вида (17.5), нужно построить регрессию (17.6) или (17.7). Аналогично для проверки нулевой гипотезы о процессе вида (17.6) нужно оценить регрессию (17.7). Однако приведенная ранее интерпретация более удачная. Поскольку статистика Дики-Фуллера имеет нестандартное распределение, для ее использования требуются специальные таблицы. Эти таблицы были состав-лены эмпирически методом Монте-Карло. Все эти статистики получены на основе одного и того же процесса вида (17.4) с ϕ = 1, но с асимптотической точки зре-ния годятся и для других процессов, несмотря на наличие мешающих параметров, которые приходится оценивать. Чтобы удобно было использовать стандартные регрессионные пакеты, урав-нения регрессии преобразуются так, чтобы зависимой переменной была первая разность. Например, в случае (17.4) имеем уравнение Δxt = δxt−1 + εt, где δ = ϕ − 1. Тогда нулевая гипотеза примет вид δ = 0. Предположение о том, что переменная следует авторегрессионному процессу первого порядка и ошибки некоррелированы, является, конечно, слишком ограни-чительным. КритерийДики-Фуллерабылмодифицировандля авторегрессионных процессов более высоких порядков и получил название дополненного критерия Дики-Фуллера (augmented Dickey-Fuller test, ADF). Базовые уравнения для него приобретают следующий вид: Δxt = (ϕ − 1)xt−1 + k j=1 γjΔxt−j + εt, (17.8)
17.4. Проверка на наличие единичных корней 557 Δxt = μ0 + (ϕ − 1)xt−1 + k j=1 γjΔxt−j + εt, (17.9) Δxt = μ0 + μ1t + (ϕ − 1)xt−1 + k j=1 γjΔxt−j + εt, (17.10) Δxt = μ0 + μ1t + μ2t2 + (ϕ − 1)xt−1 + k j=1 γjΔxt−j + εt. (17.11) Распределения этих критериев асимптотически совпадают с соответствующими обычными распределениями Дики-Фуллера и используют те же таблицы. Грубо говоря, роль дополнительной авторегрессионной компоненты сводится к тому, что-быубрать автокорреляциюиз остатков.Процедура проверки гипотез не отличается от описанной выше. Как показали эксперименты Монте-Карло, критерийДики-Фуллера чувстви-телен к наличию процесса типа скользящего среднего в ошибке. Добавление в чис-ло регрессоров распределенного лага первой разности (с достаточно большим зна-чением k) частично снимает эту проблему. На практике решающим при использовании критерия ADF является вопрос о том, как выбирать k -порядок AR-процесса в оцениваемой регрессии. Можно предложить следующие подходы. 1) Выбирать k на основе обычных t-и F -статистик.Процедура состоит в том, чтобы начать с некоторой максимальной длины лага и проверять вниз, используя t-или F -статистики для значимости самого дальнего лага (лагов). Процесс оста-навливается, когда t-статистика или F-статистика значимы. 2) Использовать информационные критерии Акаике и Шварца. Длина лага с минимальным значением информационного критерия предпочтительна. 3) Сделать остатки регрессии ADF-критерия как можно более похожими на бе-лый шум. Это можно проверить при помощи критерия на автокорреляцию. Если соответствующая статистика значима, то лаг выбран неверно и следует увели-чить k. Поскольку дополнительные лаги не меняют асимптотические результаты, то лучше взять больше лагов, чем меньше. Однако этот последний аргумент ве-рен только с асимптотической точки зрения. ADF может давать разные результаты в зависимости от того, каким выбрано количество лагов.Даже добавление лага, ко-торый "не нужен" согласно только что приведенным соображениям, может резко изменить результат проверки гипотезы. Особую проблему создает наличие сезонной компоненты в переменной. Если сезонность имеет детерминированный характер, то достаточно добавить в регрес-
558 Интегрированные процессы... сию фиктивные сезонные переменные-это не изменяет асимптотического рас-пределения ADF-статистики. Для случая стохастической сезонности также есть специальные модификации критерия. До сих пор рассматривались критерии I(1) против I(0). Временной рядмож ет быть интегрированным и более высокого порядка. Несложно понять, что критерии I(2) против I(1) сводятся к рассмотренным, если взять не уровень исследуемого ряда, а первую разность. Аналогичный подход рекомендуется для более высоких порядков интегрирования. Имитационные эксперименты, проведенные Сэдом и Дики, показали, что сле-дует проверять гипотезы последовательно, начиная с наиболее высокого порядка интегрирования, который можно ожидать априорно. То есть сначала следует про-верить гипотезу о том, что рядявл яется I(2), и лишь после этого, если гипотеза отвергнута, что он является I(1). 17.5. Коинтеграция. Регрессии с интегрированными переменными Как уже говорилось выше, привычные методы регрессионного анализа не под-ходят, если переменные нестационарны. Однако не всегда при применении МНК имеет место эффект ложной регрессии. Говорят, что I(1)-процессы {x1t} и {x2t} являются коинтегрированными по-рядка 1 и 0, коротко CI(1, 0), если существует их линейная комбинация, которая является I(0), т.е. стационарна. Таким образом, процессы {x1t} и {x2t}, интегри-рованные первого порядка I(1),-коинтегрированы, если существует коэффици-ент λ такой, что x1t − λx2t ∼I(0). Понятие коинтеграции введено Грейнджером в 1981 г. и использует модель исправления ошибок. Коинтегрированные процессы {x1t} и {x2t} связаны между собой долгосрочным стационарным соотношением, и предполагается, что существует некий корректирующий механизм, который при отклонениях возвращает x1t и x2t к их долгосрочному отношению. Если λ = 1, то разность x1t−x2t будет стационарной и, грубо говоря, x1t и x2t будут двигаться "параллельно" во времени. На рисунке 17.2 изображены две такие коинтегрированные переменные, динамика которых задана моделью исправления ошибок: Δx1t = −0.2(x1,t−1 − x2,t−1 + 2) + ε1t, (17.12) Δx2t = 0.5(x1,t−1 − x2,t−1 + 2) + ε2t, (17.13) где ε1t и ε2t -независимые случайные величины, имеющие стандартное нор-мальное распределение.
17.5. Коинтеграция. Регрессии с интегрированными переменными 559 Рис. 17.2. Два коинтегрированных процесса при λ = 1 Если переменные в регрессии не стационарны, но действительно связаны друг с другом стационарной линейной комбинацией (модель специфицирована верно), то полученные оценки коэффициентов этой линейной комбинации будут на самом деле сверхсостоятельными, т.е. будут сходиться по вероятности к истинным ко-эффициентам со скоростью, пропорциональной не квадратному корню количества наблюдений √T , как в регрессии со стационарными переменными, а со скоростью, пропорциональной просто количеству наблюдений T . Другими словами, в обыч-ной регрессии √T(λ∗−λ) имеет невырожденное асимптотическое распределение, где λ∗ -полученная из регрессии оценка λ, а в регрессии с I(1)-переменными T(λ∗ − λ) имеет невырожденное асимптотическое распределение. Обычные асимптотические аргументы сохраняют свою силу, если речь идет об оценках параметров краткосрочной динамики в модели исправления ошибок. Таким образом, можно использовать t-статистики, получаемые обычным методом наименьших квадратов, для проверки гипотез о значимости отдельных перемен-ных. Важно помнить, что это относится к оценкам краткосрочных параметров. Этот подход не годится для проверки гипотез о коэффициентах коинтеграционной комбинации. Определение коинтеграции естественным образом распространяется на слу-чай нескольких переменных произвольного порядка интегрирования. Компоненты k-мерного векторного процесса xt = (x1t, . . . , xkt) называют коинтегрирован-ными порядка d и b, что обозначается xt ∼ CI(d, b), если 1) каждая компонента xit является I(d), i = 1, . . . , k; 2) существует отличный от нуля вектор β, такой что xtβ ∼ I(d−b), d b > 0. Вектор β называют коинтегрирующим вектором.
560 Интегрированные процессы... Коинтегрирующий вектор определен с точностью до множителя. В рассмотрен-ном ранее примере коинтегрирующий вектор имеет вид β = (−1, λ). Его можно пронормировать: β = (−1/λ, 1), или так, чтобы сумма квадратов элементов была равна 1, т.е. β = −√ 1 1 +λ2 , √ λ 1 + λ2. 17.6. Оценивание коинтеграционной регрессии: подход Энгла-Грейнджера Если бы коэффициент λ был известен, то выяснение того, коинтегрированы ли переменные x1t и x2t, было бы эквивалентно выяснению того, стационар-на ли комбинация x1t − λx2t (например, с помощью критерия Дики-Фуллера). Но в практических ситуациях обычно стационарная линейная комбинация неиз-вестна. Значит, необходимо оценить коинтегрирующийвектор.После этого следует выяснить, действительно ли этот вектор дает стационарную линейнуюкомбинацию. Простейшим методом отыскания стационарной линейной комбинации является метод Энгла-Грейнджера. Энгл и Грейнджер предложили использовать оценки, полученные из обычной регрессии с помощью метода наименьших квадратов. Одна из переменных должна стоять в левой части регрессии, другая-в правой: x1t = λx2t + εt. Для того чтобы выяснить, стационарна ли полученная линейная комбинация, предлагается применить метод Дики-Фуллера к остаткам из коинтеграционной регрессии. Нулевая гипотеза состоит в том, что εt содержит единичный корень, т.е. x1t и x2t не коинтегрированы. Пусть et -остатки из этой регрессии. Про-верка нулевой гипотезы об отсутствии коинтеграции в методе Энгла-Грейнджера проводится с помощью регрессии: et = ϕet−1 + ut. (17.14) Статистика Энгла-Грейнджера представляет собой обычную t-статистику для проверки гипотезы ϕ = 1 в этой вспомогательной регрессии. Распреде-ление статистики Энгла-Грейнджера будет отличаться (даже асимптотически), от распределения DF-статистики, но имеются соответствующие таблицы. Если мы отклоняем гипотезу об отсутствии коинтеграции, то это дает уверенность в том, что полученные результаты не являются ложной регрессией. Игнорирование детерминированных компонент ведет к невернымвыводам о ко-интеграции. Чтобы этого избежать, в коинтеграционную регрессию следует доба-вить соответствующие переменные-константу, тренд, квадрат тренда, сезонные
17.7. Коинтеграция и общие тренды 561 фиктивные переменные. Например, добавляя константу и тренд, получим регрес-сию x1t = μ0+μ1t+λx2t+εt. Такое добавление, как и в случае критерияDF,меняет асимптотическое распределение критерия Энгла-Грейнджера. Следует помнить, что, в отличие от критерия Дики-Фуллера, регрессия (17.14), из которой берется t-статистика, остается неизменной, то есть в нее не нужно добавлять детермини-рованные регрессоры. В МНК-регрессии с коинтегрированными переменными оценки должны быть смещенными из-за того, что в правой части стоит эндогенная переменная x2t, коррелированная с ошибкой εt. Кроме того, ошибка содержит пропущенные пере-менные. Коинтеграционная регрессия Энгла-Грейнджера является статической по форме, т.е. не содержит лагов переменных. С асимптотической точки зрения это не приводит к смещенности оценок, поскольку ошибка является величиной мень-шего порядка, чем регрессор x2t, дисперсия которого стремится к бесконечности. Как уже говорилось, оценки на самом деле сверхсостоятельны. Однако в малых выборках смещение может быть существенным. После того как найдена стационарная линейная комбинация, можно оценить модель исправления ошибок (15.11), которая делает переменные коинтегриро-ванными. В этой регрессии используются первые разности исходных переменных и остатки из коинтеграционной регрессии, которые будут представлять корректи-рующий элемент модели исправления ошибок: Δx1t = −θlt + p−1 j=1 γjΔx1,t−j + q−1 j=0 βjΔx2,t−j + vt. (17.15) Подчеркнем роль корректирующего элемента θlt. До появления метода Энгла-Грейнджера исследователи часто оценивали регрессии в первых разно-стях, что хотя и приводило к стационарности переменных, но не учитывался ста-ционарный корректирующий член, т.е. регрессионная модель была неверно специ-фицирована (проблема пропущенной переменной). Несмотря на то, что в модели исправления ошибок (17.15) используется оцен-ка коинтегрирующего вектора ( 1−λ), оценки коэффициентов такой модели будут иметь такие же асимптотические свойства, как если бы коинтегрирующий век-тор был точно известен. В частности, можно использовать t-статистики из этой регрессии (кроме t-статистики для θ), поскольку оценки стандартных ошибок яв-ляются состоятельными. Это является следствием сверхсостоятельности оценок коинтегрирующего вектора. 17.7. Коинтеграция и общие тренды Можно предположить, что коинтеграция между двумя интегрированными пере-менными, скорее всего, проистекает из того факта, что обе они содержат одну и ту
562 Интегрированные процессы... же нестационарную компоненту, называемую общим трендом. Выше мы получили для интегрированной переменной разложение Бевериджа-Нельсона на детерми-нированный тренд, стохастический тренд и стационарную составляющую. Следует показать, что стохастические тренды в двух коинтегрированных переменных долж-ны быть одними и теми же. Проведем сначала подобный анализ для детерминированных трендов. Пусть процессы {xt} и {zt} стационарны относительно некоторого тренда f(t), не обя-зательно линейного: xt = μ0 + μ1f(t) + εt и zt = ν0 + ν1f(t) + ξt, где εt и ξt -два стационарных процесса. В каком случае линейная комбина-ция этих двух процессов будет стационарной в обычном смысле (не относительно тренда)? Найдем линейную комбинацию xt − λzt: xt − λzt = μ0 − λν0 + (μ1 − λν1)f(t) + εt − λξt. Для ее стационарности требуется, чтобы μ1 = λν1. С другой стороны, если бы {xt} содержал тренд f(t), а {zt} -отличный от него тренд g(t), то, в общем случае, не нашлось бы линейной комбинации, такой что μ1f(t) − λν1g(t) оказалась бы постоянной величиной. Следовательно, для {xt} и {zt} не нашлось бы коинтегрирующего вектора. Коинтегрирующий вектор можно найти только в том случае, если f(t) = λν1 μ1 g(t) для некоторого λ, т.е. если f(t) и g(t) линейно зависимы. Пусть теперь {xt} и {zt} -два процесса I(1), для которых существуют раз-ложения Бевериджа-Нельсона: xt = γt + vt + εt, zt = δt + wt + ξt, где vt и wt -случайные блуждания, а εt и ξt -стационарные процессы. Найдем условия, при которых линейная комбинация xt и zt, xt − λzt = γt + vt + εt − λ(δt + wt + ξt) = (γ − λδ)t + vt − λwt + εt − λξt, может быть стационарной. Для стационарности требуется, чтобы в получившейся переменной отсутствовал как детерминированный, так и стохастический тренд.Это достигается при γ = λδ и vt = λwt . При этом xt можно записать как xt = λ(δt + wt) + εt, т.е. {xt} и {zt} содержат общий тренд δt + wt.
17.8. Упражнения и задачи 563 Этот взглядна коинтеграцию развили в 1988 г. Сток и Уотсон: пусть есть k интегрированных переменных, которые коинтегрированы. Тогда каждая из этих переменных может быть записана как стационарная компонента плюс линейная комбинация меньшего количества общих трендов. 17.8. Упражнения и задачи Упражнение 1 Сгенерируйте 20 марковских процессов xt = ϕxt−1 +εt при различных коэф-фициентах авторегрессии: а) ϕ = 0.1; б) ϕ = 0.9; в) ϕ = 1; г) ϕ = 1.02. В качестве ошибки используйте нормально распределенный белый шум с единичной диспер-сией и возьмите x0 = 0. 1.1. Изобразите графики для всех 20 рядов для каждого из четырех случаев. Какой выводм ожно сделать? 1.2. Рассчитайте для каждого из четырех случаев дисперсию значений соответ-ствующих 20 рядов, рассматривая их как выборку для t = 1, . . . , 100. На-рисуйте график дисперсии по времени для всех четырех случаев. Сделайте выводы. 1.3. Оцените для всех рядов авторегрессию первого порядка и сравните распреде-ления оценок для всех 4 случаев с истинными значениями. Сделайте выводы. Упражнение 2 Покажите эффект ложной регрессии для переменных I(1) с помощью метода Монте-Карло. Сгенерируйте по 100 рядов xt и zt по модели случайного блужда-ния: xt = xt−1 + εt и zt = zt−1 + ξt, где ошибки εt ∼ N(0, 1) и ξt ∼ N(0, 1) неавтокоррелированы и некоррелированы друг с другом. 2.1. Оцените для всех 100 наборов данных регрессию xt по константе и zt: xt = azt + b + ut. Рассчитайте соответствующие статистики Стьюдента для коэффициента a и проанализируйте выборочное распределение этих статистик. В скольки
564 Интегрированные процессы... процентах случаев нулевая гипотеза (гипотеза о том, что коэффициент равен нулю) отвергается, если использовать стандартнуюграницу t-распределения с уровнем доверия 95%? Найдите оценку фактической границы уровня дове-рия 95%. 2.2. Проанализируйте для тех же 100 регрессий выборочное распределение ко-эффициента детерминации. 2.3. Возьмите пять первых наборов данных и проверьте ряды на наличие единич-ных корней с помощью теста Дики-Фуллера. 2.4. Повторите упражнения 2.1, 2.2 и 2.3, сгенерировав данные xt и zt по стаци-онарной модели AR(1) с авторегрессионным коэффициентом 0.5 и сравните с полученными ранее результатами. Сделайте выводы. Упражнение 3 Сгенерируйте 100 рядов по модели случайного блуждания с нормально распре-деленным белым шумом, имеющим единичную дисперсию. Проверьте с помощью сгенерированных данных одно из значений в таблице статистики Дики-Фуллера. Упражнение 4 Рассмотрите данные из таблицы 15.3 на стр. 520. 4.1. Преобразуйте ряды, перейдя к логарифмам, и постройте их графики.Можно ли сказать по графику, что ряды содержат единичный корень? 4.2. Проверьте формально ряды на наличие единичных корней с помощью допол-ненного теста Дики-Фуллера, включив в регрессии линейный тренди 4 лага разностей. 4.3. Используя МНК, оцените параметры модели Ct = αYt + β + εt, вычислите остатки из модели. К остатком примените тест Энгла-Грэйнджера. 4.4. Используя коэффициенты из упражнения 4.3, оцените модель исправления ошибок с четырьмя лагами разностей. Задачи 1. Задан следующий процесс: xt = 0.8xt−1 +0.2xt−2 +εt −0.9εt−1. При каком d процесс Δdxt будет стационарным?
17.8. Упражнения и задачи 565 2. Изобразите графики автокорреляционной функции и спектра для второй раз-ности стохастического процесса, содержащего квадратический тренд. 3. Дан процесс xt = ft + εt, ft = ft−1 + νt, гд е εt ∼ N(0, 2) и νt ∼ N(0, 1). Определить тип процесса, перечислить входящие в его состав компоненты и вычислить условное математическое ожидание и условную дисперсию про-цесса xt. 4. Чем отличается стохастический трендот обычного линейного тренда с точки зрения устранения проблемы ложной регрессии? 5. Процессы xt и yt заданы уравнениями xt = αxt−2 + εt и yt = βxt + + ξt + γξt−1, гд е εt и ξt - стационарные процессы. При каких условиях на параметры α, β и γ можнобылобысказать, что xt и yt коинтегрированы как CI(1, 0)? 6. Известно, что zt = t i=1 xi, гд еxi -процесс случайного блуждания, а yt = εt +0.5εt−1 + 0.25εt−2 + 0.125εt−3 + . . . . Можно ли построить ре-грессию zt от yt? Ответ обосновать. 7. Пусть xt = ε + bxt−1 + εt, гд е εt ∼ N(0, 1) и b 1, а zt -стохастический процесс, содержащий линейный тренд. Можно ли установить регрессионную связь между xt и zt? Если да, то как? 8. Получены оценки МНК в регрессии xt = ϕxt−1 + εt. Приведите вид стати-стики, используемой в тесте Дики-Фуллера. 9. Правомерно ли построение долгосрочной зависимости yt по xt, если yt - процесс случайного блуждания с шумом, xt -процесс случайного блуж-дания с дрейфом? Если нет,- ответ обосновать. Если да,-изложить эта-пы построения регрессии с приведением формул, соответствующих каждому этапу. 10. В чем преимущества дополненного теста Дики-Фуллера по сравнению с обычным DF-тестом? Привести формулы. Рекомендуемая литература 1. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000 (гл. 12-стр. 240-249). 2. Banerjee A., Dolado J.J., Galbraith J.W. and Hendry D.F., Co-integration, Error Correction and the Econometric Analysis of Non-stationary Data, Oxford University Press, 1993 (гл. 3-5)
566 Интегрированные процессы... 3. Davidson, R., and J.G.MacKinnon. Estimation and Inference in Econometrics. Oxford University Press, 1993 (Гл. 20.) 4. Dickey, D.A. and Fuller W.A., "Distributions of the Estimators for Autoregressive Time Series With a Unit Root", Journal of American Statistical Association, 75 (1979), 427-431. 5. Enders,W. Applied Econometric Time Series. John Wiley & Sons, 1995. 6. Engle, R.F. and Granger C.W.J., "Co-integration and Error Correction: Representation, Estimation and Testing", Econometrica, 55 (1987), 251-276. 7. Granger, C.W.J., and Newbold P. "Spurious Regressions in Econometrics", Journal of Econometrics, 21 (1974), 111-120. 8. Greene W.H. Econometric Analysis, Prentice-Hall, 2000. (гл.18-стр. 776- 784) 9. Said, E.S. and Dickey D.A., "Testing for Unit Roots in Autoregressive-Moving Average Models of Unknown Order", Biometrica, 71 (1984), 599-607. 10. Stock, J.H. and Watson M.W., "Testing for Common Trends", Journal of the American Statistical Association, 83 (1988), 1097-1107. 11. Stock, J.H., "Asymptotic Properties of Least Squares Estimators of Cointegrating Vectors", Econometrica, 55 (1987), 1035-1056. 12. Wooldridge Jeffrey M. Introductory Econometrics: A Modern Approach, 2nd ed., Thomson, 2003. (Ch. 18).
Часть IV Эконометрия-II 567
Это пустая страница
Глава 18 Классические критерии проверки гипотез 18.1. Оценка параметров регрессии при линейных ограничениях Рассмотрим модель линейной регрессии X = Zα+ε в случае, когда известно, что коэффициенты α удовлетворяют набору линейных ограничений Rα = r, где R -матрица размерности k×(n+1), а r -вектор свободных частей ограни-чений длины k. Метод наименьших квадратов для получения более точных оценок в этом случае следует модифицировать, приняв во внимание ограничения. Оценки наименьших квадратов при ограничениях получаются из задачи условной миними-зации: ⎧⎪⎪⎨ ⎪⎪⎩ (X − Zα)(X − Zα) → min! α Rα = r. Запишем для этой задачи функцию Лагранжа: L = (X − Zα)(X − Zα) + 2λ(Rα − r),
570 Глава 18. Классические критерии проверки гипотез где λ -вектор-столбец множителей Лагранжа. Возьмем производные (см. При-ложение A.2.2): ∂L ∂λ = −2(Rα − r), ∂L ∂α = −2(Z(X − Zα) − Rλ). Следовательно, оценки наименьших квадратов при ограничениях, a1, находятся из уравнений: Ra1 − r = 0, Z(X − Za1) − Rλ = 0. Умножим второе уравнение слева на (ZZ)−1. Получим a1 = ZZ−1 ZX − ZZ−1 Rλ. Пусть a0 -оценки α без учета ограничений, то есть a0 = (ZZ)−1 ZX. Тогд а a1 = a0 − ZZ−1 Rλ. (18.1) Если умножим это уравнение слева на R, то получим RZZ−1 Rλ = Ra0 − r. Отсюда λ = A−1 (Ra0 − r) , где мы ввели обозначение A = R(ZZ)−1 R. Это можно проинтерпретировать следующим образом: чем сильнее нарушается ограничение на оценках из регрес-сии без ограничений, тем более значимы эти ограничения. Подставим множители Лагранжа обратно в уравнение (18.1): a1 = a0 − ZZ−1 RA−1 (Ra0 − r) . (18.2) Таким образом, оценки a1 и a0 различаются тем сильнее, чем сильнее нарушается ограничение Rα = r в точке a0, т.е. чем больше невязки Ra0 − r. Докажем несмещенность оценок. Для этого в выражении для a везде заменим ¯a на ZZ−1 ZX = ZZ−1 Z(Zα + ε) = ZZ−1 Z(Zα + ε) = α + ZZ−1 Zε.
18.1. Оценка параметров регрессии при линейных ограничениях 571 Получим a1 = α + ZZ−1 Zε − ZZ−1 RA−1 Rα + RZZ−1 Zε − r. При выполнении нулевой гипотезы Rα = r можем упростить это выражение: a1 = α + ZZ−1 Zε − ZZ−1 RA−1RZZ−1 Zε = = α + I − ZZ−1 RA−1RZZ−1 Zε. Но по предположению классической модели регрессии E(ε) = 0, следователь-но, математическое ожидание второго слагаемого здесь равно нулю. А значит, E(a1) = α. Несмещенность оценки a1 доказана. Величина I − (ZZ)−1 RA−1R(ZZ)−1 Zε представляет собой отличие a1 от α. Найдем теперь ковариационную матрицу оценок a. После преобразований по-лучаемcov(a1) = E7(a1 − α)(a1 − α)8 = σ2 I − ZZ−1 RA−1RZZ−1 , где используется еще одно предположение модели регрессии-отсутствие авто-корреляции и гетероскедастичности в ошибках, т.е. E(εε) = σ2I. В то же время cov(a0) = σ2 (ZZ)−1. Таким образом, ковариационные матрицы оценок различаются на положительно (полу-) определенную матрицу σ2 (ZZ)−1 RA−1R(ZZ)−1, что можно интерпретировать в том смысле, что оцен-ки a являются более точными, чем ¯a, поскольку учитывают дополнительную ин-формацию о параметрах. Насколько отличаются между собой суммы квадратов остатков в двух рассмат-риваемых моделях? Ясно, что в регрессии с ограничениями сумма квадратов не может быть ниже (так как минимизируется та же функция при дополнительных ограничениях). Вычислим разность между двумя суммами квадратов остатков. Пусть e0 = X − Za0 -вектор остатков при оценке параметров регрессии без ограничений, а e1 = X − Za1 -вектор остатков при оценке параметров с ограничениями, и пусть RSS(a0) = e1e1, RSS(a0) = e0e0 -соответствующие суммы квадратов остатков. Из (18.2) получаем X − Za1 = X − Za0 + Z ZZ−1 RA−1 (Ra0 − r) , или e1 = e0 + Z ZZ−1 RA−1 (Ra0 − r) .
572 Глава 18. Классические критерии проверки гипотез Введем обозначение: δ = Z (ZZ)−1 RA−1 (Ra0 − r), тогда e1 = e0 + δ. Поскольку Ze0 = 0, то δe0 = 0. Следовательно, e1e1 = e0e0 + δe0 + e0δ + δδ = e0e0 + δδ. Здесь δδ = Z ZZ−1 RA−1 (Ra0 − r)Z ZZ−1 RA−1 (Ra0 − r)= = (Ra0 − r)A−1RZZ−1 RA−1 (Ra0 − r) , или, учитывая определение матрицы A, δδ = (Ra0 − r)A−1 (Ra0 − r) . Итак, RSS(a1) − RSS(a0) = e1e1 − e0e0 = (Ra0 − r)A−1 (Ra0 − r) . Как и следовало ожидать, эта разность оказывается неотрицательной. 18.2. Тестна существенность ограничения Пусть, как и раньше, e0 -вектор остатков в регрессии без ограничений, e1 -вектор остатков в регрессии с ограничениями, N -число наблюдений, n -количество факторов, k -общее количество ограничений на параметры регрессии. В случае, если ограничения Rα = r выполнены, то Fc = (e1e1 − e0e0)/k e0e0/(N − n − 1) ∼ Fk,N−n−1. (18.3) Иными словами, статистика, равная относительному приросту суммыквадратов остатков в регрессии с ограничениями по сравнению с регрессией без ограничений, скорректированному на степени свободы, имеет распределение Фишера (см. При-ложение A.3.2). Чем больше RSS(a1) будет превышать RSS(a0), тем более су-щественно ограничение и тем менее правдоподобно, что ограничение выполнено.
18.2. Тест на существенность ограничения 573 Докажем, что распределение статистики будет именно таким, если предположить, что ошибки имеют нормальное распределение: ε ∼ N(0, σ2I). Мывидели, что e1 = e0+δ . Выразим δ через ε с учетом нулевой гипотезы Rα = r: δ = Z (ZZ)−1 RA−1 (Ra0 − r) = Z (ZZ)−1 RA−1R(ZZ)−1 Zε = = Z (ZZ)−1 RR(ZZ)−1 R−1 R(ZZ)−1 Zε. Для вектора остатков e0 имеем e0 = I − Z (ZZ)−1 Zε. Совместное распределение векторов δ и e0 является нормальным (так как они линейно выражаются через ошибки ε). Эти вектора некоррелированы: E (δe0) = = Z (ZZ)−1 RR(ZZ)−1 R−1 R(ZZ)−1 ZE (εε) I − Z (ZZ)−1 Z= = σ2Z (ZZ)−1 RR(ZZ)−1 R−1 R(ZZ)−1 ZI − Z (ZZ)−1 Z= 0. Последнее равенство здесь следует из того, что ZI − Z (ZZ)−1 Z= Z− ZZ (ZZ)−1 Z= Z− Z= 0. По свойствам многомерного нормального распределения это означает, что δ и e0 независимы (см. Приложение A.3.2). Кроме того δ имеет форму Q(QQ)−1 Qε , гд еQ = Z (ZZ)−1 R. Для вектора же e0 матрица преобразования равна I − Z (ZZ)−1 Z. Обе мат-рицы преобразования являются симметричными и идемпотентными. Ранг матрицы преобразования (другими словами, количество степеней свободы) равен k для δ и N − n − 1 для e0. С учетом того, что ε ∼ N(0, σ2I), это означает: 1 σ2 δδ = 1 σ2 (e1e1 − e0e0) ∼ χ2k, 1 σ2 e0e0 ∼ χ2N−n−1, причем эти две величины независимы. Частное этих величин, деленных на количе-ство степеней свободы, распределено как Fk,N−n−1, что и доказывает утверждение. Проверяемая нулевая гипотеза имеет вид: H0 : Rα = r,
574 Глава 18. Классические критерии проверки гипотез то есть ограничения выполнены. Критерий заключается в следующем: если Fc > Fk,N−n−1, 1−θ, то нулевая гипотеза отвергается, если Fc < Fk,N−n−1, 1−θ, то она принимается. Распишем статистику подробнее: Fc = (Ra0 − r)R(ZZ)−1 R−1 (Ra0 − r) /k (X − Za0)(X − Za0) /(N − n − 1) (18.4) Покажем, что F-критерий базового курса-это частный случай данного кри-терия.Спомощью F-критерия для регрессии в целом проверяется нулевая гипоте-за о том, что все коэффициенты α, кроме свободного члена (последнего, (n + 1)-го коэффициента), равны нулю:H0 : α1 = 0, . . . , αn = 0. Запишем эти ограничения на коэффициенты в матричном виде (Rα = r). Со-ответствующие матрицы будут равны R = [In; 0n] и r = 0n. Обозначим матрицу Z без последнего столбца (константы) через Z−. В этих обозначениях Z = [Z−; 1N]. Тогд а ZZ = N⎡⎢⎣ M + ¯ Z¯ Z ¯ Z¯ Z 1 ⎤⎥⎦ и ZZ−1 = 1 N ⎡⎢⎣ M−1 −M−1 ¯ Z− ¯ZM−1 1 + ¯ZM−1 ¯ Z⎤⎥⎦, где ¯ Z = 1 N 1NZ− -вектор средних, а M = 1 N ˆ Zˆ Z -ковариационная матрица факторов Z−. Умножение (ZZ)−1 слева и справана R выделяет из этой матрицы левый верхний блок: RZZ−1 R= 1NM−1. Подставим это выражение в (18.4), обозначая через a− = Ra0 вектор из пер-вых n компонент оценок a0 (все коэффициенты за исключением константы): Fc = (a−Ma−) /n (X − Za0)(X − Za0) /(N(N − n − 1)) .
18.2. Тест на существенность ограничения 575 Здесь a−Ma− -объясненная дисперсия врегрессии, а (X−Za0)(X−Za0)/N = = s2e-остаточная дисперсия (смещенная оценка). Видим, что Fc = R2(N − n − 1) (1 − R2)n , а это стандартная форма F-статистики. Более простой способ получения этого результата состоит в том, чтобы возвра-титься к исходной задаче условной минимизации. Ограничение, что все коэффи-циенты, кроме свободного члена, равны нулю, можно подставить непосредственно в целевую функцию (сумму квадратов остатков). Очевидно, что решением задачи будет an+1 = 1 N X1N = ¯x и aj = 0, j = n + 1, т.е. a1 = ⎛⎜⎝ 0n ¯x ⎞⎟⎠. Отсюда получаем, что остатки равны центрированным значениям зависимой пере-менной: e1 = ˆX. Следовательно, из (18.3) получим Fc = (ˆXˆX− e0e0)/n e0e0/(N − n − 1) . В числителе стоит объясненная сумма квадратов, а в знаменателе-сумма квадратов остатков. Покажем, что t-критерий также является частным случаем данного критерия. Нулевая гипотеза заключается в том, что j-й параметр регрессии равен нулю. Таким образом, в этом случае k = 1, R = (0, . . . , 0,1j, 0, . . . , 0) и r = 0. При этом R(ZZ)−1R= m−1 jj , гд еm−1 jj - j-й диагональный элемент мат-рицы M−1, а M = 1 N ZZ -матрица вторых начальных моментов факторов Z. В числителе F-статистики стоит несмещенная оценка остаточной дисперсии ˆs2e= (X − Za0)(X − Za0) /(N − n − 1) . Кроме того, Ra0 − r = a0j . Окончательно получаем Fc = a2 0j m−1 jj ˆs2e/N ∼ F1, N−n−1.
576 Глава 18. Классические критерии проверки гипотез Здесь m−1 jj ˆs2e/N = s2aj -оценка дисперсии коэффициента a0j . Видим, что F-статистика имеет вид: Fc = a2 0j s2aj = a0j saj 2 = t2j, т.е. это квадрат t-статистики для коэффициента a0j . Для квантилей F-распределения выполнено F1,N−n−1, 1−θ = t2N−n−1, 1−θ. Нулевая гипотеза H0 : αj = 0 отвергается, если Fc > F1, N−n−1, 1−θ, т.е. если |tj | = √Fc > tN−n−1, 1−θ. Видим, что два критерия полностью эквивалентны. Рассмотрим регрессию, в которой факторы разбиты на два блока: X = Z1α1 + Z2α2 + ε. В качестве промежуточного случая между F-критерием для регрессии в целом и t-критерием для одного фактора рассмотрим F-критерий для группы факторов. Требуется получить ответ на вопрос о том, нужна ли в регрессии группа факторов Z2. Для проверки гипотезы H0 : α2 = 0 мы можем воспользоваться полученными выше результатами. В этом случае k = n2, гд е n2 -количество факторов во второй группе, а матрицы, задающие ограничение, равны R = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0 - - - 0 ... ... 0 - - - 0 " n#$1 % 1 0 - - - 0 0 1 - - - 0 ....... . . ... 0 0 - - - 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ " n#$2 % = [0n2×n1 ; In2 ] и r = 0n2 . Очевидно, что оценки с этими ограничениями будут равны a = ⎛⎜⎝ (Z1Z1)−1 Z1X 0n2 ⎞⎟⎠, а остатки можно найти по формуле e1 = IN − Z1 (Z1Z1)−1 Z1X. F-статистику можно рассчитать по общей формуле (18.3): Fc = (e1e1 − e0e0)/n2 e0e0/(N − n − 1) ∼ Fn2,N−n−1.
18.2. Тест на существенность ограничения 577 Мы неявно рассматривали здесь тест на исключение факторов. Можно рас-сматривать его с другой точки зрения-как тест на включение факторов, при этом формулы не поменяются. То есть мы, оценив регрессию X = Z1α1 + ε, можем проверить, следует ли включать в нее дополнительные факторы Z2. Тест на включение факторов особенно полезен для проверки того, не наруша-ются ли предположения модели регрессии. Это так называемые диагностические тесты. Все они строятся по одному и тому же принципу: если модель X = Z1α1 + ε специфицирована корректно, то любые дополнительные факторы Z2 скорее все-го будут незначимы в тесте на их включение (т.е. с большой вероятностью будет принята нулевая гипотеза α2 = 0). Факторы Z2 конструируются таким образом, чтобытест имел бо´ льшуюмощность против определенного класса нарушений пред-положений модели регрессии. Из сказанного следует, что во всех диагностических тестах нулевой гипотезой является то, что базовая модель корректно специфи-цирована. Если нулевая гипотеза будет отвергнута, то естественно искать другую модель, которая бы лучше описывала имеющиеся данные. Следует понимать, что регрессия X = Z1α1 + Z2α2 + ε в этом случае будет носить обычно вспомога-тельный характер, то есть оценки коэффициентов в ней, вообще говоря, не при-званы нести смысловой нагрузки. Она нужна только для проверки базовой модели X = Z1α1 + ε. (Хотя здесь есть, конечно, и исключения.) 18.2.1. ТестГодфрея (на автокорреляцию ошибок) Оценим регрессию X = Z1α1 + ε. Для проверки отсутствия автокорреляции p-го порядка попытаемся ввести такой набор факторов: Z2 = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 0 0 - - - 0 e1 0 - - - 0 e2 e1 - - - 0 ... .... . . ...... ... ...e1 ... ... ... ... eN−1 - - - - - - eN−p⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . Столбцы матрицы Z2 состоят из лагов остатков, дополненных нулями. Ес-ли нулевая гипотеза (α2 = 0) отвергается, то делается вывод о наличии авто-корреляции. При p = 1 тест Годфрея представляет собой близкий аналог теста
578 Глава 18. Классические критерии проверки гипотез Дарбина-Уотсона, однако при p > 1 помогает обнаруживать и автокорреляцию более высоких порядков, в чем и состоит его преимущество. 18.2.2. ТестRESET Рамсея (Ramsey RESET test) на функциональную форму уравнения Рассмотрим модель X = Z1α1 + ε, которую можно записать в виде X = X0 + ε, гд еX0 = Z1α1. Можно рассмотреть возможность наличия между X и X0 более сложной нелинейной зависимости, например, квадратичной. Для проверки линейности модели против подобной альтернативы служит тест Рамсея. Оценим регрессию X = Z1α1 + ε. Попытаемся ввести фактор Z2 = ⎛⎜⎜⎜⎜⎜⎝ (xc1)2 ... (xcN)2⎞⎟⎟⎟⎟⎟⎠ , где Xc = Z1a1 -расчетные значения из проверяемой регрессии. Если α2 = 0, то зависимость линейная, если α2 = 0, то зависимость квадратичная. Можно тем же способом проводить тест на 3-ю, 4-ю степени и т.д. Матрица добавляемых факторов имеет следующую общую форму: Z2 = ⎛⎜⎜⎜⎜⎜⎝ (xc1)2 (xc1)3 - - - (xcN)m ... ... ... (xcN)2 (xcN)3 - - - (xcN)m⎞⎟⎟⎟⎟⎟⎠ . 18.2.3. ТестЧоу (Chow-test) на постоянство модели Часто возникает сомнение в том, что для всех наблюдений 1, . . . , N модель неизменна, в частности, что параметры неизменны. Пусть все наблюдения в регрессии разбиты на две группы. В первой из них- N1 наблюдений, а во второй- N2 наблюдений, так что N1 + N2 = N. Без огра-ничения общности можно считать, что сначала идут наблюдения из первой группы, а потом из второй. Базовую регрессию X = Zα + ε можно представить в следую-щем блочном виде: ⎛⎜⎝ X1 X2⎞⎟⎠= ⎡⎢⎣ Z1 Z2⎤⎥⎦α + ⎛⎜⎝ ε1 ε2⎞⎟⎠.
18.2. Тест на существенность ограничения 579 Требуется проверить, действительно ли наблюдения в обеих группах подчиня-ются одной и той же модели x = zα + ε. 1-я форма теста Чоу В качестве альтернативы базовой модели рассмотрим регрессию ⎛⎜⎝ X1 X2⎞⎟⎠= ⎡⎢⎣ Z1 0 0 Z2⎤⎥⎦ ⎛⎜⎝ α1 α2⎞⎟⎠+ ⎛⎜⎝ ε1 ε2⎞⎟⎠. Фактически, это две разные регрессии: X1 = Z1α1 + ε1 и X2 = Z2α2 + ε2, но предполагается, что дисперсия в них одинакова. Пусть e0 -вектор остатков в регрессии на всей выборке, e1 -вектор остат-ков в регрессии с наблюдениями 1, . . . , N1, e2- вектор остатков в регрессии с N1 + 1, . . . , N наблюдениями. Требуется проверить нулевую гипотезу о равен-стве коэффициентов по двум частям выборки: H0 : α1 = α2. Для того чтобы применить здесь тест добавления переменных, обозначим α2 − α1 = δ и подставим α2 = α1 + δ в рассматриваемую модель: X2 = Z2α1 + Z2δ + ε2, Таким образом, можно рассматривать следующую регрессию (для упрощения обозначений пишем α вместо α1): ⎛⎜⎝ X1 X2⎞⎟⎠= ⎡⎢⎣ Z1 0 Z2 Z2⎤⎥⎦ ⎛⎜⎝ αδ⎞⎟⎠+ ⎛⎜⎝ ε1 ε2⎞⎟⎠. (18.5) Мы хотим проверить гипотезу H0 : δ = 0. Если нулевая гипотеза принимается, то это означает, что α1 = α2, т.е. коэф-фициенты постоянны. Заметим, что в регрессии (18.5) с ограничениями остатки окажутся равными e0, а в регрессии (18.5) без ограничений- ⎛⎜⎝ e1 e2⎞⎟⎠. Суммы квадратов остатков равны
580 Глава 18. Классические критерии проверки гипотез e0e0 и e1e1 + e2e2, соответственно. В регрессии с ограничениями оценивается n + 1 параметров, без ограничений- 2 (n + 1). Всего проверяется k = n + 1 ограничений. Используя общую формулу (18.2), получим Fc = (e0e0 − e1e1 − e2e2) /(n+ 1) (e1e1 + e2e2) /(N − 2 (n+ 1)) ∼ Fn+1,N−2(n+1). Для того чтобы применить этот тест, нужно оценить модель по двум частям выборки. Это можно сделать, когда количество наблюдений превышает количество параметров, т.е. N1 n+1 и N2 n+1. Кроме того, если Nj = n+1, то остатки ej = 0 (j = 1, 2). Таким образом, для применимости теста требуется, чтобы хотя бы в одной части количество наблюдений превышало количество параметров. 2-я форма теста Чоу То, что модель в двух частях одна и та же, можно проверить и по-другому. Пусть сначала рассчитывается регрессия по N1 < Nнаблюдениям, а затем по всем N наблюдениям. Если полученные результаты существенно отличаются, то это должно означать, что во второй части модель каким-то образом поменялась. Реализуем эту идею с помощью вспомогательной регрессии, к которой можно применить тест добавления переменных: ⎛⎜⎝ X1 X2⎞⎟⎠= ⎡⎢⎣ Z1 0N1×N2 Z2 IN2 ⎤⎥⎦ ⎛⎜⎝ αδ⎞⎟⎠+ ⎛⎜⎝ ε1 ε2⎞⎟⎠. (18.6) Эта регрессия с ограничением δ = 0 совпадает с регрессией по всем N на-блюдениям по первоначальной модели, и остатки равны e0. Еслиже оценить (18.6,18.5)без ограничений, то, какможно показать, оценки α совпадут с оценками по N1 < N наблюдениям, т.е. по регрессии X1 = Z1α1 + ε1. Остатки будут иметь вид ⎛⎜⎝ e1 0n2⎞⎟⎠.1 1Это связано с тем, что добавочные переменные являются фиктивными переменными, каждая из которых всюду равна нулю, за исключением одного из наблюдений второй части выборки. Такие фиктивные переменные приводят к "обнулению" остатков. Следовательно, задача минимизации суммыквадратов остатков по N наблюдениям здесь сводится к задачеминимизации суммыквадратов остатков по первым N1 наблюдениям.
18.2. Тест на существенность ограничения 581 Действительно, запишем модель в матричном виде. Пусть Z = ⎡⎢⎣ Z1 0 Z2 I⎤⎥⎦, X= ⎛⎜⎝ X1 X2⎞⎟⎠. Тогда оценки коэффициентов модели ⎛⎜⎝ ad⎞⎟⎠удовлетворяют нормальным уравнени-ям: ZZ ⎛⎜⎝ ad⎞⎟⎠= ZX, или ⎛⎜⎝ Z1Z1 + Z2Z2 Z2 Z2 I ⎞⎟⎠ ⎛⎜⎝ ad⎞⎟⎠= ⎛⎜⎝ Z1X1 + Z2X2 X2 ⎞⎟⎠. Получаем следующую систему уравнений для оценок коэффициентов: ⎧⎪⎨ ⎪⎩ Z1Z1a + Z2Z2a + Z2d = Z1X1 + Z2X2, Z2a + d = X2. (18.7) Умножив второе уравнение системы (18.7) слева на Z2, получим Z2Z2a + Z2d = Z2X2. После вычитания из первого уравнения системы (18.7) получим Z1Z1a = Z1X1. Таким образом, оценки коэффициентов a являются оценками МНК по первой части выборки. Далее, остатки второй части равны e2 = X2 − Z2a − d = 0. Последнее равенство следует из второго уравнения системы (18.7).
582 Глава 18. Классические критерии проверки гипотез Суммы квадратов остатков в двух моделях равны e0e0 и e1e1, соответственно. Количество ограничений k = N2. Такимобразом, получаем следующуюстатистику: Fc = (e0e0 − e1e1) /N2 e1e1/(N1 − n − 1) ∼ FN2, N1−n−1. Статистика имеет указанное распределение, если выполнена нулевая гипотеза H0 : δ = 0. Если нулевая гипотеза принимается, это означает, что модель не менялась. Заметим, что в случае, когда наблюдений в одной из частей выборки не хватает, чтобы оценить параметры, либо их столько же, сколько параметров, например, N2 n + 1, второй тест Чоу можно рассматривать как распространение на этот вырожденный случай первого теста Чоу. Второй тест Чоу можно интерпретировать также как тест на точность прогноза. Поскольку Z2a -прогнозы, полученные для второй части выборки на основе оце-нок первой части (a), то из второго уравнения системы (18.7) следует, что оценки d равны ошибкам такого прогноза: d = X2 − Z2a. Таким образом, проверяя гипотезу δ = 0, мы проверяем, насколько точны прогно-зы. Если модель по второй части выборки отличается от модели по первой части, то ошибки прогноза будут большими и мы отклоним нулевую гипотезу. 18.3. Метод максимального правдоподобия в эконометрии 18.3.1. Оценки максимального правдоподобия Методмаксимального правдоподобия-это один из классическихметодов оце-нивания, получивший широкое распространение в эконометрии благодаря своей универсальности и концептуальной простоте. Для получения оценок максимального правдоподобия следует записать функ-цию правдоподобия, а затем максимизировать ее по неизвестным параметрам мо-дели. Предположим, что изучаемая переменная x имеет распределение с плот-ностью fx(x), причем эта плотность зависит от вектора неизвестных параметров θ, что можно записать как fx(x|θ). Тогд ад ля N независимых наблюдений за переменной x, т.е. x1, . . . , xN, функция правдоподобия, по определению, есть
18.3. Метод максимального правдоподобия в эконометрии 583 плотность их совместного распределения, рассматриваемая как функция от θ при данном наборе наблюдений x1, . . . , xN: L(θ) = Ni=1 fx(xi|θ). Если изучаемая переменная имеет дискретное распределение, то fx(x|θ) сле-дует понимать как вероятность, а не как плотность. Наряду с функцией L(θ) из со-ображений удобства рассматривают также ее логарифм, называемый логарифми-ческой функцией правдоподобия. Оценки максимального правдоподобия θ∗ для параметров θ являются, по определению, аргмаксимумом функции правдоподобия (или, что то же самое, лога-рифмической функции правдоподобия). Они являются решением уравнения прав-доподобия: ∂ lnL ∂θ = 0. В более общем случае нельзя считать наблюдения за изучаемой переменной, x1, . . . , xN, независимыми и одинаково распределенными. В этом случае задается закон совместного распределения всех наблюдений, fx(x1, . . . , xN|θ) = fx(x|θ) , и функция правдоподобия для данного вектора наблюдений x полагается рав-ной fx(x|θ). Известно, что оценки максимального правдоподобия обладают свойствами со-стоятельности, асимптотической нормальности и асимптотическойэффективности. Оценку ковариационной матрицы оценок θ∗ можно получить на основе мат-рицы вторых производных (матрицы Гессе) логарифмической функции правдопо-добия: −∂2 lnL(θ∗) ∂θ∂θ−1 . Другая классическая оценка ковариационной матрицы имеет вид (I(θ∗))−1 , где I(θ) = E −∂2 lnL(θ) ∂θ∂θ-так называемая информационная матрица.
584 Глава 18. Классические критерии проверки гипотез 18.3.2. Оценки максимального правдоподобия для модели линейной регрессии Рассмотрим модель линейной регрессии xi = ziα+εi, где вектор коэффициен-тов имеет размерность n + 1, ошибки εi независимы и распределены нормально: εi ∼ N(0, σ2), а факторы zi являются детерминированными. При этом изучаемая переменная тоже имеет нормальное распределение: xi ∼ N(ziα, σ2). Плотность этого распределения равна √ 1 2πσ2 e− 1 2σ2 (xi−ziα)2 . Перемножая плотности для всех наблюдений (с учетом их независимости), получим функцию правдоподобия: L(α, σ) = 1 (2π)N/2 σN e− 1 2σ2 Ni=1 (xi−ziα)2 . Соответствующая логарифмическая функция правдоподобия равна lnL(α; σ) = −N2 ln (2π) − N ln σ − 1 2σ2 Ni=1 (xi − ziα)2, или в матричных обозначениях lnL(α; σ) = −N2 ln (2π) − N ln σ − 1 2σ2 (X − Zα)(X − Zα) . Берем производные: ∂ lnL ∂α = 1 σ2Z(X − Zα) = 0, ∂ lnL ∂σ = −Nσ + 1 σ3 (X − Zα)(X − Zα)= 0. Из первого уравнения получим оценки максимального правдоподобия для ко-эффициентов α: a = ZZ−1 ZX. Видим, что оценки наименьших квадратов и оценки максимального правдопо-добия совпадают. Из второго уравнения, подставляя в него оценки a вместо α, получим оценку дисперсии σ2: s2 = 1 N ee,
18.3. Метод максимального правдоподобия в эконометрии 585 где ee = (X − Za)(X − Za) -сумма квадратов остатков. Оценка максималь-ного правдоподобия для дисперсии ошибки смещена. Несмещенная оценка, ис-пользуемая в МНК, равна ˆs2 = 1 N − n − 1ee. Тем не менее, оценки (a, s) асимптотически несмещены, состоятельны, асимп-тотически эффективны в классе любых оценок (а не только линейных, как при МНК). Чтобы проверить, на самом ли деле мы нашли точку максимума правдоподобия, исследуем матрицу вторых производных: ∂2 lnL ∂α∂α= − 1 σ2ZZ, ∂2 lnL ∂σ2 = Nσ2 − 3 σ4 (X − Zα)(X − Zα) , ∂2 lnL ∂α∂σ = ∂2 lnL ∂σ∂α= − 2 σ3Z(X − Zα) . Таким образом, ∂2 lnL ∂(α; σ)∂(α; σ)= −⎛⎜⎜⎝ 1 σ2ZZ 2 σ3 (X − Zα)Z 2 σ3Z(X − Zα) 3 σ4 (X − Zα)(X − Zα) − Nσ2⎞⎟⎟⎠. Значение матрицы вторых производных в точке оценок (a, s) равно ∂2 lnL ∂(α; σ)∂(α; σ)a,s = − Nee ⎛⎜⎝ ZZ 00 2N⎞⎟⎠. Видно, что матрица вторых производных отрицательно определена, то есть най-дена точка максимума. Это дает оценку ковариационной матрицы оценок (a, s): ee N ⎛⎜⎝ (ZZ)−1 00 1 2N⎞⎟⎠. Таким образом, оценка ковариационной матрицы для a является смещенной (поскольку основана на смещенной оценке дисперсии): Ma = ee N ZZ−1 .
586 Глава 18. Классические критерии проверки гипотез В методе наименьших квадратов в качестве оценки берут Ma = ee N − n − 1 ZZ−1 . При N →∞ эти две оценки сходятся. Метод максимального правдоподобия дает также оценку дисперсии для s: var(s) = ee 2N2 . Рассчитаем также информационную матрицу. Для этого возьмем математиче-ское ожидание от матрицы вторых производных со знаком минус: I = E⎛⎜⎜⎝ 1 σ2ZZ 2 σ3 (X − Zα)Z 2 σ3Z(X − Zα) 3 σ4 (X − Zα)(X − Zα) − Nσ2⎞⎟⎟⎠= ⎛⎜⎜⎝ 1 σ2ZZ 00 2N σ2 ⎞⎟⎟⎠, где мы воспользовались тем, что X−Zα представляет собой вектор ошибок моде-ли ε и выполнено E(ε) = 0, E(εε) = Nσ2. Обращая информационную матрицу в точке (a, s), получим туже оценку ковариационной матрицы, что и раньше. Таким образом, оба метода дают одинаковый результат. Ln L xj j Рис. 18.1 Рассмотрим логарифмическую функцию правдоподобия как функ-цию одного из коэффициентов, αj , при остальных коэффициентах за-фиксированных на уровне оценок максимального правдоподобия, т.е. срез (n + 2)-мерного пространства (см. рис. 18.1). Видим, что оценка aj тем точнее, чем острее пик функции правдоподобия. А степень остроты пика показывает вторая производная (по абсолютному значению). Поэтому математическое ожидание матрицы вторых производных со знаком минус называется информационной матрицей. Эта матрица удовле-творяет естественным требованиям: чем больше имеем информации, тем точнее оценка. Если в логарифмическуюфункциюправдоподобия lnL(α; σ) подставить оцен-ку s2 для σ2, которая найдена из условия ∂ lnL∂σ = 0: s2 = ee N ,
18.3. Метод максимального правдоподобия в эконометрии 587 то получится так называемая концентрированная функция правдоподобия, которая зависит уже только от α: lnLc (α) = −N2 ln (2π) − N2 ln 1 N ee− N2 . Очевидно, что максимизация концентрированной функции правдоподобия эк-вивалентна методу наименьших квадратов (минимизации суммы квадратов остат-ков). 18.3.3. Три классических теста для метода максимального правдоподобия Рассмотрим линейную регрессию с нормальными ошибками. Требуется прове-рить гипотезу о том, что коэффициенты этой регрессии удовлетворяют некоторым линейным ограничениям. Пусть a0 -оценки, полученные методом максималь-ного правдоподобия без учета ограничений, а a1 -оценки, полученные тем же методом с учетом ограничений, и пусть lnL0 -значение логарифмической функ-ции правдоподобия в точке a0, а lnL1 -значение логарифмической функции правдоподобия в точке a1. Статистику для проверки такой гипотезы естественно строить как показатель, измеряющий существенность различий между двумя моде-лями-с ограничениями и без них. Если различия не очень велики (ограничения существенны), то гипотезу о том, что ограничения выполнены, следует принять, а если достаточно велики-то отвергнуть. Рассмотрим три возможных способа измерения этих различий, проиллюстрировав их графически. Ln L0 Ln L1 0 a1 a0 a Ln L Рис. 18.2 Критерий отношения правдоподобия (Likelihood ratio test-LR) основан на различии значений логарифмической функции правдоподобия в точках a0 и a1 (см. рис. 18.2), или, что то же са-мое, на логарифме отношения правдопо-добия, т.е. величине lnL0 − lnL1 = lnL0 L1. Критерий множителей Лагранжа (Lagrange multiplier test-LM) осно-ван на различии тангенса угла наклона касательной к логарифмической функции правдоподобия в точках a0 и a1. По-скольку в точке a0 он равен нулю, то следует рассмотреть, насколько тангенс угла наклона касательной в точке a1 отличен от нуля (см. рис. 18.3).
588 Глава 18. Классические критерии проверки гипотез Ln L0 Ln L1 0 a1 a0 a Ln α L Рис. 18.3 Критерий Вальда (Wald test-W) основан на невязках рассматриваемых ограничений. В точке a1 , по опреде-лению, невязки равны нулю. Таким об-разом, следует рассмотреть, насколь-ко невязки в точке a0 отличны от ну-ля. В случае одного параметра точка a1 однозначно задается ограничения-ми, и невязка в точке a0 при линей-ных ограничениях будет некоторой ли-нейной функцией разности оценок a0 и a1 (см. рис. 18.4). 0 a1 a0 a Ln L Рис. 18.4 Покажем, как соответствующие кри-терии выводятся в рассматриваемом нами случае линейной регрессии с нормальными ошибками, когда требуется проверить ли-нейные ограничения на коэффициенты. (В общем случае построение критериев про-исходит аналогичным образом.) При выво-де критериев нам понадобится следующая лемма (см. Приложение A.3.2). Лемма: Пусть χ -вектор (χ ∈ Rk) случайных величин, подчиненных мно-гомерному нормальному распределению: χ ∼ N 0, σ2Ω, где матрица Ω неособенная. Тогда 1 σ2 χΩ−1χ ∼ χ2k. Доказательство: Так как Ω положительно определена (cм. Приложения A.1.2 и A.1.2), то су-ществует неособенная квадратная матрица C, такая, что Ω−1 = CC. Рассмотрим вектор 1σCχ. Ясно, что E1σCχ= 0, а ковариационная матрица этого вектора равна 1 σ2E CχχC= CΩC= Ik. Такимобразом, вектор 1σCχ состоит из k некоррелированных и, как следствие (по свойству многомерного нормального распределения), независимых случайных
18.3. Метод максимального правдоподобия в эконометрии 589 величин, имеющих стандартное нормальное распределение. Тогда (по определению распределения χ-квадрат) сумма квадратов вектора 1σCχ распределена как χ2k. ТестВальда (W-тест) Для оценки коэффициентов регрессии без ограничений выполнено a0 = ZZ−1 ZX ∼ N α, σ2 ZZ−1. Рассмотрим невязки ограничений Ra0 − r. Чем они больше, тем более прав-доподобно, что ограничения не выполнены. Ясно, что (см. Приложение A.3.2) Ra0 − r ∼ N Rα − r; σ2A, где, как и раньше, используется обозначение A = R(ZZ)−1 R.Матрица A имеет размерность k × k, гд е k -количество ограничений. Пусть выполнена нулевая гипотеза H0: Rα = r. Тогда Ra0 − r ∼ N 0; σ2A. По лемме 1 σ2 (Ra0 − r)A−1 (Ra0 − r) ∼ χ2k. Поскольку известны лишь a0 -оценки без ограничений, то в качестве оценки неизвестной величины σ2 берем 1 N e0e0, гд е e0 = X − Za0-остатки из модели без ограничений. Отсюда получаем статистику Вальда: W = N e0e0 (Ra0 − r)RZZ−1 R−1 (Ra0 − r) . Эта статистика распределена примерно как χ2k. Тогд а, если W <χ2 k,γ, то сле-дует принять H0, что ограничения выполнены. При W >χ2 k,γ ограничения суще-ственны и следует отвергнуть H0. Можно увидеть, что статистика Вальда имеет следующую структуру: W = (Ra0 − r)RMa0R−1 (Ra0 − r) , где Ma0 = e0e0 N (ZZ)−1 -оценка ковариационной матрицыоценок a0.Фактиче-ски это общая формула для статистики Вальда, применимая в случае произвольной модели, а не только линейной регрессии с нормальными ошибками.
590 Глава 18. Классические критерии проверки гипотез Тестотношения правдоподобия (LR-тест) Рассмотрим статистику LR = −2 (lnL1 − lnL0) = −2 ln L1 L0 , называемую ста-тистикой отношения правдоподобия.Здесь L1 и L0 -значения логарифмической функции правдоподобия в точках a0 и a1: lnL0 = −N2 (1 + ln 2π) − N2 ln e0e0 N , lnL1 = −N2 (1 + ln 2π) − N2 ln e1e1 N . Суммы квадратов остатков здесь равны e0e0 = (X − Za0)(X − Za0) и e1e1 = (X − Za1)(X − Za1) = = (X − Za0)(X − Za0) + (Ra0 − r)A−1 (Ra0 − r) . Покажем, что если верна нулевая гипотеза Rα = r, топриближенно выполнено −2 ln(L1/L0) ∼ χ2k. Действительно, −2 ln L1 L0= N ln e1e1 e0e0= N ln 01 + (Ra0 − r)A−1 (Ra0 − r) (X − Za0)(X − Za0) 1. Для натурального логарифма при малых x выполнено ln (1 + x) ≈ x. Рассмот-рим последнюю дробь.При большом количестве наблюдений оценки a0 стремятся к вектору α, для которого выполнено H0 : Rα = r. Отсюда следует, что при боль-шом количестве наблюдений дробь-малая величина, и получаем приближенно LR = −2 ln L1 L0≈ N (Ra0 − r)A−1 (Ra0 − r) (X − Za0)(X − Za0) = W. Таким образом, статистика отношения правдоподобия приближенно равна ста-тистике Вальда, которая приближенно распределена как χ2k. Получили LR-тест: если LR > χ2k,γ, то H0 неверна, ограничения не выполнены, а если LR < χ2k,γ, то наоборот.
18.3. Метод максимального правдоподобия в эконометрии 591 Тестмножителей Лагранжа (LM-тест) Ранее мы получили выражение для множителей Лагранжа, соответствующих ограничению Rα = r: λ = A−1 (Ra0 − r) . Из того, что Ra0 − r ∼ N Rα − r; σ2A, следует, что λ ∼ N A−1(Rα − r); σ2A−1. Отсюда при H0 : Rα = r выполнено λ ∼ N 0; σ2A−1, поэтому в силу леммы имеем 1 σ2 λAλ ∼ χ2k. Поскольку известны только оценки с ограничением, a1, то в качестве оценки σ2 берем 1 N e1e1. Получили статистику LM = N e1e1 λAλ = N e1e1 λRZZ−1 Rλ. Если LM > χ2k,γ, то H0 отвергается, ограничения не выполнены. Если LM < χ2k,γ, то H0 принимается. Вспомним, что из нормальных уравнений для оценок при ограничениях Rλ = Z(X − Za1). В то же время ∂ lnL(a1,:e1e1/N) ∂α = N e1e1Z(X − Za1) - производная логарифмической функции правдоподобия (это функция без учета огра-ничений) по параметрам в точке оценок при ограничениях a1 и s1 =  e1e1 N . Статистика множителей Лагранжа, таким образом, имеет следующую структуру: LM = e1e1 N ∂ lnL(a1,:e1e1/N) ∂α(ZZ)−1 ∂ lnL(a1,:e1e1/N) ∂α = = ∂ lnL(a1,:e1e1/N) ∂αMa0 (a1) ∂ lnL(a1,:e1e1/N) ∂α , где Ma0 (a1) = e1e1 N (ZZ)−1 -оценка ковариационной матрицы оценок a0, вы-численная на основе информации, доступной в точке a1. Это общая формула для статистики множителей Лагранжа, применимая в случае произвольной моде-ли, а не только линейной регрессии с нормальными ошибками. В таком виде тест называется скор-тестом (score test) или тестом Рао.
592 Глава 18. Классические критерии проверки гипотез 18.3.4. Сопоставление классических тестов Величину (Ra0 − r)R(ZZ)−1 R−1 (Ra0 − r), которая фигурирует в фор-мулах для рассматриваемых статистик, можно записать также в виде e1e1 − e0e0. Таким образом, получаем следующие формулы для трех статистик через суммы квадратов остатков: W = N e1e1 − e0e0 e0e0 , LM = N e1e1 − e0e0 e1e1 , LR = N ln e1e1 e0e0. F-статистику для проверки линейных ограничений можно записать аналогич-ным образом: F = N − n − 1 k e1e1 − e0e0 e0e0 . Нетрудно увидеть, что все три статистики можно записать через F-статистику: LR = N ln 1 + k N − n − 1F, W = N N − n − 1kF, LM = N kF + N − n − 1kF. Заметим, что по свойству F-распределения kF в пределе при N → ∞ схо-дится к χ2k, чем можно доказать сходимость распределения всех трех статистик к этому распределению. Так как e1e1 e0e0, то W LM. Следовательно, тест Вальда более жест-кий, он чаще отвергает ограничения. Статистика отношения правдоподобия лежит всегда между W и LM. Чтобы это показать, обозначим x = k N − n − 1F = e1e1 − e0e0 e0e0 . Доказываемое свойство следует из того, что при x > −1 выполнено неравенство x 1 + x ln (1 + x) x.
18.4. Упражнения и задачи 593 18.4. Упражнения и задачи Упражнение 1 В Таблице 18.1 приведены данные о продаже лыж в США: SA-продажа лыж в США, млн. долл., PDI-личный располагаемый доход, млрд. долл. 1.1. Оценить регрессию SA по константе и PDI. Построить и проанализировать автокорреляционную функцию остатков. 1.2. Оценить регрессию с добавлением квартальных сезонных переменных Q1, Q2 , Q3, Q4 . Константу не включать (почему?). Оценить ту же регрессию, заменив Q4 на константу. Построить и проанализировать автокорреляцион-ную функцию остатков в регрессии с сезонными переменными. 1.3. Проверить гипотезу о том, что коэффициенты при сезонных переменных равны одновременно нулю. Есть ли сезонная составляющая в данных? Таблица 18.1. (Источник: Chatterjee, Price, Regression Analysis by Example, 1991, p.138) Квартал SA PDI Квартал SA PDI Квартал SA PDI 1965.1 37.4 118 1968.1 44.2 143 1971.1 52 180 1965.2 31.6 120 1968.2 40.4 147 1971.2 46.2 184 1965.3 34 122 1968.3 38.4 148 1971.3 47.1 187 1965.4 38.1 124 1968.4 45.4 151 1971.4 52.7 189 1966.1 40 126 1969.1 44.9 153 1972.1 52.2 191 1966.2 35 128 1969.2 41.6 156 1972.2 47 193 1966.3 34.9 130 1969.3 44 160 1972.3 47.8 194 1966.4 40.2 132 1969.4 48.1 163 1972.4 52.8 196 1967.1 41.9 133 1970.1 49.7 166 1973.1 54.1 199 1967.2 34.7 135 1970.2 43.9 171 1973.2 49.5 201 1967.3 38.8 138 1970.3 41.6 174 1973.3 49.5 202 1967.4 43.7 140 1970.4 51 175 1973.4 54.3 204
594 Глава 18. Классические критерии проверки гипотез Таблица 18.2. (Источник: M.Pokorny, An Introduction to Econometrics. Basil Blackwell, 1987, p.230) Год X L K D Год X L K D 1965 190.6 565 4.1 0.413 1973 130.2 315 4.2 0.091 1966 177.4 518 4.3 0.118 1974 109.3 300 4.2 5.628 1967 174.9 496 4.3 0.108 1975 127.8 303 4.2 0.056 1968 166.7 446 4.3 0.057 1976 122.2 297 4.3 0.078 1969 153 407 4.3 1.041 1977 120.6 299 4.4 0.097 1970 144.6 382 4.3 1.092 1978 121.7 295 4.6 0.201 1971 147.1 368 4.3 0.065 1979 120.7 288 4.9 0.128 1972 119.5 330 4.3 10.8 1980 128.2 286 5.2 0.166 1.4. Оценить ту же регрессию, считая, что коэффициент при Q1 равен коэффи-циенту при Q4 ("зимние" кварталы) и коэффициент при Q2 равен коэффи-циенту при Q3 ("летние" кварталы). 1.5. Мы предполагали выше, что константа меняется в зависимости от квартала. Теперь предположим, что в коэффициенте приPDI также имеется сезонность. Создать необходимые переменные и включить их в регрессию. Меняется ли коэффициент при PDI в зависимости от квартала? Проверить соответствую-щую гипотезу. Упражнение 2 В Таблице 18.2 приведены данные о добыче угля в Великобритании: X - общая добыча угля (млн. тонн), L -общая занятость в добыче угля (тыс. чел.), K -основные фонды в угледобывающей отрасли (восстановительная стоимость в ценах 1975 г., млн. фунтов), D -потери рабочих дней в угледобывающей от-расли из-за забастовок (млн. дней). 2.1. Оценить уравнение регрессии X = const + bK + cD. На основе графиков остатков по времени и по расчетным значениям сделать выводы относительно гетероскедастичности, автокорреляции и функциональной формы.
18.4. Упражнения и задачи 595 2.2. Провести вручную с помощью соответствующих регрессий тесты: а) на ге-тероскедастичность, б) тест Годфрея на автокорреляцию остатков, в) тест Рамсея на функциональную форму. 2.3. Провести Чоу-тест (тест на постоянство коэффициентов регрессии) с по-мощью умножения на фиктивную переменную. Данные разбить на 2 части: с 1965 по 1972 и с 1973 по 1980 гг. Сделать выводы. 2.4. Провести тест на добавление фактора L. Оценить уравнение регрессии X = const+bK + cD + aL. 2.5. Правильно ли выбранафункциональная форма регрессии?Вслучае, если она выбрана неправильно, попробовать исправить ее путем добавления квадра-тов переменных K и L. Упражнение 3 В Таблице 15.3 на стр. 520 приведены данные о совокупном доходе и потребле-нии в США в 1953-1984 гг. 3.1. Оценить потребительскую функцию (в логарифмах) lnCt = β +α ln Yt + εt. 3.2. Проверить гипотезу α = 1 с помощью: а) теста Вальда, б) преобразованной модели ln Yt = β + (α − 1) ln Yt + εt, в) теста отношения правдоподобия. 3.3. Проверить гипотезу об отсутствии автокорреляции первого порядка. 3.4. Предположим, что ошибка подчинена авторегрессионному процессу первого порядка εt = ρεt−1 + ut. Оценить соответствующую модель. 3.5. Модель с авторегрессией в ошибке можно записать в следующем виде: lnCt = β(1 − ρ) + α ln Yt − αρ ln Yt−1 + ρ lnCt−1 + ut. Эта же нелинейная модель представляется в виде линейной регрессии: lnCt = β+ αln Yt + γln Yt−1 + δlnCt−1 + ut, где α= α, β= β(1 − ρ), γ= −αρ, δ= ρ. Оцените модель как линейную регрессию. 3.6. Для той же нелинейной модели должно выполняться соотношение между ко-эффициентами линейной регрессии: αδ= −γ.Проверить даннуюгипотезу. 3.7. Оценить нелинейную регрессию. Использовать тест отношения правдоподо-бия для проверки той же гипотезы, т.е. αδ= −γ.
596 Глава 18. Классические критерии проверки гипотез Задачи 1. Оценивается функция Кобба-Дугласа (в логарифмическом виде) с ограни-чением однородности первой степени. Запишите матрицы (R и r) ограниче-ний на параметры регрессии. 2. Запишите матрицы (R и r) ограничений на параметры регрессии в случае проверки того, что 1-й и 3-й коэффициенты регрессии X = Zα + ε совпа-дают, где матрица наблюдений Z = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 1 1 3 1 0 2 1 1 3 3 −1 1 0 4 3 1 2 5 2 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . 3. Запишите матрицы (R и r) ограничений на параметры регрессии в случае проверки значимости j-го коэффициента регрессии. 4. Запишите матрицы (R и r) ограничений на параметры регрессии в случае проверки значимости уравнения регрессии в целом. 5. Исходные данные для модели линейной регрессии x = α1z1 +α2z2 +ε неиз-вестны, известно только, что количество наблюдений N = 100, сумма квад-ратов остатков равна 196, ZZ = ⎛⎜⎝ 2 −3 −3 5⎞⎟⎠и ZX = ⎛⎜⎝ 36⎞⎟⎠. а) Используя эту информацию, рассчитайте оценкиМНК. б) Рассчитайте оценкиМНК, учитывая ограничение 2α1−3α2 = 10. Най-дите сумму квадратов остатков. в) Рассчитайте статистику, с помощью которой можно проверить гипотезу 2α1 − 3α2 = 10. Какое распределение имеет эта статистика? 6. Регрессию xi = a0 + a1zi1 + a2zi2 + ei оценили без ограничений на пара-метры и получили остатки (3, −2, −4, 3), а затем оценили с ограничением a1 + a2 = 1 и получили остатки (2, −1, −4, 3). Найдите F-статистику для
18.4. Упражнения и задачи 597 проверки ограничений. С чем ее следует сравнить? В каком случае гипотеза принимается? 7. В регрессии с одним фактором и свободным членом остатки равны (1; −1; 0; −1; 1; 0). Если не включать в регрессию свободный член, то остат-ки равны (1; −2; 1; −1; 1; 0). Проверьте гипотезу о том, что свобод-ный член равен нулю, если 5%-ные границы F-распределения равны F1,1 = 161.5; F1,2 = 18.51; F1,3 = 10.13; F1,4 = 7.71; F1,5 = 6.61. 8. Как с помощью критерия Стьюдента проверить автокорреляцию первого по-рядка в остатках регрессии X = Z1α1 + ε? 9. Пусть в простой линейной регрессии остатки равны (0; 2; −2; 1; −2; 1). После добавления в исходную регрессию лага остатков (0; 0; 2; −2; 1; −2) текущие остатки оказались равны (0; 0; 0; 1; −1; 0). Проверить гипотезу об отсутствии автокорреляции ошибок, если 5%-ные границы F-распреде-ления равны F1,1 = 161.5, F1,2 = 18.51, F1,3 = 10.13, F1,4 = 7.71, F1,5 = 6.61. 10. С помощью какой регрессии можно проверить правильность функциональ-ной формы уравнения регрессии? 11. Уравнение регрессии с двумя факторами и константой оценено по времен-ным рядам длиной 10. Сумма квадратов остатков, полученная в регрессии по всем наблюдениям, равна 100, сумма квадратов остатков, полученная в ре-грессии по первым 5-ти наблюдениям, равна 40, а сумма квадратов остатков, полученная в регрессии по последним 5-ти наблюдениям, равна 20. Найдите F-статистики для гипотезы о постоянстве коэффициентов регрессии. С чем ее следует сравнить? В каком случае гипотеза принимается? 12. В исходной регрессии было 12 наблюдений, 2 фактора и константа. Сумма квадратов остатков была равна 120. Затем выборку разбили на две части, в первой из которых 6 наблюдений. В регрессии по первой части выбор-ки сумма квадратов оказалась равной 25, а по второй части 15. Проверить гипотезу о постоянстве коэффициентов в регрессии, если 5%-ные границы F-распределения равны: F2,1 = 199.5, F2,2 = 19, F2,3 = 9.55, F2,4 = 6.94, F2,5 = 5.79, F2,10 = 4.10, F3,1 = 215.7, F3,2 = 19.16, F3,3 = 9.28, F3,4 = 6.59, F3,5 = 5.41, F3,10 = 3.71. 13. Тест Чоу применили к регрессии, разбив выборку на N1 и N2 наблюдений. Количество факторов равно n. Приведите условия на N1, N2 и n, при которых невозможно использовать первую форму теста Чоу.
598 Глава 18. Классические критерии проверки гипотез 14. Тест Чоу применили к регрессии, разбив выборку на N1 и N2 наблюдений. Количество факторов равно n. Приведите условия на N1, N2 и n, при которых невозможно использовать вторую форму теста Чоу. 15. Для чего можно использовать информационную матрицу в методе макси-мального правдоподобия? 16. В регрессии X = Zα + ε матрица Z = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 1 1 2 1 3 1 4⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , а остатки ε равны (1, 1, 2, −2). Запишите информационную матрицу. 17. В регрессии X = Zα + ε матрица Z = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 1 1 2 1 1 1 2⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , а оценка дисперсии, найденная методом максимального правдоподобия, рав-на 58 . Запишите информационную матрицу. 18. Регрессию x = a0+a1z1+a2z2+e оценили без ограничений на параметры и получили остатки (0, −1, 1, 0), а затем оценили с ограничением a1+a2 = 1 и получили остатки (2, −1, −4, 3). Найдите статистику отношения прав-доподобия для проверки ограничений. С чем ее следует сравнить? В каком случае гипотеза принимается? 19. Регрессию x = a0 + a1z1 + a2z2 + e оценили без ограничений на парамет-ры и получили остатки (2, −1, −4, 3), а затем оценили с ограничением a1 + a2 = 1 и получили остатки (3, −2, −4, 3). Найдите статистику мно-жителя Лагранжа для проверки ограничений. С чем ее следует сравнить? В каком случае гипотеза принимается?
18.4. Упражнения и задачи 599 20. Регрессию x = a0 + a1z1 + a2z2 + e оценили без ограничений на парамет-ры и получили остатки (0, −1, −1, 2), а затем оценили с ограничением a1 + a2 = 1 и получили остатки (1, −2, −2, 3). Найдите статистику Валь-да для проверки ограничений. С чем ее следует сравнить? В каком случае гипотеза принимается? 21. В модели линейной регрессии x = α1z1 + α2z2 + e по некоторому на-бору данных (N = 100 наблюдений) получены следующие оценки МНК: a = (0.4,−0.7). Оценка ковариационной матрицы этих оценок равна Ma = ⎛⎜⎝ 0.01 −0.02 −0.02 0.08 ⎞⎟⎠. Используя общую формулу для статистики Вальда, проверьте следующие гипотезы на уровне 5%: а) H0 : α1 = 0.5, α2 = −0.5, б) H0 : α1 − α2 = 1, в) H0: 3α1 + α2 = 0. 22. В регрессии x = a0 + a1z1 + a2z2 + a3z3 + a4z4 + e по 40 наблюдениям с помощью теста Вальда проверяют гипотезы a1 = a4 + 1, a3 + a2 = 1. Как распределена статистика W (Вальда)? В каком случае гипотеза прини-мается? 23. Методом наименьших квадратов была оценена производственная функция: ln Y = 1.5 + 0.6 (0.3) lnK + 0.45 (0.2) ln L, где Y -объем производства, K -капитал, L -труд. В скобках указаны стандартные ошибки коэффициентов. Ковариация оценок коэффициентов при lnK и lnL равна 0.05. Коэффициент детерминации равен R2 = 0.9. Проверьте следующие гипотезы: а) как труд, так и капитал не влияют на объем производства; б) эластичности объема производства по труду и капиталу совпадают; в) производственнаяфункция характеризуется постоянной отдачей от мас-штаба (сумма эластичностей равна единице). 24. При каких условияхможноприменить критерииВальда (W),отношения прав-доподобия (LR), множителей Лагранжа (LM)?
600 Глава 18. Классические критерии проверки гипотез а) без ограничений; б) известны оценки параметров при ограничениях; в) и те и другие оценки. Для каждого из пунктов (а), (б) и (в) указажите имена тестов, которые можно применить. Рекомендуемая литература 1. Магнус Я.Р., Катышев П.К., Пересецкий А.А. Эконометрика-начальный курс.-М.: "Дело", 2000. (Гл. 3, 11). 2. Себер Дж. Линейный регрессионый анализ.-М.: "Мир", 1980. 3. Статистические методы в экспериментальной физике.-М: Атомиздат, 1976. (Гл. 5, 8-10). 4. Цыплаков А.А. Некоторые эконометрические методы. Метод максималь-ного правдоподобия в эконометрии.-Новосибирск: НГУ, 1997. 5. Baltagi, Badi H. Econometrics, 2nd edition, Springer, 1999. (Ch. 7). 6. Davidson, R., and J.G.MacKinnon. Estimation and Inference in Econometrics. Oxford University Press, 1993. (Ch. 1, 3, 8, 13). 7. Engle R.Wald, Likelihood Ratio and Lagrange Multiplier Tests in Econometrics, in Handbook of Econometrics, vol. II, Amsterdam: North Holland, 1984. 8. GreeneW.H. Econometric Analysis, Prentice-Hall, 2000. (Ch. 4, 7). 9. JudgeG.G., GriffithsW.E., Hill R.C., Luthepohl H., Lee T. Theory and Practice of Econometrics.-New York: John Wiley & Sons, 1985. (Ch. 2, 5). 10. RuudPaulA.AnIntroduction toClassicalEconometricTheory, Oxford University Press, 2000. (Ch. 4, 11, 17).
Глава 19 Байесовская регрессия Прежде чем переходить к регрессии, полезно напомнить, в чем заключается байесовский подход. Он основан на теореме Байеса (см. Приложение A.3.1) p(A|B) = p(B|A)p(A) p(B) , (19.1) которая следует из определения вероятности совместного события: p(A ∩ B) = p(A|B)p(B) = p(B|A)p(A). Пусть теперь Mi, i = 1, . . . , k -гипотезы, модели, теории, суждения об изучаемом предмете; они являются взаимоисключающими и образуют исчерпывающее множество возможных объяснений изучаемого феномена; p(Mi) -априорные (доопытные, субъективные) вероятности, выражаю-щие совокупность априорных (доопытных, субъективных) знаний об изучае-мом предмете; p(Mi) = 1; D -результат наблюдения, опыта; p(D|Mi) -правдоподобия, вероятности того, насколько правдоподобен ре-зультат, если правильна i-я теория изучаемого предмета, считаются извест-ными.
602 Глава 19. Байесовская регрессия Тогда в соответствии с (19.1) записывается следующее соотношение: p(Mi|D) = p(D|Mi)p(Mi) p(D) , (19.2) где p(D) = p(D|Mi)p(Mi), p(Mi|D) -апостериорные (послеопытные) вероятности. Это соотношение показывает, как априорные знания о предмете меняются в результате получения опытных данных, т.е. как накапливаются знания. Пример трансформации представлений преподавателя об уровне знаний студента. M1 -студент знает предмет, M2 -студент не знает предмет. Преподаватель имеет априорные оценки вероятностей этих состояний: p(M1) = 0.2, p(M2) = 0.8. Наблюдение, опыт-в данном случае это экзамен. Результат опыта: D1 -студент сдал экзамен, D2 -студент не сдал экзамен. Правдоподобия преподавателя: p(D1|M1) = 0.9 p(D2|M1) = 0.1 p(D1|M2) = 0.4 p(D2|M1) = 0.6 Пусть студент сдал экзамен.Тогда априорные оценки преподавателя корректируются следующим образом: p (M1|D1) = 0.9 - 0.2 0.9 - 0.2 + 0.4 - 0.8 = 0.36, p (M2|D1) = 0.4 - 0.8 0.9 - 0.2 + 0.4 - 0.8 = 0.64 . Если студент не сдал экзамен, то апостериорные вероятности будут такими: p (M1|D2) = 0.04 , p (M2|D2) = 0.96 .
19.1. Оценка параметров байесовской регрессии 603 19.1. Оценка параметров байесовской регрессии Для уравнения регрессии X = Zα + ε имеются априорные представления об α и σ, которые выражаются плотностью вероятности совместного распределения (α, σ). После эксперимента, результатами которого является выборка в виде вектора X и матрицы Z, эти представления корректируются. Аналогом (19.2) в данном случае выступает следующее выражение: p (α, σ|X,Z) = L(X,Z|α, σ) p (α, σ) p (X,Z) , (19.3) где p (X,Z) = α,σ L(X,Z|α, σ) dα dσ. Поскольку Z не зависит от α и σ , его можно "вынести за скобки": L(X,Z|α, σ) = PN(X|Zα, σ2I)p (Z) , p(X,Z) = p(X|Z)p(Z), и записать (19.3) в следующем виде: p (α, σ|X,Z) = PN(X|Zα, σ2I)p (α, σ) p (X|Z) . Поскольку p(X|Z) не зависит от α и σ , эту формулу можно записать, исполь-зуя знак %, который выражает отношение "пропорционально", "равно с точно-стью до константы": p (α, σ|X,Z) ∝ PN(X|Zα, σ2I)p (α, σ) . (19.4) Пусть выполнены все гипотезы основной модели линейной регрессии, включая гипотезу о нормальности. Тогда PN(X|Zα, σ2I) ∝ σ−Ne− 1 2σ2 (X−Zα)(X−Zα) = = σ−Ne− 1 2σ2 [ee+(α−a)ZZ(α−a)] ∝ σ−Ne−12 (α−a)Ω−1 a (α−a) , где a = (ZZ)−1ZX -МНК-оценка α (см. 7.13), Ωa = σ2(ZZ)−1 -матрица ковариации (см. 7.29).
604 Глава 19. Байесовская регрессия Действительно, (X − Zα)(X − Zα) = (X − Za ←−−−→ e −Z(α − a))(X − Za − Z(α − a)) = = ee − 2eZ(α − a) ←−−−−−−→ =0 +(α − a)ZZ(α − a), т.к. e и Z ортогональны (см. 7.18). Теперь предполагается, что σ известна. Тогда PN(X|Zα, σ2I) ∝ e−12 (α−a)Ω−1 a (α−a) , а соотношение (19.4) записывается в более простой форме: p (α|X,Z) ∝ PN(X|Zα, σ2I)p(α). Пусть α априорно распределен нормально с математическим ожиданием ˆα и ковариацией Ω: p (α) ∝ e−12 α−α Ω−1α−α . Тогда p (α|X,Z) ∝ e−12 α−α Ω−1α−α +(α−a)Ω−1 a (α−a). (19.5) Утверждается, что α апостериорно распределен также нормально с математи-ческим ожиданием ¯a = ¯Ω(Ω−1 a a + Ω−1 ˆα) (19.6) и ковариацией ¯Ω = Ω−1 a + Ω−1−1 , (19.7) т.е. p(α|X,Z) ∝ e−12 [(α−¯a)¯Ω−1(α−¯a)]. (19.8) Для доказательства этого утверждения необходимо и достаточно показать, что раз-ность показателей экспонент в (19.5) и (19.8) не зависит от α. Вводятся новые обозначения: x = α − a; y = α − ˆα; A = Ω−1 a ; B = Ω−1.
19.1. Оценка параметров байесовской регрессии 605 В этих обозначениях показатель степени в (19.5) записывается следующим образом (множитель −12 отбрасывается):xAx + yBy. (19.9) В этих обозначениях ¯Ω = (A + B)−1, ¯a = (A + B)−1 (A(α − x) + B (α − y)) = = (A + B)−1 ((A + B) α − (Ax + By)) = α − (A + B)−1 (Ax + By) и, следовательно, показатель степени в (19.8) выглядит так (множитель −12 также отбрасывается): (Ax − By)(A + B)−1 (Ax − By) . (19.10) Искомая разность (19.9) и (19.10) записывается следующим образом: (1) − (2) − (3) + (4), где (1) = A − A(A + B)−1 A, (2) = A(A + B)−1 B, (3) = B (A + B)−1 A, (4) = B − B(A + B)−1B. Легко показать, что все эти матрицы одинаковы и равны некоторой матрице C: (1) = A(A + B)−1 AA−1 (A + B)A−1A − I= A(A + B)−1B = (2), (2) = A7B A−1 + B−1A8−1B = AA−1 A−1 + B−1−1 BB−1 = = A−1 + B−1−1 = C, (3) = B7AA−1 + B−1B8−1A = BB−1 A−1 + B−1A−1A = C = (2), (4) = B(A + B)−1BB−1(A + B)B−1B − I= B(A + B)−1A = (3), и, следовательно, искомая разность представима в следующей форме: xCx − xCy − yCx + yCy = (x − y)C (x − y) = α− aC α− a. Что и требовалось доказать.
606 Глава 19. Байесовская регрессия Как видно из (19.5, 19.6), апостериорная ковариация (¯Ω) является результатом гармонического сложения опытной (Ωa) и априорной (Ω ) ковариаций, апосте-риорные оценки регрессии (¯a)-средневзвешенными (матричными) опытных (a) и априорных (ˆα) оценок. Если априорные оценки имеют невысокую точность, и Ω велика, то влияние их на апостериорные оценки невелико, и последние определя-ются в большой степени опытными оценками. В предельном случае, когда Ω→∞, т.е. априорная информация совершенно не надежна, ¯a → a, ¯Ω→ Ωa. 19.2. Объединение двух выборок В действительности априорная информация может быть также опытной, но по-лученной в предшествующем опыте. Тогда формулы, полученные в предыдущем пункте показывают, как информация нового опыта-по новой выборке-кор-ректирует оценки, полученные в предыдущем опыте-по старой выборке. В дан-ном пункте показывается, что в результате применения этих формул получаемая апостериорная оценка в точности равна оценке, которую можно получить по объ-единенной выборке, включающей старую и новую. Пусть имеется две выборки: старая- Z1, X1: X1 = Z1α1 + ε1, и новая- Z2, X2: X2 = Z2α2 + ε2. Считается, что σ1 и σ2 известны (как и в предыдущем пункте). Даются оценки параметров по этим двум выборкам: a1 = (Z1Z1)−1 Z1X1, Ωa1 = σ21 (Z1Z1)−1 , a2 = (Z2Z2)−1 Z2X2, Ωa2 = σ22 (Z2Z2)−1 . В предыдущем пункте первой выборке соответствовала априорная оценка, вто-рой-опытная. Теперь дается оценка параметров по объединенной выборке. При этом наблю-дения должны быть приведены к одинаковой дисперсии: ⎡⎢⎣ X1σ1 X2σ2⎤⎥⎦= ⎡⎢⎣ Z1σ1 Z2σ2⎤⎥⎦α + ⎡⎢⎣ ε1σ1 ε2σ2⎤⎥⎦. В этой объединенной регрессии остатки имеют дисперсию, равную единице.
19.3. Упражнения и задачи 607 Оценки параметров рассчитываются следующим образом: a = ⎡⎢⎣Z1σ1 Z2σ2 ⎡⎢⎣ Z1σ Z2σ2⎤⎥⎦ ⎤⎥⎦ −1Z1σ1 Z2σ2 ⎡⎢⎣ X1σ X2σ2⎤⎥⎦= = 1 σ21 Z1Z1 + 1 σ22 Z2Z2−1 1 σ21 Z1X1 + 1 σ22 Z2X2= = Ω−1 1 +Ω−1 2 −1 1 σ21 (Z1Z1)(Z1Z1)−1Z1X1 + 1 σ22 (Z2Z2)(Z2Z2)−1Z2X2= = Ω−1 1 + Ω−1 2 −1 Ω−1 1 a1 + Ω−1 2 a2. Ковариационная матрица (учитывая, что σ2 = 1): Ω = Ω−1 1 + Ω−1 2 −1 . Таким образом, оценки по объединенной выборке в терминах предыдущего пункта являются апостериорными. 19.3. Упражнения и задачи Упражнение 1 По данным таблицы 19.1: 1.1. Оцените регрессию X по Z и константе, учитывая априорную информа-цию, что математические ожидания всех коэффициентов регрессии равны 2, а их ковариационная матрица-единичная. Считать, что дисперсия ошибки равна 2. 1.2. Разделите выборку на две части. Одна часть- 20 первых наблюдений, другая часть- 20 остальных наблюдений. Считать, что дисперсия ошиб-ки в первой части равна 1, а во второй части- 4. а) Оцените обычную регрессию, воспользовавшись первой частью выбор-ки. Найдите матрицу ковариаций полученных оценок. б) Используя информацию,полученнуюнашаге (а), как априорнуюинфор-мацию о математическом ожидании и ковариационной матрице коэф-фициентов, оцените байесовскую регрессию для второй части выборки.
608 Глава 19. Байесовская регрессия Таблица 19.1 № X Z № X Z № X Z № X Z 1 6.7 2.2 11 2.4 1.2 21 4.8 1.8 31 -1.4 1.7 2 5.5 1.8 12 5.8 0.8 22 3.3 0.8 32 -0.9 2 3 4.8 1.5 13 5.7 2.5 23 5.2 2.5 33 4.2 0.3 4 3 0.3 14 -0.9 1.7 24 5.4 2.1 34 -1.4 2 5 4.9 1.9 15 9.3 2.7 25 4.5 2.8 35 -2.6 1.3 6 2.8 0.7 16 3 2.2 26 3.8 1 36 3.1 0.1 7 2.7 0.8 17 -2.9 2.8 27 3.9 1.4 37 2.5 1.3 8 7 2.1 18 -1.5 1.8 28 6.4 2.4 38 -0.8 2.4 9 5.8 1.4 19 1.8 0.7 29 2.7 0.8 39 1.7 0.7 10 6.3 2.3 20 8.3 2.9 30 4.2 0.1 40 -0.1 1.6 в) Оцените регрессию, используя все наблюдения. Регрессия должна быть взвешенной, т.е. наблюдения каждой из частей нужно разделить на ко-рень из соответствующей дисперсии. Найдите ковариационную матрицу оценок. Сравните с результатом, полученным на шаге (б). Совпадают ли коэффициенты и ковариационные матрицы? Задачи 1. Чем отличается байесовская регрессия от обычной регрессии с точки зрения информации о коэффициентах? Приведите формулы для оценки параметров по этим двум регрессиям. 2. Налоговая инспекция считает, что предприятия в среднем недоплачивают налог на прибыль в 80% случаев. Вероятность того, что в ходе проверки некоторого предприятия будет выявлено такое нарушение, равна 40% д ля предприятия, которое недоплачивает налог, и 10%для предприятия, которое полностью выплачивает налог (ошибочно). Вычислите апостериорную веро-ятность того, что данное предприятие недоплачивает налог на прибыль, если в ходе проверки не было выявлено нарушений. 3. Студент может либо знать, либо не знать предмет и либо сдать, либо не сдать экзамен по этому предмету. Вероятность того, что студент знает предмет
19.3. Упражнения и задачи 609 равна 0.3. Если студент знает предмет, то вероятность того, что он сдаст экзамен, равна 0.9, а если не знает, то 0.6. Какова вероятность, что студент не знает предмет, если он сдал экзамен? 4. Предположим, что исследователь исходит из априорной информации, что коэффициенты регрессии распределены нормально с некоторым математи-ческим ожиданием и ковариационной матрицей, а дисперсия ошибки равна некоторой известной величине. Исследователь получил какие-то данные и вычислил по ним апостериорное распределение. Затем он получил дополни-тельные данные и использовал прежнее апостериорное распределение как априорное. Можно ли утверждать, что новое апостериорное распределение будет нормальным? Ответ обоснуйте. 5. Случайная величина ξ имеет нормальное распределение с математическим ожиданием μ и дисперсией 16. Априорно известно, что μ имеет распреде-ление N(2, 9). Выборочное среднее по выборке длиной N равно 1. Найдите апостериорное распределение μ в зависимости от N. 6. Чему равна апостериорная оценка параметра, если его априорная оценка имеет нормальное распределение с математическим ожиданием 2 и диспер-сией 0.25, а выборочная оценка равна 8 по выборке длиной 10? 7. Априорная оценка параметра имеет нормальное распределение с матема-тическим ожиданием 2 и дисперсией 0.5, а выборочная оценка по выборке длиной 20 равна 2. Запишите плотность распределения апостериорных оце-нок. 8. Оценка параметра по первой части выборки равна 0 при дисперсии оценки 1, а по второй части выборки она равна 1 при дисперсии 2. Найдите оценку параметра по всей выборке. 9. Оценки регрессии по первой выборке совпадают с оценками по объединению двух выборок. Что можно сказать об оценках по второй выборке? Докажите свое утверждение. Рекомендуемая литература 1. Зельнер А. Байесовские методы в эконометрии.-М.: "Статистика", 1980. (Гл. 2, 3). 2. Лимер Э. Cатистический анализ неэксперементальных данных.-М.: "Фи-нансы и статистика", 1983.
610 Глава 19. Байесовская регрессия 3. Справочник по прикладной статистике. В 2-х т. Т 2. /Под ред. Э. Ллойда, У. Ледермана.-М.: "Финансы и статистика", 1990. (Гл. 15). 4. JudgeG.G., GriffithsW.E., Hill R.C., Luthepohl H., Lee T. Theory and Practice of Econometrics.-New York: John Wiley & Sons, 1985. (Ch. 4).
Глава 20 Дисперсионный анализ Вэтой главе продолжается рассмотрение темы, начатой в пункте 4.3. Здесь ана-лизируются модели дисперсионного анализа в общем виде и доказываются некото-рые из сделанных ранее утверждений. Как и прежде, исходная совокупность xi, i = 1, . . . , N сгруппирована по n факторам; j-й фактор может находиться на одном из kj уровней. Регрессионная модель дисперсионного анализа общего вида получается исключением из модели регрессии сфиктивными переменными, полученной в конце пункта 9.1, "обычных" регрессоров: X = G J=0 / ZJ / βJ + ε, (20.1) где / ZJ = ¯⊗ j∈J / Zj (матрица / Zj имеет размерность N × kj, и в ее ij-м столбце единицы стоят в строках тех наблюдений, в которых j-й фактор находится на ij-м уровне, остальные элементы равны 0), или, как это следует из структуры / Z и / β, представленной в пункте 9.1, в покомпонентной записи: xI, iI = β0 + G J=1 βJI(J) + εI,iI , (20.2) где I -мультииндекс конечной группы, I = I1, . . . , IK (см. обозначения в п. 1.9); iI -линейный индекс элемента в конечной группе, iI = 1, . . . , NI , NI - численность конечной группы;
612 Глава 20. Дисперсионный анализ βJI(J) (по сравнению с обозначениями, используемыми в п. 4.3, добавлен верх-ний индекс J, необходимый в данной главе для более точной идентификации пара-метра)-параметр эффекта сочетания (совместного влияния)факторов J на дан-ный элемент совокупности (на значение изучаемой переменной в данном наблюде-нии). Так, например, если n = 3, I = {2, 3, 1}, J = {1, 3}, то βJI(J) = β1,3 2,1 . В пункте 9.1 отмечено, что в модели (20.1) на регрессорах существует много линейных зависимостей и поэтому непосредственно оценить ее нельзя. Для исклю-чения линейных зависимостей регрессоров проводится следующее преобразова-ние. Предполагая, что суммы компонент вектора / βJ по всем значениям каждого элемента нижнего мультииндекса I(J) равны нулю (в принятых ниже обозначени-ях: / ZjJ/ bJ = 0 для всех j ∈ J), переходят к вектору βJ путем исключения из / βJ всех тех его компонент, для которых хотя бы один элемент нижнего мультииндекса равен единице (благодаря сделанному предположению их всегда можно восстано-вить, поскольку они линейно выражаются через оставшиеся компоненты). Теперь модель можно записать в форме без линейных зависимостей регрессоров: X = G J=0ZJβJ + ε, (20.3) где ZJ = / ZJCJ, а CJ = ⊗ j∈JCj , матрица Cj имеет следующую структуру: ⎡⎢⎣ −1kj−1 Ikj−1 ⎤⎥⎦. При этом, как и для модели (20.1), остается справедливым соотношение ZJ = ¯⊗ j∈JZj . Эквивалентность моделей (20.1) и (20.3) очевидна, т.к. /βJ = CJβJ . В этой главе сначала рассматривается частный случай, когда численности всех конечных групп NI равны единице, т.е. для каждого сочетания уровней факторов имеется строго одно наблюдение. 20.1. Дисперсионный анализ без повторений В этом случае N = K = Gkj = nj=1 kj , регрессионные модели (20.1) и (20.3) записываются без случайной ошибки, т.к. изучаемая переменная в точности раз-
20.1. Дисперсионный анализ без повторений 613 лагается по эффектам всех возможных взаимодействий факторов (здесь и далее модели записываются в оценках параметров, т.е. β меняются на b): X = G J=0 / ZJ/ bJ , (20.4) X = G J=0ZJ bJ , (20.5) а модель в покомпонентном представлении (20.2) еще и без линейного внутригруп-пового индекса: xI = b0 + G J=1 bJI(J). (20.6) Модель (20.5) можно переписать более компактно: X = Zb. (20.7) Поскольку матрицы ZJ имеют размерности N ∗ KJ−(KJ−= J(kj − 1), K0−= 1), а GJ=0KJ−= K = N (как это было показано в п. 4.3), то матрица Z квадратна, и b = Z−1X. Но для получения общих результатов, имеющих значе-ние и для частных моделей, в которых эффекты высоких порядков принимаются за случайную ошибку, используется техника регрессионного анализа: b = M−1m = ( 1 N ZZ)−1 1N ZX. В этом параграфе сделанные утверждения будут иллюстрироваться примером, в котором n = 2, k1 = k2 = 2 и модели (20.4) и (20.5) записываются следующим образом: ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x11 x12 x21 x22⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1111⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ b0 + ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 0 1 0 0 1 0 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎡⎢⎣ b11 b12 ⎤⎥⎦+ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 0 0 1 1 0 0 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎡⎢⎣ b21 b22 ⎤⎥⎦+ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ b12 11 b12 12 b12 21 b12 22⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ ,
614 Глава 20. Дисперсионный анализ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x11 x12 x21 x22⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1111⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ b0 + ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ −1 −111⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ b12+ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ −11 −11⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ b22+ ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 −1 −11⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ b12 22. Каждая из матриц / ZJ является прямымпроизведениемрядаматриц и векторов: / ZJ = ⊗G⎧⎪⎨ ⎪⎩ Ikj , если j ∈ J 1kj , если j /∈ J ⎫⎪⎬ ⎪⎭ . В этом легко убедиться, рассуждая по индукции. Так, в рассматриваемом при-мере: / Z0 = ⎡⎢⎣ 11⎤⎥⎦⊗ ⎡⎢⎣ 11⎤⎥⎦, / Z1 = ⎡⎢⎣ 1 0 0 1⎤⎥⎦⊗ ⎡⎢⎣ 11⎤⎥⎦, / Z2 = ⎡⎢⎣ 11⎤⎥⎦⊗ ⎡⎢⎣ 1 0 0 1⎤⎥⎦, / Z12 = ⎡⎢⎣ 1 0 0 1⎤⎥⎦⊗ ⎡⎢⎣ 1 0 0 1⎤⎥⎦. Матрицы CJ можно представить следующим образом: CJ = ⊗J Cj = ⊗G⎧⎪⎨ ⎪⎩ Cj , если j ∈ J 1, если j /∈ J ⎫⎪⎬ ⎪⎭ . Тогда, используя свойство коммутативности прямого и "обычного" умножения матриц (см. п. 9.1), можно показать следующее: ZJ = / ZJCJ = ⊗G⎧⎪⎨ ⎪⎩ Ikj , если j ∈ J 1kj , если j /∈ J ⎫⎪⎬ ⎪⎭ ⎧⎪⎨ ⎪⎩ Cj , если j ∈ J 1, если j /∈ J ⎫⎪⎬ ⎪⎭ = = ⊗G⎧⎪⎨ ⎪⎩ Cj , если j ∈ J 1kj , если j /∈ J ⎫⎪⎬ ⎪⎭ . (20.8)
20.1. Дисперсионный анализ без повторений 615 Теперь можно уточнить структуру матрицы M. Она состоит из блоков M ¯ JJ = 1 N Z ¯ JZJ, и все внедиагональные блоки (при ¯ J = J), благодаря (20.8), равны 0. Действительно, M ¯ JJ = 1 N ⊗G⎧⎪⎨ ⎪⎩ Cj, если j ∈ ¯ J 1kj , если j /∈ ¯ J ⎫⎪⎬ ⎪⎭ ⎧⎪⎨ ⎪⎩ Cj , если j ∈ J 1kj , если j /∈ J ⎫⎪⎬ ⎪⎭ и, если j ∈ ¯ J, /∈ J, то в ряду прямых произведений матриц возникает матрица (точнее, вектор-столбец) Cj1kj; если j /∈ ¯ J, ∈ J, то появляется матрица (вектор-строка) 1kjCj . И та, и другая матрица (вектор-столбец или вектор-строка) по построению матриц Cj равны нулю. Следовательно, M ¯ JJ = 0 при ¯ J = J. Для диагональных блоков выполняются следующие соотношения: MJJ = MJ = 1 N G−J kj⊗J CjCj = 1 KJ ⊗J CjCj = ⊗JMj , где Mj = 1 kj CjCj = 1 kj (1kj−11kj−1 + Ikj−1). В рассматриваемом примере M = I4. Вектор m состоит из блоков mJ : mJ = 1 N ZJX = 1 N CJ/ ZJX = 1 KJ CJXJ , где XJ = KJ N / ZJX -вектор-столбец средних по сочетаниям значений факто-ров J. Его компоненты в пункте 4.3 обозначались xI(J) ( xJI(J) -добавлен верх-ний индекс J -является средним значением x по тем наблюдениям, в которых 1-й фактор из множества J находится на ij1-м уровне, 2-й-на ij2-м уровне и т.д.); X0 = ¯x, XG = X. Это следует из структуры матрицы / ZJ. После решения системы нормальных уравнений mJ = MJ bJ, J = 1, . . . , G и перехода к "полным" векторам параметров эффектов получается следующее: / bJ = CJ(CJCJ )−1CJXJ = BJXJ = ⊗J BjXJ , где Bj = Cj(CjCj)−1Cj= Ikj − 1 kj 1kj ( 1kj = 1kj 1kj ), B0 = 1.
616 Глава 20. Дисперсионный анализ В рассматриваемом примере B0 = 1, B1 = B2 = 12⎡⎢⎣ 1 −1 −1 1⎤⎥⎦, B12 = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 1 −1 −1 1 −1 1 1 −1 −1 1 1 −1 1 −1 −1 1⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . В силу блочной диагональности матрицы B, параметры разных эффектов / bJ (разных по J) не зависят друг от друга, и исключение из уравнения некоторых из них не повлияет на значения параметров оставшихся эффектов. Кроме того, это доказывает справедливость приведенного в пункте 4.3 дисперсионного тождества (4.41). Действительно, воспользовавшись одной из формул (6.18) для объясненной дисперсии, которая в данном случае равна полной дисперсии, можно получить следующее: s2 = G J=1 bJMJ bJ = G J=1 1 KJ bJCJCJbJ = G J=1 1 KJ/ bJ/ bJ = G J=1 s2J, т.е. то, что и требуется. Введенное в пункте 4.3 рекуррентное правило расчета параметров эффектов, когда параметры более младших эффектов рассчитываются по значениям парамет-ров более старших эффектов, действует, поскольку наряду с соотношениями (20.4) и (20.6) выполняются аналогичные соотношения для всех средних: XJ = 0, ¯ J∈J / Z ¯ JJ/ b¯ J , (20.9) где суммирование ведется от нуля и по всем подмножествам J (J¯⊂ J), а / Z ¯ JJ - матрица фиктивных переменных для сочетания факторов ¯ J в модели, для которой полным набором факторов является J, т.е. / Z ¯ JJ = ⊗J ⎧⎪⎨ ⎪⎩ Ikj , если j ∈ ¯ J 1kj , если j /∈ ¯ J ⎫⎪⎬ ⎪⎭ XG = X, / ZJG = / ZJ, xJI(J) = b0 +¯ J∈J b ¯ JI( ¯ J). (20.10)
20.1. Дисперсионный анализ без повторений 617 Для доказательства этого факта обе части соотношения (20.5) умножаются слева на KJ N / ZJ(текущим множеством в сумме становится ¯ J): KJ N / ZJX = G ¯ J=0 KJ N / ZJZ ¯ Jb ¯ J , (20.11) и рассматривается произведение / ZJZ ¯ J из правой части полученного соотноше-ния, которое представляется следующим образом: ⊗G⎧⎪⎨ ⎪⎩ Ikj , если j ∈ J 1kj , если j /∈ J ⎫⎪⎬ ⎪⎭ ⎧⎪⎨ ⎪⎩ Cj , если j ∈ ¯ J 1kj , если j /∈ ¯ J ⎫⎪⎬ ⎪⎭ . (20.12) Возможны четыре случая. 1) j /∈ J, j ∈ ¯ J, тогда в этом произведении возникает сомножитель 1kjCj , который равен нулю, т.е. в правой части соотношения (20.11) остаются только такие слагаемые, для которых ¯ J ∈ J. 2) j /∈ J, j /∈¯ J, тогда возникает сомножитель kj , и, следовательно, каждое слагаемое в правой части (20.11) получает сомножитель N KJ , который сокраща-ется с уже имеющимся сомножителем KJ N . 3) j ∈ J, j ∈ ¯ J, тогда возникает сомножитель Cj . 4) j ∈ J, j /∈¯ J, тогда возникает сомножитель 1kj . Таким образом, рассматриваемое произведение в точности равно Z ¯ JJ. По-скольку левая часть соотношения есть XJ по определению, доказательство за-вершено. Соотношение (20.9) дает правило расчета / bJ , если все параметры более стар-ших эффектов известны. При J = 0 это соотношение означает X0 = ¯x = b0. Далее последовательно рассчитываются параметры все более младших эффектов. Техника применения F-критерия для проверки степени значимости отдельных факторов и их сочетаний приведена в пункте 4.3. Здесь важно отметить, что она применима только в рамках гипотезы о нормальности распределения x.
618 Глава 20. Дисперсионный анализ 20.2. Дисперсионный анализ с повторениями Переходя к более общему и более сложному случаю модели дисперсионного анализа с повторениями (20.1), полезно воспользоваться следующим подходом. Если в модели регрессионного анализа X = Zα + ε несколько строк матрицы Z одинаковы, то можно перейти к сокращенной моде-ли, в которой из всех этих строк оставлена одна, а в качестве соответствующей компоненты вектора X взято среднее по этим наблюдениям с одинаковыми зна-чениями независимых факторов. Это агрегированное наблюдение в соответствии с требованием ОМНК должно быть взято с весом 'Ng, гд еNg -количество одинаковых строк в исходной модели, поскольку, как известно, дисперсия средней ошибки в этом наблюдении в Ng раз меньше дисперсии исходных ошибок. Зна-чения оценок параметров в исходной и сокращенной моделях будут одинаковыми, но полная и остаточная суммы квадратов в исходной модели будут больше, чем в сокращенной, на сумму квадратов отклонений переменных x по исключенным наблюдениям от своей средней. При доказательстве этого утверждения считается, что одинаковы первые N1 строк в матрице Z: ⎡⎢⎣ x1 X⎤⎥⎦= ⎡⎢⎣ 1N1 ⊗ z1 Z ⎤⎥⎦a + ⎡⎢⎣ e1 e ⎤⎥⎦. Система нормальных уравнений для оценки a записывается следующим обра-зом: 1N1 ⊗ z1 Z⎡⎢⎣ x1 X⎤⎥⎦= 1N1 ⊗ z1 Z⎡⎢⎣ 1N1 ⊗ z1 Z ⎤⎥⎦a или, после умножения векторов и матриц, 1N1 ⊗ z1 x1 + ZX = (1N1 ⊗ z1 1N1 ⊗ z1 + ZZ)a1N1⊗z1 x1=1N1⊗z1 x1⊗1 ⇒ ⇒ N1z1 ¯x1 + ZX = (N1z1z1 + ZZ)a. Сокращенная модель записывается следующим образом: ⎡⎢⎣ 'N1¯x1 X ⎤⎥⎦= ⎡⎢⎣ 'N1z1 Z ⎤⎥⎦a + ⎡⎢⎣ 'N1¯e1 e ⎤⎥⎦.
20.2. Дисперсионный анализ с повторениями 619 Видно, что система нормальных уравнений для оценки параметров этой моде-ли в точности совпадает с системой нормальных уравнений для исходной модели, т.е. оценки параметров в исходной и сокращенной моделях одинаковы. Остаточная сумма квадратов в исходной модели равна e1e1 + ee, (20.13) в сокращенной модели- N1¯e21+ ee. (20.14) Пусть первые N1 наблюдений в исходной модели имеют нижний индекс 1i, гд е i = 1, . . . , N1. Тогд а e1i = x1i − z1a = ¯x1 + x1i − ¯x1 − z1a = ¯e1 + (x1i − ¯x1) и e1e1 =e2 1i = =(¯e1 − (x1i − ¯x1))2 = N1¯e21+ 2¯e1(x1i − ¯x1) ←−−−−−−−−−−−→ =0 +(x1i − ¯x1)2. Сравнение (20.13) и (20.14) с учетом полученного результата завершает дока-зательство. В исходной модели (20.1) строки матрицы Z, относящиеся к одной конечной группе, одинаковы, что позволяет в конечном счете перейти к сокращенной модели, существенно меньшей размерности. В исходной модели N = IKI=I1 NI, и пусть xI, s2I-средняя и дисперсия в I-й конечной группе, s2e= 1 N NIs2I-внутригрупповая дисперсия, ¯x = 1 N NIxI -общая средняя, s2q= 1 N NI (xI − ¯x)2 -общая межгрупповая дисперсия. Еще в пункте 4.3 было доказано, что s2 = s2e+ s2q. На основании этого тождества, учитывая, что количество степеней свободы внутригрупповой дисперсии равно N −K −1, а межгрупповой- K, можно про-верить статистическую гипотезу о значимости влияния всех факторов сразу на изу-чаемую переменную. Но в данном случае можно провести более детальный анализ
620 Глава 20. Дисперсионный анализ влияния отдельных факторов и их сочетаний, аналогичный тому, который прово-дился в случае модели без повторений. В таком анализе используется сокращенная модель, дающая (как это было показано выше) такие же оценки параметров ре-грессии, что и исходная модель, но представляющая не всю дисперсию, а только межгрупповую: ' / NXG = '/ NG J=0 / ZJ/ bJ = '/ NZJ bJ , (20.15) где XG -вектор средних по конечным группам xI , / N-диагональная матрица численностей конечных групп NI . Эта модель отличается от моделей (20.4) и (20.5) только наличием матричного множителя :/ N. Но это отличие принципиальное. Оно влечет потерю всех тех "хороших" свойств, которыми обладала модель без повторений. В частности, мат-рица M в общем случае перестает быть блочно-диагональной, эффекты разных сочетаний факторов становятся зависимыми, а дисперсионное тождество теряет простую структуру. С моделью (20.15) можно работать как с обычной регрессионной моделью, используя известные критерии проверки разных статистических гипотез (понимая при этом, что результаты проверки будут неоднозначны, в силу взаимозависимо-стей регрессоров). Но следует иметь в виду, что оценки параметров в этой модели смещены (что, впрочем, не влияет на результаты проверки гипотез). В частно-сти, b0 = ¯x. Для того чтобыисключить смещенность оценок, необходимо правильно строить матрицы C, используемые при устранении линейных зависимостей в матрице / Z. Это связано с тем, что теперь должны равняться нулю не простые, а взвешенные суммы компонент векторов /βJ по каждому элементу нижнего мультииндекса I(J). В частности, если Nj ij -численность группы, в которой j-й фактор находится на ij-м уровне, то Cj = ⎡⎢⎢⎣ − 1 Nj1 ( Nj 2 - - - Nj kj ) Ikj−1 ⎤⎥⎥⎦ (понятно, что когда численности всех конечных групп равны единице, эта матрица приобретает обычную структуру). Можно показать, что специальный выбор структуры матриц CJ может обес-печить максимальную "разреженность" матрицы M, т.е. обеспечить равенство нулю блоков M0G(G = 0), MJ¯J (J¯ ⊂ J). Работая со структурой матриц CJ , можно обнаружить частный случай, когда модель с повторениями обладает теми
20.3. Упражнения и задачи 621 же свойствами, что и модель без повторений. Этот случай имеет место, если каж-дый последующий (более младший) фактор делит все полученные ранее группы в одинаковой пропорции. Однако усилия, которые необходимы для доказательства этих фактов, далеко не соответствуют их практической значимости. Так, вряд ли можно ожидать, что ряд групп, имеющих разную численность, можно разбить на подгруппы в одинаковой пропорции-хотя быв силу целочисленности образуемых подгрупп. В принципе, с моделью межгрупповой дисперсии (20.15) можно работать и без сомножителя :/ N, т.е. в рамках "хороших" свойств модели без повторе-ний. Для этого достаточно предположить, что исходная модель (20.1) неоднородна по дисперсии ошибок в разных наблюдениях. А именно: считать, что дисперсия ошибки наблюдения обратно пропорциональна численности конечной группы, в которую оно входит (чем больше наблюдений-повторений-в конечной груп-пе, тем меньше дисперсия ошибки в отдельном наблюдении). Тогда сокращенная модель будет однородной по дисперсии и для ее оценки окажется применим про-стой МНК. 20.3. Упражнения и задачи Упражнение 1 Таблица 20.1 A B C I 3 0 4 II 0 7 0 III 2 8 3 Провести дисперсионный анализ (без повторений) данных, приведенных в таблице 20.1: Имеются 2 фактора по 3 уровня каждый (I, II, III и A, B, C, соответственно). Рассчитать коэффициенты / b, а также Z, / Z, b , C1, C2 , C12, B1 , B2, B12, M , m. Упражнение 2 В Таблице 20.2 приведены данные о зарплатах 52-х пре-подавателей американского колледжа: SX-пол (жен.-1, муж. - 0); ученое звание: RK1-assistant professor, RK2-associate professor, RK3-full professor; DG-ученая степень (доктор-1,магистр-0);SL-средний заработок за ака-демический год, долл. 2.1. Провести дисперсионный анализ с помощью обычной регрессии. 2.2. Провести дисперсионный анализ с помощью взвешенной регрессии, когда совокупность наблюдений с одинаковыми значениями независимых факторов заменяется одним групповым наблюдением.
622 Глава 20. Дисперсионный анализ Таблица 20.2. (Источник: S. Weisberg (1985), Applied Linear Regression, 2nd Ed, New York: Wiley, page 194) SX RK1 RK2 RK3 DG SL SX RK1 RK2 RK3 DG SL 0 0 0 1 1 36350 0 0 1 0 1 24800 0 0 0 1 1 35350 1 0 0 1 1 25500 0 0 0 1 1 28200 0 0 1 0 0 26182 1 0 0 1 1 26775 0 0 1 0 0 23725 0 0 0 1 0 33696 1 1 0 0 0 21600 0 0 0 1 1 28516 0 0 1 0 0 23300 1 0 0 1 0 24900 0 1 0 0 0 23713 0 0 0 1 1 31909 1 0 1 0 0 20690 0 0 0 1 0 31850 1 0 1 0 0 22450 0 0 0 1 0 32850 0 0 1 0 1 20850 0 0 0 1 1 27025 1 1 0 0 1 18304 0 0 1 0 1 24750 0 1 0 0 1 17095 0 0 0 1 1 28200 0 1 0 0 1 16700 0 0 1 0 0 23712 0 1 0 0 1 17600 0 0 0 1 1 25748 0 1 0 0 1 18075 0 0 0 1 1 29342 0 1 0 0 0 18000 0 0 0 1 1 31114 0 0 1 0 1 20999 0 0 1 0 0 24742 1 1 0 0 1 17250 0 0 1 0 0 22906 0 1 0 0 1 16500 0 0 0 1 0 24450 0 1 0 0 1 16094 0 1 0 0 0 19175 1 1 0 0 1 16150 0 0 1 0 0 20525 1 1 0 0 1 15350 0 0 0 1 1 27959 0 1 0 0 1 16244 1 0 0 1 1 38045 1 1 0 0 1 16686 0 0 1 0 1 24832 1 1 0 0 1 15000 0 0 0 1 1 25400 1 1 0 0 1 20300 
20.3. Упражнения и задачи 623 2.3. Учесть эффекты второго порядка: добавить в регрессию попарные произве-дения исходных фиктивных переменных. Значимы ли они? Задачи Таблица 20.3 A B I 43 2 II 4 53 III 8 1 1. Что является отличительной особенностью модели диспер-сионного анализа по сравнению с "обычными" моделями регрессионного анализа? 2. С помощью таблицы 20.3 задана классификация по двум факторам. Запишите матрицы фиктивных переменных для главных эффектов. 3. Какуюструктуру имеет матрица ковариаций оценок в дисперсионном анализе без повторений? 4. Как называется в дисперсионном анализе то, что в регрессионном анализе называется объясненной и остаточной дисперсией? 5. При проведении дисперсионного анализа с повторениями по усредненным наблюдениям используется взвешенная регрессия. С какой целью это дела-ется? 6. Если в дисперсионном анализе без повторений отбросить эффекты высшего порядка, то как изменятся значения параметров оставшихся эффектов? 7. Вмодели полного дисперсионного анализа без повторений с однимфактором, имеющим три уровня, запишите матрицу нецентральных вторых моментов для матрицы регрессоров Z. 8. Сколько наблюдений нужно иметь для применения модели дисперсионного анализа без повторений в случае четырех факторов, каждый из которых мо-жет принимать три уровня, если учитывать только эффекты первого порядка? 9. Сколько наблюдений нужно иметь для применения модели полного диспер-сионного анализа без повторений в случае двух факторов, каждый из которых может принимать три уровня? 10. Для модели дисперсионного анализа с двумя факторами, первый из которых имеет три уровня, а второй-два, рассчитать матрицу C12. 11. Рассмотрим модель дисперсионного анализа с двумя факторами, первый из которых принимает два уровня, а второй-три уровня. Рассчитайте мат-рицы Z1, Z2.
624 Глава 20. Дисперсионный анализ 12. В первой группе 20 человек, а во второй- 30 человек. Дисперсия оценок по "Эконометрии" в первой группе равна 1.5, а во второй- 1. Вычислите остаточную дисперсию в модели дисперсионного анализа. 13. В первой группе 20 человек, а во второй - 30 человек. Средняя оценка по "Эконометрии" в первой группе равна 3.5, а во второй- 4. Вычислите объясненную дисперсию в модели дисперсионного анализа. 14. В первой группе 20 человек, а во второй - 30 человек. Средняя оценка по "Философии" в первой группе равна 4.5, а во второй - 3. Вычислите коэффициенты в модели дисперсионного анализа. 15. В первой группе 20 человек, а во второй - 30 человек. Средняя оценка по "Эконометрии" в первой группе равна 3.5, а во второй- 4. Дисперсия оценок в первой группе равна 1.5, а во второй - 1. Вычислите общую дисперсию оценок двум группам. 16. Проводится дисперсионный анализ без повторений с двумя факторами, один из которых принимает три уровня, а другой - четыре. Как вычисляется статистика для проверки значимости эффектов второго порядка? Какое она имеет распределение (сколько степеней свободы)? Рекомендуемая литература 1. Болч Б., Хуань К.Дж. Многомерные статистические методы для экономи-ки.-М.: "Статистика", 1979. (Гл. 5) 2. Себер Дж. Линейный регрессионый анализ.-М.: "Мир", 1980. 3. Шеффе Г. Дисперсионный анализ.-М.: "Наука", 1980.
Глава 21 Модели с качественными зависимыми переменными При изучении экономических явлений на дезагрегированном уровне (уровне от-дельных экономических субъектов) возникает потребность в новых методах. Дело в том, что стандартные эконометрические методы, такие как классическая мо-дель регрессии, предназначены для анализа переменных, которые могут прини-мать любое значение на числовой прямой, причем предполагается фактически, что распределение изучаемой переменной похоже на нормальное. Модели, в ко-торых диапазон значений зависимой переменной ограничен, называют моделями с ограниченной зависимой переменной. Среди них важную роль играют модели, в которых изучаемая переменная дискретна и может принимать только некоторые значения (конечное число), либо даже имеет нечисловую природу (так называемые модели с качественной зависимой переменной). Модели такого рода помогают, в частности, моделировать выбор экономических субъектов. В качестве примера можно привести выбор предприятия: внедрять какую-то новую технологию или нет. Если индивидуальный выбор исследовать методами, предназначенными для непрерывных переменных, то будет неправомерно проигнорирована информация о поведенческой структуре ситуации. 21.1. Модель дискретного выбора для двух альтернатив Анализ дискретного выбора основывается на микроэкономической теории, ко-торая моделирует поведение индивидуума как выбор из данного множества аль-
626 Глава 21. Модели с качественными зависимыми переменными тернатив такой альтернативы, которая бы максимизировала его полезность. Этот выбор с точки зрения стороннего наблюдателя, однако, не полностью предопре-делен. Исследователь не может наблюдать все факторы, определяющие результат выбора конкретного индивидуума. Коль скоро ненаблюдаемые факторы случайны, то выбор двух индивидуумов может быть разным при том, что наблюдаемые фак-торы совпадают. С его точки зрения это выглядит как случайный разброс среди индивидуумов с одними и теми же наблюдаемыми характеристиками. Предполагается, что выбор осуществляется на основе ненаблюдаемой полез-ности альтернатив u(x). Если u(1) > u(0), то индивидуум выбирает x = 1, если u(1) < u(0), то индивидуум выбирает x = 0. В простейшем случае полезность является линейной функцией факторов: u(1) = zα1 и u(0) = zα0. Чтобы модель была вероятностной, ее дополняют отклоняющими факторами, так что u(1) = zα1 + ε1, u(0) = zα0 + ε0. Предполагается, что распределение отклонений ε1 и ε0 непрерывно. Заметим, что для описания выбора вполне достаточно знать разность между полезностями вместо самих полезностей: ˜x = u(1) − u(0) = z(α1 − α0) + ε1 − ε0 = zα + ε, при этом оказывается, что в основе выбора лежит переменная ˜x, которая пред-ставляет собой сумму линейной комбинации набора факторов z и случайного от-клонения ε, имеющего некоторое непрерывное распределение: ˜x = zα + ε. Эта переменная является ненаблюдаемой. Наблюдается только дискретная ве-личина x, которая связана с ˜x следующим образом: если ˜x больше нуля, то x = 1, если меньше, то x = 0. Ясно, что по наблюдениям за x и z мы могли бы оценить коэффициенты α только с точностью до множителя. Умножение ненаблюдаемых величин ˜x, α и ε на один и тот же коэффициент не окажет влияния на наблюдаемые величины x и z. Таким образом, можно произвольным образом нормировать модель, например, положить дисперсию ошибки равной единице. Кроме того, в этой модели есть дополнительный источник неоднозначности: одним и тем же коэффициентам α могут соответствовать разные пары α0 и α1. Таким образом, можно сделать вывод, что исходная модель выбора принципиально неидентифицируема. Однако это не мешает ее использованию для предсказания результата выбора, что мы продемонстрируем в дальнейшем.
21.1 Оценивание модели с биномиальной зависимой переменной 627 Без доказательства отметим, что если в модели выбора ε1 и ε0 имеют распре-деление F(y) = e−e−y (распределение экстремального значения) и независимы, то ε = ε1 − ε0 имеет логистическое распределение. При этом получается модель, называемая логит. Если ε1 и ε0 имеют нормальное распределение с параметрами 0 и 12 и неза-висимы, то ε = ε1 − ε0 имеет стандартное нормальное распределение. При этом получается модель, называемая пробит. Модели логит и пробит рассматривались в главе 9. 21.2. Оценивание модели с биномиальной зависимой переменной методом максимального правдоподобия Предыдущие рассуждения приводят к следующей модели: ˜x = zα + ε, x =⎧⎪⎨ ⎪⎩ 0, ˜x < 0, 1, ˜x > 0. Пусть Fε(-) -функция распределения отклонения ε. Выведем из распреде-ления ε распределение ˜x, а из распределения ˜x -распределение x: Pr(x = 1) = Pr(˜x > 0) = Pr(zα +ε > 0) = Pr(ε > −zα) = 1 − Fε(−zα). Для удобства обозначим F(y) = 1−Fε(−y). (При симметричности относитель-но нуля распределения ε будет выполнено F(y) = 1 − Fε(−y) = Fε(y).) Таким образом, Pr(x = 1) = F(zα). Пусть имеются N наблюдений, (xi, zi), i = 1, . . . , N, которые соответствуют этой модели, так что xi имеют в основе ненаблюдаемую величину ˜xi = ziα + εi. Предполагаем, что ошибки εi имеют нулевое математическое ожидание, одина-ково распределены и независимы. Рассмотрим, как получить оценки коэффициен-тов α методом максимального правдоподобия. Обозначим через pi = pi(α) = F(ziα). Также пусть I0 = {i| xi = 0}, I1 = {i| xi = 1}. Функция правдоподобия, то есть вероятность получения наблю-дений xi при данных zi, имеет вид: L(α) = i∈I1 pi(α)i∈I0 (1 − pi(α)).
628 Глава 21. Модели с качественными зависимыми переменными Вместо самой функции правдоподобия удобно использовать логарифмическую функцию правдоподобия: lnL(α) = i∈I1 ln pi(α) +i∈I0 ln(1 − pi(α)), которую можно записать как lnL(α) = Ni=1 xi ln pi(α) + (1 − xi) ln(1 − pi(α)). (21.1) В результате максимизации этой функции по α получаем оценки максималь-ного правдоподобия. Условия первого порядка максимума (уравнения правдоподо-бия), т.е. ∂ lnL(α) ∂α = 0, имеют простой вид: Ni=1 (xi − pi) f(ziα) pi(1 − pi)zi = 0, где мы учли, что ∂pi(α) ∂α = dF(ziα) dα = f(ziα)zi, где f -производнаяфункции F(-).Поскольку F(-) представляет собойфункцию распределения, то f(-) -плотность распределения. Можно использовать следующий метод, который дает те же оценки, что и ме-тод максимального правдоподобия. Пусть a0 -некоторая приближенная оценка коэффициентов модели. Аппроксимируем функцию F(-) ее касательной в точке ziα (т.е. применим линеаризацию): F(ziα) ≈ F(zia0) + f(zia0)zi α − a0. Подставим затем эту аппроксимацию в исходную модель: xi − pi(a) ≈ f(zia)zi(α − a) + ξi, или xi − pi(a0) + f(zia0)zia0 ≈ f(zia0)ziα + ξi.
21.2 Оценивание модели с биномиальной зависимой переменной 629 При данном a0 это линейная регрессия. Как несложно проверить, дисперсия ошибки ξi равна pi(α)(1 − pi(α)), т.е. ошибки гетероскедастичны. К этой модели можно применить взвешенную регрессию.Следует разделить левую и правую части на корень из оценки дисперсии ошибки ξi, т.е. на 'pi(a0)(1 − pi(a0)): xi − pi(a0) + f(zia0)zia0 'pi(a0)(1 − pi(a0)) ≈ f(zia0)zi 'pi(a0)(1 − pi(a0))α + ξi 'pi(a0)(1 − pi(a0)) . Оценивая эту вспомогательную регрессию, мы на основе оценок a0 получим новые оценки, скажем a1. Повторяя эту процедуру, получим последовательность оценок {ak}. Если процедура сойдется, т.е. ak → a при k → ∞, то a будут оценками максимального правдоподобия. В качестве оценки ковариационной матрицы оценок a можно использовать −∂2 lnL(a) ∂α∂α−1 . По диагонали этой матрицы стоят оценки дисперсий коэффициентов. На их ос-нове обычным способом можно получить аналоги t-статистик для проверки гипо-тезы о равенстве отдельного коэффициента нулю. Такой тест будет разновидностью теста Вальда. Для проверки набора ограничений удобно использовать статистику отноше-ния правдоподобия LR = 2(lnL(a) − lnL(aR)), гд е lnL(a) -логарифмическая функция правдоподобия из 21.1, a -оценка методом максимума правдоподобия без ограничений, aR -оценка при ограничениях. Эту же статистику можно использовать для построения показателя каче-ства модели, аналогичного F-статистике для линейной регрессии. Она позволя-ет проверить гипотезу о равенстве нулю коэффициентов при всех регрессорах, кроме константы. Соответствующая статистика отношения правдоподобия равна LR0 = 2(lnL(a) − lnL0), гд е lnL0 -максимум логарифмической функции прав-доподобия для модели с одной константой. Она распределена асимптотически как χ2 с n степенями свободы, где n -количество параметров в исходной моде-ли, не включая константу. Величина lnL0 получается следующим образом. Пусть N -общее количество наблюдений, N0 -количество наблюдений, для которых xi = 0, N1 -количество наблюдений, для которых xi = 1. Тогда предсказанная вероятность появления xi = 1 в модели с одной константой будет равна для всех наблюдений N1/N. Отсюда lnL0 = N0 lnN0 + N1 lnN1 − N ln N.
630 Глава 21. Модели с качественными зависимыми переменными 21.2.1. Регрессия с упорядоченной зависимой переменной Регрессия с упорядоченной зависимой переменной имеет дело с альтернати-вами, которые можно расположить в определенном порядке. Например, это могут быть оценки, полученные на экзамене, или качество товара, которое может ха-рактеризоваться сортом от "высшего" до "третьего". Будем предполагать, что альтернативы пронумерованы от 0 до S. Переменная x принимает значение s, если выбрана альтернатива s. Предполагается, что в основе выбора лежит нена-блюдаемая величина ˜x = zα + ε. При этом x = 0 выбирается, если ˜x меньше нижнего (нулевого) порогового значения, x = 1, если ˜x попадает в промежуток от нулевого до первого порогового значения и т. д.; x = S выбирается, если ˜x превышает верхнее пороговое значение: x =⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ 0, ˜x < γ0, 1, γ0 < ˜x < γ1, - - - S, ˜x > γS−1. Если среди регрессоров z есть константа, то невозможно однозначно иден-тифицировать γ. В связи с этим следует использовать какую-либо нормировку. Можно, например, положить γ0 = 0. Это оставляет S−1 неизвестных пороговых параметров. Пусть Fε(-) -функция распределения ошибки ε. Тогда вероятность того, что x = s, гд е s = 1, . . . , S − 1, равна Pr(x = s) = Pr(γs−1 < zα+ ε < γs) = = Pr(γs−1 − zα < ε < γs − zα) = Fε(γs − zα) − Fε(γs−1 − zα). Аналогично для s = 0 и s = S получаем Pr(x = 0) = Pr(ε < γ0 − zα) = Fε(γ0 − zα), Pr(x = S) = Pr(γS−1 − zα < ε) = 1 − Fε(γS−1 − zα). Пусть (xi, zi), i = 1, . . . , N -имеющиеся наблюдения. По этим наблюдени-ям можно получить оценки максимального правдоподобия. Обозначим pis(α, γ) = Pr(xi = s).
21.2 Оценивание модели с биномиальной зависимой переменной 631 Соответствующая логарифмическая функция правдоподобия равна lnL(α, γ) = S s=0i∈Is ln pis(α, γ), гд еIs = {i| xi = s}. Максимизируя эту функцию по α и γ, получим требуемые оценки. На практике обычно используют одну из двух моделей: упорядоченный пробит, то есть модель с нормально распределенным отклонением ε, или упорядоченный логит, то есть модель, основанную на логистическом распределении. 21.2.2. Мультиномиальный логит Предположим, что принимающий решение индивидуум стоит перед выбором из S альтернатив, s = 0, . . . , S − 1. Предполагается, что выбор делается на основе функции полезности u(s). В линейной модели u(s) = zsαs, гд е zs - матрица факторов, αs -неизвестные параметры. Обычно есть также факторы, не отра-женные в zs из-за их ненаблюдаемости, которые тоже влияют на полезность. Такие характеристики представлены случайной ошибкой u(s) = zsαs + εs. При этом x выбирается равным s, если u(s) > u(t), ∀s = t. В самой простой модели принимается, что ошибки εs подчинены распре-делению экстремального значения и независимы между собой. Распределение экстремального значения1 в стандартной форме имеет функцию распределе-ния F(y) = e−e−y . Распределение экстремального значения обладает следующи-ми важными для рассматриваемой модели свойствами: максимум нескольких ве-личин, имеющих распределение экстремального значения, также имеет распреде-ление экстремального значения, а разность двух величин, имеющих распределе-ние экстремального значения, имеет логистическое распределение. Используя эти свойства, можно вывести, что в данной модели Pr(x = s) = ezsαs S−1 t=0 eztαt . Эта мод ель называется мультиномиальным логитом. Относительно функций zsαs обычно делаются какие-либо упрощающие до-пущения, например, что факторы для всех альтернатив одни и те же, то есть 1Это "распределение экстремального значения первого рода" (согласно теореме Гнеденко есть еще два распределения экстремального значения) или, как его еще называют, распределение Гумбе-ля. Данное распределение также изредка называют распределением Вейбулла. Кроме того, именем Вейбулла называют и другие распределения (в частности, "распределение экстремального значения третьего рода"), поэтому может возникнуть путаница.
632 Глава 21. Модели с качественными зависимыми переменными u(s) = zαs + εs, или что функция имеет один и тот же вид, коэффициенты в за-висимости от s не меняются, а меняются только факторы, определяющие выбор, то есть u(s) = zsα + εs. В первом случае z можно интерпретировать как харак-теристики индивидуума, принимающего решение. Это собственно мультиномиаль-ный логит. Во втором случае zs можно интерпретировать как характеристики s-й альтернативы. Этот второй вариант называют условным логитом. Можно предложить модель, которая включает оба указанных варианта. Обо-значим через w характеристики индивидуума, а через zs характеристики s-ой аль-тернативы (в том числе те, которые специфичны для конкретных индивидуумов). Например, при изучении выбора покупателями супермаркета альтернативами яв-ляются имеющиеся супермаркеты, w мог бы включать информацию о доходах и т.п., а в zs следует включить информацию о супермаркетах (уровень цен,широта ассортимента и т.п.) и характеристики пары покупатель-супермаркет, такие как расстояние до супермаркета от места жительства потребителя. В такой мод ели u(s) = zsα+wδs +εs и вероятности вычисляются по формуле Pr(x = s) = ezsα+wδs S−1 t=0 eztα+wδt . Заметим, что в этой модели есть неоднозначность. В частности, если прибавить к коэффициентам δs один и тотже вектор Δ -это все равно, что умножить числи-тель и знаменатель на ewΔ. Таким образом, для идентификации модели требуется какая-либо нормировка векторов δs. Можно, например, положить δ0 = 0. Для оценивания модели используется метод максимального правдоподобия. Пусть (xi, zi0, . . . , zi,S−1,wi), i = 1, . . . , N -имеющиеся наблюдения.Обозна-чим pis(α, δ) = Pr(xi = s) = ezisα+wiδs S−1 t=0 ezitα+wiδt . Тогда логарифмическая функция правдоподобия имеет вид lnL(α, δ) = S−1 s=0i∈Is ln pis(α, δ), гд еIs = {i| xi = s}. На основе xi можно ввести набор фиктивных переменных dis, таких что dis =⎧⎪⎨ ⎪⎩ 1, xi = s, 0, xi = s.
21.2 Оценивание модели с биномиальной зависимой переменной 633 В этих обозначениях функция правдоподобия приобретет вид lnL(α, δ) = Ni=1 S−1 s=0 dis ln pis(α, δ). 21.2.3. Моделирование зависимости от посторонних альтернатив в мультиномиальных моделях Длямультиномиального логитаотношение вероятностей двух альтернатив ("со-отношение шансов") равно Pr(x = s) Pr(x = t) = e(zsα+wδs) e(ztα+wδt) = e((zs−zt)α+w(δs−δt)). Оно зависит только от характеристик этих двух альтернатив, но не от характери-стик остальных альтернатив. Это свойство называется независимостью от посто-ронних альтернатив. Оно позволяет оценивать мультиномиальные модели на под-множестве полного множества альтернатив и получать корректные (состоятель-ные) оценки. Однако это свойство мультиномиального логита во многих ситуациях выбора не очень реалистично. Рассмотрим, например, выбор между передвижением на поезде, на самолете авиакомпании A и на самолете авиакомпании B. Известно, что 50% пассажиров выбирает поезд, 25%-авиакомпанию A и 25%-авиакомпанию B. Допустим, авиакомпании предоставляют примерно одинаковые услуги по схожей цене, и пас-сажиры предпочитают одну из двух авиакомпаний по каким-то чисто субъективным причинам. Если авиакомпании объединятся, то естественно ожидать, что соотно-шение шансов для поезда и самолета будет равно один к одному. Однако с точки зрения мультиномиального логита соотношение шансов должно остаться два к од-ному, поскольку характеристики передвижения поездом и передвижения самолетом остались теми же. Предложено несколько модификаций этой модели, которые уже не демонстри-руют независимость от посторонних альтернатив, и, следовательно, более реали-стичны. Вмод ели вложенного логита используется иерархическая структура альтерна-тив. Вдвухуровневой модели сначала делается выбор между группами альтернатив, а затем делается выбор внутри выбранной группы. В приведенном примере есть две группы альтернатив: "самолет" и "поезд". Внутри группы "самолет" дела-ется выбор между авиакомпаниями A и B. Группа "поезд" содержит только одну альтернативу, поэтому выбор внутри нее тривиален. Пусть имеется l групп альтернатив.Обозначим через Sk множество альтерна-тив, принадлежащих k-й группе. Безусловная вероятность того, что будет выбрана
634 Глава 21. Модели с качественными зависимыми переменными альтернатива s из группы k в модели вложенного логита, определяется формулой (запишем ее только для условного логита, т.е. модели, где от альтернативы зависят факторы, но не коэффициенты) Pr(x = s) = e(z˙kα˙+zsα) lm=1t∈Sm e(z˙mα˙+ztα) = ez˙kα˙ ezsα lm=1 e ˙ zmα˙t∈Sm eztα . Если альтернативы s и t принадлежат одной и той же группе k, то отношение вероятностей равно Pr(x = s) Pr(x = t) = ez˙kα˙ ezsα ez˙kα˙ eztα = ezsα eztα . Это отношение, как и в обычном мультиномиальном логите, зависит только от характеристик этих альтернатив. В то же время, если альтернативы s и t при-надлежат разным группам, k и m соответственно, то отношение вероятностей равно Pr(x = s) Pr(x = t) = ez˙kα˙ ezsα ez˙mα˙ eztα = ez˙kα˙+zsα ez˙mα˙+ztα . Это отношение зависит, кроме характеристик самих альтернатив, также от ха-рактеристик групп, к которым они принадлежат. Другое направление модификации модели мультиномиального логита исходит из того, что независимость от посторонних альтернатив является следствием двух предположений, лежащих в основе модели: то, что ошибки εs одинаково распре-делены и, следовательно, имеют одинаковую дисперсию, и то, что они независимы. Во-первых, можно предположить, что имеет место гетероскедастичность. (Имеется в виду не гетероскедастичность по наблюдениям, а гетероскедастичность по альтернативам.) Для того чтобы ввести гетероскедастичность в модель, доста-точно дополнить распределенияошибок масштабирующими коэффициентами.При этом ошибка εs имеет функцию распределения Fs(y) = e−e−y/σs . Поскольку одновременно все σs идентифицировать нельзя, то требуется норми-ровка. Например, можно принять, что σ0 = 1. С помощью такой модификации мы получим гетероскедастичную модель с распределением экстремального значения. Во-вторых, можно предположить, что ошибки εs могут быть коррелирован-ными друг с другом. Обычно в таком случае используют многомерное нормальное
21.3. Упражнения и задачи 635 распределение ошибок: ε = ⎛⎜⎜⎜⎜⎜⎝ ε0... εS−1 ⎞⎟⎟⎟⎟⎟⎠ ∼ N(0,Σε). Здесь Σε - ковариационная матрица ошибок, которая обычно предполагается неизвестной. С помощью такой модификации мы получим модель мультиномиаль-ного пробита. Ковариационная матрица Σε не полностью идентифицирована. Дело в том, что, во-первых, важны разности между ошибками, а не сами ошибки, а во-вторых, ковариационная матрица разностей между ошибками идентифицируется только с точностью до множителя. Можно предложить различные варианты нормиров-ки. Как следствие нормировки, количество неизвестных параметров в матрице Σε существенно уменьшается. Если в исходной матрице их S(S +1)/2, то после нор-мировки остается S(S − 1)/2 − 1 неизвестных параметров. К сожалению, не существует аналитических формул для расчета вероятностей альтернатив в мультиномиальном пробите. Вероятности имеют видмногомер ных интегралов. Обозначим через Bs множество таких ошибок ε, которые приводят к выбору s-й альтернативы, т.е. Bs = {ε|u(s) > u(t), ∀s = t} = {ε|zsαs + εs > ztαt + εt, ∀s = t}, а через ϕ(ε) -многомерную плотность распределения ε. Тогда вероятность того, что будет выбрана альтернатива s, равна2 Pr(x = s) = ε∈Bsϕ(ε)dε. Для вычисления таких интегралов, как правило, используется методМонте-Карло. 21.3. Упражнения и задачи Упражнение 1 В Таблице 9.3 на стр. 306 приведены данные о голосовании по поводу увеличе-ния налогов на содержание школ в городе Троя штата Мичиган в 1973 г. Наблю-дения относятся к 95-ти индивидуумам. Приводятся различные их характеристики: 2Реально требуется вычислить не S-мерный интеграл, а (S − 1)-мерный, поскольку важны не сами ошибки, а разности между ними.
636 Глава 21. Модели с качественными зависимыми переменными Pub = 1, если хотя бы один ребенок посещает государственную школу, иначе 0; Priv = 1, если хотя бы один ребенок посещает частную школу, иначе 0; Years- срок проживания в данном районе; Teach = 1, если человек работает учителем, иначе 0; LnInc-логарифм годового дохода семьи в долл.; PropTax-логарифм налогов на имущество в долл. за год (заменяет плату за обучение-плата зависит от имущественного положения); Yes = 1, если человек проголосовал на референ-думе "за", 0, если "против". Зависимая переменная-Yes. В модель включаются все перечисленные факторы, а также квадрат Years. 1.1. Получите приближенные оценки для логита и пробита с помощью линейной регрессии. 1.2. Оцените логит и пробит с помощьюММПи сравните с предыдущим пунктом. 1.3. Вычислите коэффициенты логита через коэффициенты пробита и сравните с предыдущими результатами. 1.4. На основе оценокМПдля логита найдите маргинальные значения для Teach, LnInc и PropTax при среднем уровне факторов. 1.5. Постройте график вероятности голосования "за" в зависимости от Years при среднем уровне остальных факторов. 1.6. Постройте аналогичный график маргинального значения Years. Упражнение 2 Рассматривается модель мультиномиального логита. Вмодели имеется три аль-тернативы: 0, 1 и 2. Для каждой из альтернатив s = 0, 1, 2 полезность рассчиты-вается по формуле us = zsα + βs + εs, гд е α = 2, βs = s/5, а ошибки εs имеют распределение экстремального значения. Поскольку функция распределения для распределения экстремального значения имеет вид F(ε) = e−e−ε, то ошибкимож-но генерировать по формуле ε = −ln (−ln (ξ)), ξ имеет равномерное распределе-ние на отрезке [0; 1]. Зависимая переменная x принимает одно из трех возможных значений (0, 1 или 2) в зависимости от того, какая полезность выше. 2.1. Пусть z1 = 0.4, z2 = 0.3, z3 = 0.2. Проверить методом Монте-Карло формулу для вероятностей: Pr(x = s) = ezsα+βs 2 t=0 eztα+βt ,
21.3. Упражнения и задачи 637 сгенерировав выборку из 1000 наблюдений для x и рассчитав эмпирические частоты. 2.2. Сгенерировать данные по модели, взяв zs ∼ N(0, 2) для всех s. Сгенери-ровав набор из 1000 наблюдений (xi, z0i, z1i, z2i), гд е i = 1, . . . , 1000, по-лучить оценки параметров модели мультиномиального логита, предполагая, что β0 = 0. Сравнить с истинными значениями параметров. Задачи 1. Чему равны оценки максимального правдоподобия по модели логит с одной константой? 2. Запишите 7 терминов, которые имеют отношение к моделям с качественной зависимой переменной. 3. Рассмотрите модель с биномиальной зависимой переменной x, принимаю-щейзначения 0или1и зависящейотфиктивнойпеременной z,принимающей значения 0 или 1. Модель включает также константу. Данные резюмируются следующей таблицей (в клетках стоят количества соответствующих наблю-дений): x = 0 x = 1 z = 0 N00 N01 z = 1 N10 N11 а) Пусть в основе модели лежит некоторая дифференцируемая функция распределения F (-), заданная на всей действительной прямой. Найдите Pr (x = 1) при z = 0 и при z = 1. б) Запишите в компактном виде логарифмическую функцию правдоподо-бия. в) Запишите условия первого порядка для оценок максимального правдо-подобия, обозначая F(y) = f (y). г) Для N00 = 15, N01 = 5, N10 = 5, N11 = 15 получите оценки логита методом максимального правдоподобия. д) Для тех же данных получите оценки пробита методом максимального правдоподобия, используя таблицы стандартного нормального распре-деления. е) Как можно определить, значима лификтивная переменная z?Запишите формулу соответствующей статистики и укажите, как она распределена.
638 Глава 21. Модели с качественными зависимыми переменными ж) Получите формулу для приближенных оценок логита методом усредне-ния (используя линейность отношения шансов для логита). Сравните с формулой для оценок максимального правдоподобия. 4. Изучается зависимость курения среди студентов от пола. В следующей таб-лице приведены данные по 40 студентам: Пол Количество наблюдений Доля курящих Муж. 20 0.3 Жен. 20 0.4 Оцените по этим данным модель логит методом максимального правдоподо-бия. Используйте при этом то, что ln 2 = 0.693, ln 3 = 1.099 и ln 11 = 2.398. 5. Пусть переменная x, принимающая значения 0 или 1, зависит от одного фактора z. Модель включает также константу. Данные приведены в таблице: x 0 0 1 1 0 1 0 1 0 1 z 1 2 3 4 5 6 7 8 9 10 Запишите для этих данных логарифмическую функцию правдоподобия моде-ли с биномиальной зависимой переменной. 6. Оцените упорядоченный пробит методом максимального правдоподобия по следующим данным: x 0 1 2 3 4 количество на-блюдений 50 40 45 80 35 7. Модель с биномиальной зависимой переменной имеет вид: ˜x = αz + β + ε, x =⎧⎪⎨ ⎪⎩ 1, ˜x > 0, 0, ˜x < 0, где z -фиктивная переменная.Связьмежду x и z задана таблицей (в клет-ках указано количество наблюданий):
21.3. Упражнения и задачи 639 x 0 1 0 24 28 z 1 32 16 а) Найдите оценки коэффициентов логита и пробита по методу усреднения сгруппированных наблюдений. б) Найдите оценки максимального правдоподобия. в) Проверьте значимость модели в целом по статистике отношения прав-доподобия. 8. По некоторым данным был оценен рядмод елей с биномиальной зависимой переменной и факторами z1 и z2. В таблице приведены результаты оцени-вания этих моделей методом максимального правдоподобия. В скобках за-писаны стандартные ошибки коэффициентов. Прочерк означает, что данный фактор не был включен в модель. В последней строке приведено значение логарифмической функции правдоподобия в максимуме. Логит Пробит I II III IV V IV VII VIII Константа 1.87 (0.38) 0.28 (0.20) 1.88 (0.38) 0.28 (0.20) 1.14 (0.21) 0.17 (0.12) 1.16 (0.21) 0.18 (0.12) Z1 -0.08 (0.33) 0.0012 (0.19) - - -0.06 (0.19) 0.0011 (0.12) - - Z2 -2.00 (0.44) - -1.99 (0.44) - -1.21 (0.24) - -1.20 (0.25) - ln L -44.4 -68.2 -44.5 -68.3 -44.2 -68.3 -44.4 -68.5 Какую из моделей следует выбрать? Обоснуйте свой ответ. 9. Рассмотрите модель дискретного выбора из двух альтернатив с линейной случайной функцией полезности вида: u(s) = αzs + β + εs, s= 0.1, где все ошибки εs имеют равномерное распределение U[−γ, γ] и независи-мы по уравнениям и по наблюдениям. а) Найдите вероятности выбора s = 0 и s = 1 для такой модели.
640 Глава 21. Модели с качественными зависимыми переменными б) Объясните, идентифицируемы ли одновременно параметры α, β и γ. Если нет, то предложите идентифицирующую нормировку. в) Запишите функцию правдоподобия для этой модели. 10. Покажите, что логарифмическая функция правдоподобия для биномиального логита является всюду вогнутой по параметрам.Какие преимущества дает это свойство? 11. Рассмотрите модель дискретного выбора из двух альтернатив: s = 1 и s = 2, в основе которого лежит случайная полезность ui(s) = zisα+εis, предпола-гая, что ошибки двух альтернатив коррелированыи распределенынормально: ⎛⎜⎝ ε1i ε2i ⎞⎟⎠∼ N ⎛⎜⎝ ⎛⎜⎝ 00 ⎞⎟⎠,⎛⎜⎝ σ21 σ12 σ12 σ22 ⎞⎟⎠ ⎞⎟⎠. Какие параметры идентифицируемы? Аргументируйте свой ответ. Предло-жите нормировки, которые позволят оценить такую модель биномиального пробита. Каким методом можно оценить такой "коррелированный" пробит? 12. Пусть Λ(-), Φ(-) -функции распределения логистического и стандартного нормального распределения соответственно. а) Покажите, что выпуклая комбинация F(y) = (1 − α)Λ(y) + αΦ(y), α ∈ [0; 1], также задает функцию распределения (удовлетворяющую всем должным требованиям). б) Постройте на основе F(y) модель, которая охватывает как логит, так и пробит. в) Запишите логарифмическуюфункцию правдоподобия для такой модели. г) Запишите условия первого порядка для оценок максимального правдо-подобия. д) Является ли параметр α идентифицируемым? (Аргументируйте свой ответ формально.) 13. Рассмотрите модель дискретного выбора из двух альтернатив с линейной случайной функцией полезности вида: u(s) = zsα + εs, s= 0.1, где все ошибки ε0 и ε1 независимы и их функция распределения имеет вид F(y) = e−e−y .
21.3. Упражнения и задачи 641 а) Покажите, что Pr ε1 − ε0 < y= ey 1 + ey . б) Найдите вероятности выбора s = 0 и s = 1 для такой модели. Пока-жите, что данная модель совпадает с логитом. 14. Пусть в упорядоченном логите зависимая переменная x принимает три зна-чения (0, 1, 2). Найдите, как вероятность того, что x = 2, зависит от пара-метра γ1 (границы между 1 и 2), т.е. найдите соответствующее маргинальное значение. 15. Выведите формулу оценок максимального правдоподобия для регрессии с упорядоченной зависимой переменной с одной константой. Для количества наблюдений, соответствующих выбору альтернативы s, используйте обозна-чение Ns. (Подсказка: удобно перейти от исходных параметров к вероятно-стям ps = Pr (x = s).) 16. Рассмотрите использование упорядоченной регрессии для моделирования ре-шения индивидуума о получении образования. Пусть в основе принимаемого решения имеется некоторый индекс, выражающий полезность от образо-вания: Ui = Ziα + εi, εi ∼ N(0; σ2). Чем выше индекс, тем более вероятен выбор более высокого уровня об-разования. Более конкретно, пусть имеются некоторые известные заранее пороговые значения для индекса, γ1 и γ2, такие что: - при Ui > γ2 индивидуум i заканчивает вуз; - при γ1 < Ui γ2 индивидуум i заканчивает среднюю школу, но не получает высшего образования; - при Ui γ1 индивидуум i получает только неполное среднее образо-вание. а) Какой видмож ет иметь зависимая переменная в такой модели? б) Покажите, что в данной модели нельзя однозначно идентифицировать как β, так и σ. в) Можно ли однозначно идентифицировать β/σ? г) Можно ли однозначно идентифицировать β, если положить σ = 1?
642 Глава 21. Модели с качественными зависимыми переменными д) Возможно было бы идентифицировать γ1 и γ2, если бы они были неиз-вестны? е) Запишите функцию правдоподобия для данной модели. 17. В модели регрессии с упорядоченной зависимой переменной альтернативами были числа s = 0, . . . , S. Как поменяются оценки максимального правдо-подобия, если альтернативами будут числа 1, 2, 22, . . . , 2S? Аргументируйте свой ответ. 18. Ввыборах участвуют три кандидата: Иванов (s = 1),Петров (s = 2) и "про-тив всех" (s = 0). Передв ыборами был проведен опрос населения. Для каждого из опрошенных собраны данные о том, какого он пола (F или M) и за кого собирается голосовать. В результате получено 6 чисел: NsF , NsM (s = 0, 1, 2)-количество женщин и мужчин, собирающихся голосовать за каждого из трех кандидатов. Выведите функцию правдоподобия для соот-ветствующей модели мультиномиального логита. 19. С помощью мультиномиального логита изучается выбор индивидуумами спо-соба передвижения между домом и работой: пешком, на автобусе или на лич-ном автомобиле. Имеются следующие данные: среднее время передвижения от дома до работы для каждого индивидуума каждым из способов и сред-ний доход каждого индивидуума. Введите требуемые обозначения и запишите формулы вероятностей выбора каждого из способов передвижения. Предло-жите нормировку, которая позволяет идентифицировать модель. 20. Работники кафе быстрого обслуживания "Томато-пицца" могут выбрать один из видов фирменной униформы: брюки или юбку,-причем одного из двух цветов: красного и темно-красного. Какой из моделей вы бы описали такую ситуацию? Объясните. 21. Рассмотрите модель дискретного выбора из трех альтернатив с линейной функцией полезности, соответствующую модели мультиномиального проби-та. Предложите нормировки, которые позволят оценить такую модель. 22. В чем состоят преимущества и недостатки мультиномиального пробита по сравнению с мультиномиальным логитом? Рекомендуемая литература 1. Cramer J.S. The Logit Model for Economists.-Adward Arnold, 1991.
21.3. Упражнения и задачи 643 2. Davidson R., MacKinnon J.G. Estimation and Inference in Econometrics.- Oxford University Press, 1993. (Ch. 15). 3. GreeneW.H. Econometric Analysis.-Prentice-Hall, 2000. (Ch. 8, 19). 4. Maddala G.S. Limited-Dependent and Qualitative Variables in Econometrics, Cambridge University Press, 1983. (Ch. 2, 3, 5). 5. Wooldridge Jeffrey M. Introductory Econometrics: A Modern Approach, 2nd ed.-Thomson, 2003. (Ch. 17). 6. Baltagi, Badi H. Econometrics, 2nd edition, Springer, 1999 (Ch. 13). 7. RuudPaulA.AnIntroduction toClassicalEconometricTheory, Oxford University Press, 2000 (Ch. 27).
Глава 22 Эффективные оценки параметров модели ARMA В главе 14 "Линейные стохастические модели ARIMA" были рассмотрены два метода оценки параметров моделей ARMA: линейная регрессия для оценивания авторегрессий и метод моментов для общей модели ARMA. Эти методы не обес-печивают эффективность оценок параметров модели. Можно предложить способы оценивания, которые дают более точные оценки. Для этого можно использовать метод максимального правдоподобия. Рассмотрению этого метода и посвящена данная глава. 22.1. Оценки параметров модели AR(1) Рассмотрим только случай модели AR(1): xt = εt + ϕxt−1, t = 1, . . . , T, на примере которого хорошо видна и общая ситуация. Чтобы воспользоваться методом максимума правдоподобия, вычислим плотно-сти распределения вероятности наблюдений x1, x2, . . . , xT : f(x1, . . . , xT) = f(x1) - f(x2|x1) - f(x3|x2) - . . . - f(xT |xT−1). Предположим, что условное распределения xt при известном xt−1 нормально. В соответствии с моделью AR(1) это распределение имеет среднее ϕxt−1 и дис-
22.1. Оценки параметров модели AR(1) 645 персию σ2ε . Значит, f(x1, . . . , xT) = f(x1) T t=2(2πσ2ε )−12 - e− 1 2σ2ε (xt−ϕxt−1)2= = f(x1)(2πσ2ε )−T−1 2 e− 1 2σ2ε T t=2(xt−ϕxt−1)2 . Плотность какфункция параметров ϕ и σε является функцией правдоподобия. Вместо полной плотности f(x1, . . . , xT ) в качестве приближения рассмотрим плотность условного распределения x2, . . . , xT, считая x1 заданным, т.е. будем оперировать с f(x2, . . . , xT |x1). Тем самым мы потеряем одну степень свободы. Приближенная функция прав-доподобия равна L−(ϕ, σ2ε) = 2πσ2ε−T−1 2 e− 1 2σ2ε Tt=2 (xt−ϕxt−1)2 = 2πσ2ε−T−1 2 e− 1 2σ2ε s(ϕ), где s(ϕ) = Tt=2 (xt − ϕxt−1)2. Максимизируя ее по σ2ε , выразим σ2ε через ϕ : σ2ε = s(ϕ) T − 1. Подставляя это выражение в L−(ϕ, σ2ε ), получим концентрированную функцию правдоподобия: Lc−(ϕ) = 2π s(ϕ) T − 1−T−1 2 e−T−1 2 . Максимизация Lc−(ϕ) по ϕ эквивалентна минимизации суммы квадратов s(ϕ) = Tt=2(xt − ϕxt−1)2 = Tt=2 εt2. Таким образом, задача сводится к обычному МНК. Минимум этого выражения по ϕ равен просто ϕ∗ = Tt=2 xtxt−1 T−1 t=1 x2t . Получили условную МНК-оценку. Несложно обобщить этот методна случай AR(p) при p > 1.
646 Глава 22. Эффективные оценки параметров модели ARMA Мызнаем, что в качестве оценки ϕ можно использовать выборочную автокор-реляцию r1. Но так как условная МНК-оценка несколько иная, в вырожденных случаях можно получить значения |ϕ| > 1. Это можно обойти, учитывая информа-цию о x1. Для этого воспользуемся тем, что частное распределение x1 является нормальным со средним 0 и дисперсией σ2x1 = σ2ε 1 − ϕ2 . Можно воспользоваться здесь взвешенным МНК. Сумма квадратов остатков после преобразования в пространстве наблюдений равна h(ϕ) = 1 − ϕ2x21+ T t=2 (xt − ϕxt−1)2. Получим точную МНК-оценку ˆϕ∗ = argmin ϕ h(ϕ). Плотность частного распределения x1 равна f (x1) = 2π σ2ε 1−ϕ2 −12 e−(1−ϕ2)x21 2σ2ε . Отсюда f (x1, . . . , xT) = 1 − ϕ212 2πσ2ε −T2 e −1 2σ2ε h(ϕ). Будем рассматривать эту плотность как функцию правдоподобия, обозначая через L(ϕ, σ2ε ). Точную ММП-оценку находим из условия L(ϕ, σ2ε ) → max σ2ε ,ϕ . Оценкой σ2ε будет h(ϕ)T . Концентрируя функцию правдоподобия по σ2ε , получим: Lc(ϕ) = 1 − ϕ212 2π h (ϕ) T −T2 e−T2 → max ϕ ! Это эквивалентно решению задачи 1 − ϕ2− 1T h (ϕ) → min! ϕ Отсюда найдем ММП-оценку ˜ϕ∗ и ˜σ2∗ ε = h( ˜ ϕ∗) T . Множитель 1 − ϕ2− 1T обеспечивает существование минимума в допустимом интервале −1 < ϕ < 1, хотя теперь для нахождения минимума требуются ите-рационные процедуры. Для таких процедур оценка r1 может послужить хорошим начальным приближением. Величина 1− ϕ2 не зависит от T , и с ростом T множитель (1 − ϕ2)− 1T стре-мится к единице. Поэтому этот множитель существенен при малых объемах выбо-рок и |ϕ| близких к 1. При больших T и |ϕ| не очень близких к 1 без него можно
22.2. Оценка параметров модели MA(1) 647 обойтись, соглашаясь с незначительными потерями точности оценки, но сильно сокращая объем вычислений. Этим обстоятельством объясняется использование МНК-оценок. 22.2. Оценка параметров модели MA(1) Продемонстрировать общий метод оценивания модели MA(p) можно, рассмат-ривая простейший случай модели MA(1): xt = εt − θεt−1. Отталкиваясь от наблюдений x1, x2, . . . , xT , воспользуемся методом макси-мального правдоподобия (ММП). Для этого необходимо вычислить для модели функцию плотности распределения вероятности. Это легче всего сделать, перейдя от последовательности xt к последовательности εt. x1 = ε1 − θε0 ⇒ ε1 = x1 + θε0, x2 = ε2 − θε1 ⇒ x2 = ε2 − θx1 − θ2ε0 ⇒ ε2 = x2 + θx1 + θ2ε0 и так далее. Получаем систему: ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ ε1 ε2 ε3... εT⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ θ θ2 θ3... θT⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ ε0 + ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 0 0 - - - 0 θ 1 0 - - - 0 θ2 θ 1 - - - 0 ... ... .... . . ... θT−1 θT−2 θT−3 - - - 1⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ x1 x2 x3... xT⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Это система уравнений относительно θ. В векторной форме ε = ˜θε0 + Qx, (22.1) где мы обозначили ˜θ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ θ θ2 θ3... θT⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ и Q = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 0 0 - - - 0 θ 1 0 - - - 0 θ2 θ 1 - - - 0 ... ... .... . . ... θT−1 θT−2 θT−3 - - - 1⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ .
648 Глава 22. Эффективные оценки параметров модели ARMA Будем полагать, что εt -последовательность независимых случайных величин, имеющих одинаковое нормальное распределение со средним 0 и дисперсией σ2ε . Плотность распределения вероятности записывается в виде f(ε0, . . . , εT) = f(ε0, x1, . . . , xT) = (2πσ2ε )−T+1 2 e− 1 2σ2ε Tt=0 ε2t . (22.2) Метод максимального правдоподобия заключается в нахождении такого значе-ния θ, при котором достигается максимум (22.2), или, что эквивалентно, достигает минимума сумма квадратов S (θ|ε0) = T t=0 ε2t= ε20+ εε = 1 + ˜θ˜θε20+ 2xQ˜θε0 + xQQx. (22.3) Необходимо рассмотреть теперь проблему определения величины ε0. Первый подход-положить ε0 = 0. Тогда требуется минимизировать xQQX → min! θ Эта нелинейная задача решается разными вычислительными методами. Полу-ченные таким путем решения называется условным МНК-решением, а θ∗ = = argmin θ xQQX -условной МНК-оценкой. При втором подходе величина ε0 вместе с θ входит в число подлежащих мини-мизации свободных параметров. Так как величина ε0 входит в выражение для εt линейно, то можно частично облегчить оптимизационнуюпроцедуру, поставив вме-сто ε0 значение ˆε0, минимизирующее функцию s при данном θ. То есть на первом шаге решаем задачу S(θ|ε0) → min! ε0 ∂S(θ|ε0) ∂ε0 = 2(1+ ˜ θ˜θ)ε0 + 2xQ˜θ = 0 ⇒ ˆε0 = − xQ˜θ 1 + ˜ θ˜θ . Далее, подставим ˆε0 = ˆε0(θ) в S(θ|ε0) (22.3) и решим задачу: S (θ|ˆε0 (θ)) → min! θ S (θ|ˆε0 (θ)) = 1 + ˜θ˜θxQ˜θ 1 + ˜ θ˜θ2 + xQQx − 2(xQ˜θ)2 1 + ˜θ˜θ = = xQQx − (xQ˜θ)2 1 + ˜ θ˜θ → min! ε0, θ Полученное при таком подходе значение θ∗, минимизирующее функцию S(θ), называют точной МНК-оценкой для θ и ˆε0 = − xQ∗˜θ∗ 1 + ˜θ∗˜θ∗ .
22.2. Оценка параметров модели MA(1) 649 Небольшой дополнительный анализ приводит к точным ММП-оценкам. Обозначим 1 + ˜ θ˜θ = K. Функцию правдоподобия можно представить в виде f (ε0, . . . , εT) = f (ε0, x1, . . . , xT) = = 2πσ2ε K −12 e− K 2σ2ε (ε0−ˆε0)2 ∗ K−12 2πσ2ε−T2 e− 1 2σ2ε S(θ), где S (θ) = xQQx − (xQ˜θ)2 1 + ˜ θ˜θ . Видим, что первая часть этой записи-это функция плотности распределе-ния ε0 ∈ N(ˆε0, σ2ε K ), т.е первая часть представляет собой условное распреде-ление f (ε0|x1, . . . , xT ) неизвестного значения ε0 при известных наблюдениях x1, x2, . . . , xT . Вторая часть записи-это частная функция плотности распределения вероятности наблюдений x1, x2, . . . , xT, т.е. f (x1, . . . , xT ). Действительно, x = ε0c + Dε, гд е c = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ −θ 0... 0 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ , D= ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 0 - - - 0 −θ 1 . . . ....... . . . . . 0 0 - - - −θ 1⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Ковариационная матрица ряда x равна Γ = E(xx) = E(ε0c + Dε) (ε0c + Dε)= = Eε20cc+ EDεεD= σ2ε (cc+ DD). Обратная к ней: Γ−1 = 1 σ2ε (cc+ DD)−1 = = 1 σ2ε DD−1 − DD−1 c 1 + cDD−1 c−1 cDD−1. Заметим, что D−1 = Q, (DD)−1 = QQ и −D−1c = −Qc = ˜θ. Тогд а Γ−1 = 1 σ2ε QQ − QθθQ 1 + θθ .
650 Глава 22. Эффективные оценки параметров модели ARMA Определитель ковариационной матрицы Γ: |Γ| = σ2T ε cc+ DD= σ2T ε 1 + cDD−1 cDD= = σ2T ε 1 + cDD−1 c= σ2T ε 1 + ˜θ˜θ, где мы воспользовались тем, что |D| = 1 и |DD| = |D||D| = 1. По формуле плотности многомерного нормального распределения f (x1, . . . , xT) = (2π)−T2 |Γ|−12 e−12xΓ−1x = = 2πσε2−T2 1 + ˜ θ˜θ−12 e− 1 2σ2ε xQQ−QθθQ 1+θθ x. Поэтому f (x1, . . . , xT) = K−12 2πσ2ε−T2 e− 1 2σ2ε S(θ). Итак, необходимо решить задачу: f (ε0, . . . , εT) = f (ε0, x1, . . . , xT) = = f (ε0|x1, . . . , xT ) - f (x1, . . . , xT ) → max! ε0, θ От ε0 зависит только первая часть ⇒ ε∗0 = ˆε0, а задача приобретает вид f (x1, . . . , xT ) → max! θ ⇔ K−12 2πσε2−T2 e− 1 2σ2ε S(θ) → max! σ2ε , Нетрудно получить оценку по σ2ε : ˆσ2ε = S(θ)T .Концентрируемфункцию прав-доподобия: fc (x1, . . . , xT) = K−12 2πS (θ) T −T2 e−T2 → max! θ Собираем в данной функции вместе все, что зависит от θ, и получаем 1 K 1T S (θ)T2 .T 2πeT2 → max! θ ⇔ K 1T S (θ) → min! θ Значение ˆθ∗, минимизирующее функцию K1/T S(θ), называют точной ММП-оценкой. Заметим, что функция K зависит лишь от θ , не зависит от наблюдений, и с ро-стом T величина K1/T стремится к единице. Поэтому этот множитель существе-нен лишь при малых объемах выборок и в этом случае не представляет труда для вычислений, а при умеренно больших T без него можно обойтись, соглашаясь с незначительным смещением оценки, но сильно сокращая объем вычислений, осо-бенно в случае MA(q). Этим обстоятельством объясняется использование точных МНК-оценок.
22.3. Оценки параметров модели ARMA(P, Q) 651 22.3. Оценки параметров модели ARMA(p, q) Рассмотрим модели ARMA(p, q) ряда {xt}: xt − ϕ1xt−1 −- - -−ϕpxt−p = εt − θ1εt−1 −- - -−θqεt−q. Будем полагать, что εt -последовательность независимых случайных вели-чин, имеющих одинаковое нормальное распределение со средним 0 и дисперси-ей σ2ε . Через автоковариационную функцию стационарного ARMA-процесса γi = E[xtxt−i] можно выразить ковариационную матрицу x = (x1, . . . , xT ) ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ γ0 γ1 - - - γT−1 γ1 γ0 - - - γT−2 ... .... . . ... γT−1 γT−2 - - - γ0 ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ = Γ. Она является симметричной тёплицевой матрицей и обозначается как τ [γ0, . . . , γT−1]. Так как x ∼ NT (0, Γ), то логарифмическая функция правдоподобия процесса равна lnL(θ,ϕ, σ2ε) = −T2 ln(2πσ2ε ) − 12 ln |Γ| − 12xΓ−1x. (22.4) Через ri обозначаем автоковариацию, нормированную на дисперсию ошибок, т.е. ri = γi σ2ε , а через R обозначаем матрицу τγ0 σ2ε , . . . , γT − 1 σ2ε . В терминах нормированной R логарифмическая функция правдоподобия (22.4) записывается следующим образом: lnL(θ,ϕ, σ2ε) = −T2 ln(2πσ2ε ) − 12 ln |R| − 1 2σ2ε xR−1x → max! Воспользовавшись условиями первого порядка ∂ lnL(θ,ϕ, σ2ε ) ∂σ2ε = 0, получим оценку σ2ε как функцию от θ и ϕ : σ2ε = σ2ε (θ,ϕ) = xR−1x T .
652 Глава 22. Эффективные оценки параметров модели ARMA Поставив оценку σ2ε в логарифмическую функцию правдоподобия, получим концентрированную функцию правдоподобия lnLc(θ,ϕ) = −T2 ln(2π) − T2 − 12 ln |R| − T2 lnxR−1x T → max! Точные оценки параметров ARMA для процесса xt можно найти, максимизируя функцию lnLc(θ,ϕ), что делается с помощью численных методов. 22.4. Упражнения и задачи Упражнение 1 Сгенерируйте ряд длиной 200 наблюдений по модели MA(1) с параметром θ1 = 0.5 и нормально распределенной неавтокоррелированной ошибкой с единич-ной дисперсией. По этому ряду оцените модель MA(1) методом моментов (прирав-нивая теоретические и выборочные автокорреляции), условным методом наимень-ших квадратов и точным методом максимального правдоподобия. Сравните все эти оценки с истинным значением. Упражнение 2 Сгенерируйте ряд длиной 200 наблюдений по модели AR(1) с параметром ϕ1 = 0.5 и нормально распределенной неавтокоррелированной ошибкой с еди-ничной дисперсией. По этому ряду оцените модель AR(1) методом моментов (при-равнивая теоретические и выборочные автокорреляции), условным методом наи-меньших квадратов и точным методом максимального правдоподобия. Сравните все эти оценки с истинным значением. Упражнение 3 Сгенерируйте ряд длиной 200 наблюдений по модели ARMA(1, 1) с парамет-рами ϕ1 = 0.5 и θ1 = 0.5 и нормально распределенной неавтокоррелирован-ной ошибкой с единичной дисперсией. По этому ряду оцените модель ARMA(1, 1) методом моментов (приравнивая теоретические и выборочные автокорреляции), условным методом наименьших квадратов и точным методом максимального прав-доподобия. Сравните все эти оценки с истинными значениями. Задачи 1. Какие предположения должны выполняться, чтобы можно было оценить мо-дель MA(1) с помощью метода максимального правдоподобия?
22.4. Упражнения и задачи 653 2. Сформулируйте кратко отличие между условной и точной оценкой МНК для модели MA(1) и связь между ними (с пояснением обозначений). 3. Как можно найти оценку параметра для модели AR(1), исходя из предпо-ложения, что первое наблюдение не является случайной величиной? Как называется такая оценка? 4. Запишите функцию правдоподобия для модели авторегрессии первого поряд-ка, выделив множитель, который является причиной отличия точной ММП-оценки от условной оценки. Плотности распределения какой величины соот-ветствует этот множитель? 5. Запишитефункциюправдоподобия для модели скользящегосреднего первого порядка. Рекомендуемая литература 1. Бокс Дж., Дженкинс Г. Анализ временных рядов. Прогноз и управление. (Вып. 1, 2).-М.: "Мир", 1972. 2. Песаран М., Слейтер Л. Динамическая регрессия: теория и алгоритмы.- М: "Финансы и статистика", 1984. (Гл. 2-4). 3. Engle Robert F. Autoregressive Conditional Heteroskedasticity with Estimates of the Variance of U.K. Inflation //Econometrica, 50, 1982, 987-1008. 4. Hamilton James D. Time Series Analysis.-Princeton University Press, 1994. (Ch. 5). 5. JudgeG.G., GriffithsW.E., Hill R.C., Luthepohl H., Lee T. Theory and Practice of Econometrics.-New York: John Wiley & Sons, 1985. (Ch. 8). 6. (*) Справочник по прикладной статистике: В 2-х т. Т. 2. /Под ред. Э. Ллойда, У. Ледермана.-М.: "Финансы и статистика", 1990. (Гл. 18).
Глава 23 Векторные авторегрессии 23.1. Векторная авторегрессия: формулировка и идентификация Модели векторной авторегрессии (VAR) представляют собой удобный инстру-мент для одновременногомоделирования нескольких рядов. Векторная авторегрес-сия-это такаямодель, в которой несколько зависимых переменных, и зависят они от собственных лагов и от лагов других переменных. Если в обычной авторегрессии коэффициенты являются скалярами, то здесь следует рассматривать уже матрицы коэффициентов. В отличие от модели регрессии, в VAR-модели нет нужды делить переменные на изучаемые переменные и независимые факторы. Любая экономическая пере-менная модели VAR по умолчанию включается в состав изучаемых величин (хотя есть возможность часть переменных рассматривать как внешние к модели, экзо-генные). Отметим, что естественным расширением модели VAR является модель VARMA, включающая ошибку в виде скользящего среднего.ОднакомодельVARMA не получила очень широкого распространения из-за сложности оценивания. Ав-торегрессию легче оценивать, так как выполнено предположение об отсутствии автокорреляции ошибок. В то же время, члены скользящего среднего приходит-ся оценивать методом максимального правдоподобия. Так как каждый обратимый процесс скользящего среднего может быть представлен в виде AR(∞), чистые ав-торегрессии могут приближать векторные процессы скользящего среднего, если
23.1. Векторная авторегрессия: формулировка и идентификация 655 добавить достаточное число лагов. Предполагается, что при этом ошибка не бу-дет автокоррелированной, что позволяет с приемлемой точностью моделировать временные ряды, описываемые моделью VARMA, при помощи авторегрессии до-статочно высокого порядка. Пусть xt -вектор-строка k изучаемых переменных, zt -вектор-строка независимых факторов (в него может входить константа, тренд, сезонные пере-менные и т.п.). Как и традиционные системы одновременных уравнений, модели векторной авторегрессии имеют две формы записи: структурную и приведенную. Структурная векторная авторегрессия (SVAR) p-го порядка-это модель следующего вида: xt = p j=0 xt−jΦj + ztA + εt, где (Φ0)ll = 0. Здесь Φj -матрица k×k коэффициентов авторегрессии для j-го лага xt, A - матрица коэффициентов при независимых факторах. Коэффициенты, относящие-ся к отдельному уравнению, стоят по столбцам этих матриц. Относительно матри-цы Φj предполагается, что ее диагональные элементы1 равны нулю, (Φ0)ll = 0, l = 1, . . . , k. Это означает, что отдельная переменная xlt не влияет сама на себя в тот же момент времени. При этом предполагается, что ковариационнаяматрица одновременных ошибок диагональна: var(εt) = diag(ω21, . . . , ω2k) = Ω. (23.1) Некоторые из коэффициентов здесь известны, поэтому такая модель называется структурной. Обозначим B = I − Φ0, Bll = 1. Тогда SVAR можно переписать как xtB = p j=1 xt−jΦj + ztA + εt. (23.2) 1Если матрица имеет индекс, то для обозначения ее элемента мы будем заключать матрицу в скоб-ки. Например, (A1)ij .
656 Глава 23. Векторные авторегрессии Структурная векторная регрессия фактически является "гибридом" моделей авторегрессии и систем одновременных уравнений. Соответственно, анализ таких моделей должен учитывать и динамические свойства, характерные для моделей авторегрессии, и черты, присущие системам одновременных уравнений. Уравнение структурной векторной авторегрессии представляет собой систе-му одновременных регрессионных уравнений, в которой среди факторов имеются лаги изучаемых переменных. Для того чтобы показать это в явном виде, введем следующие обозначения: z˜t = (xt−1, . . . , xt−p, zt) и A˜ = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ Φ1 ... Φp A ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . В таких обозначениях xtB = ˜zt A˜ + εt, или в матричной записи XB = Z˜A˜ + ε. Как и в случае систем одновременных уравнений, нельзя оценить параметры структурной формы методом непосредственно наименьших квадратов, поскольку, если матрица B недиагональна, найдутся уравнения, в которых будет более чем одна эндогенная переменная. В i-м уравнении системы будет столько же эндоген-ных переменных, сколько ненулевых элементов в i-м столбце матрицы B. Таким образом, в общем случае уравнения системы будут взаимозависимы, и, следова-тельно, оценки их по отдельности методом наименьших квадратов будут несостоя-тельными. Классический частный случай, в котором все-таки можно применять МНК- это случай рекурсивной системы. Рекурсивной является система одновременных уравнений, в которой матрица B является верхней треугольной, а матрица ко-вариаций ошибок Ω (23.1) является диагональной. Последнее условие в случае SVAR выполнено по определению. При этом первая переменная зависит только от экзогенных переменных, вторая переменная зависит только от первой и от экзо-генных переменных и т.д. Поскольку ошибки в разных уравнениях некоррелирова-ны, то каждая эндогенная переменная коррелирована только с ошибками из своего
23.1. Векторная авторегрессия: формулировка и идентификация 657 и предыдущих уравнений и не коррелирована с ошибками тех уравнений, в кото-рые она входит в качестве регрессора. Таким образом, ни в одном из уравнений не нарушаются предположенияМНК о некоррелированности ошибки и регрессоров, т.е. оценки МНК состоятельны. В общем случае, когда модель SVAR не обязательно рекурсивная, чтобы изба-виться от одновременных зависимостей, можно умножить2 23.2 справа на B−1: xt = p j=1 xt−jΦjB−1 + ztAB−1 + εtB−1. Далее, обозначим D = AB−1, Πj = ΦjB−1, vt = εtB−1. Это дает приведенную форму векторной авторегрессии: xt = p j=1 xt−jΠj + ztD + vt. Ковариационная матрица одновременных ошибок приведенной формы равна var(vt) = Σ. Она связана с ковариационной матрицей одновременных ошибок структурной формы Ω (см. 23.1) соотношением BΣB = Ω. Как и в случае обычных одновременных уравнений, при оценивании структур-ных векторных авторегрессий возникает проблема идентификации. Существует несколько типов идентифицирующих ограничений, которые можно использовать для решения этой проблемы. 1) Нормирующие ограничения, которые только закрепляют единицыизмерения коэффициентов. В данном случае в качестве нормирующих ограничений использу-ются ограничения Bll = 1 (диагональные элементы матрицы B равны 1). 2) Ограничения на коэффициенты структурных уравнений. Ограничения на ко-эффициенты бывают двух видов: ограничение на коэффициенты в пределах одного и того же уравнения (важный частный случай такого ограничения-исключение переменной из уравнения) и ограничение на коэффициенты нескольких уравнений. 3) Ограничения на ковариационную матрицу ошибок. В структурной вектор-ной авторегрессии используется крайний случай таких ограничений: матрица ко-вариаций ошибок Ω в этой модели диагональна (см. 23.1), т.е. так называемое ограничение ортогональности ошибок. 4) Долгосрочные ограничения. Это ограничения на долгосрочные взаимодей-ствия переменных, резюмируемые долгосрочным мультипликатором M, о котором речь пойдет ниже (см. 23.5). 2Мы исходим из предположения, что B -неособенная матрица.
658 Глава 23. Векторные авторегрессии В отличие от векторной авторегрессии, в классических системах одновремен-ных уравнений редко используют ограничения на ковариационную матрицу, а здесь они входят в определение модели, причем в виде жесткого ограничения ортого-нальности ошибок. Стандартные идентифицирующие ограничения, которые неявно подразумева-лись в ранних статьях по векторной авторегрессии, состоят в том, что матри-ца B является верхней треугольной. Это дает рекурсивную векторную авторе-грессию. Рекурсивную векторную авторегрессию можно оценить методом наименьших квадратов по причинам, о которых упоминалось выше. Другой способ состоит в том, чтобы оценить приведенную форму модели и восстановить из нее коэффициен-ты структурной формы. Для этого надо использовать так называемое разложение Холецкого (триангуляризацию) для ковариационной матрицы приведенной фор-мы: Σ = UΩU, гд е Ω -диагональная матрица с положительными элементами, U -верхняя треугольная матрица с единицами на диагонали.Естественно, вместо истинной матрицы Σ используют ее оценку. Тогда полученная матрица Ω будет оценкой ковариационной матрицы ошибок структурной формы, а U−1 -оценкой матрицы B. Однако использование рекурсивной векторной авторегрессии нежелательно, если только нет каких-либо оснований считать, что одновременные взаимодей-ствия между переменными действительно являются рекурсивными. Дело в том, что эти идентифицирующие ограничения совершенно произвольны и зависят от того, в каком порядке расположены переменные в векторе xt. В общем случае оценивание структурной VAR производят примерно теми же методами, что и оценивание одновременных уравнений. В частности, можно использовать метод максимального правдоподобия. Специфичность методов оце-нивания состоит в том, что они должны учитывать ограничение ортогональности ошибок. 23.2. Стационарность векторной авторегрессии Чтобы анализировать условия и следствия стационарности векторной авто-регрессии, удобно отвлечься от структурной формы этой модели и пользоваться приведенной формой. Для упрощения анализа мы без потери общности будем рас-сматривать векторную авторегрессию без детерминированных членов: xt = p j=1 xt−jΠj + vt,
23.2. Стационарность векторной авторегрессии 659 или в операторном виде3: xtI − p j=1ΠjLj= vt. Многие свойства процесса VAR(p)можно получить из свойств процесса VAR(1), если воспользоваться соответствующим представлением: ˜xt = ˜xt−1 ˜Π+ ˜vt, где вводятся следующие обозначения: ˜xt = (xt, xt−1, . . . , xt−p+1) , ˜vt = vt, 0k, . . . , 0kи ˜Π = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ Π1 Ik 0k×k - - - 0k×k Π2 0k×k Ik - - - 0k×k ... ... .... . . ... Πp−1 0k×k 0k×k - - - Ik Πp 0k×k 0k×k - - - 0k×k ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ . Используя рекуррентные подстановки ˜xt = (˜xt−2 ˜Π+ ˜vt)˜Π+ ˜vt = ˜xt−2˜Π2 + ˜vt ˜Π+ ˜vt, ˜xt = (˜xt−3 ˜Π+ ˜vt)˜Π2 + ˜vt ˜Π+ ˜vt = ˜xt−3˜Π3 + ˜vt˜Π2 + ˜vt ˜Π+ ˜vt и т.д., несложно получить для VAR(1) представление в виде бесконечного скользя-щего среднего: ˜xt = ˜vt + ˜vt ˜Π+ ˜vt˜Π2 + ˜vt˜Π3 + . . . = ∞i=0 ˜vt−i˜Πi. Для того чтобы этот ряд сходился, необходимо, чтобы его члены затухали, т.е. чтобы в пределе при i→∞ последовательность матриц ˜Πi стремилась к ну-лю. Для этого требуется, чтобы собственные значения матрицы ˜Πлежали внутри 3Здесь оператор стоит после переменной, на которую действует, чтобы не нарушать правила умножения матриц.
660 Глава 23. Векторные авторегрессии единичного круга. Собственные значения матрицы ˜Π, по определению, удовлетво-ряют уравнению: ˜Π− λITp= 0. Определитель в этой формуле можно выразить через матрицы Πj (доказательство этого требует довольно громоздких вычислений): ˜Π− λITp= (−1)TpIT λp − Π1λp−1 − . . . − Πp−1λ − Πp. Таким образом, уравнение для собственных значений эквивалентно следую-щему: IT λp − Π1λp−1 − . . . − Πp−1λ − Πp= 0. Процесс VAR(p) слабо стационарен тогда и только тогда, когда корни этого уравнения меньше единицы по абсолютной величине. Этиусловия стационарностиможнопереформулировать в терминахматричного характеристического многочлена процесса VAR(p), который равен Π(z) = I − p j=1Πjzj . Если возьмем определитель этого многочлена, то получится скалярный харак-теристический многочлен |Π(z)| =I − p j=1Πjzj. Он будет многочленом, поскольку определитель-это многочлен от своих эле-ментов. Соответствующее характеристическое уравнение имеет вид: |Π(z)| = 0. Условия стационарности состоят в том, что корни этого характеристического урав-нения лежат за пределами единичного круга. 23.3. Анализ реакции на импульсы Для содержательной интерпретации стационарной векторной авторегрессии следует выразить изучаемую переменную xt через ошибки εt структурной формы, которые, по определению модели, взаимно некоррелированы.
23.3. Анализ реакции на импульсы 661 Запишем приведенную форму модели (без детерминированных членов) с ис-пользованием лагового оператора L: xtI − p j=1ΠjLj= vt = εtB−1. В предположении стационарности процесса xt можно обратить лаговый поли-ном и получить xt = εtB−1I − p j=1ΠjLj−1. (23.3) Это дает представление в виде бесконечного скользящего среднего (представ-ление Вольда) для VAR: xt = ∞i=0 εt−iΨi. (23.4) Матрицы Ψi представляют так называемую функцию реакции на импуль-сы (IRF-impulse response function) для структурной векторной авторегрессии и могут быть символически записаны в виде Ψi = dxt dεt−i . Более точно, функция реакции на импульсы-это последовательность (Ψi)lr, i = 0, 1, 2, где l и r -индексы пары изучаемых переменных. Величина (Ψi)lr показывает, как влияет ошибка εtl (которая соответствует уравнению для пере-менной xtl) на переменную xtr при запаздывании на i периодов. Эти матрицы можно рассчитать рекуррентно: Ψi = p j=1Ψi−jΠj, i = 1, 2, . . . , начиная с Ψ0 = B−1 и Ψi = 0k×k, i<0. Накопленная реакция на импульсы определяется следующим образом: Ψs = s i=0 Ψi.
662 Глава 23. Векторные авторегрессии Она показывает суммарное запаздывающее влияние ошибок на изучаемую пе-ременную для всех лагов от 0 до некоторого s. Долгосрочное влияние резюмируется матрицей M, определяемой как M = lim s→∞ Ψs = ∞i=0 Ψi. (23.5) Эту матрицу называют долгосрочным мультипликатором. Ее также можно записать в виде M = B−1I − p j=1Πj−1. Последняя формула следует из того, что B−1I − p j=1Πj−1 = ∞i=0 ΨiLi (см. 23.3 и 23.3). Для того чтобы это показать, надо подставить 1 вместо L. 23.4. Прогнозирование с помощью векторной авторегрессии и разложение дисперсии Поскольку лаги исследуемых переменных полагаются величинами известны-ми, то построение прогнозов по ним в гораздо меньшей степени, чем в системах одновременных уравнений, осложняется проблемой получения точных значений факторов. Для упрощения формул мы будем исходить из того, что нам известны истин-ные параметры процесса. Пусть известны значения xt временного ряда VAR для t = 1, . . . , T. Сделаем прогноз на ( T +1)-й период. Это математическое ожидание xT+1, условное относительно имеющейся на момент T информации x1, . . . , xT . При расчетах удобно действовать так, как если бы была известна вся предыстория процесса: ΩT = (xT , . . . , x1, x0, . . . ). Выводы от этого не изменятся. Таким образом, будем использовать ожидания, условные относительно ΩT . Искомый прогноз равен xpT (1) = E(xT+1|ΩT) = p j=1E(xT+1−j |ΩT )Πj + E(vT+1|ΩT) = p j=1 xT+1−jΠj ,
23.4. Прогнозирование с помощью векторной авторегрессии 663 где мы воспользовались тем, что E(vT+1|ΩT) = 0 и что все xt в правой части уравнения регрессии входят в предысторию ΩT . Чтобы получить формулу прогноза на s периодов, возьмем от обеих частей уравнения для процесса VAR математическое ожидание, условное относительно ΩT . Получим xpT (s) = E(xT+s|ΩT) = p j=1E(xT+s−j |ΩT )Πj . По этой формуле прогнозы вычисляются рекуррентно, причем E(xt|ΩT) = xt при t T, и E(xt|ΩT) = xpT (t − T) при t > T. Заметим, что построение прогнозов не требует знания структурной формы мо-дели. Таким образом, чтобы построить прогноз, достаточно оценить приведенную форму без наложения ограничений обычнымМНК. Это делает VAR очень удобным инструментом прогнозирования: не требуется анализировать, как взаимосвязаны переменные, какая переменная на какую влияет. Ошибка прогноза-это ds = xT+s − xpT+s = xT+s − E(xT+s|ΩT ). Чтобы найти эту ошибку, воспользуемся разложением Вольда для xT+k: xT+s = ∞i=0 εT+s−iΨi. Очевидно, что ошибки εT+1, . . . , εT+s непредсказуемы и их ожидаемые зна-чения относительно ΩT равны нулю, поэтому xpT (s) = E(xT+s|ΩT) = ∞i=s εT+s−iΨi. Таким образом, ошибка прогноза равна: ds = s−1 i=0 εT+s−iΨi. Прогноз является несмещенным, поскольку E(d) = 0. Ковариационная матрица ошибки прогноза находится по формуле: Σds = E(dsds|ΩT) = E⎛⎝s−1 i=0 εT+s−iΨis−1 i=0 εT+s−iΨi|ΩT⎞⎠ = = s−1 i=0 ΨiEεT+s−iεT+s−i|ΩT Ψi = s−1 i=0 ΨiΩΨi.
664 Глава 23. Векторные авторегрессии При выводе формулы мы воспользовались тем, что ошибки структурной фор-мы εt не автокоррелированы и их ковариационная матрица равна var(εt) = Ω. Можно выразить ковариационную матрицу ошибки прогноза также и через кова-риационную матрицу ошибок приведенной формы: Σds = s−1 i=0 ΨiBΣBΨi. Поскольку в структурной форме ошибки разных уравнений некоррелированы, т.е. Ω = diag(ω21, . . . , ω2k), то можно разложить ковариационную матрицу ошибки прогноза на составляющие, соответствующие отдельным ошибкам εtl . Обозначим l-ю строку матрицы Ψi через ψil . Вектор ψil представляет собойфункциюреакции на импульсы влияния εt−i,l на все переменные xt. Тогд а Σds = s−1 i=0 ΨiΩΨi = s−1 i=0 k l=1 ψilω2l ψil = k l=1 s−1 i=0 ψilω2l ψil. Таким образом, ошибке l-го уравнения соответствует вкладв ковариационную матрицу ошибки прогноза равный s−1 i=0 ψilω2l ψil. Можно интерпретировать j-й диагональный элемент этой матрицы как вклад ошибки l-го уравнения εt,l в дисперсию ошибки прогноза j-й изучаемой пере-менной xt,j при прогнозе на s периодов. Обозначим этот вклад через σ2jl,s: σ2jl,s = s−1 i=0 ψilω2l ψiljj . Можем вычислить также долю каждой из ошибок в общей дисперсии ошибки прогноза j-й изучаемой переменной: R2jl,s = σ2jl,s kr=1 σ2jr,s . Набор этих долей R2jl,s, гд е l = 1, . . . , k, представляет собой так называе-мое разложение дисперсии ошибки прогноза для структурной векторной авторе-грессии.
23.5. Причинность по Грейнджеру 665 23.5. Причинность по Грейнджеру В эконометрике наиболее популярной концепцией причинности является при-чинность по Грейнджеру. Это связано, прежде всего, с ее относительной просто-той, а также с относительной легкостью определения ее на практике. Причинность по Грейнджеру применяется к компонентам стационарного век-торного случайного процесса: может ли одна из этих переменных быть причиной другой переменной. В основе определения лежит хорошо известный постулат, что будущее не может повлиять на прошлое. Этот постулат Грейнджер рассматривал в информационном аспекте. Для того чтобы определить, является ли переменная x причиной переменной y, следует выяснить, какую часть дисперсии текущего значения переменной y можно объяс-нить прошлыми значениями самой переменной y и может ли добавление прошлых значений переменной x улучшить это объяснение. Переменную x называют при-чиной y, если x помогает в предсказании y с точки зрения уменьшения дисперсии. В контексте векторной авторегрессии переменная x будет причиной y, если ко-эффициенты при лагах x статистически значимы. Заметим, что часто наблюдается двухсторонняя причинная связь: x является причиной y, и y является причиной x. Рассмотрим причинность по Грейнджеру для двух переменных. Приведенная форма модели имеет вид:xt = p j=1 ajxt−j + p j=1 bjyt−j + vt, yt = p j=1 cjxt−j + p j=1 djyt−j + wt. Отсутствие причинной связи от x к y означает, что cj = 0 при j = 1, . . . , p, т.е. что прошлые значения x не влияют на y . Отсутствие причинной связи от y к x означает, что bj = 0 при j = 1, . . . , p. Когда процесс стационарен, тогда гипотезы о причинной связи можно прове-рять с помощью F-статистики. Нулевая гипотеза заключается в том, что одна переменная не является причиной по Грейнджеру для другой переменной. Дли-ну лага p следует выбрать по самому дальнему лагу, который еще может помочь в прогнозировании. Следует понимать, что причинность по Грейнджеру-это не всегда то, что при-нято называть причинностью в общемсмысле.Причинность по Грейнджеру связана скорее с определением того, что предшествует чему, а также с информативностью переменной с точки зрения прогнозирования другой переменной.
666 Глава 23. Векторные авторегрессии 23.6. Коинтеграция в векторной авторегрессии Векторная авторегрессия представляет собой также удобный инструмент для моделирования нестационарных процессов и коинтеграции между ними (о ко-интеграции см. в гл. 17). Предположим, что в векторной авторегрессии xt, задаваемой уравнением xt = p j=1 xt−jΠj + vt, (23.6) отдельные составляющие процессы xtj либо стационарны, I(0), либо интегриро-ваны первого порядка, I(1). Рассмотрим в этой ситуации коинтеграцию CI(1, 0). Для упрощения забудем о том, что согласно точному определению коинтегриро-ванные вектора сами по себе должны быть нестационарными. Линейную комби-нацию стационарных процессов по этому упрощающему определению тоже будем называть коинтегрирующей комбинацией. Таким образом, будем называть коин-тегрирующим вектором рассматриваемого процесса векторной авторегрессии xt такой вектор c = 0, что xtc является I(0). Если векторный процесс состоит из более чем двух процессов, то может суще-ствовать несколько коинтегрирующих векторов. Поскольку коинтегрирующая комбинация-это линейная комбинация, то как следствие, любая линейная комбинация коинтегрирующих векторов, не равная нулю, есть опять коинтегрирующий вектор. В частности, если c1 и c2 -два ко-интегрирующих вектора и c = α1c1 + α2c2 = 0, то c -тоже коинтегрирующий вектор. Таким образом, коинтегрирующие вектора фактически образуют линейное подпространство с выколотым нулем, которое принято называть коинтегрирую-щим подпространством. Обозначим через β матрицу, соответствующую произвольному базису коинте-грирующего подпространства процесса xt. Это k × r матрица, где r -размер-ность коинтегрирующего подпространства. Размерность r называют рангом ко-интеграции. Столбцы β -это линейно независимые коинтегрирующие вектора. Для удобства анализа преобразуем исходную модель (23.6) векторной авторе-грессии. Всегда можно переписать векторную авторегрессию в форме векторной модели исправления ошибок (vector error-correction model, VECM): Δxt = xt−1Π+p−1 j=1Δxt−jΓj + vt, где Γj = − p i=j+1Πi, Π = −(I − p i=1Πi) = −Π(1).
23.6. Коинтеграция в векторной авторегрессии 667 При сделанных нами предположениях первые разности Δxt должны быть ста-ционарными. Отсюда следует, что процесс xt−1Π = Δxt − p−1 j=1Δxt−jΓj − vt ста-ционарен как линейная комбинация стационарных процессов. Это означает, что столбцы матрицы Π -это коинтегрирующие вектора (либо нулевые вектора). Любой такой вектор можно разложить по базису коинтегрирующего подпростран-ства, β. Составим из коэффициентов таких разложений k × r матрицу α, так что Π = βα. Ранг матрицы Π не может превышать r. Укажем без доказательства, что при сделанных предположениях ранг матрицы Π в точности равен r. Таким образом, мы получили следующую запись для векторной модели исправ-ления ошибок: Δxt = xt−1βα+ p−1 j=1Δxt−jΓj + vt, где α отвечает за скорость исправления отклонений от равновесия (матрица кор-ректирующих коэффициентов), β -матрица коинтеграционных векторов. Можно рассмотреть два крайних случая: r = 0 и r = k. Если r = 0, то Π = 0 и не существует стационарных линейных комбинаций процесса xt. Если r = k, то Π имеет полный ранг и любая комбинация xt стационарна, т.е. все составляющие процессы являются I(0). О собственно коинтеграции можно говорить лишь при 0 < r < k. До сих пор мы не вводили в модель детерминированные компоненты. Одна-ко, вообще говоря, можно ожидать, что в модель входят константы и линейные тренды, причем они могут содержаться как в самих рядах, так и в коинтеграцион-ных уравнениях. Рассмотрим векторную модель исправления ошибок с константой и трендом: Δxt = μ0 + μ1t + xt−1βα+ p−1 j=1Δxt−jΓj + vt, где μ0 и μ1 -вектора-строки длиной k. Вектор μ0 соответствует константам, а вектор μ1 -коэффициентам линейных трендов.Можновыделить пять основных случаев, касающихся статуса векторов μ0 и μ1 в модели. В таблице 23.1 они перечислены в порядке перехода от частного к более общему. Здесь γ0 и γ1 -вектора-строки длины r. Случай 0 легко понять-константы и тренды в модели полностью отсутствуют. В случае 1∗ константа входит в коинтеграционное пространство и тем самым в корректирующие механизмы, но не входит в сам процесс xt в виде дрейфа. Это
668 Глава 23. Векторные авторегрессии Таблица 23.1 Случай 0 μ0 = 0 μ1 = 0 Случай 1∗ μ0 = γ0αμ1 = 0 Случай 1 μ0 произвольный μ1 = 0 Случай 2∗ μ0 произвольный μ1 = γ1αСлучай 2 μ0 произвольный μ1 произвольный несложно увидеть, если переписать модель следующим образом: Δxt = (γ0 + xt−1β) α+ p−1 j=1Δxt−jΓj + vt. В случае 1 μ0 можно записать как μ0 = μ∗0+γ0α, гд е μ0 входит в коинтегра-ционное пространство, а μ∗0 соответствует дрейфу в векторной модели исправления ошибок: Δxt = μ∗0 + (γ0 + xt−1β) α+ p−1 j=1Δxt−jΓj + vt. Дрейф в модели исправления ошибок означает, что процесс xt содержит ли-нейный тренд4. Аналогичные рассуждения верны по отношению к временному тренду в случаях 2∗ и 2. В случае 2∗ тренд входит в коинтеграционное пространство, но не входит в xt в виде квадратичного тренда. В случае 2 тренд входит и в коинтеграционное пространство, и в xt в виде квадратичного тренда. 23.7. Метод Йохансена Наряду с методом Энгла-Грейнджера (см. п. 17.6), еще одним популярным ме-тодом нахождения стационарных комбинаций нестационарных переменных явля-ется метод Йо´ хансена. Этот метод, по сути дела, распространяет методику Дики- Фуллера (см. 17.4) на случай векторной авторегрессии. Помимо оценивания ко-интегрирующих векторов, методЙох ансена также позволяет проверить гипотезы о ранге коинтеграции (количестве коинтегрирующих векторов) и гипотезы о виде коинтегрирующих векторов. 4Это аналог ситуации для скалярного авторегрессионного процесса с дрейфом.
23.7. Метод Йохансена 669 Перечислим преимущества, которые дает метод Йохансена по сравнению с ме-тодом Энгла-Грейнджера: 1) Метод Энгла-Грейнджера применим, только когдамежду нестационарными переменными есть всего одно коинтегрирующее соотношение. Если ранг коинте-грации больше 1, то методд ает бессмысленные результаты. 2) МетодЭнгл а-Грейнджера статичен, в нем не учитывается краткосрочная динамика. 3) Результаты метода Йохансена не зависят от нормировки, использованной при оценивании, в то время как метод Энгла-Грейнджера может дать существенно отличающиеся результаты в зависимости от того, какая переменная стоит в левой части оцениваемой коинтеграционной регрессии. Пусть векторный процесс xt = (x1t, . . . , xkt) описывается векторной авторе-грессией p-го порядка, причем каждая из компонент является I(1) или I(0). Пред -полагается, что ошибки, относящиеся к разным моментам времени, независимы и распределены нормально с нулевым математическим ожиданием и ковариацион-ной матрицей Σ.Как указывалось выше,можно записать векторную авторегрессию в форме векторной модели исправления ошибок: Δxt = xt−1Π+p−1 j=1Δxt−jΓj + vt. В методе Йохансена оцениваемыми параметрами являются k × k матрицы коэффициентов Γj и Π, а также ковариационная матрица Σ.Имеяоценки Γj и Π, можно получить оценки коэффициентов приведенной формымодели по следующим формулам: Π1 = I +Γ1 +Π, Πj = Γj − Γj−1, j = 2, . . . , p − 1, Πp = −Γp−1. Ранг коинтеграции r считается известным. Ограничения на ранг коинтеграции задаются как ограничения на матрицу Π. Как сказано выше, в предположении, что ранг коинтеграции равен r, ранг матрицы Π тоже равен r, и эту матрицу можно представить в виде произведения двух матриц: Π = βα, где α и β имеют размерность k × r. Таким образом, в дальнейших выкладках используется представление: Δxt = xt−1βα+ p−1 j=1Δxt−jΓj + vt.
670 Глава 23. Векторные авторегрессии Матрица β состоит из коинтегрирующих векторов. Заметим, что если бы мат-рица β была известна (естественно, с точностью до нормировки), то отклонения от равновесия xtβ тоже были бы известны, и мы имели бы дело с линейными уравнениями регрессии, которые можно оценить посредствомМНК. В методе Йо-хансена исходят из того, что матрицу β требуется оценить. Для оценивания модели используется метод максимального правдоподобия. Плотность распределения ошибок vt по формуле для многомерного нормаль-ного распределения равна (2π)−k/2 |Σ|−1/2 e(−12 vtΣ−1vt). Обозначим через δ вектор, состоящий из параметров α, β и Γj . Для данного δ остатки модели равны vt(δ) = Δxt − xt−1βα− p−1 j=1Δxt−jΓj . Используя это обозначение, можем записать функцию правдоподобия: L(δ, Σ) = (2π)−kT/2 |Σ|−T/2 e(−12Tt=p vt(δ)Σ−1vt(δ)). Заметим, что это функция правдоподобия, в которой за неимением данных в сумме пропущены первые p наблюдений. Логарифмическая функция правдоподобия равна lnL(δ, Σ) = −kT 2 ln(2π) + T2 ln |Σ−1| − 12 T t=p vt(δ)Σ−1vt(δ). При данном δ максимум функции правдоподобия по Σ достигается при Σ = Σ(δ) = 1T T t=p vt(δ)vt(δ). Это можно доказать, дифференцируя логарифмическую функцию правдоподобия по Σ−1 (см. Приложение A.2.2). Можно показать, что для этой матрицы выполнено T t=p vt(δ)Σ−1(δ)vt(δ) = Tk.
23.7. Метод Йохансена 671 С учетом этого максимизация функции правдоподобия эквивалентна миними-зации определителя матрицы Σ(δ) по δ или T t=p vt(δ)vt(δ)→ min δ ! Тем самым мы получили, что метод максимального правдоподобия сводится к максимизации некоторой обобщенной суммы квадратов. По аналогии со сказан-ным выше ясно, что при данной матрице β можно получить оценки максимального правдоподобия для остальных неизвестных параметров обычнымметодом наимень-ших квадратов. Кроме того, при данных α, β можно получить оценки Γj методом наименьших квадратов из регрессий: Δxt − xt−pβα= p−1 j=1Δxt−jΓj + vt. Оценим отдельно регрессии Δxt и xt−p по переменным, стоящим в правой части данного уравнения: Δxt = p−1 j=1Δxt−jSj + r0t, xt−p = p−1 j=1Δxt−jTj + rpt, где Sj, Tj -коэффициенты регрессий. Получим из них остатки r0t и rpt. Отсюда при данных α и β получим остатки исходной модели: vt = vt(α, β) = r0t − rptβα. В этих обозначениях задача нахождения оценок приобретает вид: T t=p (r0t − rptβα)(r0t − rptβα)→ min α, β ! Для нахождения матриц α и β Йохансен использовал процедуру, известную как регрессия с пониженнымрангом.Онасостоит в следующем.На основе остатков r0t и rpt формируются выборочные ковариационные матрицы остатков: Mij = 1T T t=1 ritrjt, i,j= 0, p,
672 Глава 23. Векторные авторегрессии и задача переписывается в виде M00 − αβMp0 −M0pβα+ αβMppβα→ min α,β ! Минимизация по α при данном β дает α(β) = M0pβ(βMppβ)−1, откуда следует задача минимизации уже только по β: M00 −M0pβ(βMppβ)−1βMp0→ min β ! Очевидно, что отсюда нельзя однозначно найти β. Для нахождения β удобно ввести следующую нормировку: βMppβ = Ir. Оказывается, что с учетом данного нормирующего ограничения последняя задача минимизации эквивалентна следую-щей обобщенной задаче поиска собственных значений5: Mp0M−1 00 M0p − λMppc = 0, где c -собственный вектор, а собственные значения λ находятся как решения уравнения: Mp0M−1 00 M0p − λMpp= 0. Матрица β находится в виде собственных векторов, соответствующих r наи-большимсобственнымзначениям, где r -ранг коинтеграции.Пусть собственные числа упорядочены по убыванию, т.е. λ1 λ2 . . . λk. Тогда следует выбрать первые r этих чисел, λ1, . . . , λr. Столбцами матрицы β будут соответствующие вектора c1, . . . , cr . Обобщеннуюзадачу поиска собственных значений можно свести к стандартной задаче, если найти какую-либо матрицу M1/2 pp , являющуюся квадратным корнем матрицы Mpp, т.е. Mpp = M1/2 pp M1/2 pp : M−1/2 pp Mp0M−1 00 M0pM−1/2 pp − λIc= 0, 5Она напоминает метод наименьшего дисперсионного отношения для оценивания систем одно-временных уравнений (см. п. 10.3). Также это напрямую связано с так называемой теорией кано-нических корреляций. (См. Бриллинджер Д. Временные ряды. Обработка данных и теория.-М.: "Мир", 1980.)
23.7. Метод Йохансена 673 где c= M1/2 pp c. Напомним еще раз, что β определяется только с точностью до некоторой нор-мировки. Мы уже использовали нормировку βMppβ = Ir, которая выбрана из соображений удобства вычислений. Однако нормировку предпочтительнее выби-рать, исходя из экономической теории рассматриваемых процессов. Поэтому сле-дует умножить полученную оценку β справа на квадратную неособенную r × r матрицу, которая бы привела коинтеграционные вектора к более удобному для экономической интерпретации виду. После того как найдена оценка максимального правдоподобия для β, вычисля-ются оценки других параметров. Для этого можно использовать в обратном поряд-ке все те подстановки, которые мы сделали при упрощении задачи максимизации функции правдоподобия. Можно также использовать эквивалентную процедуру: сразу получить оценки α и Γj , применив метод наименьших квадратов к исходной регрессии. Для проверки гипотез о ранге коинтеграции r используется статистикаотноше-ния правдоподобия. Пусть нулевая гипотеза состоит в том, что ранг коинтеграции равен r0, а альтернативная гипотеза-что ранг коинтеграции равен ra ( ra > r0). Обозначим максимальное значение функции правдоподобия, полученное в предпо-ложении, что ранг равен r, через Lr. Тогда статистика отношения правдоподобия равна (см. раздел 18.3.3):LR(r0, ra) = 2(lnLra − lnLr0). Подставив в функцию правдоподобия полученные оценки, можно вывести сле-дующее выражение для максимального значения логарифма функции правдоподо-бия (с точностью до константы): lnLr = −T2 r i=1 ln(1 − λi) + const. Поэтому LR(r0, ra) = −T ra i=r0+1 ln(1 − λi). Особенно полезны с точки зрения поиска коинтеграционного ранга два частных случая статистики отношения правдоподобия. Статистика следа используется для проверки нулевой гипотезы о том, что ранг коинтеграции равен r, против альтернативной гипотезы о том, что ранг равен k (количеству переменных). Статистика имеет вид: LRtrace = LR(r, k) = −T k i=r+1 ln(1 − λi).
674 Глава 23. Векторные авторегрессии Проверка гипотез проводится последовательно для r = k − 1, . . . , 0 и за-канчивается, когда в первый раз не удается отклонить нулевую гипотезу. Можно проводить проверку гипотез и в обратном порядке r = 0, . . . , k −1. В этом случае она заканчивается, когда нулевая гипотеза будет отвергнута в первый раз. Можно также использовать статистику максимального собственного значе-ния, которая используется для проверки нулевой гипотезы о том, что ранг равен r, против альтернативной гипотезы о том, что ранг равен r+1. Эта статистика равна: LRλ−max = LR(r, r + 1) = −ln(1 − λr+1). Обе статистики имеют нестандартные асимптотические распределения. Эти асимптотические распределения не зависят от мешающих параметров, а зависят только от k−r, и от того, как входят в модель константа и тренд (см. перечисленные на стр. 668 пять основных случаев). Методом Монте-Карло получены таблицы LRtrace и LRλ−max для всех пяти случаев и нескольких значений k − r (на данный момент имеются таблицы для k − r = 1, . . . , 12). Также в последние годы разрабатываются различного рода аппроксимации для этих нестандартных распределений. Как и в случае критерия ADF (см. п. 17.4), очень важным вопросом является выбор длины лага p (порядка авторегрессии). Способы, по сути дела, являются теми же самыми. Для проверки гипотез о длине лага можно использовать критерий отношения правдоподобия, который в данном случае имеет обычное распределе-ние χ2. Если процесс состоит из k компонент, и проверяется гипотеза о том, что следует увеличить p на единицу, то количество степеней свободы соответствую-щей статистики равно k. Важно также, чтобы в выбранной модели отсутствовала автокорреляция остатков, поскольку это одно из предположений модели. Метод Йохансена можно использовать также для оценивания моделей с ли-нейными ограничениями на матрицу коинтегрирующих векторов β или на матрицу корректирующих коэффициентов α. Для проверки таких ограничений удобно ис-пользовать все тот же тест отношения правдоподобия, который здесь имеет обыч-ное асимптотическое распределение χ2. 23.8. Коинтеграция и общие тренды Рассмотрим модель VAR(1) с интегрированными переменными и ее представ-ление в виде модели исправления ошибок: Δxt = xt−1βα+ vt. (23.7) Поскольку матрица β состоит из коинтегрирующих векторов, то отклонения от равновесия xtβ являются I(0), и для них существует разложение Вольда. Выведем
23.8. Коинтеграция и общие тренды 675 его. Для этого сначала умножим уравнение модели на β: Δxtβ = xt−1βαβ + vtβ, откуда: xtβ = xt−1β(αβ + I) + vtβ. Имеем для стационарного вектора xtβ авторегрессию, из которойможемпред-ставить xtβ в виде бесконечного скользящего среднего (разложение Вольда): xtβ = ∞i=0 vt−iβ(αβ + I)i. Подставив это выражение в исходную модель (23.7), получим также разложение Вольда для приростов Δxt: Δxt = ∞i=1 vt−iβ(αβ + I)i−1α+ vt, или Δxt = ∞i=0 vt−iCi = vtC(L), где мы ввели обозначения Ci для матричных коэффициентов скользящего средне-го. Несложно подсчитать, пользуясь формулой бесконечной геометрической про-грессии, что соответствующий долгосрочный мультипликатор равен C(1) = I − β(αβ)−1α. (23.8) Обозначим C∗i = − ∞j=i+1Ci и C∗(L) = ∞i=0C∗i Li. При этом выполнено следующее разложение для C(L): C(L) = ΔC∗(L) + C(1). Кроме того, можно показать, что C∗(L) соответствует стационарному процессу. Все это позволяет разделить процесс Δxt на сумму двух составляющих: Δxt = ΔvtC∗(L) + vtC(1). Следовательно, xt можно представить следующим образом: xt = vtC∗(L) + v∗t C(1), (23.9)
676 Глава 23. Векторные авторегрессии где v∗t -это векторный процесс случайного блуждания, построенный на основе vt (проинтегрированный vt): vt = Δv∗t . Первое слагаемое в разложении xt (23.9) стационарно, а второе представляет собой линейную комбинацию процессов I(1). Если воспользоваться тождеством I = α⊥(β⊥α⊥)−1β⊥ + β(αβ)−1α, где α⊥ и β⊥ -это (k − r) × k матрицы полного ранга, такие что αα⊥ = 0 и ββ⊥ = 0, то матрицу C(1), опираясь на (23.8), можно представить как C(1) = α⊥(β⊥α⊥)−1β⊥. Таким образом, процесс xt можно представить в виде6: xt = vtC∗(L) + v∗t α⊥(β⊥α⊥)−1β⊥. Элементы вектора v∗t α⊥ являются общими стохастическими трендами. Отсюда видно, что k-мерныйVAR-процесс с рангом коинтеграции r можно выразить через k − r линейно независимых общих трендов. Матрица (β⊥α⊥)−1β⊥ содержит коэффициенты ("нагрузки") этих общих трендов. Представление через общие тренды служит основой для еще одного метода оценивания коинтеграционных регрессий-метода Стока-Уотсона. 23.9. Упражнения и задачи Упражнение 1 В таблице 23.2 приведены данные о потребительских расходах C и доходах Y в США в млрд. долл., очищенные от сезонности. 1.1. Нарисуйте график потребления и доходов. Что можно сказать об этих рядах по графикам? 1.2. Создайте первые разности логарифмов для обоих рядов. Нарисуйте график и сделайте выводы. 1.3. Предположим, что существует структурная зависимость между потреблени-ем и доходами. А именно, потребление C зависит от текущих доходов и, вследствие привычек, от лагов потребления: Ct = α1 + α2Yt + α3Ct−1 + εCt. 6Это так называемое разбиение на цикл и трендБ евериджа-Нельсона (см. п. 17.2).
23.9. Упражнения и задачи 677 Таблица 23.2. (Источник: Temple University Department of Economics Econometrics II Multivariate Time Series; 􀀀􀀀􀀀) Квартал C Y Квартал C Y Квартал C 1947.1 192.5 202.3 1952.1 220 231.1 1957.1 268.9 291.1 1947.2 196.1 197.1 1952.2 227.7 240.9 1957.2 270.4 294.6 1947.3 196.9 202.9 1952.3 223.8 245.8 1957.3 273.4 296.1 1947.4 197 202.2 1952.4 230.2 248.8 1957.4 272.1 293.3 1948.1 198.1 203.5 1953.1 234 253.3 1958.1 268.9 291.3 1948.2 199 211.7 1953.2 236.2 256.1 1958.2 270.9 292.6 1948.3 199.4 215.3 1953.3 236 255.9 1958.3 274.4 299.9 1948.4 200.6 215.1 1953.4 234.1 255.9 1958.4 278.7 302.1 1949.1 199.9 212.9 1954.1 233.4 254.4 1959.1 283.8 305.9 1949.2 203.6 213.9 1954.2 236.4 254.8 1959.2 289.7 312.5 1949.3 204.8 214 1954.3 239 257 1959.3 290.8 311.3 1949.4 209 214.9 1954.4 243.2 260.9 1959.4 292.8 313.2 1950.1 210.7 228 1955.1 248.7 263 1960.1 295.4 315.4 1950.2 214.2 227.3 1955.2 253.7 271.5 1960.2 299.5 320.3 1950.3 225.6 232 1955.3 259.9 276.5 1960.3 298.6 321 1950.4 217 236.1 1955.4 261.8 281.4 1960.4 299.6 320.1 1951.1 223.3 230.9 1956.1 263.2 282 1951.2 214.5 236.3 1956.2 263.7 286.2 1951.3 217.5 239.1 1956.3 263.4 287.7 1951.4 219.8 240.8 1956.4 266.9 291 В свою очередь, текущие доходы зависят от лагов доходов (из-за инерции) и от лагов потребления (по принципу мультипликатора): Yt = β1 + β2Yt−1 + β3Ct−1 + εYt. Оцените параметры структурной формы модели при помощи МНК по ис-ходным данным. Затем проделайте то же самое, используя преобразованные данные из пункта 1.2 (разности логарифмов). Объясните, имеют ли два по-лученных набора оценок одинаковый смысл. Какие оценки предпочтительнее и почему? 1.4. Перепишите модель в приведенной форме. Укажите взаимосвязь между ко-эффициентами структурной и приведенной форм.Оцените приведеннуюфор-му модели по исходным данным и по преобразованным данным. Какие оценки предпочтительнее и почему?
678 Глава 23. Векторные авторегрессии 1.5. Добавьте еще по одному лагу в оба уравнения приведенной формы. Оцените коэффициенты по исходным данным и по преобразованным данным и про-ведите тесты причинности по Грейнджеру. Что можно сказать о направлении причинности по полученным результатам? 1.6. Проверьте ряды на наличие единичных корней, используя тест Дики- Фуллера. 1.7. Примените к исходным данным и к логарифмам исходных данных метод Йо-хансена, используя в модели 4 лага разностей. Упражнение 2 В таблице 23.3 даны макроэкономические показатели по США(поквартальные данные получены на основе помесячных данных): Infl-темп инфляции, рассчи-танный по формуле: 400(ln(CPIt) − ln(CPIt−1)), где CPI-индекс потребитель-ских цен, т.е. логарифмический темп прироста цен в процентах из расчета за год; UnRate-уровень безработицы (процент населения); FedFunds-эффективная процентная ставка по межбанковским краткосрочным кредитам овернайт (в про-центах годовых). 2.1. Оцените параметры векторной авторегрессии 4-го порядка, предполагая, что текущая инфляция влияет на текущую безработицу и процентную ставку, а текущая безработица влияет на текущую процентную ставку (рекурсивная система). 2.2. Оцените модель в приведенной форме, пропуская последнее наблюдение, и получите точечный прогноз на один период. Сравните прогнозы с фактиче-скими значениями. Упражнение 3 В таблице 23.4 приведены данные из статьи Турмана и Фишера, посвящен-ной вопросу о том, что первично-куры или яйца. Это годовые данные по США за 1930-1983 гг. о производстве яиц в миллионах дюжин и о поголовье кур (ис-ключая бройлерные). 3.1. Проведите тест причинности по Грейнджеру между двумя рядами. Сделайте выводы относительно направления причинности. 3.2. Для каждого ряда проведите тест Дики-Фуллера на наличие единичных корней, используя лаги от 0 до 12. Выберите нужную длину лага, используя
23.9. Упражнения и задачи 679 Таблица 23.3. (Источник: The Federal Reserve Bank of St. Louis, database FRED II, 􀀀􀀀) Квартал Infl UnRate FedFunds 1954-3 -1.490 5.967 1.027 1954-4 0.000 5.333 0.987 1955-1 0.000 4.733 1.343 1955-2 -1.495 4.400 1.500 1955-3 2.985 4.100 1.940 1955-4 0.000 4.233 2.357 1956-1 0.000 4.033 2.483 1956-2 4.436 4.200 2.693 1956-3 2.930 4.133 2.810 1956-4 2.909 4.133 2.927 1957-1 4.324 3.933 2.933 1957-2 2.857 4.100 3.000 1957-3 2.837 4.233 3.233 1957-4 2.817 4.933 3.253 1958-1 5.575 6.300 1.863 1958-2 0.000 7.367 0.940 1958-3 0.000 7.333 1.323 1958-4 1.382 6.367 2.163 1959-1 0.000 5.833 2.570 1959-2 1.377 5.100 3.083 1959-3 2.740 5.267 3.577 1959-4 1.363 5.600 3.990 1960-1 0.000 5.133 3.933 1960-2 2.712 5.233 3.697 1960-3 0.000 5.533 2.937 1960-4 2.694 6.267 2.297 1961-1 0.000 6.800 2.003 1961-2 0.000 7.000 1.733 1961-3 2.676 6.767 1.683 1961-4 0.000 6.200 2.400 1962-1 2.658 5.633 2.457 1962-2 0.000 5.533 2.607 1962-3 2.640 5.567 2.847 Квартал Infl UnRate FedFunds 1962-4 0.000 5.533 2.923 1963-1 1.314 5.767 2.967 1963-2 1.309 5.733 2.963 1963-3 1.305 5.500 3.330 1963-4 2.597 5.567 3.453 1964-1 0.000 5.467 3.463 1964-2 1.292 5.200 3.490 1964-3 1.288 5.000 3.457 1964-4 2.564 4.967 3.577 1965-1 0.000 4.900 3.973 1965-2 3.816 4.667 4.077 1965-3 0.000 4.367 4.073 1965-4 3.780 4.100 4.167 1966-1 3.744 3.867 4.557 1966-2 2.477 3.833 4.913 1966-3 4.908 3.767 5.410 1966-4 1.218 3.700 5.563 1967-1 1.214 3.833 4.823 1967-2 3.620 3.833 3.990 1967-3 3.587 3.800 3.893 1967-4 4.734 3.900 4.173 1968-1 3.514 3.733 4.787 1968-2 4.638 3.567 5.980 1968-3 4.585 3.533 5.943 1968-4 5.658 3.400 5.917 1969-1 5.579 3.400 6.567 1969-2 5.502 3.433 8.327 1969-3 5.427 3.567 8.983 1969-4 6.417 3.567 8.940 1970-1 6.316 4.167 8.573 1970-2 5.188 4.767 7.880 1970-3 4.103 5.167 6.703 1970-4 6.076 5.833 5.567 
680 Глава 23. Векторные авторегрессии Таблица 23.3. (продолжение) Квартал Infl UnRate FedFunds 1971-1 2.005 5.933 3.857 1971-2 4.969 5.900 4.563 1971-3 2.952 6.033 5.473 1971-4 2.930 5.933 4.750 1972-1 2.909 5.767 3.540 1972-2 2.888 5.700 4.300 1972-3 3.819 5.567 4.740 1972-4 3.783 5.367 5.143 1973-1 8.382 4.933 6.537 1973-2 7.306 4.933 7.817 1973-3 8.949 4.800 10.560 1973-4 9.618 4.767 9.997 1974-1 12.753 5.133 9.323 1974-2 9.918 5.200 11.250 1974-3 12.853 5.633 12.090 1974-4 10.147 6.600 9.347 1975-1 6.877 8.267 6.303 1975-2 5.268 8.867 5.420 1975-3 8.141 8.467 6.160 1975-4 7.260 8.300 5.413 1976-1 2.867 7.733 4.827 1976-2 4.969 7.567 5.197 1976-3 6.299 7.733 5.283 1976-4 5.517 7.767 4.873 1977-1 8.136 7.500 4.660 1977-2 5.995 7.133 5.157 1977-3 5.255 6.900 5.820 1977-4 6.473 6.667 6.513 1978-1 7.001 6.333 6.757 1978-2 9.969 6.000 7.283 1978-3 9.126 6.033 8.100 1978-4 8.334 5.900 9.583 1979-1 11.612 5.867 10.073 Квартал Infl UnRate FedFunds 1979-2 12.950 5.700 10.180 1979-3 12.006 5.867 10.947 1979-4 13.220 5.967 13.577 1980-1 16.308 6.300 15.047 1980-2 11.809 7.333 12.687 1980-3 6.731 7.667 9.837 1980-4 11.745 7.400 15.853 1981-1 10.058 7.433 16.570 1981-2 8.487 7.400 17.780 1981-3 11.330 7.400 17.577 1981-4 4.274 8.233 13.587 1982-1 2.542 8.833 14.227 1982-2 9.599 9.433 14.513 1982-3 2.876 9.900 11.007 1982-4 0.000 10.667 9.287 1983-1 1.634 10.367 8.653 1983-2 5.266 10.133 8.803 1983-3 4.004 9.367 9.460 1983-4 3.964 8.533 9.430 1984-1 5.874 7.867 9.687 1984-2 3.098 7.433 10.557 1984-3 3.839 7.433 11.390 1984-4 3.045 7.300 9.267 1985-1 4.899 7.233 8.477 1985-2 2.613 7.300 7.923 1985-3 2.226 7.200 7.900 1985-4 5.147 7.033 8.103 1986-1 -1.464 7.033 7.827 1986-2 1.098 7.167 6.920 1986-3 2.188 6.967 6.207 1986-4 2.899 6.833 6.267 1987-1 5.022 6.600 6.220 1987-2 4.608 6.267 6.650 
23.9. Упражнения и задачи 681 Таблица 23.3. (продолжение) Квартал Infl UnRate FedFunds 1987-3 4.207 6.000 6.843 1987-4 3.126 5.833 6.917 1988-1 3.102 5.700 6.663 1988-2 5.117 5.467 7.157 1988-3 5.053 5.467 7.983 1988-4 3.997 5.333 8.470 1989-1 4.940 5.200 9.443 1989-2 6.171 5.233 9.727 1989-3 2.250 5.233 9.083 1989-4 4.779 5.367 8.613 1990-1 7.219 5.300 8.250 1990-2 4.023 5.333 8.243 1990-3 7.927 5.700 8.160 1990-4 5.099 6.133 7.743 1991-1 1.784 6.600 6.427 1991-2 3.545 6.833 5.863 1991-3 2.930 6.867 5.643 1991-4 3.488 7.100 4.817 1992-1 2.596 7.367 4.023 1992-2 2.865 7.600 3.770 1992-3 2.845 7.633 3.257 1992-4 3.387 7.367 3.037 1993-1 2.801 7.133 3.040 1993-2 2.782 7.067 3.000 1993-3 1.936 6.800 3.060 1993-4 3.570 6.633 2.990 1994-1 2.181 6.567 3.213 1994-2 2.169 6.200 3.940 1994-3 3.769 6.000 4.487 1994-4 2.138 5.633 5.167 1995-1 2.921 5.467 5.810 1995-2 3.162 5.667 6.020 1995-3 1.833 5.667 5.797 Квартал Infl UnRate FedFunds 1995-4 2.085 5.567 5.720 1996-1 4.137 5.533 5.363 1996-2 3.075 5.500 5.243 1996-3 2.545 5.267 5.307 1996-4 3.535 5.333 5.280 1997-1 1.756 5.233 5.277 1997-2 1.000 5.000 5.523 1997-3 2.489 4.867 5.533 1997-4 1.486 4.667 5.507 1998-1 0.494 4.633 5.520 1998-2 1.970 4.400 5.500 1998-3 1.716 4.533 5.533 1998-4 2.196 4.433 4.860 1999-1 0.972 4.300 4.733 1999-2 3.143 4.267 4.747 1999-3 4.073 4.233 5.093 1999-4 2.377 4.067 5.307 2000-1 5.180 4.033 5.677 2000-2 3.029 3.967 6.273 2000-3 3.007 4.067 6.520 2000-4 2.298 3.933 6.473 2001-1 3.195 4.167 5.593 2001-2 4.295 4.467 4.327 2001-3 0.449 4.833 3.497 2001-4 -1.801 5.600 2.133 2002-1 2.698 5.633 1.733 2002-2 2.903 5.833 1.750 2002-3 2.440 5.767 1.740 2002-4 1.545 5.900 1.443 2003-1 5.034 5.767 1.250 2003-2 -0.653 6.167 1.247 2003-3 3.039 6.133 1.017 
682 Глава 23. Векторные авторегрессии Таблица 23.4. (Источник: Thurman, Walter N. and Mark E. Fisher, "Chickens, Eggs, and Causality", American Journal of Agricultural Economics 70 (1988), p. 237-238.) куры яйца куры яйца 1930 468491 3581 1957 391363 5442 1931 449743 3532 1958 374281 5442 1932 436815 3327 1959 387002 5542 1933 444523 3255 1960 369484 5339 1934 433937 3156 1961 366082 5358 1935 389958 3081 1962 377392 5403 1936 403446 3166 1963 375575 5345 1937 423921 3443 1964 382262 5435 1938 389624 3424 1965 394118 5474 1939 418591 3561 1966 393019 5540 1940 438288 3640 1967 428746 5836 1941 422841 3840 1968 425158 5777 1942 476935 4456 1969 422096 5629 1943 542047 5000 1970 433280 5704 1944 582197 5366 1971 421763 5806 1945 516497 5154 1972 404191 5742 1946 523227 5130 1973 408769 5502 1947 467217 5077 1974 394101 5461 1948 499644 5032 1975 379754 5382 1949 430876 5148 1976 378361 5377 1950 456549 5404 1977 386518 5408 1951 430988 5322 1978 396933 5608 1952 426555 5323 1979 400585 5777 1953 398156 5307 1980 392110 5825 1954 396776 5402 1981 384838 5625 1955 390708 5407 1982 378609 5800 1956 383690 5500 1983 364584 5656 
23.9. Упражнения и задачи 683 информационные критерии Акаике и Шварца. Сделайте вывод о том, явля-ются ли ряды стационарными. Как полученный результат может повлиять на интерпретацию результатов упражнения 3.1? 3.3. Проверьте с помощью методов Энгла-Грейнджера иЙохансена, коинтегри-рованы ли ряды. Как полученный результат может повлиять на интерпрета-цию результатов упражнения 3.1? Упражнение 4 Втаблице 23.5 приведеныпоквартальныемакропоказатели по Великобритании из обзорной статьи Мускателли и Хурна: M -величина денежной массы (агрегат M1); Y -общие конечные расходы на товары и услуги (TFE) в постоянных ценах (переменная, моделирующая реальные доходы); P -дефлятор TFE (индекс цен); R -ставка по краткосрочным казначейским векселям (переменная, соответству-ющая альтернативной стоимости хранения денег). Изучается связь между тремя переменными: lnM − ln P -реальная денежная масса (в логарифмах); ln Y - реальный доход (в логарифмах); R -процентная ставка. 4.1. Найдите ранг коинтеграции и коинтегрирующие вектора методом Йохансе-на, используя векторную модель исправления ошибок (VECM) с четырьмя разностями в правой части и с сезонными фиктивными переменными. Сде-лайте это при разных возможных предположениях о том, как входят в модель константа и тренд: а) константа входит в коинтеграционное пространство, но не входит в мо-дель исправления ошибок в виде дрейфа; б) константа входит в коинтеграционное пространство, а также в модель исправления ошибок в виде дрейфа, так что данные содержат тренд; в) тренд входит в коинтеграционное пространство, но данные не содержат квадратичный тренд. 4.2. С помощью тестов отношения правдоподобия определить, как должны вхо-дить в модель константа и тренд. Убедитесь в том, что случай 4.1а, который рассматривался в статье Мускателли и Хурна, отвергается тестами. 4.3. На основе найденного коинтегрирующего вектора оцените остальные коэф-фициенты VECM. Рассмотрите полученные коэффициенты модели и сде-лайте выводо том, насколько они соответствуют экономической теории. (Подсказка: интерпретируйте коинтегрирующую комбинацию как уравнение спроса на деньги и обратите внимания на знак при ln Y ).
684 Глава 23. Векторные авторегрессии Таблица 23.5. (Источник: Muscatelli, V.A., Hurn, S. Cointegration and Dynamic Time Series Models. Journal of Economic Surveys 6 (1992, No. 1), 1-43.) Квартал lnM R lnY lnP Квартал lnM R lnY lnP 1963-1 8.8881 0.0351 10.678 -1.635 1974-1 9.5017 0.1199 11.068 -0.95787 1963-2 8.9156 0.0369 10.746 -1.6189 1974-2 9.5325 0.1136 11.103 -0.89963 1963-3 8.9305 0.0372 10.752 -1.6205 1974-3 9.5584 0.1118 11.125 -0.85112 1963-4 8.9846 0.0372 10.789 -1.6028 1974-4 9.6458 0.1096 11.139 -0.8025 1964-1 8.9584 0.0398 10.756 -1.6045 1975-1 9.6438 0.1001 11.066 -0.73806 1964-2 8.9693 0.0436 10.8 -1.5843 1975-2 9.6742 0.0938 11.074 -0.6802 1964-3 8.9911 0.0462 10.802 -1.5813 1975-3 9.7275 0.1017 11.088 -0.63297 1964-4 9.0142 0.0547 10.838 -1.5677 1975-4 9.7689 0.1112 11.124 -0.59904 1965-1 8.9872 0.0651 10.779 -1.5557 1976-1 9.787 0.0904 11.092 -0.56265 1965-2 9.0005 0.0612 10.818 -1.5438 1976-2 9.8141 0.1019 11.105 -0.52675 1965-3 9.0116 0.0556 10.832 -1.5408 1976-3 9.8641 0.1126 11.134 -0.49414 1965-4 9.0506 0.0545 10.851 -1.5272 1976-4 9.8765 0.1399 11.172 -0.45642 1966-1 9.0394 0.0556 10.814 -1.519 1977-1 9.8815 0.112 11.112 -0.41982 1966-2 9.0336 0.0565 10.838 -1.5037 1977-2 9.9238 0.0772 11.122 -0.38322 1966-3 9.0438 0.0658 10.849 -1.4964 1977-3 10.001 0.0655 11.14 -0.36159 1966-4 9.0491 0.0662 10.86 -1.4845 1977-4 10.071 0.0544 11.177 -0.35041 1967-1 9.0388 0.06 10.841 -1.4842 1978-1 10.097 0.0597 11.143 -0.32276 1967-2 9.055 0.053 10.874 -1.4769 1978-2 10.117 0.0949 11.165 -0.29942 1967-3 9.0975 0.0544 10.881 -1.4726 1978-3 10.168 0.0938 11.182 -0.27619 1967-4 9.1326 0.0657 10.9 -1.4661 1978-4 10.223 0.1191 11.203 -0.25279 1968-1 9.1029 0.074 10.89 -1.4459 1979-1 10.222 0.1178 11.159 -0.22607 1968-2 9.1204 0.0714 10.901 -1.4285 1979-2 10.236 0.1379 11.215 -0.1898 1968-3 9.1351 0.0695 10.929 -1.4141 1979-3 10.274 0.1382 11.221 -0.13856 1968-4 9.1733 0.0666 10.961 -1.4044 1979-4 10.304 0.1649 11.242 -0.10048 1969-1 9.1182 0.0718 10.891 -1.3898 1980-1 10.274 0.1697 11.199 -0.05655 1969-2 9.0992 0.0783 10.932 -1.3825 1980-2 10.293 0.1632 11.171 -0.01134 1969-3 9.1157 0.0782 10.943 -1.3697 1980-3 10.294 0.1486 11.187 0.02109 1969-4 9.1744 0.0771 10.979 -1.3573 1980-4 10.343 0.1358 11.188 0.04439 1970-1 9.1371 0.0754 10.905 -1.3368 1981-1 10.356 0.1187 11.144 0.06282 1970-2 9.178 0.0689 10.962 -1.3168 1981-2 10.39 0.1224 11.144 0.09367 1970-3 9.1981 0.0683 10.971 -1.2939 1981-3 10.407 0.1572 11.191 0.11541 1970-4 9.2643 0.0682 11.017 -1.2791 1981-4 10.506 0.1539 11.206 0.13245 1971-1 9.2693 0.0674 10.938 -1.2585 1982-1 10.501 0.1292 11.175 0.14663 1971-2 9.2836 0.0567 10.99 -1.2345 1982-2 10.526 0.1266 11.172 0.1703 1971-3 9.3221 0.0539 11.01 -1.2132 1982-3 10.551 0.1012 11.193 0.1829 1971-4 9.3679 0.0452 11.044 -1.1992 1982-4 10.613 0.0996 11.22 0.19315 1972-1 9.3742 0.0436 10.98 -1.1853 1983-1 10.639 0.1049 11.209 0.21273 1972-2 9.4185 0.0459 11.025 -1.1707 1983-2 10.664 0.0951 11.195 0.2242 1972-3 9.4356 0.0597 11.02 -1.1439 1983-3 10.675 0.0917 11.244 0.23412 1972-4 9.4951 0.0715 11.1 -1.1282 1983-4 10.719 0.0904 11.268 0.24285 1973-1 9.4681 0.0813 11.093 -1.1044 1984-1 10.754 0.0856 11.241 0.25738 1973-2 9.5347 0.0754 11.102 -1.0913 1984-2 10.798 0.0906 11.233 0.27467 1973-3 9.5119 0.1025 11.117 -1.0493 1984-3 10.827 0.1024 11.268 0.28672 1973-4 9.5445 0.1162 11.141 -1.0048 1984-4 10.862 0.0933 11.317 0.29773 
23.9. Упражнения и задачи 685 Задачи 1. Рассмотрите приведенную форму процесса VAR(1): (xt, yt) = (xt−1, yt−1)⎛⎜⎝ 0.2 0.4 0.2 0 ⎞⎟⎠+ (vxt, vyt) , где ошибки vxt, vyt не автокоррелированы и их ковариационная матрица равна ⎛⎜⎝ 1 −0.5 −0.5 4.25 ⎞⎟⎠. а) Является ли процесс стационарным? б) Найдите структурную форму модели (матрицу коэффициентов и кова-риационную матрицу), если известно, что она является рекурсивной (yt входит в уравнение для xt, но xt не входит в уравнение для yt). в) Найдите (матричный) долгосрочный мультипликатор. 2. Векторная регрессия с двумя переменными xt, yt задается следующимиурав-нениями: xt = αxt−1 + βyt−1 + vt, yt = γxt−1 + δyt−1 + wt. Ковариационная матрица ошибок vt,wt имеет вид: Σ = ⎛⎜⎝ 1 ρ ρ 1 ⎞⎟⎠, где |ρ| < 1. а) Запишите модель в матричном виде. б) Представьте матрицу Σ в вид еΣ = UΩU, гд е U -верхняя треуголь-ная матрица, Ω - диагональная матрица с положительными диаго-нальными элементами. в) Умножьте уравнение модели справа на матрицу U. Что можно сказать о получившемся представлении модели? г) Повторите задание, поменяв порядок переменных yt, xt. Сравните и сделайте выводы.
686 Глава 23. Векторные авторегрессии 3. Предположим, что темпы прироста объемов производства, yt, и денежной массы, mt, связаны следующими структурными уравнениями: mt = αmt−1 + εmt, yt = βmt + γmt−1 + δyt−1 + εyt, где ошибки εmt, εyt не автокоррелированы, не коррелированы друг с другом, а их дисперсии равны σ2mи σ2y , соответственно. а) Запишите структурные уравнения в стандартном матричномвидемодели SVAR. б) Запишите модель в приведенной форме. в) Какой видимеет функция реакции на импульсы для монетарных шо-ков εmt и шоков производительности εyt? Как эта функция связана с представлением модели в виде бесконечного скользящего среднего (разложением Вольда)? г) Найдите (матричный) долгосрочный мультипликатор. 4. Рассмотрите двумерную векторную авторегрессию первого порядка: (x1t, x2t) = (x1, t−1, x2, t−1)⎛⎜⎝ π11 π12 π21 π22 ⎞⎟⎠+ (v1t, v2t) , где ошибки v1t, v2t являются белым шумом и независимы между собой. а) При каких параметрах модель является рекурсивной? Объясните. б) При каких параметрах x1t и x2t представляют собой два независимых случайных блуждания? Объясните. в) Известно, что x1t не является причиной x2t в смысле Грейнджера. Какие ограничения этот факт накладывает на параметры? 5. Рассмотрите авторегрессию второго порядка: xt = ϕ1xt−1 +ϕ2xt−2 +εt, гд е ошибка εt представляет собой белый шум. а) Обозначьте xt−1 = yt и запишите данную модель в виде векторной авторегрессии первого порядка для переменных xt и yt. б) Чему равна ковариационная матрица одновременных ошибок в полу-чившейся векторной авторегрессии? в) Сопоставьте условия стационарности исходной модели AR(2) и полу-ченной модели VAR(1).
23.9. Упражнения и задачи 687 6. Представьте векторную авторегрессию второго порядка в виде векторной авторегрессии первого порядка (с расшифровкой обозначений). 7. Рассмотрите двумерную модель VAR(1): (x1t, x2t) = (x1, t−1, x2, t−1)Π + vt, где Π = ⎛⎜⎝ 12 1 0 14 ⎞⎟⎠. а) Найдите корни характеристического многочлена, соответствующего этой модели. Является ли процесс стационарным? б) Найдите собственные числа матрицы Π. Как они связаны с корнями характеристического многочлена, найденными в пункте (a)? 8. Рассмотрите векторную авторегрессию первого порядка: (x1t, x2t) = (x1, t−1, x2, t−1)⎛⎜⎝ π11 π12 π21 π22 ⎞⎟⎠+ (v1t, v2t) . При каких параметрах процесс является стационарным: а) π11 = 1, π12 = 0.5, π21 = 0, π22 = 1; б) π11 = 0.3, π12 = 0.1, π21 = −0.1, π22 = 0.5; в) π11 = 2, π12 = 0, π21 = 1, π22 = 0.5; г) π11 = 0.5, π12 = −1, π21 = 1, π22 = 0.5; д) π11 = 0.5, π12 = −1, π21 = 1, π22 = 0.5; е) π11 = 0.3, π12 = −0.2, π21 = 0.2, π22 = 0.3? Аргументируйте свой ответ. 9. В стране чудес динамика темпа прироста ВВП, yt, темпа прироста денежной массы M2, mt, и ставки процента, rt, описывается следующей моделью
688 Глава 23. Векторные авторегрессии VAR(2): (yt,mt, rt) = (2, 1, 0) + (yt−1,mt−1, rt−1)⎛⎜⎜⎜⎜⎜⎝ 0.7 0 0.9 0.1 0.4 0 0 0.1 0.8 ⎞⎟⎟⎟⎟⎟⎠ + + (yt−2,mt−2, rt−2)⎛⎜⎜⎜⎜⎜⎝ −0.2 0 0 0 0.1 0 0 0.1 0 ⎞⎟⎟⎟⎟⎟⎠ + (vyt, vmt, vrt) , где ошибки vyt, vmt, vrt представляют собой белый шум. а) Покажите, что все три переменные являются стационарными. б) Найдите безусловные математические ожидания этих переменных. в) Запишите модель в виде векторной модели исправления ошибок. 10. Опишите поэтапно возможную процедуру построения прогнозов для вектор-ной авторегрессии. Необходимо ли для построения прогноза знать ограни-чения, накладываемые структурной формой? (Объясните.) На каком этапе построения прогноза можно было бы учесть структурные ограничения? 11. Объясните различие между структурной и приведенной формой векторной авторегрессии. В чем причина того, что разложение дисперсии ошибки про-гноза основывают на структурной форме, а не на приведенной форме? 12. Рассмотрите векторный процесс (xt, yt): xt = xt−1 + εt, yt = λxt + ϕyt−1 + ξt (λ = 0, |ϕ| < 1). а) Покажите, что xt и yt являются коинтегрированными CI(1, 0). Укажи-те ранг коинтеграции и общий видк оинтегрирующих векторов. б) Запишите процесс в виде векторной модели исправления ошибок. Ука-жите соответствующую матрицу корректирующих коэффициентов и матрицу коинтегрирующих векторов. 13. Пусть в векторной модели исправления ошибок константа входит в коин-теграционное пространство. Какие ограничения это налагает на параметры модели?
23.9. Упражнения и задачи 689 14. На примере векторной авторегрессии первого порядка с двумя переменными, коинтегрированными как CI(1, 0), покажите, что наличие константы (дрей-фа) в коинтеграционном пространстве означает, что переменные содержат линейный тренд. 15. Пусть в векторной модели исправления ошибок Δxt = xt−1Π+p−1 j=1Δxt−jΓj + vt матрица Π = ⎛⎜⎜⎜⎜⎜⎝ 1 6 1 2 4 2 3 2 1 ⎞⎟⎟⎟⎟⎟⎠ . Найдите ранг коинтеграции, матрицу коинтегрирующих векторов β и мат-рицу корректирующих коэффициентов α. 16. Объясните, почему процедура Йохансена позволяет не проверять перемен-ные на наличие единичных корней. 17. Пусть в векторной модели исправления ошибок Δxt = xt−1Π+p−1 j=1Δxt−jΓj + vt коинтегрирующие векторы равны (1; −1; 0) и (1; 1; 1), а матрица коррек-тирующих коэффициентов равна α = ⎛⎜⎜⎜⎜⎜⎝ 1 0 0 2 0 −1 ⎞⎟⎟⎟⎟⎟⎠ . Найдите матрицу Π. Рекомендуемая литература 1. Amisano Gianni, Carlo Giannini. Topics in Structural VAR Econometrics, 2nd ed.-Springer, 1997.
690 Глава 23. Векторные авторегрессии 2. Banerjee A., J.J.Dolado, J.W.GalbraithandD.F.Hendry, Co-integration, Error Correction and the Econometric Analysis of Non-stationary Data.-Oxford University Press, 1993. (Ch. 5, 8.) 3. Canova F. "VAR Models: Specification, Estimation, Inference and Forecasting" in H. Pesaran and M. Wickens (eds.) Handbook of Applied Econometrics.- Basil Blackwell, 1994. 4. Granger C. W. J. Investigating Causal Relations by Econometric Models and Cross-Spectral Methods. //Econometrica, 37 (1969), 424-438. 5. GreeneW.H. Econometric Analysis.-Prentice-Hall, 2000. (Ch. 17, 18). 6. Hamilton, J.D. Time Series Analysis.-PrincetonUniversity Press, 1994. (Ch. 10, 11). 7. Johansen S. Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models. //Econometrica, 59 (1991), 1551-1580. 8. Lutkepohl Helmut. Introduction to Multiple Time Series Analysis, second edition.-Berlin: Springer, 1993. (Ch. 2, 10, 11). 9. Muscatelli V.A. and Hurn S. Cointegration and dynamic time series models. //Journal of Economic Surveys, 6 (1992), 1-43. 10. Sims C. A. Macroeconomics and Reality. //Econometrica, 48 (1980), 1-48. 11. Stock J.H. and Watson M.W. Testing for Common Trends. //Journal of the American Statistical Association, 83 (1988), 1097-1107. 12. Watson Mark W. Vector Autoregressions and Cointegration. //Handbook of Econometrics, Vol. IV. Robert Engle and Daniel McFadden, eds. Elsevier, 1994, 2844-2915. 13. Mills TerenceC. Time Series Techniques for Economists.-Cambridge University Press, 1990. (Ch. 14). 14. Mills Terence C. The Econometric Financial Modelling Time Series.-Cambridge University Press, 1999. (Ch. 7, 8).
Приложение A Вспомогательные сведения из высшей математики A.1. Матричная алгебра A.1.1. Определения x = {xi}i=1, ..., n = ⎛⎜⎜⎜⎜⎜⎝ x1... xn⎞⎟⎟⎟⎟⎟⎠ называется вектор-столбцом размерности n. x = (x1, , . . . , xn) называется вектор-строкой размерности n. A = {aij} i=1, ...,m j=1, ..., n = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ a11 a12 . . . a1n a21 a22 . . . a2n ... ... ... am1 am2 . . . amn ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ называется матрицей размерности m × n. 691
692 Приложение A. Вспомогательные сведения из высшей математики * Сумма матриц A и B (m × n): C = A + B = {aij + bij}, C (m × n). * Произведение матриц A (m × n) и B (n × k): C = AB = n t=1 aitbtj;, C (m × k). * Скалярное произведение вектор-столбцов a (m × 1) и b (m × 1): ab = mi=1 aibi. * Квадратичная форма вектор-столбца x (m × 1) и матрицы A (m × m): xAx = mi=1 mj=1 aijxixj . * Произведение матрицы A (m × n) на скаляр α: B = αA = {αaij}, B (m × n). * Транспонирование матрицы A (m × n): B = A= {aji}, B (n × m). * Следм атрицы A (m × m): tr (A) = mi=1 aii. * Рангом (rank(A)) матрицы A называется количество линейно независи-мых столбцов (равное количеству линейно независимых строк). Матри-ца A (m × n) имеет полный ранг по столбцам, если rank(A) = n. Мат-рица A (m × n) имеет полный ранг по строкам, если rank(A) = m. * Матрица A (m × m) называется невырожденной (неособенной), если rank(A) = m. В противном случае она называется вырожденной. * Матрица A (m × m) называется диагональной, если aij = 0 при i = j. Для диагональной матрицы используется обозначение A = = diag(a11, . . . , amm). * Матрица Im = diag(1, . . . , 1) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 0 - - - 0 0 1 - - - 0 ... .... . . ... 0 0 - - - 1 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ (m × m) называется единичной. * Матрица A (m × m) называется симметричной (симметрической), если A = A.
A.1. Матричная алгебра 693 * Матрица A (m × m) называется верхней треугольной, если aij = 0 при i > j.Матрица A (m×m) называется нижней треугольной, если aij = 0 при i < j. * Матрица A−1 (m×m) называется обратной матрицей к матрице A (m×m), если AA−1 = A−1A = Im. * Матрица A (m ×m) называется идемпотентной, если AA = A2 = A. * Векторы-столбцы a (m×1) и b (m×1) называются ортогональными, если их скалярное произведение равно нулю: ab = 0. * Матрица A (m×n), где m n, называется ортогональной, если ее столбцы ортогональны, т.е. AA = In. * Матрица A (m × m) называется положительно определенной, если для любого вектор-столбца x = 0 (m × 1) выполняется xAx > 0. Матрица A (m × m) называется отрицательно определенной, если для любого вектор-столбца x = 0 (m × 1) выполняется xAx < 0. * Матрица A (m × m) называется положительно полуопределенной (неот-рицательно определенной), если для любого вектора-столбца x (m × 1) выполняется xAx 0.Матрица A (m×m) называется отрицательно по-луопределенной (неположительно определенной), если для любого вектора-столбца x (m × 1) выполняется xAx 0. * Определителем матрицы A (m × m) называется |A| = det(A) = mj=1 aij(−1)i+j |Aij |, где i -номер любой строки, а матрицы Aij ((m− 1) × (m− 1)) получены из матрицы A путем вычеркивания i-й строки и j-го столбца. * Для матрицы A (m × m) уравнение |A − λIm| = 0 называется характери-стическим уравнением. Решение этого уравнения λ называется собственным числом (собственным значением) матрицы A. Вектор x = 0 (m × 1) назы-вается собственным вектором матрицы A, соответствующим собственному числу λ, если (A − λIm) x = 0.
694 Приложение A. Вспомогательные сведения из высшей математики * Прямое произведение (произведение Кронекера) матриц A (m × n) и B (p × q) это матрица C (mp × nq): C = A ⊗ B = ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ a11B a12B - - - a1nB a21B a22B - - - a2nB ... .... . . ... am1B am2B - - - amnB ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ . A.1.2. Свойства матриц Сложение матриц : A + B = B + A (коммутативность). : (A + B) + C = A + (B + C) (ассоциативность). Произведение матриц : В общем случае AB = BA (свойство коммутативности не выполнено). : (AB)C = A(BC) (ассоциативность). : A(B + C) = AB + AC (A + B)C = AC + BC (дистрибутивность). : AIm = ImA = A для матрицы A (m ×m). : ⎛⎜⎝ A B C D ⎞⎟⎠ ⎛⎜⎝ E F G H⎞⎟⎠= ⎛⎜⎝ AE + BG AF + BH CE + DG CF + DH ⎞⎟⎠. Ранг : Для матрицы A (m × n) выполнено rank(A) min{m, n}. : rank(AB) min {rank(A), rank(B)}. : Если матрица B (m × m) является невырожденной, то для матрицы A (m × n) выполнено rank(A) = rank(BA). Если матрица B (n × n) яв-ляется невырожденной, то для матрицы A (m × n) выполнено rank(A) = = rank(AB). : rank(AA) = rank(AA) = rank(A).
A.1. Матричная алгебра 695 Cлед : tr (A + B) = tr (A) + tr (B). : tr (αA) = α - tr (A). : tr (A) = tr (A). : tr (AB) = tr (BA). : tr (ABC) = tr (CAB) = tr (BCA). : tr (AA) = tr (AA) = mi=1 n j=1 a2ij . : tr (Im) = m. : tr A(AA)−1A= n, где матрица A (m×n) имеет полный ранг по столб-цам, т.е. rank(A) = n. : tr⎛⎜⎝ A B C D ⎞⎟⎠= tr (A) + tr (D), гд еA и D -квадратные матрицы. Транспонирование : (A + B)= A+ B. : (AB)= BA. Определитель : Для матрицы A (2 × 2): |A| = a11a22 − a12a21. : |A| |B| = |AB|. : |I| = 1. : |αA| = αm |A| для матрицы A (m ×m). : |A| = |A|. : A−1= 1 |A| .
696 Приложение A. Вспомогательные сведения из высшей математики : Если матрица A (m × m) является треугольной (например, диагональной), то |A| = mi=1 aii. : |I + AB| = |I + BA|. : A + BD−1C|D| =D + CA−1B|A|. : |A + xy| = |A| (1 + yAx) для матрицы A (m×m) и вектор-столбцов x, y (m × 1). : A 0 0 B = |A| |B|, гд еA и B -квадратные матрицы. : A B C D = A − BD−1C|D| = D− CA−1B|A|, гд еA и D -квад-ратные невырожденные матрицы. : Матрица A (m×m) является невырожденной (rank(A) = m) тогда и только тогда, когда |A| = 0. Обращение : Если обратная матрица существует, то она единственна (в частности, левая и правая обратные матрицы совпадают). : Матрица A (m × m) имеет обратную A−1 тогда и только тогда, когда она является невырожденной, т.е. rank(A) = m. : Матрица A (m×m) имеет обратную A−1 тогда и только тогда, когда |A| = = 0. : Обозначим через aij элементы обратной матрицы A−1. Тогд а aij = (−1)i+j |Aji| |A| , гд е Aji ((m−1)×(m−1)) получены из матрицы A путем вычеркивания j-й строки и i-го столбца. Во всех приводимых ниже формулах предполагается, что существуют обратные матрицы там, где это требуется.
A.1. Матричная алгебра 697 : Для матрицы A (2 × 2): A−1 = 1 |A| ⎛⎜⎝ a22 −a21 −a12 a11 ⎞⎟⎠= 1 a11a22 − a12a21 ⎛⎜⎝ a22 −a21 −a12 a11 ⎞⎟⎠. : Ax = y, x = A−1y. : (AB)−1 = B−1A−1. : A−1−1 = A. : (A)−1 = A−1. : Если A (m × m)-ортогональная матрица, то A= A−1. : Для диагональной матрицы A = diag(a11, . . . , amm) выполнено: A−1 = diag(1/a11, . . . , 1/amm). : (A + B)−1 = A−1 A−1 + B−1−1 B−1. : A + BD−1C−1 = A−1 − A−1B(D + CA−1B)−1CA−1. : (I + AB)−1 = I − A(I + BA)−1B. : ⎛⎜⎝ A 0 0 B ⎞⎟⎠ −1 = ⎛⎜⎝ A−1 0 0 B−1 ⎞⎟⎠, гд еA и B -квадратные матрицы. : ⎛⎜⎝ A B C D ⎞⎟⎠ −1 = ⎛⎜⎝ (A − BD−1C)−1 −A−1B(D −CA−1B)−1 −D−1C(A − BD−1C)−1 (D− CA−1B)−1 ⎞⎟⎠, где A и D -квадратные матрицы. Положительно определенные матрицы : Если матрица A положительно определенная, то |A| > 0. Если матрица A положительно полуопределенная, то |A| 0. : Если матрица A положительно (полу-)определенная, то матрица −A от-рицательно (полу-)определенная.
698 Приложение A. Вспомогательные сведения из высшей математики : Если матрица A положительно определенная, то обратная матрица A−1 также положительно определенная. : Если матрицы A и B положительно (полу-)определенные, то матрицы A + B и AB также положительно (полу-)определенные. : Если матрица A положительно определенная, а B положительно полу-определенная, то |A + B| |A|. Если B положительно определенная, то |A + B| > |A|. : Матрицы AA и ABA (n × n) являются симметричными положительно полуопределенными для любой матрицы A (m × n) и симметричной поло-жительно полуопределенной матрицы B (m × m). : Если матрица A (m × n) имеет полный ранг по столбцам, то матри-ца AA (n × n) симметричная положительно определенная. Если мат-рица B (m × m) симметричная положительно определенная, то матрица ABA (n × n) симметричная положительно определенная. : Если матрица A (m × m) положительно полуопределенная, то существует верхняя треугольная матрица U (m×m), такая что A = UU. Также суще-ствует нижняя треугольная матрица L (m × m), такая что A = LL. Такое представление матрицы называется разложением Холецкого (триангуляри-зацией). Идемпотентные матрицы : Если матрица A идемпотентная, то матрица I − A тоже идемпотентная, причем A(I − A) = 0. : Если матрица A симметричная и идемпотентная, то rank(A) = tr (A). : Матрицы A(AA)−1Aи Im − A(AA)−1Aявляются симметричными и идемпотентными для любой матрицы A (m × n), имеющей полный ранг по столбцам.При этом tr A(AA)−1A= n и tr Im − A(AA)−1A= = m − n. Собственные числа и векторы : Для матрицы A (m × m) |A − λIm| является многочленом m-й степени (характеристическим многочленом) и имеет m корней, λ1, . . . , λm, в общем случае комплексных, среди которых могут быть кратные. По определению, λ1, . . . , λm являются собственными числами матрицы A.
A.1. Матричная алгебра 699 : У матрицы A (m × m) существует не больше m различных собственных чисел. : Если x -собственный вектор матрицы A, соответствующий собственному числу λ, то для любого скаляра α = 0, αx -тоже собственный вектор, соответствующий собственному числу λ. : Если λ1, . . . , λm -собственные числа матрицы A, то tr(A) = mi=1 λi, |A| = mi=1 λi. : Если матрица A идемпотентная, то все ее собственные числа равны 0 или 1. : Все собственные числа вещественной симметричной матрицы вещественны. : Если x и y -собственные векторы вещественной симметричной матрицы, соответствующие двум различным собственным числам, то они ортогональ-ны: xy = 0. : Если матрица A (m × m) является вещественной и симметричной, то существуют матрицы H и Λ, гд еH (m × m)-ортогональная матри-ца (H= H−1), столбцы которой - собственные векторы матрицы A, а Λ (m × m)-диагональная матрица, состоящая из соответствующих соб-ственных чисел матрицы A, такие что выполнено A = HΛH. : Если матрица A (m×m) является вещественной, симметричной, невырож-денной, то A−1 = HΛ−1H. : Вещественная симметричная матрица является положительно полуопреде-ленной (определенной) тогда и только тогда, когда все ее собственные числа неотрицательны (положительны). Вещественная симметричная матрица яв-ляется отрицательно полуопределенной (определенной) тогда и только тогда, когда все ее собственные числа неположительны (отрицательны). : Если матрица A (m×m) является вещественной, симметричной и положи-тельно полуопределенной, то A = BB = B2, гд еB = HΛ1/2H(m × m)- вещественная, симметричная и положительно полуопределенная матрица; Λ1/2 = diag{λ1/2 g }. : Пусть λ1 - - - λm - собственные числа вещественной симметрич-ной матрицы A (m × m). Тогда собственый вектор x1, соответствующий
700 Приложение A. Вспомогательные сведения из высшей математики наименьшему собственому числу λ1, является решением задачи ⎧⎪⎪⎨ ⎪⎪⎩ xAx → min! x xx = 1. : Пусть λ1 - - - λm - собственные числа вещественной симметричной матрицы A (m × m). Тогда λm = max x xAx xx и λ1 = min x xAx xx . Произведение Кронекера : A ⊗ (B + C) = A ⊗ B + A ⊗ C и (A + B) ⊗ C = A ⊗ C + B ⊗ C. : A ⊗ (B ⊗ C) = (A ⊗ B) ⊗ C. : α ⊗A = α - A = A ⊗ α. : (A ⊗ B)= A⊗ B. : (A ⊗ B) (C ⊗ D) = (AC) ⊗ (BD). : (A ⊗ B)−1 = A−1 ⊗ B−1. : |A ⊗ B| = |A|n |B|m для матриц A (m × m) и B (n × n). : tr (A ⊗ B) = tr (A) - tr (B). : rank(A ⊗ B) = rank(A) - rank(B). A.2. Матричное дифференцирование A.2.1. Определения * Производной скалярной функции s(x) по вектор-столбцу x (n × 1) или, другими словами, градиентом является вектор-столбец (n × 1) ∂s ∂x = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝ ∂s ∂x1 ... ∂s ∂xn ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠ .
A.2. Матричное дифференцирование 701 * Производной скалярной функции s(x) по вектор-строке x (1×n) является вектор-строка (1 × n) ∂s ∂x = ∂s ∂x1 , . . . , ∂s ∂xn. * Производной векторной функции y(x) (n × 1) по вектору x (1 × m) или, другими словами, матрицей Якоби является матрица (n × m) ∂y ∂x = < ∂yi ∂xj @i=1, ..., n j=1, ...,m . : Производной векторной функции y(x) (1×n) по вектору x (m×1) является матрица (m × n) ∂y ∂x = ∂yj ∂xi ;i=1, ...,m j=1, ..., n . * Производной скалярной функции s(A) по матрице A (m × n) является матрица (m × n) ∂s ∂A = < ∂s ∂aij @i=1, ...,m j=1, ..., n . * Производной матричной функции A(s) по скаляру s является матрица (m× × n) ∂A ∂s = ∂aij ∂s ;i=1, ...,m j=1, ..., n . * Второй производной скалярной функции s(x) по вектору-столбцу x (n × 1) или, другими словами, матрицей Гессе является матрица (n × n) ∂2s ∂x∂x= < ∂2s ∂xi∂xj @i=1, ...,m j=1, ..., n . A.2.2. Свойства : ∂x∂x = I и ∂x ∂x= I. : ∂Ax ∂x= A и ∂xA ∂x = A.
702 Приложение A. Вспомогательные сведения из высшей математики : ∂xy ∂x = ∂yx ∂x = y. : ∂xx ∂x = 2x. : ∂xAy ∂x = Ay и ∂xAy ∂y= xA. : ∂xAx ∂x = (A + A)x. Для симметричной матрицы A: ∂xAx ∂x = 2Ax = 2Ax. : ∂xAy ∂A = xy. : ∂xA−1y ∂A = (A)−1 xy(A)−1. : ∂ tr (A) ∂A = I. : ∂ tr (AB) ∂A = Bи ∂ tr (AB) ∂B = A. : ∂ tr (AA) ∂A = 2A. : ∂ tr (ABA) ∂A = (B + B)A. : ∂ tr (ABA) ∂B = AA. : ∂ |A| ∂A = |A| (A)−1. : ∂ ln |A| ∂A = (A)−1. : ∂ ln |ABA| ∂A = BA(ABA)−1 + BA(ABA)−1. : ∂ (AB) ∂s = A∂B ∂s + ∂A ∂s B. : ∂A−1 ∂s = −A−1 ∂A ∂s A−1.
A.3. Сведения из теории вероятностей и математической статистики703 : ds (A) dt = tr ∂s ∂A dAdt = tr ∂s ∂AdA dt . : ∂ tr (A) ∂s = tr ∂A ∂s . : ∂ ln |A| ∂s = tr A−1 ∂A ∂s . : dy (x) ds = ∂y ∂xdx ds . A.3. Сведения из теории вероятностей и математической статистики A.3.1. Характеристики случайных величин Определения : Функцией распределения случайной величины x называется функция Fx(z) = Pr(x z), сопоставляющаячислу z вероятность того, что x не пре-вышает z.Функция распределения полностью характеризует отдельную слу-чайную величину. : Если случайная величина x непрерывна, то она имеет плотность fx(-), ко-торая связана с функцией распределения соотношениями fx(z) = Fx(z). : Квантилью уровня F, гд е F ∈ [0; 1], (F-квантилью) непрерывной случайной величины x называется число xF , такое что Fx(xF) = xF −∞ fx(t)dt = F. : Медианой x0,5 называется 0, 5-квантиль. : Модой непрерывной случайной величины называется величина, при которой плотность распределения достигает максимума, т.е. *x= argmax z fx(z). : Если распределение непрерывной случайной величины x симметрично от-носительно нуля, т.е. fx(z) = fx(−z) и Fx(−xF) = 1 − Fx(xF ), то двусто-ронней F-квантилью называется число xF , такое что Fx(xF ) − Fx(−xF) = xF −xF fx(t)dt = F.
704 Приложение A. Вспомогательные сведения из высшей математики : Математическим ожиданием непрерывной случайной величины x называ-ется E(x) = +∞ −∞ tfx(t)dt. : Математическое ожидание является начальным моментом первого по-рядка. Начальным (нецентральным) моментом q-го порядка называется E(xq) = +∞ −∞ tqfx(t)dt. : По случайной величине x может быть построена соответствующая ей цен-трированная величина ˆx : ˆx = x − E(x), имеющая аналогичные законы распределения и нулевое математическое ожидание. : Центральным моментом q-го порядка случайной величины x называется на-чальный момент q-го порядка для соответствующей центрированной вели-чины ˆx, т.е. E(ˆxq) = E[(x − E(x))q]. Для непрерывной случайной величины центральный момент q-го порядка равен μq = +∞ −∞ (t − E(x))qfx(t)dt. : Дисперсией случайной величины называется центральный момент второго порядка. Для непрерывной случайной величины дисперсия равна var(x) = σ2x= E(ˆx2) = E(x − E(x))2= +∞ −∞ (t − E(x))2fx(t)dt. : Среднеквадратическим отклонением называется квадратный корень из дис-персии σx = :σ2x. Нормированной (стандартизованной) случайной величи-ной называется x − E(x) σx . : Коэффициентомасимметрии называется начальныймомент третьегопорядка нормированной случайной величины, т.е. δ3 = E0x − E(x) σx 31 = μ3 σ3x. : Куртозисом называется начальный момент четвертого порядка нормирован-ной случайной величины, т.е. δ4 = E0x − E(x) σx 41 = μ4 σ4x. Коэффициентом эксцесса называется δ4 − 3.
A.3 Сведения из теории вероятностей и матем. статистики 705 : Для n-мерного случайного вектора x = (x1, . . . , xn)(многомерной слу-чайной величины) функцией распределения называется Fx1, ..., xn(z1, . . . , zn) = Pr(x1 z1, . . . , xn zn). : Если распределение случайного вектора x непрерывно, то он имеет плотность fx(-) (называемую совместной плотностью случайных величин x1, . . . , xn), которая связана с функцией распределения соотношениями fx1, ..., xn(z) = ∂nFx1, ..., xn(z) ∂x1 - - - ∂xn . Случайные величины x1, . . . , xn называются независимыми (в совокупно-сти), если Fx1, ..., xn(z1, . . . , zn) = Fx1(z1) - - - Fxn(zn). : Ковариацией случайных величин x и y называется cov(x, y) = E[(x − E(x)) (y − E(y))] . : Корреляцией случайных величин x и y называется ρx,y = cov(x, y) :var(x)var(x) . : Ковариационной матрицей n-мерной случайной величины x = (x1, . . . , xn)называется Γx = var(x) = ⎛⎜⎜⎜⎜⎜⎝ cov(x1, x1) - - - cov(x1, xn) .... . . ... cov(x1, xn) - - - cov(xn, xn) ⎞⎟⎟⎟⎟⎟⎠ = = E(x − E(x)) (x − E(x)). : Корреляционной матрицей n-мерной случайной величины x = (x1, . . . , xn)называется Px = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ 1 ρx1,x2 - - - ρx1,xn ρx1,x2 1 - - - ρx2,xn ... .... . . ... ρx1,xn ρx2,xn - - - 1 ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ .
706 Приложение A. Вспомогательные сведения из высшей математики Функция распределения и плотность : Функция распределения имеет следующие свойства: это неубывающая, непрерывная справа функция, 0 Fx(z) 1, причем lim z→−∞Fx(z) = 0 и lim z→∞Fx(z) = 1. : Fx(z) = z −∞ fx(t)dt. : fx(z) 0. : +∞ −∞ fx(t)dt = 1. : Вероятность того, что x ∈ [a, b] , равна Pr(a x b) = b a fx(t)dt. : Для многомерной случайной величины Fx1, ..., xn(z1, . . . , zn) = z1 −∞ - - - zn −∞ fx1, ..., xn(t1, . . . , tn)dtn . . . dt1. : Если случайные величины x1, . . . , xn независимы, то fx1, ..., xn(z1, . . . , zn) = fx1(z1) - . . . - fxn(zn). Математическое ожидание : Если c -константа, то E(c) = c. : Если x и y -любые две случайные величины, то E(x + y) = E(x) + E(y). : Если c -константа, то E(cx) = cE(x). : В общем случае E(xy) = E(x)E(y). : Если функция f(-) вогнута, то выполнено неравенство Йенсена: E(f(x)) f (E(x)) . : Для симметричного распределения выполено E(x) = x0,5.
A.3 Сведения из теории вероятностей и матем. статистики 707 Дисперсия : var(x) = E(x2) − E(x)2. : Для любой случайной величины x выполнено var(x) 0. : Если c -константа, то выполнено: var(c) = 0; var(c + x) = var(x); var(cx) = c2var(x). : Если x и y -любые две случайные величины, то в общем случае: var(x + y) = var(x) + var(y). : Неравенство Чебышёва: Pr (|x − E(x)| > α) var(x) α2 для любого поло-жительного числа α. Ковариация : cov(x, y) = E(xy) − E(x)E(y). : cov(x, y) = cov(y, x). : cov(cx, y) = c - cov(x, y). : cov(x + y, z) = cov(x, z) + cov(y, z). : cov(x, x) = var(x). : Если x и y независимы, то cov(x, y) = 0.Обратное, вообще говоря, невер-но. Корреляция : ρx,y = cov(˜x, ˜y), гд е ˜x = x − E(x) σx и ˜y = y − E(y) σy -соответствую-щие центрированные нормированные случайные величины. Следовательно, свойства корреляции аналогичны свойствам ковариации. : ρx,x = 1. : −1 ρx,y 1. : Если ρx,y = 0, то E(xy) = E(x)E(y).
708 Приложение A. Вспомогательные сведения из высшей математики Условные распределения : Условной вероятностью события A относительно события B называется Pr(A|B) = Pr(A ∩ B)/Pr(B). Из определения следует, что Pr(A∩B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A). : Для независимых событий A и B выполнено Pr(A|B) = Pr(A). : Теорема Байеса: Пусть A1, . . . , An,B -события, такие что (1) Ai ∩ Aj = ∅ при i = j, (2) B ⊂ ni=1Ai, (3) Pr(B) > 0. Тогда Pr(Ai|B) = Pr(B|Ai)Pr(Ai) Pr(B) = Pr(B|Ai)Pr(Ai) nj=1 Pr(B|Aj)Pr(Aj) . : Пусть (x, y) -случайный вектор, имеющий непрерывное распределение, где вектор x имеет размерность m × 1, а y - n × 1. Плотностью мар-гинального распределения x называется fx(x) = Rn fx,y(x, y)dy. Плотно-стью условного распределения x относительно y называется fx|y(x|y) = fx,y(x, y) fy(y) = fx,y(x, y) Rm fx,y(x, y)dx. : Если x и y независимы, то плотность условного распределения совпадает с плотностью маргинального, т.е. fx|y(x|y) = fx(x). : Условным математическим ожиданием x относительно y называется E(x|y) = Rm xfx|y(x|y)dx. : Условная дисперсия x относительно y равна var (x|y) = E(x − E(x|y))2 |y.
A.3 Сведения из теории вероятностей и матем. статистики 709 Свойства условного ожидания и дисперсии : E(α(y)|y) = α(y). : E(α(y)x|y) = α(y)E(x|y). : E(x1 + x2|y) = E(x1|y) + E(x2|y). : Правило повторного ожидания: E(E(x|y, z) |y) = E(x|y). : Если x и y независимы , то E(x|y) = E(x). : var (α(y)|y) = 0. : var (α(y) + x|y) = var (x|y). : var (α(y)x|y) = α2(y)var (x|y). A.3.2. Распределения, связанные с нормальным Нормальное распределение Нормальное (или гауссовское) распределение с математическим ожиданием μ и дисперсией σ2 обозначается N μ, σ2и имеет плотность распределения (f(z) = √ 1 2πσ2 e−(z−μ)2 2σ2 . Нормальное распределение симметрично относительно μ, и для него выполня-ется E(x) = x0,5 = *x= μ. Моменты нормального распределения: μ2k+1 = 0 и μ2k = (2k − 1)!! - σ2k = = 1- 3 - . . . - σ2k при целых k, в частности, μ4 = 3σ4. Коэффициент асимметрии: δ3 = 0. Куртозис δ4 = 3, коэффициент эксцесса равен нулю. Стандартным нормальным распределением называется N (0, 1). Его плотность ϕ(z) = √12π e−z2 2 ; функция распределения Φ(z) = z −∞ √12π e−t2 2 dt.
710 Приложение A. Вспомогательные сведения из высшей математики Распределение хи-квадрат Распределение хи-квадрат с k степенями свободы обозначается χ2k. Его плот-ность: ⎧⎪⎪⎨ ⎪⎪⎩ f(x) = (x/2)k/2−1 2k/2Γ(k/2) e−x/2, x0, f(x) = 0, x<0, где Γ(-) -гамма-функция. Если xi ∼ N(0, 1), i = 1, . . . , k и независимы в совокупности, то k i=1 x2i∼ χ2k. Если x ∼ χ2k, то E(x) = k и var(x) = 2k. Коэффициент асимметрии: δ3 = 2√ √ 2 k > 0. Куртозис: δ4 = 12 k + 3, коэффициент эксцесса δ4 −3 = 12 k > 0. При больших k распределение хи-квадрат похоже на N (k, 2k). Распределение Стьюдента Распределение Стьюдента с k степенями свободы обозначается через tk. Его также называют t-распределением. Его плотность: f(x) = Γ√((k + 1)/2) kπΓ(k/2) 1 + x2 k −k+1 2 . Если x1 ∼ N(0, 1), x2 ∼ χ2k и независимы, то x1 :x2/k ∼ tk. Распределение Стьюдента симметрично относительно нуля и x0,5 = *x= 0. Математическое ожидание существует при k > 1 и E(x) = 0. При k n не существует n-го момента. Дисперсия: var(x) = k k − 2 (существует при k > 2). Коэффициент асимметрии: δ3 = 0 (существует при k > 3). Куртозис: δ4 = 3k − 2 k − 4 ; коэффициент эксцесса: δ4 −3 = 6 k − 4 (существуют при k > 4). При больших k распределение Стьюдента похоже на N (0, 1).
A.3 Сведения из теории вероятностей и матем. статистики 711 РаспределениеФишера Распределение Фишера с k1 и k2 степенями свободы обозначается Fk1,k2 . Его также называютF-распределением или распределениемФишера-Снедекора. Его плотность: ⎧⎪⎪⎨ ⎪⎪⎩ f(x) = Γ((k1 + k2)/2) Γ(k1/2)Γ(k2/2)kk1/2 1 kk2/2 2 xk1/2−1 (k1x + k2)(k1+k2)/2, x 0, f(x) = 0, x<0. Если x1 ∼ χ2k1 , x2 ∼ χ2k2 и независимы, то x1/k1 x2/k2 ∼ Fk1,k2 . Если x ∼ Fk1,k2, то E(x) = k2 k2 − 2, при k2 > 2, var(x) = 2k22(k1 + k2 − 2) k1(k2 − 2)2(k2 − 4), при k2 > 4, δ3 = 2(2k1 + k2 − 2) k2 − 6 ! 2(k2 − 4) k1(k1 + k2 − 2) , при k2 > 6, δ4 −3 = 127(k2 − 2)2(k2 − 4) + k1(5k2 − 22)(k1 + k2 − 2)8 k1(k1 + k2 − 2)(k2 − 6)(k2 − 8) , при k2 > 8. Многомерное нормальное распределение n-мерное нормальное распределение с математическим ожиданием μ (n × 1) и ковариационной матрицей Σ (n × n) обозначается N (μ, Σ). Его плотность: f(z) = (2π)−n/2 |Σ|−1/2 e−12 (z−μ)Σ−1(z−μ). Свойства многомерного нормального распределения: : Если x ∼ N (μ, Σ), то Ax + b ∼ N (Aμ + b,AΣA). : Если x ∼ N 0n, σ2In, то xx σ2 ∼ χ2n. : Если x ∼ N (0, Σ), гд е Σ (n × n)-невырожденная матрица, то xΣ−1x ∼ χ2n.
712 Приложение A. Вспомогательные сведения из высшей математики : Если x ∼ N 0,A(AA)−1A, гд еA (n × k)-матрица, имеющая пол-ный ранг по столбцам, то xx ∼ χ2k. Если x ∼ N 0, I − A(AA)−1A, где A (n × k)-матрица, имеющая полный ранг по столбцам, то xx ∼ χ2n−k. : Если x = (x1, . . . , xn) ∼ N μ, diag(σ21, . . . , σ2n), то x1, . . . , xn независи-мы в совокупности и xi ∼ N μi, σ2i . : Если совместное распределение случайных векторов x и y является много-мерным нормальным: ⎛⎜⎝ xy ⎞⎟⎠∼ N ⎛⎜⎝ ⎛⎜⎝ μx μy ⎞⎟⎠,⎛⎜⎝ Σxx Σxy Σyx Σyy ⎞⎟⎠ ⎞⎟⎠, то маргинальное распределение x имеет вид x ∼ N (μx,Σxx), а условное распределение x относительно y имеет вид x|y ∼ N μx +ΣxyΣ−1 yy (y − μy) ,Σxx − ΣxyΣ−1 yy Σyx. Аналогично y ∼ N (μy,Σyy) и y|x ∼ N μy +ΣyxΣ−1 xx (x − μx) ,Σyy − ΣyxΣ−1 xxΣxy. A.3.3. Проверка гипотез Пусть x1, . . . , xn -случайная выборка из распределения Fθ, заданного па-раметром θ ∈ Θ. Нулевая гипотеза H0 относительно параметра θ состоит в том, что он при-надлежит некоторому более узкому множеству: θ ∈ Θ0, гд еΘ0 ⊂ Θ. Альтерна-тивная гипотеза H1 состоит в том, что параметр принадлежит другому множеству: θ ∈ Θ1, гд еΘ1 = Θ\Θ0. Рассматривается некоторая статистика s, которая яв-ляется функцией от выборки: s = s(x1, . . . , xn). Процедуру (правило) проверки гипотезы называют статистическим критерием или статистическим тестом. Суть проверки гипотезы H0 против альтернативной гипотезы H1 состоит в том, что за-даются две непересекающиеся области, S0 и S1, такие что S0 ∩S1 -вся область значений статистики s. Если s ∈ S0, то нулевая гипотеза (H0) принимается, а если s ∈ S1, то нулевая гипотеза отвергается.
A.3 Сведения из теории вероятностей и матем. статистики 713 Обычно S0 = (−∞, s∗] и S1 = (s∗,+∞), гд е s∗ -критическая граница. Та-кой критерий называется односторонним.При этом критерий состоит в следующем: если s < s∗, то H0 принимается, если s > s∗, то H0 отвергается. S0 и S1 выбираются так, чтобы в случае, когда H0 верна, вероятность того, что s ∈ S1 , была быравна некоторой заданной малой вероятности α. Как правило, на практике используют вероятность α = 0.05 (хотя это не имеет подсобой каких-либо теоретических оснований). Ошибкой первого рода называется ошибка, состоящая в том, что отвергается верная нулевая гипотеза. Вероятность ошибки первого рода равна α. Вероятность ошибки первого рода называется уровнем значимости или размером. Вероятность 1 − α называют уровнем доверия. Ошибкой второго рода называется ошибка, состоящая в том, что принимается неверная нулевая гипотеза. Вероятность ошибки второго рода обозначают β. Мощностью критерия называют величину 1−β. Мощность характеризует, на-сколько хорошо работает критерий. Мощность должна быть как можно большей при данном α. Требуется, по крайней мере, чтобы α < 1 − β. Критерий, не удо-влетворяющий этому условию, называют смещенным. Альтернативный способ проверки гипотез использует вероятность ошибки пер-вого рода, если принять s∗ равной s, т.е. вероятность того, что s > s∗. Эту вероят-ность называют уровнем значимости или P-значением. Обозначим ее pv. При за-данной вероятности α критерий состоит в следующем: если pv > α, то H0 принимается, если pv < α, то H0 отвергается. Еще один способ проверки гипотез основан на доверительных областях для па-раметра θ. Пусть D -доверительная область для параметра θ, такая что θ ∈ D с некоторой веротностью 1 − α, и пусть проверяется гипотеза H0: θ = θ0 против альтернативной гипотезы H1: θ = θ0. Критерий состоит в следующем: если θ0 ∈ D, то H0 принимается, если θ0 /∈ D, то H0 отвергается. Отметим, что в этом случае θ0 не случайная величина; случайной является доверительная область D.
714 Приложение A. Вспомогательные сведения из высшей математики A.4. Линейные конечно-разностные уравнения Конечно-разностное уравнение p-го порядка имеет вид: α0yt + α1yt−1 + - - - + αpyt−p = ut, где ut -известная последовательность, α0, α1, . . . , αp -известные коэффици-енты, а последовательность yt следует найти. Это уравнение такжеможно записать через лаговый многочлен: α (L) yt = α0 + α1L + α2L2 + - - - + αpLpyt = ut, где L -лаговый оператор (Lyt = yt−1). Если даны p последовательных значений последовательности yt, например, y1, . . . , yp, то другие значения можно найти по рекуррентной формуле. При t > p получаем yt = 1 α0 (ut − (α1yt−1 + - - - + αpyt−p)) . Конечно-разностное уравнение называется однородным, если ut = 0. Общее решение конечно-разностного уравнения имеет вид: yt = y0t + ˜yt, где y0t - общее решение соответствующего однородного уравнения, а ˜yt = α−1 (L) ut -частное решение неоднородного уравнения. A.4.1. Решение однородного конечно-разностного уравне-ния Если λj, j = 1, . . . , p -корни характеристического уравнения α (λ) = α0 + α1λ + α2λ2 + - - - + αpλp = 0, тогда α (λ) = α0 p j=1 (1 − λ/λj). Последовательность y(j) t = λ−t j является решением однородного конечно-разностного уравнения. Действительно, в разложении α (L) на множители имеет-ся множитель 1 − L/λj и (1 − L/λj) y(j) t = (1 − L/λj) λ−t j = λ−t j − Lλ−t−1 j = λ−t j − λ−t j = 0.
A.5. Комплексные числа 715 Если все корни λj, j = 1, . . . , p различные, то общее решение однородного конечно-разностного уравнения имеет вид: y0t = C1y(1) t + . . . + Cpy(p) t = C1λ−t 1 + . . . + Cpλ−t p . Если не все корни различны, то для корня λj кратности m соответствующее слагаемое имеет вид C1j + C2j t + . . . + Cmjtm−1λ−t j . Если λ1 и λ2 -пара комплексно-сопряженных корней, т.е. λ1 = R(cos (ϕ) + i sin (ϕ)) = Reiϕ и λ2 = R(cos (ϕ) − i sin (ϕ)) = Re−iϕ, то два слагаемых C1y(1) t + C2y(2) t = C1λ−t 1 + C2λ−t 2 , гд е C1,C2 являются комплексно-сопряженными, можно заменить на C1λ−t 1 + C2λ−t 2 = C1(Reiϕ)−t + C2(Re−iϕ)−t = = R−t C1e−iϕt + C2eiϕt= R−t (Acos(ϕt) + B sin(ϕt)) . Если известны p значений последовательности y0t , например, y01, . . . , y0p, то коэффициенты Cj,Cjk,A,B находятся из решения системы p линейных урав-нений. Например, если все корни λj, j = 1, . . . , p различны, то коэффициенты Cj находятся из системы уравнений ⎛⎜⎜⎜⎜⎜⎝ λ−1 1 - - - λ−1 p ... ... λ−p 1 - - - λ−p p ⎞⎟⎟⎟⎟⎟⎠ ⎛⎜⎜⎜⎜⎜⎝ C1... Cp ⎞⎟⎟⎟⎟⎟⎠ = ⎛⎜⎜⎜⎜⎜⎝ y01... y0p ⎞⎟⎟⎟⎟⎟⎠ . A.5. Комплексные числа Комплексным числом z называется пара (a, b), гд е a-действительная часть, а b - мнимая часть числа. Комплексное число записывают в виде z = a + ib, где i = √−1. : Сложение: если z1 = a1 + ib1, z2 = a2 + ib2, то z1 + z2 = (a1 + a2) + + i (b1 + b2); если z = a + ib, то −z = (−a) + i(−b) = −a − ib. : Вычитание: если z1 = a1 + ib1, z2 = a2 + ib2, то z1 − z2 = (a1 − a2) + + i (b1 − b2).
716 Приложение A. Вспомогательные сведения из высшей математики : Умножение: если z1 = a1 + ib1, z2 = a2 + ib2, то z1z2 = a1a2 − b1b2 + + i (a2b1 + a1b2). : Обратное число: если z = a+ib, то z−1 = a a2 + b2 +i(− b a2 + b2) = a − ib a2 + b2 . : Деление: z1 = a1 + ib1, z2 = a2 + ib2, то z1 z2 = z1z−1 2 . : Для числа z = a+ib число z∗ = a−ib называют комплексно-сопряженным. : Модуль: если z = a + ib, то |z| = √a2 + b2. : Выполняется свойство zz∗ = |z|2 : Любое комплексное число z = a+ib можно представит в виде z = |z| cos ϕ+ +i|z| sin ϕ, где угол ϕ такой что cos ϕ = a |z| и sinϕ = b |z| . Угол ϕ называют аргументом z. : Формула Эйлера: cos ϕ + i sin ϕ = eiϕ и cos ϕ − i sin ϕ = e−iϕ. cos ϕ = 12(eiϕ + e−iϕ) sinϕ = 1 2i (eiϕ − e−iϕ). : Формула Муавра: если z = cosϕ + i sin ϕ = eiϕ, то zn = cos(nϕ) + + i sin(nϕ) = einϕ.
Приложение B Статистические таблицы Нормальное распределение ε = 1.96 0.06 1.9 √12π +∞ 1.96 e−(1/2)x2dx = 0.0250 Функция нормального распределения: √12π +∞ ε e−(1/2)x2 dx. Плотность функции нормального распределения -4 -3 -2 -1 0 1 2 3 4 2.5% 1.96 Рис. B.1График плотности нормального распределения с нулевым математическим ожиданием и единичной дисперсией N(0, 1) 717
718 Приложение B. Статистические таблицы Таблица А.1. Значения функции плотности стандартного нормального распределения ε 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.5000 0.4960 0.4920 0.4880 0.4840 0.4801 0.4761 0.4721 0.4681 0.4641 0.1 0.4602 0.4562 0.4522 0.4483 0.4443 0.4404 0.4364 0.4325 0.4286 0.4247 0.2 0.4207 0.4168 0.4129 0.4090 0.4052 0.4013 0.3974 0.3936 0.3897 0.3859 0.3 0.3821 0.3783 0.3745 0.3707 0.3669 0.3632 0.3594 0.3557 0.3520 0.3483 0.4 0.3446 0.3409 0.3372 0.3336 0.3300 0.3264 0.3228 0.3192 0.3156 0.3121 0.5 0.3085 0.3050 0.3015 0.2981 0.2946 0.2912 0.2877 0.2843 0.2810 0.2776 0.6 0.2743 0.2709 0.2676 0.2643 0.2611 0.2578 0.2546 0.2514 0.2483 0.2451 0.7 0.2420 0.2389 0.2358 0.2327 0.2296 0.2266 0.2236 0.2206 0.2177 0.2148 0.8 0.2119 0.2090 0.2061 0.2033 0.2005 0.1977 0.1949 0.1922 0.1894 0.1867 0.9 0.1841 0.1814 0.1788 0.1762 0.1736 0.1711 0.1685 0.1660 0.1635 0.1611 1 0.1587 0.1562 0.1539 0.1515 0.1492 0.1469 0.1446 0.1423 0.1401 0.1379 1.1 0.1357 0.1335 0.1314 0.1292 0.1271 0.1251 0.1230 0.1210 0.1190 0.1170 1.2 0.1151 0.1131 0.1112 0.1093 0.1075 0.1056 0.1038 0.1020 0.1003 0.0985 1.3 0.0968 0.0951 0.0934 0.0918 0.0901 0.0885 0.0869 0.0853 0.0838 0.0823 1.4 0.0808 0.0793 0.0778 0.0764 0.0749 0.0735 0.0721 0.0708 0.0694 0.0681 1.5 0.0668 0.0655 0.0643 0.0630 0.0618 0.0606 0.0594 0.0582 0.0571 0.0559 1.6 0.0548 0.0537 0.0526 0.0516 0.0505 0.0495 0.0485 0.0475 0.0465 0.0455 1.7 0.0446 0.0436 0.0427 0.0418 0.0409 0.0401 0.0392 0.0384 0.0375 0.0367 1.8 0.0359 0.0351 0.0344 0.0336 0.0329 0.0322 0.0314 0.0307 0.0301 0.0294 1.9 0.0287 0.0281 0.0274 0.0268 0.0262 0.0256 0.0250 0.0244 0.0239 0.0233 2 0.0228 0.0222 0.0217 0.0212 0.0207 0.0202 0.0197 0.0192 0.0188 0.0183 2.1 0.0179 0.0174 0.0170 0.0166 0.0162 0.0158 0.0154 0.0150 0.0146 0.0143 2.2 0.0139 0.0136 0.0132 0.0129 0.0125 0.0122 0.0119 0.0116 0.0113 0.0110 2.3 0.0107 0.0104 0.0102 0.0099 0.0096 0.0094 0.0091 0.0089 0.0087 0.0084 2.4 0.0082 0.0080 0.0078 0.0075 0.0073 0.0071 0.0069 0.0068 0.0066 0.0064 2.5 0.0062 0.0060 0.0059 0.0057 0.0055 0.0054 0.0052 0.0051 0.0049 0.0048 2.6 0.0047 0.0045 0.0044 0.0043 0.0041 0.0040 0.0039 0.0038 0.0037 0.0036 2.7 0.0035 0.0034 0.0033 0.0032 0.0031 0.0030 0.0029 0.0028 0.0027 0.0026 2.8 0.0026 0.0025 0.0024 0.0023 0.0023 0.0022 0.0021 0.0021 0.0020 0.0019 2.9 0.0019 0.0018 0.0018 0.0017 0.0016 0.0016 0.0015 0.0015 0.0014 0.0014 3 0.0013 0.0013 0.0013 0.0012 0.0012 0.0011 0.0011 0.0011 0.0010 0.0010 
719 Распределение t-Cтьюдента Pr(t > tk,1−θ) = θ Границы t-распределения с k степенями свободы tk,1−θ θ = 0.025 k = 5 2.5%, t0.975 = 2.571 Распределение tСтьюдента -4 -3 -2 -1 0 1 2 3 4 2.5% 2.571 Рис. B.2График плотности распределения t-Стьюдента для k = 5
720 Приложение B. Статистические таблицы Таблица А.2. Граница t-распределения с k степенями свободы Односторонние квантили θ k 0.15 0.1 0.05 0.025 0.01 0.005 1 1.963 3.078 6.314 12.706 31.821 63.656 2 1.386 1.886 2.920 4.303 6.965 9.925 3 1.250 1.638 2.353 3.182 4.541 5.841 4 1.190 1.533 2.132 2.776 3.747 4.604 5 1.156 1.476 2.015 2.571 3.365 4.032 6 1.134 1.440 1.943 2.447 3.143 3.707 7 1.119 1.415 1.895 2.365 2.998 3.499 8 1.108 1.397 1.860 2.306 2.896 3.355 9 1.100 1.383 1.833 2.262 2.821 3.250 10 1.093 1.372 1.812 2.228 2.764 3.169 11 1.088 1.363 1.796 2.201 2.718 3.106 12 1.083 1.356 1.782 2.179 2.681 3.055 13 1.079 1.350 1.771 2.160 2.650 3.012 14 1.076 1.345 1.761 2.145 2.624 2.977 15 1.074 1.341 1.753 2.131 2.602 2.947 16 1.071 1.337 1.746 2.120 2.583 2.921 17 1.069 1.333 1.740 2.110 2.567 2.898 18 1.067 1.330 1.734 2.101 2.552 2.878 19 1.066 1.328 1.729 2.093 2.539 2.861 20 1.064 1.325 1.725 2.086 2.528 2.845 21 1.063 1.323 1.721 2.080 2.518 2.831 22 1.061 1.321 1.717 2.074 2.508 2.819 23 1.060 1.319 1.714 2.069 2.500 2.807 24 1.059 1.318 1.711 2.064 2.492 2.797 25 1.058 1.316 1.708 2.060 2.485 2.787 26 1.058 1.315 1.706 2.056 2.479 2.779 27 1.057 1.314 1.703 2.052 2.473 2.771 28 1.056 1.313 1.701 2.048 2.467 2.763 Односторонние квантили θ k 0.15 0.1 0.05 0.025 0.01 0.005 29 1.055 1.311 1.699 2.045 2.462 2.756 30 1.055 1.310 1.697 2.042 2.457 2.750 35 1.052 1.306 1.690 2.030 2.438 2.724 40 1.050 1.303 1.684 2.021 2.423 2.704 45 1.049 1.301 1.679 2.014 2.412 2.690 50 1.047 1.299 1.676 2.009 2.403 2.678 55 1.046 1.297 1.673 2.004 2.396 2.668 60 1.045 1.296 1.671 2.000 2.390 2.660 65 1.045 1.295 1.669 1.997 2.385 2.654 70 1.044 1.294 1.667 1.994 2.381 2.648 75 1.044 1.293 1.665 1.992 2.377 2.643 80 1.043 1.292 1.664 1.990 2.374 2.639 85 1.043 1.292 1.663 1.988 2.371 2.635 90 1.042 1.291 1.662 1.987 2.368 2.632 95 1.042 1.291 1.661 1.985 2.366 2.629 100 1.042 1.290 1.660 1.984 2.364 2.626 110 1.041 1.289 1.659 1.982 2.361 2.621 120 1.041 1.289 1.658 1.980 2.358 2.617 130 1.041 1.288 1.657 1.978 2.355 2.614 140 1.040 1.288 1.656 1.977 2.353 2.611 150 1.040 1.287 1.655 1.976 2.351 2.609 160 1.040 1.287 1.654 1.975 2.350 2.607 170 1.040 1.287 1.654 1.974 2.348 2.605 180 1.039 1.286 1.653 1.973 2.347 2.603 190 1.039 1.286 1.653 1.973 2.346 2.602 200 1.039 1.286 1.653 1.972 2.345 2.601 1.036 1.282 1.645 1.960 2.326 2.576 
721 Распределение хи-квадрат (χ2) Pr(χ2 > χ2k,1−θ) = θ. Границы χ2-распределения с k степенями свободы χ2k,1−θ. θ = 0.05 k = 5 5% , χ0.95 = 11.07 0 5 10 15 20 5% 11.07 Распределение хи-квадрат (χ2) Рис. B.3График плотности распределения χ2 для k =5
722 Приложение B. Статистические таблицы Таблица А.3. Границы χ2-распределения с k степенями свободы k 0.9 0.75 0.5 0.25 0.15 0.1 0.05 0.025 0.01 0.005 1 0.016 0.102 0.455 1.323 2.072 2.706 3.841 5.024 6.635 7.879 2 0.211 0.575 1.386 2.773 3.794 4.605 5.991 7.378 9.210 10.597 3 0.584 1.213 2.366 4.108 5.317 6.251 7.815 9.348 11.345 12.838 4 1.064 1.923 3.357 5.385 6.745 7.779 9.488 11.143 13.277 14.860 5 1.610 2.675 4.351 6.626 8.115 9.236 11.070 12.832 15.086 16.750 6 2.204 3.455 5.348 7.841 9.446 10.645 12.592 14.449 16.812 18.548 7 2.833 4.255 6.346 9.037 10.748 12.017 14.067 16.013 18.475 20.278 8 3.490 5.071 7.344 10.219 12.027 13.362 15.507 17.535 20.090 21.955 9 4.168 5.899 8.343 11.389 13.288 14.684 16.919 19.023 21.666 23.589 10 4.865 6.737 9.342 12.549 14.534 15.987 18.307 20.483 23.209 25.188 11 5.578 7.584 10.341 13.701 15.767 17.275 19.675 21.920 24.725 26.757 12 6.304 8.438 11.340 14.845 16.989 18.549 21.026 23.337 26.217 28.300 13 7.041 9.299 12.340 15.984 18.202 19.812 22.362 24.736 27.688 29.819 14 7.790 10.165 13.339 17.117 19.406 21.064 23.685 26.119 29.141 31.319 15 8.547 11.037 14.339 18.245 20.603 22.307 24.996 27.488 30.578 32.801 16 9.312 11.912 15.338 19.369 21.793 23.542 26.296 28.845 32.000 34.267 17 10.085 12.792 16.338 20.489 22.977 24.769 27.587 30.191 33.409 35.718 18 10.865 13.675 17.338 21.605 24.155 25.989 28.869 31.526 34.805 37.156 19 11.651 14.562 18.338 22.718 25.329 27.204 30.144 32.852 36.191 38.582 20 12.443 15.452 19.337 23.828 26.498 28.412 31.410 34.170 37.566 39.997 21 13.240 16.344 20.337 24.935 27.662 29.615 32.671 35.479 38.932 41.401 22 14.041 17.240 21.337 26.039 28.822 30.813 33.924 36.781 40.289 42.796 23 14.848 18.137 22.337 27.141 29.979 32.007 35.172 38.076 41.638 44.181 24 15.659 19.037 23.337 28.241 31.132 33.196 36.415 39.364 42.980 45.558 25 16.473 19.939 24.337 29.339 32.282 34.382 37.652 40.646 44.314 46.928 26 17.292 20.843 25.336 30.435 33.429 35.563 38.885 41.923 45.642 48.290 27 18.114 21.749 26.336 31.528 34.574 36.741 40.113 43.195 46.963 49.645 28 18.939 22.657 27.336 32.620 35.715 37.916 41.337 44.461 48.278 50.994 29 19.768 23.567 28.336 33.711 36.854 39.087 42.557 45.722 49.588 52.335 30 20.599 24.478 29.336 34.800 37.990 40.256 43.773 46.979 50.892 53.672 35 24.797 29.054 34.336 40.223 43.640 46.059 49.802 53.203 57.342 60.275 40 29.051 33.660 39.335 45.616 49.244 51.805 55.758 59.342 63.691 66.766 45 33.350 38.291 44.335 50.985 54.810 57.505 61.656 65.410 69.957 73.166 50 37.689 42.942 49.335 56.334 60.346 63.167 67.505 71.420 76.154 79.490 55 42.060 47.610 54.335 61.665 65.855 68.796 73.311 77.380 82.292 85.749 60 46.459 52.294 59.335 66.981 71.341 74.397 79.082 83.298 88.379 91.952 65 50.883 56.990 64.335 72.285 76.807 79.973 84.821 89.177 94.422 98.105 70 55.329 61.698 69.334 77.577 82.255 85.527 90.531 95.023 100.425 104.215 75 59.795 66.417 74.334 82.858 87.688 91.061 96.217 100.839 106.393 110.285 80 64.278 71.145 79.334 88.130 93.106 96.578 101.879 106.629 112.329 116.321 85 68.777 75.881 84.334 93.394 98.511 102.079 107.522 112.393 118.236 122.324 90 73.291 80.625 89.334 98.650 103.904 107.565 113.145 118.136 124.116 128.299 θ
723 Таблица А.3. Границы χ2-распределения с k степенями свободы (продолжение) k 0.9 0.75 0.5 0.25 0.15 0.1 0.05 0.025 0.01 0.005 95 77.818 85.376 94.334 103.899 109.286 113.038 118.752 123.858 129.973 134.247 100 82.358 90.133 99.334 109.141 114.659 118.498 124.342 129.561 135.807 140.170 110 91.471 99.666 109.334 119.608 125.376 129.385 135.480 140.916 147.414 151.948 120 100.624 109.220 119.334 130.055 136.062 140.233 146.567 152.211 158.950 163.648 130 109.811 118.792 129.334 140.482 146.719 151.045 157.610 163.453 170.423 175.278 140 119.029 128.380 139.334 150.894 157.352 161.827 168.613 174.648 181.841 186.847 150 128.275 137.983 149.334 161.291 167.962 172.581 179.581 185.800 193.207 198.360 160 137.546 147.599 159.334 171.675 178.552 183.311 190.516 196.915 204.530 209.824 170 146.839 157.227 169.334 182.047 189.123 194.017 201.423 207.995 215.812 221.242 180 156.153 166.865 179.334 192.409 199.679 204.704 212.304 219.044 227.056 232.620 190 165.485 176.514 189.334 202.760 210.218 215.371 223.160 230.064 238.266 243.959 200 174.835 186.172 199.334 213.102 220.744 226.021 233.994 241.058 249.445 255.264 θ Распределение F-Фишера k1 k2 k1 = 5 k2 = 10 5%, F0.95 = 3.33 1%, F0.99 = 5.64 Pr(F >Fk1,k2,0.95) = 0.05, Pr(F >Fk1,k2,0.99) = 0.01. Границы F-распределения с k1, k2 степенями свободы для 5% и 1%вероятности Fk1, k2, 1−θ 0 1 2 3 4 5 6 7 8 5% 1% 3.3 5.64 Распределение F Фишера Рис. B.4График плотности распределения для k1 = 5, k2 = 10
724 Приложение B. Статистические таблицы Таблица А.4. Границы F -распределения с k1 и k2 степенями свободы для 5% и 1% вероятности k1 k2 1 2 3 4 5 6 7 8 9 10 11 12 1 161.4 4052 199.5 4999 215.7 5404 224.6 5624 230.2 5764 234.0 5859 236.8 5928 238.9 5981 240.5 6022 241.9 6056 243.0 6083 243.9 6107 2 18.51 98.50 19.00 99.00 19.16 99.16 19.25 99.25 19.30 99.30 19.33 99.33 19.35 99.36 19.37 99.38 19.38 99.39 19.40 99.40 19.40 99.41 19.41 99.42 3 10.13 34.12 9.55 30.82 9.28 29.46 9.12 28.71 9.01 28.24 8.94 27.91 8.89 27.67 8.85 27.49 8.81 27.34 8.79 27.23 8.76 27.13 8.74 27.05 4 7.71 21.20 6.94 18.00 6.59 16.69 6.39 15.98 6.26 15.52 6.16 15.21 6.09 14.98 6.04 14.80 6.00 14.66 5.96 14.55 5.94 14.45 5.91 14.37 5 6.61 16.26 5.79 13.27 5.41 12.06 5.19 11.39 5.05 10.97 4.95 10.67 4.88 10.46 4.82 10.29 4.77 10.16 4.74 10.05 4.70 9.96 4.68 9.89 6 5.99 13.75 5.14 10.92 4.76 9.78 4.53 9.15 4.39 8.75 4.28 8.47 4.21 8.26 4.15 8.10 4.10 7.98 4.06 7.87 4.03 7.79 4.00 7.72 7 5.59 12.25 4.74 9.55 4.35 8.45 4.12 7.85 3.97 7.46 3.87 7.19 3.79 6.99 3.73 6.84 3.68 6.72 3.64 6.62 3.60 6.54 3.57 6.47 8 5.32 11.26 4.46 8.65 4.07 7.59 3.84 7.01 3.69 6.63 3.58 6.37 3.50 6.18 3.44 6.03 3.39 5.91 3.35 5.81 3.31 5.73 3.28 5.67 9 5.12 10.56 4.26 8.02 3.86 6.99 3.63 6.42 3.48 6.06 3.37 5.80 3.29 5.61 3.23 5.47 3.18 5.35 3.14 5.26 3.10 5.18 3.07 5.11 10 4.96 10.04 4.10 7.56 3.71 6.55 3.48 5.99 3.33 5.64 3.22 5.39 3.14 5.20 3.07 5.06 3.02 4.94 2.98 4.85 2.94 4.77 2.91 4.71 11 4.84 9.65 3.98 7.21 3.59 6.22 3.36 5.67 3.20 5.32 3.09 5.07 3.01 4.89 2.95 4.74 2.90 4.63 2.85 4.54 2.82 4.46 2.79 4.40 12 4.75 9.33 3.89 6.93 3.49 5.95 3.26 5.41 3.11 5.06 3.00 4.82 2.91 4.64 2.85 4.50 2.80 4.39 2.75 4.30 2.72 4.22 2.69 4.16 13 4.67 9.07 3.81 6.70 3.41 5.74 3.18 5.21 3.03 4.86 2.92 4.62 2.83 4.44 2.77 4.30 2.71 4.19 2.67 4.10 2.63 4.02 2.60 3.96 14 4.60 8.86 3.74 6.51 3.34 5.56 3.11 5.04 2.96 4.69 2.85 4.46 2.76 4.28 2.70 4.14 2.65 4.03 2.60 3.94 2.57 3.86 2.53 3.80 15 4.54 8.68 3.68 6.36 3.29 5.42 3.06 4.89 2.90 4.56 2.79 4.32 2.71 4.14 2.64 4.00 2.59 3.89 2.54 3.80 2.51 3.73 2.48 3.67 16 4.49 8.53 3.63 6.23 3.24 5.29 3.01 4.77 2.85 4.44 2.74 4.20 2.66 4.03 2.59 3.89 2.54 3.78 2.49 3.69 2.46 3.62 2.42 3.55 17 4.45 8.40 3.59 6.11 3.20 5.19 2.96 4.67 2.81 4.34 2.70 4.10 2.61 3.93 2.55 3.79 2.49 3.68 2.45 3.59 2.41 3.52 2.38 3.46 18 4.41 8.29 3.55 6.01 3.16 5.09 2.93 4.58 2.77 4.25 2.66 4.01 2.58 3.84 2.51 3.71 2.46 3.60 2.41 3.51 2.37 3.43 2.34 3.37 19 4.38 8.18 3.52 5.93 3.13 5.01 2.90 4.50 2.74 4.17 2.63 3.94 2.54 3.77 2.48 3.63 2.42 3.52 2.38 3.43 2.34 3.36 2.31 3.30 20 4.35 8.10 3.49 5.85 3.10 4.94 2.87 4.43 2.71 4.10 2.60 3.87 2.51 3.70 2.45 3.56 2.39 3.46 2.35 3.37 2.31 3.29 2.28 3.23 21 4.32 8.02 3.47 5.78 3.07 4.87 2.84 4.37 2.68 4.04 2.57 3.81 2.49 3.64 2.42 3.51 2.37 3.40 2.32 3.31 2.28 3.24 2.25 3.17 22 4.30 7.95 3.44 5.72 3.05 4.82 2.82 4.31 2.66 3.99 2.55 3.76 2.46 3.59 2.40 3.45 2.34 3.35 2.30 3.26 2.26 3.18 2.23 3.12 23 4.28 7.88 3.42 5.66 3.03 4.76 2.80 4.26 2.64 3.94 2.53 3.71 2.44 3.54 2.37 3.41 2.32 3.30 2.27 3.21 2.24 3.14 2.20 3.07 24 4.26 7.82 3.40 5.61 3.01 4.72 2.78 4.22 2.62 3.90 2.51 3.67 2.42 3.50 2.36 3.36 2.30 3.26 2.25 3.17 2.22 3.09 2.18 3.03 25 4.24 7.77 3.39 5.57 2.99 4.68 2.76 4.18 2.60 3.85 2.49 3.63 2.40 3.46 2.34 3.32 2.28 3.22 2.24 3.13 2.20 3.06 2.16 2.99 26 4.23 7.72 3.37 5.53 2.98 4.64 2.74 4.14 2.59 3.82 2.47 3.59 2.39 3.42 2.32 3.29 2.27 3.18 2.22 3.09 2.18 3.02 2.15 2.96 
725 Таблица А.4. Границы F -распределения с k1 и k2 степенями свободы для 5% и 1%вероятности (продолжение) k1 k2 1 2 3 4 5 6 7 8 9 10 11 12 27 4.21 7.68 3.35 5.49 2.96 4.60 2.73 4.11 2.57 3.78 2.46 3.56 2.37 3.39 2.31 3.26 2.25 3.15 2.20 3.06 2.17 2.99 2.13 2.93 28 4.20 7.64 3.34 5.45 2.95 4.57 2.71 4.07 2.56 3.75 2.45 3.53 2.36 3.36 2.29 3.23 2.24 3.12 2.19 3.03 2.15 2.96 2.12 2.90 29 4.18 7.60 3.33 5.42 2.93 4.54 2.70 4.04 2.55 3.73 2.43 3.50 2.35 3.33 2.28 3.20 2.22 3.09 2.18 3.00 2.14 2.93 2.10 2.87 30 4.17 7.56 3.32 5.39 2.92 4.51 2.69 4.02 2.53 3.70 2.42 3.47 2.33 3.30 2.27 3.17 2.21 3.07 2.16 2.98 2.13 2.91 2.09 2.84 32 4.15 7.50 3.29 5.34 2.90 4.46 2.67 3.97 2.51 3.65 2.40 3.43 2.31 3.26 2.24 3.13 2.19 3.02 2.14 2.93 2.10 2.86 2.07 2.80 34 4.13 7.44 3.28 5.29 2.88 4.42 2.65 3.93 2.49 3.61 2.38 3.39 2.29 3.22 2.23 3.09 2.17 2.98 2.12 2.89 2.08 2.82 2.05 2.76 36 4.11 7.40 3.26 5.25 2.87 4.38 2.63 3.89 2.48 3.57 2.36 3.35 2.28 3.18 2.21 3.05 2.15 2.95 2.11 2.86 2.07 2.79 2.03 2.72 38 4.10 7.35 3.24 5.21 2.85 4.34 2.62 3.86 2.46 3.54 2.35 3.32 2.26 3.15 2.19 3.02 2.14 2.92 2.09 2.83 2.05 2.75 2.02 2.69 40 4.08 7.31 3.23 5.18 2.84 4.31 2.61 3.83 2.45 3.51 2.34 3.29 2.25 3.12 2.18 2.99 2.12 2.89 2.08 2.80 2.04 2.73 2.00 2.66 42 4.07 7.28 3.22 5.15 2.83 4.29 2.59 3.80 2.44 3.49 2.32 3.27 2.24 3.10 2.17 2.97 2.11 2.86 2.06 2.78 2.03 2.70 1.99 2.64 44 4.06 7.25 3.21 5.12 2.82 4.26 2.58 3.78 2.43 3.47 2.31 3.24 2.23 3.08 2.16 2.95 2.10 2.84 2.05 2.75 2.01 2.68 1.98 2.62 46 4.05 7.22 3.20 5.10 2.81 4.24 2.57 3.76 2.42 3.44 2.30 3.22 2.22 3.06 2.15 2.93 2.09 2.82 2.04 2.73 2.00 2.66 1.97 2.60 48 4.04 7.19 3.19 5.08 2.80 4.22 2.57 3.74 2.41 3.43 2.29 3.20 2.21 3.04 2.14 2.91 2.08 2.80 2.03 2.71 1.99 2.64 1.96 2.58 50 4.03 7.17 3.18 5.06 2.79 4.20 2.56 3.72 2.40 3.41 2.29 3.19 2.20 3.02 2.13 2.89 2.07 2.78 2.03 2.70 1.99 2.63 1.95 2.56 55 4.02 7.12 3.16 5.01 2.77 4.16 2.54 3.68 2.38 3.37 2.27 3.15 2.18 2.98 2.11 2.85 2.06 2.75 2.01 2.66 1.97 2.59 1.93 2.53 60 4.00 7.08 3.15 4.98 2.76 4.13 2.53 3.65 2.37 3.34 2.25 3.12 2.17 2.95 2.10 2.82 2.04 2.72 1.99 2.63 1.95 2.56 1.92 2.50 65 3.99 7.04 3.14 4.95 2.75 4.10 2.51 3.62 2.36 3.31 2.24 3.09 2.15 2.93 2.08 2.80 2.03 2.69 1.98 2.61 1.94 2.53 1.90 2.47 70 3.98 7.01 3.13 4.92 2.74 4.07 2.50 3.60 2.35 3.29 2.23 3.07 2.14 2.91 2.07 2.78 2.02 2.67 1.97 2.59 1.93 2.51 1.89 2.45 80 3.96 6.96 3.11 4.88 2.72 4.04 2.49 3.56 2.33 3.26 2.21 3.04 2.13 2.87 2.06 2.74 2.00 2.64 1.95 2.55 1.91 2.48 1.88 2.42 90 3.95 6.93 3.10 4.85 2.71 4.01 2.47 3.53 2.32 3.23 2.20 3.01 2.11 2.84 2.04 2.72 1.99 2.61 1.94 2.52 1.90 2.45 1.86 2.39 100 3.94 6.90 3.09 4.82 2.70 3.98 2.46 3.51 2.31 3.21 2.19 2.99 2.10 2.82 2.03 2.69 1.97 2.59 1.93 2.50 1.89 2.43 1.85 2.37 125 3.92 6.84 3.07 4.78 2.68 3.94 2.44 3.47 2.29 3.17 2.17 2.95 2.08 2.79 2.01 2.66 1.96 2.55 1.91 2.47 1.87 2.39 1.83 2.33 150 3.90 6.81 3.06 4.75 2.66 3.91 2.43 3.45 2.27 3.14 2.16 2.92 2.07 2.76 2.00 2.63 1.94 2.53 1.89 2.44 1.85 2.37 1.82 2.31 200 3.89 6.76 3.04 4.71 2.65 3.88 2.42 3.41 2.26 3.11 2.14 2.89 2.06 2.73 1.98 2.60 1.93 2.50 1.88 2.41 1.84 2.34 1.80 2.27 400 3.86 6.70 3.02 4.66 2.63 3.83 2.39 3.37 2.24 3.06 2.12 2.85 2.03 2.68 1.96 2.56 1.90 2.45 1.85 2.37 1.81 2.29 1.78 2.23 1000 3.85 6.66 3.00 4.63 2.61 3.80 2.38 3.34 2.22 3.04 2.11 2.82 2.02 2.66 1.95 2.53 1.89 2.43 1.84 2.34 1.80 2.27 1.76 2.20 
726 Приложение B. Статистические таблицы Таблица А.4. Границы F -распределения с k1 и k2 степенями свободы для 5% и 1%вероятности (продолжение) k1 k2 14 16 18 20 24 30 40 50 75 100 200 500 1 245.4 6143 246.5 6170 247.3 6191 248.0 6209 249.1 6234 250.1 6260 251.1 6286 251.8 6302 252.6 6324 253.0 6334 253.7 6350 254.1 6360 2 19.42 99.43 19.43 99.44 19.44 99.44 19.45 99.45 19.45 99.46 19.46 99.47 19.47 99.48 19.48 99.48 19.48 99.48 19.49 99.49 19.49 99.49 19.49 99.50 3 8.71 26.92 8.69 26.83 8.67 26.75 8.66 26.69 8.64 26.60 8.62 26.50 8.59 26.41 8.58 26.35 8.56 26.28 8.55 26.24 8.54 26.18 8.53 26.15 4 5.87 14.25 5.84 14.15 5.82 14.08 5.80 14.02 5.77 13.93 5.75 13.84 5.72 13.75 5.70 13.69 5.68 13.61 5.66 13.58 5.65 13.52 5.64 13.49 5 4.64 9.77 4.60 9.68 4.58 9.61 4.56 9.55 4.53 9.47 4.50 9.38 4.46 9.29 4.44 9.24 4.42 9.17 4.41 9.13 4.39 9.08 4.37 9.04 6 3.96 7.60 3.92 7.52 3.90 7.45 3.87 7.40 3.84 7.31 3.81 7.23 3.77 7.14 3.75 7.09 3.73 7.02 3.71 6.99 3.69 6.93 3.68 6.90 7 3.53 6.36 3.49 6.28 3.47 6.21 3.44 6.16 3.41 6.07 3.38 5.99 3.34 5.91 3.32 5.86 3.29 5.79 3.27 5.75 3.25 5.70 3.24 5.67 8 3.24 5.56 3.20 5.48 3.17 5.41 3.15 5.36 3.12 5.28 3.08 5.20 3.04 5.12 3.02 5.07 2.99 5.00 2.97 4.96 2.95 4.91 2.94 4.88 9 3.03 5.01 2.99 4.92 2.96 4.86 2.94 4.81 2.90 4.73 2.86 4.65 2.83 4.57 2.80 4.52 2.77 4.45 2.76 4.41 2.73 4.36 2.72 4.33 10 2.86 4.60 2.83 4.52 2.80 4.46 2.77 4.41 2.74 4.33 2.70 4.25 2.66 4.17 2.64 4.12 2.60 4.05 2.59 4.01 2.56 3.96 2.55 3.93 11 2.74 4.29 2.70 4.21 2.67 4.15 2.65 4.10 2.61 4.02 2.57 3.94 2.53 3.86 2.51 3.81 2.47 3.74 2.46 3.71 2.43 3.66 2.42 3.62 12 2.64 4.05 2.60 3.97 2.57 3.91 2.54 3.86 2.51 3.78 2.47 3.70 2.43 3.62 2.40 3.57 2.37 3.50 2.35 3.47 2.32 3.41 2.31 3.38 13 2.55 3.86 2.51 3.78 2.48 3.72 2.46 3.66 2.42 3.59 2.38 3.51 2.34 3.43 2.31 3.38 2.28 3.31 2.26 3.27 2.23 3.22 2.22 3.19 14 2.48 3.70 2.44 3.62 2.41 3.56 2.39 3.51 2.35 3.43 2.31 3.35 2.27 3.27 2.24 3.22 2.21 3.15 2.19 3.11 2.16 3.06 2.14 3.03 15 2.42 3.56 2.38 3.49 2.35 3.42 2.33 3.37 2.29 3.29 2.25 3.21 2.20 3.13 2.18 3.08 2.14 3.01 2.12 2.98 2.10 2.92 2.08 2.89 16 2.37 3.45 2.33 3.37 2.30 3.31 2.28 3.26 2.24 3.18 2.19 3.10 2.15 3.02 2.12 2.97 2.09 2.90 2.07 2.86 2.04 2.81 2.02 2.78 17 2.33 3.35 2.29 3.27 2.26 3.21 2.23 3.16 2.19 3.08 2.15 3.00 2.10 2.92 2.08 2.87 2.04 2.80 2.02 2.76 1.99 2.71 1.97 2.68 18 2.29 3.27 2.25 3.19 2.22 3.13 2.19 3.08 2.15 3.00 2.11 2.92 2.06 2.84 2.04 2.78 2.00 2.71 1.98 2.68 1.95 2.62 1.93 2.59 19 2.26 3.19 2.21 3.12 2.18 3.05 2.16 3.00 2.11 2.92 2.07 2.84 2.03 2.76 2.00 2.71 1.96 2.64 1.94 2.60 1.91 2.55 1.89 2.51 20 2.22 3.13 2.18 3.05 2.15 2.99 2.12 2.94 2.08 2.86 2.04 2.78 1.99 2.69 1.97 2.64 1.93 2.57 1.91 2.54 1.88 2.48 1.86 2.44 21 2.20 3.07 2.16 2.99 2.12 2.93 2.10 2.88 2.05 2.80 2.01 2.72 1.96 2.64 1.94 2.58 1.90 2.51 1.88 2.48 1.84 2.42 1.83 2.38 22 2.17 3.02 2.13 2.94 2.10 2.88 2.07 2.83 2.03 2.75 1.98 2.67 1.94 2.58 1.91 2.53 1.87 2.46 1.85 2.42 1.82 2.36 1.80 2.33 23 2.15 2.97 2.11 2.89 2.08 2.83 2.05 2.78 2.01 2.70 1.96 2.62 1.91 2.54 1.88 2.48 1.84 2.41 1.82 2.37 1.79 2.32 1.77 2.28 24 2.13 2.93 2.09 2.85 2.05 2.79 2.03 2.74 1.98 2.66 1.94 2.58 1.89 2.49 1.86 2.44 1.82 2.37 1.80 2.33 1.77 2.27 1.75 2.24 25 2.11 2.89 2.07 2.81 2.04 2.75 2.01 2.70 1.96 2.62 1.92 2.54 1.87 2.45 1.84 2.40 1.80 2.33 1.78 2.29 1.75 2.23 1.73 2.19 26 2.09 2.86 2.05 2.78 2.02 2.72 1.99 2.66 1.95 2.58 1.90 2.50 1.85 2.42 1.82 2.36 1.78 2.29 1.76 2.25 1.73 2.19 1.71 2.16 
727 Таблица А.4. Границы F -распределения с k1 и k2 степенями свободы для 5% и 1%вероятности (продолжение) k1 k2 14 16 18 20 24 30 40 50 75 100 200 500 27 2.08 2.82 2.04 2.75 2.00 2.68 1.97 2.63 1.93 2.55 1.88 2.47 1.84 2.38 1.81 2.33 1.76 2.26 1.74 2.22 1.71 2.16 1.69 2.12 28 2.06 2.79 2.02 2.72 1.99 2.65 1.96 2.60 1.91 2.52 1.87 2.44 1.82 2.35 1.79 2.30 1.75 2.23 1.73 2.19 1.69 2.13 1.67 2.09 29 2.05 2.77 2.01 2.69 1.97 2.63 1.94 2.57 1.90 2.49 1.85 2.41 1.81 2.33 1.77 2.27 1.73 2.20 1.71 2.16 1.67 2.10 1.65 2.06 30 2.04 2.74 1.99 2.66 1.96 2.60 1.93 2.55 1.89 2.47 1.84 2.39 1.79 2.30 1.76 2.25 1.72 2.17 1.70 2.13 1.66 2.07 1.64 2.03 32 2.01 2.70 1.97 2.62 1.94 2.55 1.91 2.50 1.86 2.42 1.82 2.34 1.77 2.25 1.74 2.20 1.69 2.12 1.67 2.08 1.63 2.02 1.61 1.98 34 1.99 2.66 1.95 2.58 1.92 2.51 1.89 2.46 1.84 2.38 1.80 2.30 1.75 2.21 1.71 2.16 1.67 2.08 1.65 2.04 1.61 1.98 1.59 1.94 36 1.98 2.62 1.93 2.54 1.90 2.48 1.87 2.43 1.82 2.35 1.78 2.26 1.73 2.18 1.69 2.12 1.65 2.04 1.62 2.00 1.59 1.94 1.56 1.90 38 1.96 2.59 1.92 2.51 1.88 2.45 1.85 2.40 1.81 2.32 1.76 2.23 1.71 2.14 1.68 2.09 1.63 2.01 1.61 1.97 1.57 1.90 1.54 1.86 40 1.95 2.56 1.90 2.48 1.87 2.42 1.84 2.37 1.79 2.29 1.74 2.20 1.69 2.11 1.66 2.06 1.61 1.98 1.59 1.94 1.55 1.87 1.53 1.83 42 1.94 2.54 1.89 2.46 1.86 2.40 1.83 2.34 1.78 2.26 1.73 2.18 1.68 2.09 1.65 2.03 1.60 1.95 1.57 1.91 1.53 1.85 1.51 1.80 44 1.92 2.52 1.88 2.44 1.84 2.37 1.81 2.32 1.77 2.24 1.72 2.15 1.67 2.07 1.63 2.01 1.59 1.93 1.56 1.89 1.52 1.82 1.49 1.78 46 1.91 2.50 1.87 2.42 1.83 2.35 1.80 2.30 1.76 2.22 1.71 2.13 1.65 2.04 1.62 1.99 1.57 1.91 1.55 1.86 1.51 1.80 1.48 1.76 48 1.90 2.48 1.86 2.40 1.82 2.33 1.79 2.28 1.75 2.20 1.70 2.12 1.64 2.02 1.61 1.97 1.56 1.89 1.54 1.84 1.49 1.78 1.47 1.73 50 1.89 2.46 1.85 2.38 1.81 2.32 1.78 2.27 1.74 2.18 1.69 2.10 1.63 2.01 1.60 1.95 1.55 1.87 1.52 1.82 1.48 1.76 1.46 1.71 55 1.88 2.42 1.83 2.34 1.79 2.28 1.76 2.23 1.72 2.15 1.67 2.06 1.61 1.97 1.58 1.91 1.53 1.83 1.50 1.78 1.46 1.71 1.43 1.67 60 1.86 2.39 1.82 2.31 1.78 2.25 1.75 2.20 1.70 2.12 1.65 2.03 1.59 1.94 1.56 1.88 1.51 1.79 1.48 1.75 1.44 1.68 1.41 1.63 65 1.85 2.37 1.80 2.29 1.76 2.23 1.73 2.17 1.69 2.09 1.63 2.00 1.58 1.91 1.54 1.85 1.49 1.77 1.46 1.72 1.42 1.65 1.39 1.60 70 1.84 2.35 1.79 2.27 1.75 2.20 1.72 2.15 1.67 2.07 1.62 1.98 1.57 1.89 1.53 1.83 1.48 1.74 1.45 1.70 1.40 1.62 1.37 1.57 80 1.82 2.31 1.77 2.23 1.73 2.17 1.70 2.12 1.65 2.03 1.60 1.94 1.54 1.85 1.51 1.79 1.45 1.70 1.43 1.65 1.38 1.58 1.35 1.53 90 1.80 2.29 1.76 2.21 1.72 2.14 1.69 2.09 1.64 2.00 1.59 1.92 1.53 1.82 1.49 1.76 1.44 1.67 1.41 1.62 1.36 1.55 1.33 1.49 100 1.79 2.27 1.75 2.19 1.71 2.12 1.68 2.07 1.63 1.98 1.57 1.89 1.52 1.80 1.48 1.74 1.42 1.65 1.39 1.60 1.34 1.52 1.31 1.47 125 1.77 2.23 1.73 2.15 1.69 2.08 1.66 2.03 1.60 1.94 1.55 1.85 1.49 1.76 1.45 1.69 1.40 1.60 1.36 1.55 1.31 1.47 1.27 1.41 150 1.76 2.20 1.71 2.12 1.67 2.06 1.64 2.00 1.59 1.92 1.54 1.83 1.48 1.73 1.44 1.66 1.38 1.57 1.34 1.52 1.29 1.43 1.25 1.38 200 1.74 2.17 1.69 2.09 1.66 2.03 1.62 1.97 1.57 1.89 1.52 1.79 1.46 1.69 1.41 1.63 1.35 1.53 1.32 1.48 1.26 1.39 1.22 1.33 400 1.72 2.13 1.67 2.05 1.63 1.98 1.60 1.92 1.54 1.84 1.49 1.75 1.42 1.64 1.38 1.58 1.32 1.48 1.28 1.42 1.22 1.32 1.17 1.25 1000 1.70 2.10 1.65 2.02 1.61 1.95 1.58 1.90 1.53 1.81 1.47 1.72 1.41 1.61 1.36 1.54 1.30 1.44 1.26 1.38 1.19 1.28 1.13 1.19 
728 Приложение B. Статистические таблицы Критерий Дарбина-Уотсона Значащие точки dL и dU, для 5% уровня значимости. N -количество наблюдений, n -количество объясняющих переменных (без учета постоянного члена). Критерий Дарбина-Уотсона 0 dL dU 4-dU 4-dL 2.0 4.0 Положительная автокорреляция Зона неопределенности Нет автокорреляции Отрицательная автокорреляция Рис. B.5
729 Таблица. А.5 Значения статистики dL и dU критерия Дарбина-Уотсона n = 1 n = 2 n = 3 n = 4 n = 5 n = 6 n = 7 n = 8 n = 9 n = 10 N dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL 6 0.610 1.400 7 0.700 1.356 0.467 1.896 8 0.763 1.332 0.559 1.777 0.368 2.287 9 0.824 1.320 0.629 1.699 0.455 2.128 0.296 2.588 10 0.879 1.320 0.697 1.641 0.525 2.016 0.376 2.414 0.243 2.822 11 0.927 1.324 0.758 1.604 0.595 1.928 0.444 2.283 0.316 2.645 0.203 3.005 12 0.971 1.331 0.812 1.579 0.658 1.864 0.512 2.177 0.379 2.506 0.268 2.832 0.171 3.149 13 1.010 1.340 0.861 1.562 0.715 1.816 0.574 2.094 0.445 2.390 0.328 2.692 0.230 2.985 0.147 3.266 14 1.045 1.350 0.905 1.551 0.767 1.779 0.632 2.030 0.505 2.296 0.389 2.572 0.286 2.848 0.200 3.111 0.127 3.360 15 1.077 1.361 0.946 1.543 0.814 1.750 0.685 1.977 0.562 2.220 0.447 2.472 0.343 2.727 0.251 2.979 0.175 3.216 0.111 3.438 16 1.106 1.371 0.982 1.539 0.857 1.728 0.734 1.935 0.615 2.157 0.502 2.388 0.398 2.624 0.304 2.860 0.222 3.090 0.155 3.304 17 1.133 1.381 1.015 1.536 0.897 1.710 0.779 1.900 0.664 2.104 0.554 2.318 0.451 2.537 0.356 2.757 0.272 2.975 0.198 3.184 18 1.158 1.391 1.046 1.535 0.933 1.696 0.820 1.872 0.710 2.060 0.603 2.257 0.502 2.461 0.407 2.667 0.321 2.873 0.244 3.073 19 1.180 1.401 1.074 1.536 0.967 1.685 0.859 1.848 0.752 2.023 0.649 2.206 0.549 2.396 0.456 2.589 0.369 2.783 0.290 2.974 20 1.201 1.411 1.100 1.537 0.998 1.676 0.894 1.828 0.792 1.991 0.692 2.162 0.595 2.339 0.502 2.521 0.416 2.704 0.336 2.885 21 1.221 1.420 1.125 1.538 1.026 1.669 0.927 1.812 0.829 1.964 0.732 2.124 0.637 2.290 0.547 2.460 0.461 2.633 0.380 2.806 22 1.239 1.429 1.147 1.541 1.053 1.664 0.958 1.797 0.863 1.940 0.769 2.090 0.677 2.246 0.588 2.407 0.504 2.571 0.424 2.734 23 1.257 1.437 1.168 1.543 1.078 1.660 0.986 1.785 0.895 1.920 0.804 2.061 0.715 2.208 0.628 2.360 0.545 2.514 0.465 2.670 24 1.273 1.446 1.188 1.546 1.101 1.656 1.013 1.775 0.925 1.902 0.837 2.035 0.751 2.174 0.666 2.318 0.584 2.464 0.506 2.613 25 1.288 1.454 1.206 1.550 1.123 1.654 1.038 1.767 0.953 1.886 0.868 2.012 0.784 2.144 0.702 2.280 0.621 2.419 0.544 2.560 26 1.302 1.461 1.224 1.553 1.143 1.652 1.062 1.759 0.979 1.873 0.897 1.992 0.816 2.117 0.735 2.246 0.657 2.379 0.581 2.513 27 1.316 1.469 1.240 1.556 1.162 1.651 1.084 1.753 1.004 1.861 0.925 1.974 0.845 2.093 0.767 2.216 0.691 2.342 0.616 2.470 28 1.328 1.476 1.255 1.560 1.181 1.650 1.104 1.747 1.028 1.850 0.951 1.958 0.874 2.071 0.798 2.188 0.723 2.309 0.650 2.431 29 1.341 1.483 1.270 1.563 1.198 1.650 1.124 1.743 1.050 1.841 0.975 1.944 0.900 2.052 0.826 2.164 0.753 2.278 0.682 2.396 30 1.352 1.489 1.284 1.567 1.214 1.650 1.143 1.739 1.071 1.833 0.998 1.931 0.926 2.034 0.854 2.141 0.782 2.251 0.712 2.363
730 Приложение B. Статистические таблицы Таблица. А.5 Значения статистики dL и dU критерия Дарбина-Уотсона (продолжение) n = 1 n = 2 n = 3 n = 4 n = 5 n = 6 n = 7 n = 8 n = 9 n = 10 N dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL 31 1.363 1.496 1.297 1.570 1.229 1.650 1.160 1.735 1.090 1.825 1.020 1.920 0.950 2.018 0.879 2.120 0.810 2.226 0.741 2.333 32 1.373 1.502 1.309 1.574 1.244 1.650 1.177 1.732 1.109 1.819 1.041 1.909 0.972 2.004 0.904 2.102 0.836 2.203 0.769 2.306 33 1.383 1.508 1.321 1.577 1.258 1.651 1.193 1.730 1.127 1.813 1.061 1.900 0.994 1.991 0.927 2.085 0.861 2.181 0.795 2.281 34 1.393 1.514 1.333 1.580 1.271 1.652 1.208 1.728 1.144 1.808 1.080 1.891 1.015 1.979 0.950 2.069 0.885 2.162 0.821 2.257 35 1.402 1.519 1.343 1.584 1.283 1.653 1.222 1.726 1.160 1.803 1.097 1.884 1.034 1.967 0.971 2.054 0.908 2.144 0.845 2.236 36 1.411 1.525 1.354 1.587 1.295 1.654 1.236 1.724 1.175 1.799 1.114 1.877 1.053 1.957 0.991 2.041 0.930 2.127 0.868 2.216 37 1.419 1.530 1.364 1.590 1.307 1.655 1.249 1.723 1.190 1.795 1.131 1.870 1.071 1.948 1.011 2.029 0.951 2.112 0.891 2.198 38 1.427 1.535 1.373 1.594 1.318 1.656 1.261 1.722 1.204 1.792 1.146 1.864 1.088 1.939 1.029 2.017 0.970 2.098 0.912 2.180 39 1.435 1.540 1.382 1.597 1.328 1.658 1.273 1.722 1.218 1.789 1.161 1.859 1.104 1.932 1.047 2.007 0.990 2.085 0.932 2.164 40 1.442 1.544 1.391 1.600 1.338 1.659 1.285 1.721 1.230 1.786 1.175 1.854 1.120 1.924 1.064 1.997 1.008 2.072 0.945 2.149 45 1.475 1.566 1.430 1.615 1.383 1.666 1.336 1.720 1.287 1.776 1.238 1.835 1.189 1.895 1.139 1.958 1.089 2.022 1.038 2.088 50 1.503 1.585 1.462 1.628 1.421 1.674 1.378 1.721 1.335 1.771 1.291 1.822 1.246 1.875 1.201 1.930 1.156 1.986 1.110 2.044 55 1.528 1.601 1.490 1.641 1.452 1.681 1.414 1.724 1.374 1.768 1.334 1.814 1.294 1.861 1.253 1.909 1.212 1.959 1.170 2.010 60 1.549 1.616 1.514 1.652 1.480 1.689 1.444 1.727 1.408 1.767 1.372 1.808 1.335 1.850 1.298 1.894 1.260 1.939 1.222 1.984 65 1.567 1.629 1.536 1.662 1.503 1.696 1.471 1.731 1.438 1.767 1.404 1.805 1.370 1.843 1.336 1.882 1.301 1.923 1.266 1.964 70 1.583 1.641 1.554 1.672 1.525 1.703 1.494 1.735 1.464 1.768 1.433 1.802 1.401 1.837 1.369 1.873 1.337 1.910 1.305 1.948 75 1.598 1.652 1.571 1.680 1.543 1.709 1.515 1.739 1.487 1.770 1.458 1.801 1.428 1.834 1.399 1.867 1.369 1.901 1.339 1.935 80 1.611 1.662 1.586 1.688 1.560 1.715 1.534 1.743 1.507 1.772 1.480 1.801 1.453 1.831 1.425 1.861 1.397 1.893 1.369 1.925 85 1.624 1.671 1.600 1.696 1.575 1.721 1.550 1.747 1.525 1.774 1.500 1.801 1.474 1.829 1.448 1.857 1.422 1.886 1.396 1.916 90 1.635 1.679 1.612 1.703 1.589 1.726 1.566 1.751 1.542 1.776 1.518 1.801 1.494 1.827 1.469 1.854 1.445 1.881 1.420 1.909 95 1.645 1.687 1.623 1.709 1.602 1.732 1.579 1.755 1.557 1.778 1.535 1.802 1.512 1.827 1.489 1.852 1.465 1.877 1.442 1.903 100 1.654 1.694 1.634 1.715 1.613 1.736 1.592 1.758 1.571 1.780 1.550 1.803 1.528 1.826 1.506 1.850 1.484 1.874 1.462 1.898 150 1.720 1.746 1.706 1.760 1.693 1.774 1.679 1.788 1.665 1.802 1.651 1.817 1.637 1.832 1.622 1.847 1.608 1.862 1.594 1.877 200 1.758 1.778 1.748 1.789 1.738 1.799 1.728 1.810 1.718 1.820 1.707 1.831 1.697 1.841 1.686 1.852 1.675 1.863 1.665 1.874
731 Таблица А.5. Значения статистики dL и dU критерия Дарбина-Уотсона (продолжение) n = 11 n = 12 n = 13 n = 14 n = 15 n = 16 n = 17 n = 18 n = 19 N = 20 N dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL 16 0.098 3.503 17 0.138 3.378 0.087 3.557 18 0.177 3.265 0.123 3.441 0.078 3.603 19 0.220 3.159 0.160 3.335 0.111 3.496 0.070 3.642 20 0.263 3.063 0.200 3.234 0.145 3.395 0.100 3.542 0.063 3.676 21 0.307 2.976 0.240 3.141 0.182 3.300 0.132 3.448 0.091 3.583 0.058 3.705 22 0.349 2.897 0.281 3.057 0.220 3.211 0.166 3.358 0.120 3.495 0.083 3.619 0.052 3.731 23 0.391 2.826 0.322 2.979 0.259 3.128 0.202 3.272 0.153 3.409 0.110 3.535 0.076 3.650 0.048 3.753 24 0.431 2.761 0.362 2.908 0.297 3.053 0.239 3.193 0.186 3.327 0.141 3.454 0.101 3.572 0.070 3.678 0.044 3.773 25 0.470 2.702 0.400 2.844 0.335 2.983 0.275 3.119 0.221 3.251 0.172 3.376 0.130 3.494 0.094 3.604 0.065 3.702 0.041 3.790 26 0.508 2.649 0.438 2.784 0.373 2.919 0.312 3.051 0.256 3.179 0.205 3.303 0.160 3.420 0.120 3.531 0.087 3.632 0.060 3.724 27 0.544 2.600 0.475 2.730 0.409 2.859 0.348 2.987 0.291 3.112 0.238 3.233 0.191 3.349 0.149 3.460 0.112 3.563 0.081 3.658 28 0.578 2.555 0.510 2.680 0.445 2.805 0.383 2.928 0.325 3.050 0.271 3.168 0.222 3.283 0.178 3.392 0.138 3.495 0.104 3.592 29 0.612 2.515 0.544 2.634 0.479 2.755 0.418 2.874 0.359 2.992 0.305 3.107 0.254 3.219 0.208 3.327 0.166 3.431 0.129 3.528 30 0.643 2.477 0.577 2.592 0.512 2.708 0.451 2.823 0.392 2.937 0.337 3.050 0.286 3.160 0.238 3.266 0.195 3.368 0.156 3.465 31 0.674 2.443 0.608 2.553 0.545 2.665 0.484 2.776 0.425 2.887 0.370 2.996 0.317 3.103 0.269 3.208 0.224 3.309 0.183 3.406 32 0.703 2.411 0.638 2.517 0.576 2.625 0.515 2.733 0.457 2.840 0.401 2.946 0.349 3.050 0.299 3.153 0.253 3.252 0.211 3.348 33 0.731 2.382 0.668 2.484 0.606 2.588 0.546 2.692 0.488 2.796 0.432 2.899 0.379 3.000 0.329 3.100 0.283 3.198 0.239 3.293 34 0.758 2.355 0.695 2.454 0.634 2.554 0.575 2.654 0.518 2.754 0.462 2.854 0.409 2.954 0.359 3.051 0.312 3.147 0.267 3.240 35 0.783 2.330 0.722 2.425 0.662 2.521 0.604 2.619 0.547 2.716 0.492 2.813 0.439 2.910 0.388 3.005 0.340 3.099 0.295 3.190
732 Приложение B. Статистические таблицы Таблица А.5. Значения статистики dL и dU критерия Дарбина-Уотсона (продолжение) n = 11 n = 12 n = 13 n = 14 n = 15 n = 16 n = 17 n = 18 n = 19 N = 20 N dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL dU dL 36 0.808 2.306 0.748 2.398 0.689 2.492 0.631 2.586 0.575 2.680 0.520 2.774 0.467 2.868 0.417 2.961 0.369 3.053 0.323 3.142 37 0.831 2.285 0.772 2.374 0.714 2.464 0.657 2.555 0.602 2.646 0.548 2.738 0.495 2.829 0.445 2.920 0.397 3.009 0.351 3.097 38 0.854 2.265 0.796 2.351 0.739 2.438 0.683 2.526 0.628 2.614 0.575 2.703 0.522 2.792 0.472 2.880 0.424 2.968 0.378 3.054 39 0.875 2.246 0.819 2.329 0.763 2.413 0.707 2.499 0.653 2.585 0.600 2.671 0.549 2.757 0.499 2.843 0.451 2.929 0.404 3.013 40 0.896 2.228 0.840 2.309 0.785 2.391 0.731 2.473 0.678 2.557 0.626 2.641 0.575 2.724 0.525 2.808 0.477 2.892 0.430 2.974 45 0.988 2.156 0.938 2.225 0.887 2.296 0.838 2.367 0.788 2.439 0.740 2.512 0.692 2.586 0.644 2.659 0.598 2.733 0.553 2.807 50 1.064 2.103 1.019 2.163 0.973 2.225 0.927 2.287 0.882 2.350 0.836 2.414 0.792 2.479 0.747 2.544 0.703 2.610 0.660 2.675 55 1.129 2.062 1.087 2.116 1.045 2.170 1.003 2.225 0.961 2.281 0.919 2.338 0.877 2.396 0.836 2.454 0.795 2.512 0.754 2.571 60 1.184 2.031 1.145 2.079 1.106 2.127 1.068 2.177 1.029 2.227 0.990 2.278 0.951 2.330 0.913 2.382 0.874 2.434 0.836 2.487 65 1.231 2.006 1.195 2.049 1.160 2.093 1.124 2.138 1.088 2.183 1.052 2.229 1.016 2.276 0.980 2.323 0.944 2.371 0.908 2.419 70 1.272 1.986 1.239 2.026 1.206 2.066 1.172 2.106 1.139 2.148 1.105 2.189 1.072 2.232 1.038 2.275 1.005 2.318 0.971 2.362 75 1.308 1.970 1.277 2.006 1.247 2.043 1.215 2.080 1.184 2.118 1.153 2.156 1.121 2.195 1.090 2.235 1.058 2.275 1.027 2.315 80 1.340 1.957 1.311 1.991 1.283 2.024 1.253 2.059 1.224 2.093 1.195 2.129 1.165 2.165 1.136 2.201 1.106 2.238 1.076 2.275 85 1.369 1.946 1.342 1.977 1.315 2.009 1.287 2.040 1.260 2.073 1.232 2.105 1.205 2.139 1.177 2.172 1.149 2.206 1.121 2.241 90 1.395 1.937 1.369 1.966 1.344 1.995 1.318 2.025 1.292 2.055 1.266 2.085 1.240 2.116 1.213 2.148 1.187 2.179 1.160 2.211 95 1.418 1.929 1.394 1.956 1.370 1.984 1.345 2.012 1.321 2.040 1.296 2.068 1.271 2.097 1.247 2.126 1.222 2.156 1.197 2.186 100 1.439 1.923 1.416 1.948 1.393 1.974 1.371 2.000 1.347 2.026 1.324 2.053 1.301 2.080 1.277 2.108 1.253 2.135 1.229 2.164 150 1.579 1.892 1.564 1.908 1.550 1.924 1.535 1.940 1.519 1.956 1.504 1.972 1.489 1.989 1.474 2.006 1.458 2.023 1.443 2.040 200 1.654 1.885 1.643 1.896 1.632 1.908 1.621 1.919 1.610 1.931 1.599 1.943 1.588 1.955 1.576 1.967 1.565 1.979 1.554 1.991
Именной указатель Акаике Х. (Akaike H.), 239, 377 Байес Т. (Bayes Th.), 23, 601, 708 Бартлетт М. С. (BartlettM. S.), 260, 366 Беверидж С. (Beveridge S.), 550 Бернулли Д. (Bernoulli D.), 23 Бокс, 457, 474 Боллерслев Т. (Bollerslev T.), 527 ВальдА . (Wald A.), 587 Винер Н. (Wiener N.), 22, 414 Вольд, 463 Гальтон Ф. (Galton F.), 18, 147 Гаусс К. Ф. (Gauss C. F.), 18, 51 Герман К.Ф., 18 Годфрей Л. (Godfrey L.), 577 Голдфельд С. М. (Goldfeld S. M.), 262, 372 Готтелинг Г. (Hotteling H.), 358 Грейнджер К. (Granger C.), 558, 560, 665 Дженкинс, 457 Дивизиа Ф. (Divisia F.), 109 Дики Д. (Dickey D.), 554 Зинес Дж.Л. (Zinnes J.L.), 25 Йохансен С. (Johansen S.), 668 Квандт Р. (Quandt R.), 262, 372 Кейнс Дж. М. (Keynes J.M.), 23 Кетле А. (Quetelet A.), 18 Койк Л. (Koyck L.), 504 Кэмпбел Н.Р. (Campbell, N.R.), 24 Лаплас П. С. (Laplace P. S.), 18 Ласпейрес Э. (Laspeyres E.), 99 Лоренц М. (Lorenz M.), 75 Льюнг, 475 Марков, 432 Моргенштерн О. (Morgenstern O.), 25 Нейман Дж. фон (von Neumann J.), 25 Нельсон Ч. (Nelson Ch.), 536, 550 Пааше Г. (Paasche G.), 99 Парзен Э. (Parzen Е.), 420 Петти В. (PettyW.), 17 Пирс, 474 Пирсон К. (Pearson K.), 18 ПуассонС. Д. (Poisson S. D.), 23 Пфанцагль И. (Pfanzagle J.), 24 Рамсей Дж. (Ramsey J.), 578 Рамсей Ф. (Ramsey F.), 23 Синклер Дж. (Sinclair J.), 17 Спирмен Ч. (Spearman Ch.), 370 Стивенc С.С (Stevens S.S.), 24 Сток Дж. (Stock J.), 563 733
734 Именной указатель Суппес П. (Suppes, P.), 25 Тарский А. (Tarski A.), 25 Тьюки Дж. У. (Tukey J. W.), 420 Уинтерс П. Р. (Winters, P. R.), 402 Уокер, 438 Уотсон М. У. (Watson M.W.), 563 Фишер И. (Fisher I.), 99 Фуллер У. А. (FullerW. A.), 554 Ханин Г.И., 31 Хинчин А. Я., 414 Чоу Г. (Chow G.), 578 Шварц Г. (Schwarz G.), 239, 377 Энгл Р. (Engle R.), 524, 560 Юл, 435, 438
Предметный указатель ADF, 556, 557 ADL, 507 ARCH, 524 ARFIMA, 539 ARIMA, 465 ARMA, 457, 644 BLUE, 185, 229 ECM, 512 EGARCH, 536 FIGARCH, 539 GARCH, 527, 537 Q-статистика, 367, 475 RESET, 578 SVAR, 655 VAR, 654 VARMA, 654 VECM, 666 ОМНК, 257, 317 автоковариационная матрица, 351, 354, 383 автоковариационная функция, 351, 458 автоковариация, 351, 353, 355, 419 автокорреляционная матрица, 351, 471 автокорреляционная функция, 351, 355, 365, 366, 427, 438, 439, 453, 469, 529 автокорреляция, 265, 351, 353, 355, 363, 366, 367, 577 авторегрессионная модель с распреде-ленным лагом, 506 авторегрессионная условная гетероске-дастичность, 526 авторегрессия, 431, 449, 507, 644 амплитуда, 407 анализ данных, 22 аномальное наблюдение, 350, 360, 361 апостериорные вероятности, 602 априорные вероятности, 601 асимметрия, 51, 76, 80, 704 байесовская регрессия, 603 байесовский информационный критерий, 239, 377, 557 белый шум, 353, 356, 365, 367, 385, 416, 427 биномиальная зависимая переменная, 295, 627 векторная авторегрессия, 654 векторная модель исправления ошибок, 666 735
736 Предметный указатель веса лага, 376 взвешенная регрессия, 257, 264, 300 взрывной процесс, 434, 547 вложенный логит, 633 внутригрупповая дисперсия, 161 волатильность, 523 временной ряд, 347, 349 выборка, 52 выборочный спектр, 413 выброс, 350, 360, 361 гармонический тренд, 357, 387 гауссовские процессы, 349 гауссовский белый шум, 353, 427 гауссовский процесс, 479 генеральная совокупность, 24, 52 геометрический лаг, 504 гетероскедастичность, 259, 553 гистограмма, 50 главные компоненты, 208 главные факторы, 208 главные эффекты, 163 гомоскедастичность, 259, 363 группировка, 38, 160 двухшаговый метод наименьших квадра-тов, 275, 328 детерминированный процесс, 349 дефлятор, 99 дециль, 70 дисперсионное отношение, 262, 372 дисперсионное тождество, 165, 204 дисперсионный анализ, 160, 611 дисперсионный анализ без повторений, 160, 612 дисперсионный анализ с повторениями, 160, 618 дисперсия, 71, 353, 584, 704, 707 дисперсия ошибки прогноза, 240, 362, 479, 532, 664 доверительная область, 81, 713 доверительный интервал, 81, 188, 191, 231, 247, 364, 479 долгосрочная память, 538 долгосрочная эластичность, 501 долгосрочное стационарное состояние, 512 долгосрочный мультипликатор, 501, 509, 513, 662 доходность, 63 дробно-интегрированный процесс, 539 единичный корень, 463, 548, 553 идемпотентная матрица, 693, 698 идентификация, 234, 276, 319, 630, 632, 634, 635, 657 изучаемая величина, 182 изучаемая переменная, 37, 142, 201, 222 индекс, 89, 91, 106 Дивизиа, 109, 110, 116, 118 Ласпейреса, 99 Пааше, 99 Торнквиста, 119 Фишера, 99 объема, 93 переменного состава, 93 постоянного состава, 93 стоимости, 93 цены, 93 инструментальные переменные, 203, 273, 468 интегрированный процесс, 463, 550, 666 интервальная шкала, 26 интервальный прогноз, 247, 362, 364, 479 интерполирование, 244 информационная матрица, 583 информационный критерий Акаике, 239, 377, 557 информационный критерий Шварца, 239, 377, 557 календарный эффект, 350 канонические корреляции, 672 качественная зависимая переменная,625 качественный признак, 38, 290 качественный фактор, 290 квантиль, 70, 73, 703 квантильный размах, 72 квартиль, 70 кластерный анализ, 44 ковариационная матрица оценок, 227, 257, 583, 585
Предметный указатель 737 ковариация, 141, 705, 707 коинтеграция, 558, 666 коинтегрирующий вектор, 559, 666 конечно-разностное уравнение, 714 коррелограмма, 355, 369 корреляционное окно, 419 косвенный методн аименьших квадратов, 325 коэффициент автокорреляции, 266, 475 коэффициент вариации, 73 коэффициент детерминации, 151, 158, 204, 223, 236 скорректированный, 238, 239 коэффициент корреляции, 142, 151, 158, 705, 707 коэффициент множественной корреля-ции, 158 коэффициент ранговой корреляции Спирмена, 370 коэффициент регрессии, 199 кривая Лоренца, 75, 76 критерий, см. тест Бартлетта, 260 Бокса-Пирса, 367 Вальда, 587 Глейзера, 263 Годф рея, 263 Голдфельда-Квандта, 262, 372 Дарбина-Уотсона, 266 Дики-Фуллера, 554 Дики-Фуллера дополненный, 556, 674 Льюнга-Бокса, 368 Маллоуза, 239 Пирсона, 139 Спирмена, 369 Стьюдента, 575 Фишера, 144, 574 множителей Лагранжа, 587 отношения правдоподобия, 587 случайности, 365 сравнения средних, 371 критическая область, 81 кросс-ковариация, 353, 355 кросс-корреляция, 353, 355 кумулята, 50 кумулятивная частота, 50 куртозис, 82, 526, 704 лаг, 373 лаговый многочлен, 374 лаговый оператор, 373, 714 линейная регрессия, 146, 199 линейно детерминированный процесс, 384, 427 линейные оценки, 185, 227 линейный прогноз, 381, 387 линейный тренд, 357, 364, 488 линейный фильтр, 350, 376, 386, 387 логистическая кривая, 358 логистическая функция, 357, 358 логистическое распределение, 297, 298, 627 логит, 296, 298, 627 ложная корреляция, 137 ложная регрессия, 552 маргинальное распределение, 131, 712 маргинальные значения, 302 марковский процесс, 432, 445, 644 математическое моделирование, 24 математическое ожидание, 66, 73, 704, 706 матрица, 691 матрица Гессе, 583 медиана, 66, 703 межгрупповая дисперсия, 161 межотраслевой баланс, 20 методГоттелинга, 359 методЙоханс ена, 668 методКо чрена-Оркатта, 269, 472 методРод са, 359 методЭнгла-Грейнджера, 560, 669 методинс трументальных переменных, 274 метод максимального правдоподобия, 187, 582 методмоментов, 450, 468, 469 методнаимень шего дисперсионного от-ношения, 330, 672 методнаимень ших квадратов, 146, 183, 202, 206, 223, 381, 569
738 Предметный указатель многомерное нормальное распределе-ние, 230, 382, 711, 712 множественная регрессия, 154, 222 мода, 66, 703 модель Бокса-Дженкинса, 457, 463, 465, 466 модель Койка, 504 модель Уинтерса, 402 модель адаптивных ожиданий, 510 модель дискретного выбора, 625 модель исправления ошибок, 512, 666 модель линейного фильтра, 426, 428, 430-433, 447, 452, 453, 462, 476 модель общего тренда с нерегулярно-стью, 488 модель частичного приспособления, 509 момент, 70 мощность критерия, 713 мультииндекс, 41, 611 мультиколлинеарность, 235, 381, 502 мультиномиальный логит, 631 мультиномиальный пробит, 635 мультипликативность индекса, 91, 110 наблюдение, 37, 48 накопленные частоты, 50 начальный момент, 71, 224 невзаимозависимая система уравнений, 314 независимость от посторонних альтерна-тив, 633 несмещенность, 184, 227, 570 несмещенность прогноза, 244, 362, 380, 478, 663 неэкспериментальные данные, 19 номинальная шкала, 25 нормальное распределение, 51, 181, 186, 584, 709 нормальные уравнения, 203, 380 нулевая гипотеза, 81, 712 обобщенный методмомент ов, 276 обратимость индекса, 90 обратимость процесса MA, 455, 456, 469 обратная регрессия, 152 общий тренд, 562, 674 объективная вероятность, 191 объясненная дисперсия, 144, 203, 223, 619 ограниченная зависимая переменная, 625 одновременные уравнения, 318, 655, 656 окно Парзена, 420 окно Тьюки-Хэннинга, 420, 421 ортогональная матрица, 693 ортогональная регрессия, 153, 205, 272 остатки, 147, 182, 186, 201, 209 остаточная дисперсия, 144, 151, 200, 201, 223, 228, 239, 619 относительная частота, 49, 129 оценка Ньюи-Уэста, 269 оценка Уайта, 264 оценки k -класса, 331 ошибка, 29, 31, 146, 182, 183, 270 ошибка второго рода, 713 ошибка первого рода, 713 ошибка прогноза, 244, 362, 379, 383, 387, 477, 532, 663 ошибки измерения, 183, 192, 270 параметр, 199 пассивное наблюдение, 19 первые разности, 375 период, 406 периодограмма, 412, 417 плотность распределения, 50 полигон, 50 полиномиальный лаг, 502 полиномиальный тренд, 168, 357, 360, 361, 391 положительно определенная матрица, 693, 697 положительно полуопределенная матри-ца, 693 правило повторного взятия ожидания, 378, 709 предыстория, 475, 531 преобразование Койка, 505, 507, 528 преобразование Фурье, 406 преобразование в пространстве наблю-дений, 210, 258, 471
Предметный указатель 739 преобразование в пространстве пере-менных, 211 приближение Бартлетта, 366, 368 приведенная форма, 318, 657 признак, 21 причинность по Грейнджеру, 665 пробит, 296, 298, 627 проверка гипотез, 81, 233, 712 прогноз, 361, 362 прогнозирование, 240, 244, 361, 378, 380, 385, 476, 480, 531, 662 прогнозный интервал, 247, 362 произведение Кронекера, 694, 700 простая регрессия, 154, 201, 272 процентиль, 70 процессЮла, 435, 439, 447 прямая регрессия, 152 прямое произведение матриц, 693 равномерное распределение, 52 разложение Бевериджа-Нельсона, 550, 551, 562 разложение Вольда, 386, 427, 447, 456, 463, 476, 550, 551, 674 разложение временного ряда, 349 разложение дисперсии, 151, 161, 167, 204, 664 разностный оператор, 375 ранг, 692, 694 ранг коинтеграции, 666 ранговая шкала, 26 распределение χ2 , 140, 181, 189, 231, 710 распределение Бернулли, 296, 298 распределение Стьюдента, 181, 364, 710 распределение Фишера, 144, 181, 233, 372, 572, 711 распределение частот, 43 распределение экстремального значе-ния, 627, 631 распределенный лаг, 375, 500 расчетные значения, 149, 203 регрессия, 145, 147, 199, 222 регрессор, 222 рекурсивная система, 332, 656 рядФу рье, 409 ряднаблюд ений, 48 сверхидентифицированность, 319 сверхидентифицируемость, 276 сглаживание временного ряда, 169, 391 сглаживание выборочного спектра, 417 сезонность, 291, 349, 356, 359, 399 симметричное распределение, 51 симметричность индекса, 97 система регрессионных уравнений, 314 скользящее среднее, 169, 376, 391, 394, 452, 647 скошенность, 51, 76 слабая стационарность, 352 след, 692 случайная флуктуация, 350 случайное блуждание, 434, 465, 485, 547 случайное блуждание с дрейфом, 486 случайное блуждание с шумом, 487 случайные ошибки, 29 случайный процесс, 348 смешанный процесс авторегрессии - скользящего среднего, 457 собственное число, 699 совместное распределение, 130 сортировка, 39 состоятельность, 185, 228, 583 спектр, 413, 445, 453, 461 спектральная плотность, 416 спектральное окно, 418 спектрограмма, 413 сплайн, 168 среднее, 353 арифметическое, 53 гармоническое, 57 геометрическое, 57 квадратичное, 57 степенное, 57 хронологическое, 54, 56 среднее линейное отклонение, 72 среднеквадратическое отклонение, 72, 704 средний квадрат ошибки прогноза, 362, 379, 380, 383, 385, 387, 477 средняя величина, 53 статистика, 17-19
740 Предметный указатель Бокса-Пирса, 367, 474 Дарбина-Уотсона, 578 Дики-Фуллера, 555 Льюнга-Бокса, 368, 475 Стьюдента, 191, 231 Фишера, 152, 158, 162, 166, 171, 575, 592 Энгла-Грейнджера, 560 максимального собственного значе-ния, 674 отношения правдоподобия, 673 следа, 673 статистическая совокупность, 36 статистический показатель, 21 статистическое наблюдение, 21 стационарность, 351, 432, 445, 546, 547, 658 слабая, 351, 385, 525, 546 строгая, 436 стационарность относительно тренда, 549 стационарный процесс, 349 степени свободы, 140 стохастическая волатильность, 537 стохастический тренд, 350, 485, 547 строгая стационарность, 351 структура лага, 376 структурная векторная авторегрессия, 655 структурная форма, 318 структурный сдвиг, 350, 361, 371 субъективная вероятность, 23, 601 сумма квадратов остатков, 148, 183, 186, 571 таблица сопряженности, 132 темп прироста, 34, 35, 55, 123 темп роста, 34, 35, 61 тенденция, 349, 370, 371 теорема Байеса, 135, 601, 708 теорема Винера-Хинчина, 414 теорема Вольда, 386, 427 теорема Гаусса-Маркова, 229 теорема Парсеваля, 411 тест, см. критерий Вальда, 589 Годфрея, 577 Рамсея, 578 Чоу, 578 множителей Лагранжа, 591 отношения правдоподобия, 590 тестирование, 81 точечный прогноз, 244, 362 точная ММП-оценка, 646, 649, 650 точнаяМНК-оценка, 646, 648 транзитивность индекса, 90 тренд, 167, 349, 350, 356, 357, 360, 361, 364, 369, 547, 554 трехшаговый методнаименьших квадра-тов, 332 унимодальное распределение, 52 упорядоченная зависимая переменная, 630 упорядоченный логит, 631 упорядоченный пробит, 631 уравненияЮла-Уокера, 438, 450, 458 уровень доверия, 713 уровень значимости, 141, 232, 713 условнаяМНК-оценка, 645, 648 условная дисперсия, 143, 377, 526, 708 условное математическоеожидание, 143, 377, 380, 708 условное распределение, 134, 377, 382, 708, 712 условный логит, 632 фаза, 407 фактор, 37, 142, 182, 201, 222, 234 фиктивные переменные, 289, 359, 611 функциональная форма, 578 функция Кобба-Дугласа, 20, 57 функция правдоподобия, 187, 582 функция распределения, 50, 703 функция реакции на импульсы, 462, 501, 661 характеристический многочлен, 431, 435, 436, 443, 456, 698 характеристическое уравнение, 431, 435, 436, 440, 443, 448, 455, 456, 459, 463, 660, 714
Предметный указатель 741 центральный момент, 71, 76, 223, 704 цепное свойство индекса, 90 цикл, 167, 349 частная автокорреляционная функция, 451, 453 частота, 406 шаговая регрессия, 241 ширина окна, 422 шкала отношений, 26 шкалирование, 26 шум, 350 экономическая величина, 20, 21, 28, 32 экономические измерения, 27 эксперимент, 19 экспоненциальная средняя, 399 экспоненциальное сглаживание, 171, 398, 401 экспоненциальный тренд, 168, 357 экстраполирование, 244, 361 эксцесс, 82, 526, 704 эластичность, 33 эффективность, 184, 188, 230, 469, 583, 644 эффекты 1-го порядка, 163 эффекты 2-го порядка, 164 эффекты взаимодействия, 164, 295, 612
