 9. КОНТЕКСТНО-ЗАВИСИМАЯ КЛАССИФИКАКЦИЯ
1. Постановка задачи
Рассмотренные ранее задачи предполагали, что нет зависимости между различными классами, т.е. имея вектор  из класса , мы могли получить следующий вектор из любого класса. Далее мы будем предполагать зависимость классов, т.е. классификация каждого нового вектора осуществляется в зависимости от классификации предыдущих векторов. Выбор класса, к которому следует отнести вектор, зависит от его собственного значения, значений других векторов, существующих отношений между различными классами. 
Такие задачи возникают во многих приложения: распознавание речи, обработка изображений и др.
Эта классификация называется контекстно-зависимой. 
Отправной точкой является Байесовский классификатор. Но зависимость между различными классами требует более общей формулировки проблемы. Общая информация, которая присутствует в векторах, требует, чтобы классификация была выполнена с использованием всех векторов одновременно и также была организованна в той же последовательности, в которой получена в экспериментах. Поэтому мы будем называть вектор признаков наблюдением, выстроенным в последовательность  из  наблюдений.
2. Байесовский классификатор
Пусть  - последовательность  наблюдений и ,  - классы, в которые эти вектора можно классифицировать. Пусть также  - одна из возможных последовательностей соответствия классов последовательности наблюдений, где , . Общее число таких последовательностей классов  есть . Задача заключается в том, чтобы решить, к какой последовательности классов отнести последовательность наблюдений. Это эквивалентно отнесению  к ,  к  и т.д.
Подходом к решению проблемы является рассмотрение каждой конкретной последовательности  как расширенного вектора признаков на ,  как на возможных классах. В данном случае Байесовское правило
, при эквивалентно
, при .
Рассмотрим, как это правило выглядит для некоторого типичного класса контекстно-зависимых моделей.
3. Модель Марковской цепи
Одна из наиболее используемых моделей, описывающих зависимость классов, является правило Марковской цепи. Если  есть последовательность классов, то Марковская модель предполагает, что

Тогда зависимость классов ограничивается только внутри двух последовательных классов. Такой класс моделей называется Марковской моделью первого порядка. Возможны обобщения на второй, третий и т.д. порядок. 
Другими словами, даны наблюдения , принадлежащие классам  соответственно. Вероятность того, что наблюдение  на шаге  принадлежит классу , зависит только от того класса, к которому принадлежит наблюдение  на шаге .

или
 
Сделаем два общих предположения:
1) в последовательности классов наблюдения статистически независимы;
2) функция плотности вероятностей в одном классе не зависит от других классов.
Это означает, что зависимость существует только на последовательности, в которой классы встречаются, но внутри классов наблюдений "подчиняются" собственным правилам. Таким образом, получаем, что
 
Комбинируя  и , получаем Байесовское правило в виде следующего утверждения.
	Байесовское правило: для последовательности наблюдений векторов  проводим их классификацию в соответствующие последовательности классов  так, чтобы величина
 
была максимальной.
	Поиск требует вычисления последнего выражения для каждого , , что, в свою очередь, требует  умножений, а это очень много. Но существуют пути экономии вычислений. Если в  и  отличаются только последние классы, т.е. , при  и , то большая часть вычислений дублируется.
4. Алгоритм Витерби ( Viterbi )
Пусть задано  столбцов; каждая точка в столбце соответствует одному из  возможных классов ; столбцы соответствуют наблюдениям , . Стрелками обозначены переходы от одного класса к другому в последовательности получения наблюдений. Каждая последовательность классов  соответствует конкретному маршруту последовательных переходов. Каждый переход от  i -го класса к  j -му  характеризуется вероятностью , которая предполагается известной. Предположим, что эти вероятности одинаковы для всех . Далее предположим, что условные вероятности - плотности , ,  - также известны. Тогда задача максимизации  ставится как поиск последовательности переходов.
Пусть  - цена, связанная с переходом . Начальное условие при  есть . Учитывая данные предположения, получаем общую формулу, которую нужно оптимизировать:
 
 или, логарифмируя, имеем

Используем принцип Беллмана:
, при .
Обозначим через
.
Получаем обратный ход для вычисления . Получаем число операций , что существенней меньше . Данная процедура динамического программирования известна как алгоритм Витерби.
5. Скрытые Марковские модели
Теперь рассмотрим системы, в которых состояния напрямую не наблюдаются и могут быть лишь оценены из последовательности наблюдений с помощью некоторой оптимизационной техники. Этот тип Марковских моделей известен как скрытые Марковские модели (НММ). НММ - это тип стохастической аппроксимации нестационарных стохастических последовательностей со статистическими свойствами, которые подвергаются различным случайным переходам среди множества различных стационарных процессов. Иными словами, НММ моделирует последовательность наблюдений как кусочно-стационарный процесс. 
Такие модели широко используются в распознавании речи. Рассматриваются так называемые высказывания - это может быть слово, часть слова, даже предложение или параграф. Статистические свойства речевого сигнала внутри высказывания подвергаются серии переходов. Например, слово содержит порцию гласных и согласных звуков. Они характеризуются различными статистическими свойствами, которые в свою очередь отражены в переходах в речевых сигналах от одной к другой. Такие примеры дает распознавание рукописного текста, распознавание текстур, где успешно применяется НММ.
НММ есть в основе своей конечный автомат, который генерирует строку наблюдений - последовательность векторов наблюдений . Таким образом, НММ содержит  состояний, и строка наблюдений получается как результат последовательных переходов из одного состояния  в другое состояние . Нам подходит так называемая модель "машины Моора", в соответствии с которой наблюдения получаются как результаты (выходы) из состояний на прибытие (по переходу) в каждом состоянии.
Пример. НММ с тремя состояниями. Стрелки обозначают переходы. Такая модель может соответствовать короткому слову с тремя различными стационарными частями, например, для слова "оса".
Модель предоставляет информацию о последовательных переходах между состояниями , . Такой тип НММ известен как "слева-направо", поскольку индекс состояний определяется выделенным числом фонем в одном слове. В действительности, несколько состояний (обычно 3 или 4) используется для каждой фонемы.






3




