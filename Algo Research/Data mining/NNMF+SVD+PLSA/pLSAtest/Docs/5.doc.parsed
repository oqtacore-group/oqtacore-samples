 12. Обучение по прецедентам (по Вапнику, Червоненкису)
1. Задача построения классификатора
Пусть 
 - пространство образов,
 - признаковое пространство,
,  - индикаторная функция,
 - множество признаков.
Тогда .
	Пусть также
,  - множество прецедентов,
 - решающее правило.
Тогда .
Выбор решающего правила исходит из минимизации , где  - метрика, мера близости функций и. Построение  называют задачей обучения.  - это ученик, процедура формирования - это учитель, прецеденты - это обучающая последовательность.
2. Качество обучения классификатора
	Относительная доля несовпадений классификации с учителем для решающего правила есть: , где . Надежность обучения классификатора - это вероятность получения решающего правила с заданным качеством.
	Пусть  - класс дискриминантных функций, где ( ( A  - параметр. Число степеней свободы при выборе конкретной функции в классе определяется количеством параметров в векторе ( , т.е. размерностью  A . 
Например, для классов линейных и квадратичных функций имеем:
Линейная дискриминантная функция: . В таком случае имеем  степень свободы.
Квадратичная дискриминантная функция: . В таком случае имеем  + степеней свободы.
С увеличением степеней свободы увеличивается способность классификатора по разделению.
3. Вероятностная модель
	Пусть прецеденты - это результат реализации случайных величин. Рассмотрим величину риска (т.е. ошибки) связанной с классификацией. Определим понятия риск среднего и риска эмпирического.
	Пусть на  заданы -алгебра и мера . Пусть также
 - вектор признаков,
 - класс функций, из которых выбирается решающее правило,
 - решающее правило (результат классификации), которое принимает значение 0 или 1 при фиксированном векторе параметра,
 - характеристическая функция множества,
 - множество параметров, описывающие различные функции в .
Тогда  , где  и , .
	В данных обозначениях средний риск выглядит следующим образом:
.
Для случая двух классов, при , имеем:

или
,
где  - это вероятностная мера на пространстве .

4. Задача поиска наилучшего классификатора
	Рассмотрим минимизацию функционала:
 
 Задача же поиска наилучшего классификатора состоит в нахождении  такого, что
.
Если же минимума не существует, то надо найти  такое, что
.
Другими словами, необходимо решить задачу минимизации среднего риска.
Поскольку  неизвестно, будем решать задачу минимизации эмпирического риска. Пусть  - число прецедентов. Тогда эмпирический риск задается выражением:
.
Таким образом, задача минимизации эмпирического риска выглядит так:
,
где случайные величины мы минимизируем по параметру  - любой возможный параметр.
В идеале надо получить взаимосвязанные оценки эмпирического и среднего риска.
Отметим, что чем меньше , тем легче построить  такую, что  обращается в ноль, либо очень мало. Но при этом истинное значение  может сильно отличаться от . Необходимо выбрать  такую, чтобы имела место равномерная сходимость по  выражения:
.
Фактически это есть сходимость частот к математическому ожиданию.
	В дальнейшем будем считать,  что  в  зависимости  от  конкретного  набора  прецедентов  можем  получить любые . Но необходимо, чтобы полученные эмпирическое решающее хорошо работало (отражало общие свойства) для всех образов. Поэтому в формуле присутствует равномерная сходимость.

5. Сходимость эмпирического риска к среднему. Случай конечного числа решающих правил. 
	Пусть
 - математическое ожидание ошибки классификатора ,
 - событие - ошибка классификатора при решающем правиле ,
 - вероятность,
 - частота в  испытаниях.
Воспользуемся неравенством Бернштейна, тогда

есть оценка - соотношение между частотой и вероятностью при заданном количестве испытаний.
	Пусть  - случайная величина. Тогда  - математическое ожидание ,  - дисперсия, причем . Обозначим . Тогда соответствующая оценка имеет вид:
, где .
 и ,
где  - необходимое количество прецедентов для обеспечения близости.
	Теорема. Пусть из множества, состоящего из  решающих правил, выбирается правило, частота ошибок которого на прецедентах составляет . Тогда с вероятностью  можно утверждать, что вероятность ошибочной классификации с помощью данного правила  составит величину, меньшую , если длина обучающей последовательности не меньше , где ,  и  заданы и последовательность независима. 
	Данная теорема справедлива для случая конечного числа решающих правил. Вапник и Червоненкис смогли обобщить эти оценки на случай бесконечного числа решающих правил.
6. Случай бесконечного числа решающих правил 
Введем понятие "разнообразия класса функций для бесконечного множества". Пусть  - прецеденты.
	Определение. Дихотомией называется разбиение множества на два подмножества.
В нашем случае имеем  дихотомий. Итак, пусть ,  - это класс решающих правил, причем . Пусть  есть количество дихотомий на классе решающих правил. Тогда зададим энтропию следующим образом:
,
где математическое ожидание берется по всем выборкам . Тогда

есть энтропия класса  решающих правил на выборках длины .
6.1. Критерий равномерной сходимости  к вероятностям 
Теорема. Для равномерной сходимости  к  по классу  необходимо и достаточно, чтобы .
Суть данного критерия - не пытаться выделить очень точный классификатор, так как это отдаляет от общности.
Сразу же возникает проблема необходимость перехода к бесконечным системам решающих правил. Существенно, что значение имеет лишь конечное подмножество систем решающих правил, необходимое для разделения конечного числа прецедентов.
6.2. Достаточное условие равномерной сходимости 
Проверка условия критерия равномерной сходимости по вероятности затрудняется неопределенностью распределения выборки. Поэтому достаточные условия формулируются таким образом, чтобы не зависеть от распределения и при этом гарантировать равномерную сходимость. В таком случае вместо энтропии рассматривается величина:
,
где  - это функция роста класса решающих функций .
	Т.к. логарифм максимума равен максимуму логарифмов, что, в свою очередь, не меньше математического ожидания от логарифма, то
.
	Если 
,
то по свойствам пределов
.
	Данное условие легко проверятся для различных классов решающих правил.
	Другими словами  можно трактовать как максимальное число способов разделения  точек на два класса с помощью решающих правил , .
	Теорема. Функция роста либо тождественно равна , либо, мажорируется функцией , где  - минимальное значение , при котором , т.е. либо , либо .
	В свою очередь 
.
Значит 
, 
где , и  - степенная функция, мажорирующая .
	Существует максимум  точка, которая еще разбивается всеми возможными способами с помощью правила , но никакие  точек этим свойством не обладают.
	Определение.  называется емкостью класса решающих функций или мера разнообразия решающих правил в классе  или  VC -размерностью класса - универсальная характеристика класса решающих функций.
	Отметим, что если  для всех , то емкость бесконечна.
	Теорема. Если емкость класса решающих функций конечна, то всегда имеет место равномерная сходимость частот к вероятностям такое, что

и достаточное условие выполнено.
6.3. Скорость сходимости 
Запишем оценку для бесконечного числа решающих правил. Ее вид аналогичен случаю конечного числа решающих правил:
.
Если емкость бесконечна, то оценка тривиальная (не больше единицы). Пусть  - конечная емкость класса решающих функций. Тогда
.
Введем обозначение: ,
Тогда .
Отсюда следует, что.
Значит, с вероятностью, превышающей  качество эмпирического оптимального решающего правила отличается от истинно оптимально решающего правила не более чем на величину .
	В следующей таблице представлен некоторый итог наших рассуждений.
	Малая емкость класса решающих функций (бедный)	Большая емкость класса решающих функций (богатый)		Близость эмпирического решающего правила к оптимальному решающему правилу	Хорошая	Плохая		Качество разделения (минимизация ошибки)	Низкое	Высокое		Таким образом, необходимо минимизировать степени свободы.
6.4. Случай класса линейных решающих функций 
Пусть  - линейная решающая функция,  - размерность пространства.
Как уже отмечалось выше, имеем  дихотомий, где  - длина выборки. Хотим выяснить, какое количество дихотомий реализуется с помощью гиперплоскостей?
Максимальное число точек в пространстве размерности , которое с помощью гиперплоскостей можно разбить всеми возможными способами на два класса есть . Если ,
то линейный риск будет равномерно сходиться к среднему риску. Емкость класса конечна и равна .
 
